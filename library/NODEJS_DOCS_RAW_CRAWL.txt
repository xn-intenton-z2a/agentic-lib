Node.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
            
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      

      
        


About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
            
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      

      
        


About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker\n\n\n\nRun JavaScript EverywhereNode.js® is a free, open-source, cross-platform JavaScript runtime environment
that lets developers create servers, web apps, command line tools and scripts.Download Node.js (LTS)Download Node.js (LTS)Downloads Node.js v22.14.01 with long-term support. Node.js can also be installed via version managers.Want new features sooner? Get Node.js v23.11.01 instead.
Create an HTTP ServerWrite TestsRead and Hash a FileStreams PipelineWork with Threads// server.mjs
import { createServer } from 'node:http';

const server = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello World!\n');
});

// starts a simple http server locally on port 3000
server.listen(3000, '127.0.0.1', () => {
  console.log('Listening on 127.0.0.1:3000');
});

// run with `node server.mjs`
JavaScriptCopy to clipboardLearn more what Node.js is able to offer with our Learning materials.\n\n\n\nRun JavaScript EverywhereNode.js® is a free, open-source, cross-platform JavaScript runtime environment
that lets developers create servers, web apps, command line tools and scripts.Download Node.js (LTS)Download Node.js (LTS)Downloads Node.js v22.14.01 with long-term support. Node.js can also be installed via version managers.Want new features sooner? Get Node.js v23.11.01 instead.
Create an HTTP ServerWrite TestsRead and Hash a FileStreams PipelineWork with Threads// server.mjs
import { createServer } from 'node:http';

const server = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello World!\n');
});

// starts a simple http server locally on port 3000
server.listen(3000, '127.0.0.1', () => {
  console.log('Listening on 127.0.0.1:3000');
});

// run with `node server.mjs`
JavaScriptCopy to clipboardLearn more what Node.js is able to offer with our Learning materials.\n\n\n\nIntroduction to Node.js
Node.js is an open-source and cross-platform JavaScript runtime environment. It is a popular tool for almost any kind of project!
Node.js runs the V8 JavaScript engine, the core of Google Chrome, outside of the browser. This allows Node.js to be very performant.
A Node.js app runs in a single process, without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in its standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are written using non-blocking paradigms, making blocking behavior the exception rather than the norm.
When Node.js performs an I/O operation, like reading from the network, accessing a database or the filesystem, instead of blocking the thread and wasting CPU cycles waiting, Node.js will resume the operations when the response comes back.
This allows Node.js to handle thousands of concurrent connections with a single server without introducing the burden of managing thread concurrency, which could be a significant source of bugs.
Node.js has a unique advantage because millions of frontend developers that write JavaScript for the browser are now able to write the server-side code in addition to the client-side code without the need to learn a completely different language.
In Node.js the new ECMAScript standards can be used without problems, as you don't have to wait for all your users to update their browsers - you are in charge of deciding which ECMAScript version to use by changing the Node.js version, and you can also enable specific experimental features by running Node.js with flags.
An Example Node.js Application
The most common example Hello World of Node.js is a web server:
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardTo run this snippet, save it as a server.js file and run node server.js in your terminal.
If you use mjs version of the code, you should save it as a server.mjs file and run node server.mjs in your terminal.
This code first includes the Node.js http module.
Node.js has a fantastic standard library, including first-class support for networking.
The createServer() method of http creates a new HTTP server and returns it.
The server is set to listen on the specified port and host name. When the server is ready, the callback function is called, in this case informing us that the server is running.
Whenever a new request is received, the request event is called, providing two objects: a request (an http.IncomingMessage object) and a response (an http.ServerResponse object).
Those 2 objects are essential to handle the HTTP call.
The first provides the request details. In this simple example, this is not used, but you could access the request headers and request data.
The second is used to return data to the caller.
In this case with:
res.statusCode = 200;
JavaScriptCopy to clipboard
we set the statusCode property to 200, to indicate a successful response.
We set the Content-Type header:
res.setHeader('Content-Type', 'text/plain');
JavaScriptCopy to clipboard
and we close the response, adding the content as an argument to end():
res.end('Hello World\n');
JavaScriptCopy to clipboard
If you haven't already done so, download Node.js.NextHow much JavaScript do you need to know to use Node.js?\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nDownload Node.js®Get Node.js® v22.14.0 (LTS) for Unknown using  with npmBashCopy to clipboard and their installation scripts are not maintained by the Node.js project. If you encounter any issues please visit 's websiteOr get a prebuilt Node.js® for Unknown running a Unknown architecture.N/A Installer (.gz)Standalone Binary (.gz)
Read the changelog for this version.Read the blog post for this version.Learn how to verify signed SHASUMS.Looking for Node.js source? Download a signed Node.js source tarball.Check out our nightly binaries or
all previous releases
or the unofficial binaries for other platforms.\n\n\n\nMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025Previous12345...158Next\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
            
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      

      
        


About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker\n\n\n\nNode.js Releases
Major Node.js versions enter Current release status for six months, which gives library authors time to add support for them.
After six months, odd-numbered releases (9, 11, etc.) become unsupported, and even-numbered releases (10, 12, etc.) move to Active LTS status and are ready for general use.
LTS release status is "long-term support", which typically guarantees that critical bugs will be fixed for a total of 30 months.
Production applications should only use Active LTS or Maintenance LTS releases.
Release Schedule

Full details regarding the Node.js release schedule are available on GitHub.
Commercial Support
Commercial support for versions past the Maintenance phase is available through our OpenJS Ecosystem Sustainability Program partner HeroDevs.
Looking for the latest release of a version branch?
Node.jsN-APICodenameReleased atnpmv23.11.0v131-2025-04-01v10.9.2ReleasesChangelogDocsv22.14.0v127Jod2025-02-11v10.9.2ReleasesChangelogDocsv21.7.3v120-2024-04-10v10.5.0ReleasesChangelogDocsv20.19.0v115Iron2025-03-13v10.8.2ReleasesChangelogDocsv19.9.0v111-2023-04-10v9.6.3ReleasesChangelogDocsv18.20.8v108Hydrogen2025-03-27v10.8.2ReleasesChangelogDocsv17.9.1v102-2022-06-01v8.11.0ReleasesChangelogDocsv16.20.2v93Gallium2023-08-08v8.19.4ReleasesChangelogDocsv15.14.0v88-2021-04-06v7.7.6ReleasesChangelogDocsv14.21.3v83Fermium2023-02-16v6.14.18ReleasesChangelogDocsv13.14.0v79-2020-04-29v6.14.4ReleasesChangelogDocsv12.22.12v72Erbium2022-04-05v6.14.16ReleasesChangelogDocsv11.15.0v67-2019-04-30v6.7.0ReleasesChangelogDocsv10.24.1v64Dubnium2021-04-06v6.14.12ReleasesChangelogDocsv9.11.2v59-2018-06-12v5.6.0ReleasesChangelogDocsv8.17.0v57Carbon2019-12-17v6.13.4ReleasesChangelogDocsv7.10.1v51-2017-07-11v4.2.0ReleasesChangelogDocsv6.17.1v48Boron2019-04-03v3.10.10ReleasesChangelogDocsv5.12.0v47-2016-06-23v3.8.6ReleasesChangelogDocsv4.9.1v46Argon2018-03-29v2.15.11ReleasesChangelogDocsv0.12.18v14-2017-02-22v2.15.11ReleasesChangelogDocs
Official vs. Community Installation Methods
The Node.js website provides several non-interactive installation methods, including command-line interfaces (CLIs), operating system (OS) package managers (e.g., brew), and Node.js version managers (e.g., nvm).
To highlight and promote community contributions, the Node.js project introduced a revised Downloads page categorizing installation methods as either “Official” or “Community.” This provides users with increased flexibility and choice. To ensure clarity, we’ve defined criteria for each category.
Official Installation Methods
Installation methods designated as “Official” must meet the following requirements:
Requirements (Official Installation Methods)New Node.js releases must be available simultaneously with the official release.Project maintainers must have a close relationship with the Node.js project, including direct communication channels.Installation method must download official binaries bundled by the Node.js project.Installation method must not build from source when pre-built binaries are available, nor should it alter the official binaries.
Community Installation Methods
Community installation methods included on the self-service download page (/download) must also adhere to a minimum set of criteria:

Version Support: Must support all currently supported, non-End-of-Life (EOL) Node.js versions.
OS Compatibility: Must function on at least one officially supported Operating System (OS).
Broad OS Support: Cannot be limited to a subset of OS distributions or versions.

For example, an installation method claiming compatibility with “Windows” must function on “Windows 10”, “Windows 11”, and all their editions (including server versions).
Similarly, an installation method claiming compatibility with “Linux” must be installable on all major Linux distributions, not just a specific subset. It cannot rely on distribution-specific package managers like apt or dnf.


Free and Open Source: Must be free to use and open source, must not be sold as a commercial product, and must not be a paid service.\n\n\n\n\n\nIntroduction to Node.js
Node.js is an open-source and cross-platform JavaScript runtime environment. It is a popular tool for almost any kind of project!
Node.js runs the V8 JavaScript engine, the core of Google Chrome, outside of the browser. This allows Node.js to be very performant.
A Node.js app runs in a single process, without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in its standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are written using non-blocking paradigms, making blocking behavior the exception rather than the norm.
When Node.js performs an I/O operation, like reading from the network, accessing a database or the filesystem, instead of blocking the thread and wasting CPU cycles waiting, Node.js will resume the operations when the response comes back.
This allows Node.js to handle thousands of concurrent connections with a single server without introducing the burden of managing thread concurrency, which could be a significant source of bugs.
Node.js has a unique advantage because millions of frontend developers that write JavaScript for the browser are now able to write the server-side code in addition to the client-side code without the need to learn a completely different language.
In Node.js the new ECMAScript standards can be used without problems, as you don't have to wait for all your users to update their browsers - you are in charge of deciding which ECMAScript version to use by changing the Node.js version, and you can also enable specific experimental features by running Node.js with flags.
An Example Node.js Application
The most common example Hello World of Node.js is a web server:
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardTo run this snippet, save it as a server.js file and run node server.js in your terminal.
If you use mjs version of the code, you should save it as a server.mjs file and run node server.mjs in your terminal.
This code first includes the Node.js http module.
Node.js has a fantastic standard library, including first-class support for networking.
The createServer() method of http creates a new HTTP server and returns it.
The server is set to listen on the specified port and host name. When the server is ready, the callback function is called, in this case informing us that the server is running.
Whenever a new request is received, the request event is called, providing two objects: a request (an http.IncomingMessage object) and a response (an http.ServerResponse object).
Those 2 objects are essential to handle the HTTP call.
The first provides the request details. In this simple example, this is not used, but you could access the request headers and request data.
The second is used to return data to the caller.
In this case with:
res.statusCode = 200;
JavaScriptCopy to clipboard
we set the statusCode property to 200, to indicate a successful response.
We set the Content-Type header:
res.setHeader('Content-Type', 'text/plain');
JavaScriptCopy to clipboard
and we close the response, adding the content as an argument to end():
res.end('Hello World\n');
JavaScriptCopy to clipboard
If you haven't already done so, download Node.js.NextHow much JavaScript do you need to know to use Node.js?\n\n\n\nHow much JavaScript do you need to know to use Node.js?
As a beginner, it's hard to get to a point where you are confident enough in your programming abilities. While learning to code, you might also be confused at where does JavaScript end, and where Node.js begins, and vice versa.
What is recommended to learn before diving deep with Node.js?

Lexical Structure
Expressions
Data Types
Classes
Variables
Functions
this operator
Arrow Functions
Loops
Scopes
Arrays
Template Literals
Strict Mode
ECMAScript 2015 (ES6) and beyond
Asynchronous JavaScript

With those concepts in mind, you are well on your road to become a proficient JavaScript developer, in both the browser and in Node.js.
Asynchronous Programming
The following concepts are also key to understand asynchronous programming, which is one of the fundamental parts of Node.js:

Asynchronous programming and callbacks
Timers
Promises
Async and Await
Closures
The Event Loop
PrevIntroduction to Node.jsNextDifferences between Node.js and the Browser\n\n\n\nDifferences between Node.js and the Browser
Both the browser and Node.js use JavaScript as their programming language. Building apps that run in the browser is completely different from building a Node.js application. Despite the fact that it's always JavaScript, there are some key differences that make the experience radically different.
From the perspective of a frontend developer who extensively uses JavaScript, Node.js apps bring with them a huge advantage: the comfort of programming everything - the frontend and the backend - in a single language.
You have a huge opportunity because we know how hard it is to fully, deeply learn a programming language, and by using the same language to perform all your work on the web - both on the client and on the server, you're in a unique position of advantage.

What changes is the ecosystem.

In the browser, most of the time what you are doing is interacting with the DOM, or other Web Platform APIs like Cookies. Those do not exist in Node.js, of course. You don't have the document, window and all the other objects that are provided by the browser.
And in the browser, we don't have all the nice APIs that Node.js provides through its modules, like the filesystem access functionality.
Another big difference is that in Node.js you control the environment. Unless you are building an open source application that anyone can deploy anywhere, you know which version of Node.js you will run the application on. Compared to the browser environment, where you don't get the luxury to choose what browser your visitors will use, this is very convenient.
This means that you can write all the modern ES2015+ JavaScript that your Node.js version supports. Since JavaScript moves so fast, but browsers can be a bit slow to upgrade, sometimes on the web you are stuck with using older JavaScript / ECMAScript releases. You can use Babel to transform your code to be ES5-compatible before shipping it to the browser, but in Node.js, you won't need that.
Another difference is that Node.js supports both the CommonJS and ES module systems (since Node.js v12), while in the browser, we are starting to see the ES Modules standard being implemented.
In practice, this means that you can use both require() and import in Node.js, while you are limited to import in the browser.PrevHow much JavaScript do you need to know to use Node.js?NextThe V8 JavaScript Engine\n\n\n\nThe V8 JavaScript Engine
V8 is the name of the JavaScript engine that powers Google Chrome. It's the thing that takes our JavaScript and executes it while browsing with Chrome.
V8 is the JavaScript engine i.e. it parses and executes JavaScript code. The DOM, and the other Web Platform APIs (they all makeup runtime environment) are provided by the browser.
The cool thing is that the JavaScript engine is independent of the browser in which it's hosted. This key feature enabled the rise of Node.js. V8 was chosen to be the engine that powered Node.js back in 2009, and as the popularity of Node.js exploded, V8 became the engine that now powers an incredible amount of server-side code written in JavaScript.
The Node.js ecosystem is huge and thanks to V8 which also powers desktop apps, with projects like Electron.
Other JS engines
Other browsers have their own JavaScript engine:

Firefox has SpiderMonkey
Safari has JavaScriptCore (also called Nitro)
Edge was originally based on Chakra but has more recently been rebuilt using Chromium and the V8 engine.

and many others exist as well.
All those engines implement the ECMA ES-262 standard, also called ECMAScript, the standard used by JavaScript.
The quest for performance
V8 is written in C++, and it's continuously improved. It is portable and runs on Mac, Windows, Linux and several other systems.
In this V8 introduction, we will ignore the implementation details of V8: they can be found on more authoritative sites (e.g. the V8 official site), and they change over time, often radically.
V8 is always evolving, just like the other JavaScript engines around, to speed up the Web and the Node.js ecosystem.
On the web, there is a race for performance that's been going on for years, and we (as users and developers) benefit a lot from this competition because we get faster and more optimized machines year after year.
Compilation
JavaScript is generally considered an interpreted language, but modern JavaScript engines no longer just interpret JavaScript, they compile it.
This has been happening since 2009, when the SpiderMonkey JavaScript compiler was added to Firefox 3.5, and everyone followed this idea.
JavaScript is internally compiled by V8 with just-in-time (JIT) compilation to speed up the execution.
This might seem counter-intuitive, but since the introduction of Google Maps in 2004, JavaScript has evolved from a language that was generally executing a few dozens of lines of code to complete applications with thousands to hundreds of thousands of lines running in the browser.
Our applications can now run for hours inside a browser, rather than being just a few form validation rules or simple scripts.
In this new world, compiling JavaScript makes perfect sense because while it might take a little bit more to have the JavaScript ready, once done it's going to be much more performant than purely interpreted code.PrevDifferences between Node.js and the BrowserNextAn introduction to the npm package manager\n\n\n\nAn introduction to the npm package manager
Introduction to npm
npm is the standard package manager for Node.js.
In September 2022 over 2.1 million packages were reported being listed in the npm registry, making it the biggest single language code repository on Earth, and you can be sure there is a package for (almost!) everything.
It started as a way to download and manage dependencies of Node.js packages, but it has since become a tool used also in frontend JavaScript.

Yarn and pnpm are alternatives to npm cli. You can check them out as well.

Packages
npm installs, updates and manages downloads of dependencies of your project. Dependencies are pre-built pieces of code, such as libraries and packages, that your Node.js application needs to work.
Installing all dependencies
If a project has a package.json file, by running
npm install
ShellCopy to clipboard
it will install everything the project needs, in the node_modules folder, creating it if it's not existing already.
Installing a single package
You can also install a specific package by running
npm install <package-name>
ShellCopy to clipboard
Furthermore, since npm 5, this command adds <package-name> to the package.json file dependencies. Before version 5, you needed to add the flag --save.
Often you'll see more flags added to this command:

--save-dev installs and adds the entry to the package.json file devDependencies
--no-save installs but does not add the entry to the package.json file dependencies
--save-optional installs and adds the entry to the package.json file optionalDependencies
--no-optional will prevent optional dependencies from being installed

Shorthands of the flags can also be used:

-S: --save
-D: --save-dev
-O: --save-optional

The difference between devDependencies and dependencies is that the former contains development tools, like a testing library, while the latter is bundled with the app in production.
As for the optionalDependencies the difference is that build failure of the dependency will not cause installation to fail. But it is your program's responsibility to handle the lack of the dependency. Read more about optional dependencies.
Updating packages
Updating is also made easy, by running
npm update
ShellCopy to clipboard
npm will check all packages for a newer version that satisfies your versioning constraints.
You can specify a single package to update as well:
npm update <package-name>
ShellCopy to clipboard
Versioning
In addition to plain downloads, npm also manages versioning, so you can specify any specific version of a package, or require a version higher or lower than what you need.
Many times you'll find that a library is only compatible with a major release of another library.
Or a bug in the latest release of a lib, still unfixed, is causing an issue.
Specifying an explicit version of a library also helps to keep everyone on the same exact version of a package, so that the whole team runs the same version until the package.json file is updated.
In all those cases, versioning helps a lot, and npm follows the semantic versioning (semver) standard.
You can install a specific version of a package, by running
npm install <package-name>@<version>
ShellCopy to clipboard
Running Tasks
The package.json file supports a format for specifying command line tasks that can be run by using
npm run <task-name>
ShellCopy to clipboard
For example:
{
  "scripts": {
    "start-dev": "node lib/server-development",
    "start": "node lib/server-production"
  }
}
JSONCopy to clipboard
It's very common to use this feature to run Webpack:
{
  "scripts": {
    "watch": "webpack --watch --progress --colors --config webpack.conf.js",
    "dev": "webpack --progress --colors --config webpack.conf.js",
    "prod": "NODE_ENV=production webpack -p --config webpack.conf.js"
  }
}
JSONCopy to clipboard
So instead of typing those long commands, which are easy to forget or mistype, you can run
$ npm run watch
$ npm run dev
$ npm run prod
Shell SessionCopy to clipboardPrevThe V8 JavaScript EngineNextECMAScript 2015 (ES6) and beyond\n\n\n\nECMAScript 2015 (ES6) and beyond
Node.js is built against modern versions of V8. By keeping up-to-date with the latest releases of this engine, we ensure new features from the JavaScript ECMA-262 specification are brought to Node.js developers in a timely manner, as well as continued performance and stability improvements.
All ECMAScript 2015 (ES6) features are split into three groups for shipping, staged, and in progress features:

All shipping features, which V8 considers stable, are turned on by default on Node.js and do NOT require any kind of runtime flag.
Staged features, which are almost-completed features that are not considered stable by the V8 team, require a runtime flag: --harmony.
In progress features can be activated individually by their respective harmony flag, although this is highly discouraged unless for testing purposes. Note: these flags are exposed by V8 and will potentially change without any deprecation notice.

Which features ship with which Node.js version by default?
The website node.green provides an excellent overview over supported ECMAScript features in various versions of Node.js, based on kangax's compat-table.
Which features are in progress?
New features are constantly being added to the V8 engine. Generally speaking, expect them to land on a future Node.js release, although timing is unknown.
You may list all the in progress features available on each Node.js release by grepping through the --v8-options argument. Please note that these are incomplete and possibly broken features of V8, so use them at your own risk:
node --v8-options | grep "in progress"
ShellCopy to clipboard
I have my infrastructure set up to leverage the --harmony flag. Should I remove it?
The current behavior of the --harmony flag on Node.js is to enable staged features only. After all, it is now a synonym of --es_staging. As mentioned above, these are completed features that have not been considered stable yet. If you want to play safe, especially on production environments, consider removing this runtime flag until it ships by default on V8 and, consequently, on Node.js. If you keep this enabled, you should be prepared for further Node.js upgrades to break your code if V8 changes their semantics to more closely follow the standard.
How do I find which version of V8 ships with a particular version of Node.js?
Node.js provides a simple way to list all dependencies and respective versions that ship with a specific binary through the process global object. In case of the V8 engine, type the following in your terminal to retrieve its version:
node -p process.versions.v8
ShellCopy to clipboardPrevAn introduction to the npm package managerNextNode.js, the difference between development and production\n\n\n\nNode.js, the difference between development and production
There is no difference between development and production in Node.js, i.e., there are no specific settings you need to apply to make Node.js work in a production configuration.
However, a few libraries in the npm registry recognize using the NODE_ENV variable and default it to a development setting.
Always run your Node.js with the NODE_ENV=production set.
A popular way of configuring your application is by using the twelve factor methodology.
Why is NODE_ENV considered an antipattern?
An environment is a digital platform or a system where engineers can build, test, deploy, and manage software products. Conventionally, there are four stages or types of environments where our application is run:

Development
Testing
Staging
Production

The fundamental problem of NODE_ENV stems from developers combining optimizations and software behavior with the environment their software is running on. The result is code like the following:
if (process.env.NODE_ENV === 'development') {
  // ...
}

if (process.env.NODE_ENV === 'production') {
  // ...
}

if (['production', 'staging'].includes(process.env.NODE_ENV)) {
  // ...
}
JavaScriptCopy to clipboard
While this might look harmless, it makes the production and staging environments different, thus making reliable testing impossible. For example a test and thus a functionality of your product could pass when NODE_ENV is set to development but fail when setting NODE_ENV to production.
Therefore, setting NODE_ENV to anything but production is considered an antipattern.PrevECMAScript 2015 (ES6) and beyondNextNode.js with WebAssembly\n\n\n\nNode.js with WebAssembly
WebAssembly is a high-performance assembly-like language that can be compiled from various languages, including C/C++, Rust, and AssemblyScript. Currently, it is supported by Chrome, Firefox, Safari, Edge, and Node.js!
The WebAssembly specification details two file formats, a binary format called a WebAssembly Module with a .wasm extension and corresponding text representation called WebAssembly Text format with a .wat extension.
Key Concepts

Module - A compiled WebAssembly binary, ie a .wasm file.
Memory - A resizable ArrayBuffer.
Table - A resizable typed array of references not stored in Memory.
Instance - An instantiation of a Module with its Memory, Table, and variables.

In order to use WebAssembly, you need a .wasm binary file and a set of APIs to communicate with WebAssembly. Node.js provides the necessary APIs via the global WebAssembly object.
console.log(WebAssembly);
/*
Object [WebAssembly] {
  compile: [Function: compile],
  validate: [Function: validate],
  instantiate: [Function: instantiate]
}
*/
JavaScriptCopy to clipboard
Generating WebAssembly Modules
There are multiple methods available to generate WebAssembly binary files including:

Writing WebAssembly (.wat) by hand and converting to binary format using tools such as wabt
Using emscripten with a C/C++ application
Using wasm-pack with a Rust application
Using AssemblyScript if you prefer a TypeScript-like experience


Some of these tools generate not only the binary file, but the JavaScript "glue" code and corresponding HTML files to run in the browser.

How to use it
Once you have a WebAssembly module, you can use the Node.js WebAssembly object to instantiate it.
JSMJS// Assume add.wasm file exists that contains a single function adding 2 provided arguments
const fs = require('node:fs');

// Use the readFileSync function to read the contents of the "add.wasm" file
const wasmBuffer = fs.readFileSync('/path/to/add.wasm');

// Use the WebAssembly.instantiate method to instantiate the WebAssembly module
WebAssembly.instantiate(wasmBuffer).then(wasmModule => {
  // Exported function lives under instance.exports object
  const { add } = wasmModule.instance.exports;
  const sum = add(5, 6);
  console.log(sum); // Outputs: 11
});
JavaScriptCopy to clipboardInteracting with the OS
WebAssembly modules cannot directly access OS functionality on its own. A third-party tool Wasmtime can be used to access this functionality. Wasmtime utilizes the WASI API to access the OS functionality.
Resources

General WebAssembly Information
MDN Docs
Write WebAssembly by hand
PrevNode.js, the difference between development and productionNextDebugging Node.js\n\n\n\nDebugging Node.js
This guide will help you get started debugging your Node.js apps and scripts.
Enable Inspector
When started with the --inspect switch, a Node.js process listens for a
debugging client. By default, it will listen at host and port 127.0.0.1:9229.
Each process is also assigned a unique UUID.
Inspector clients must know and specify host address, port, and UUID to connect.
A full URL will look something like
ws://127.0.0.1:9229/0f2c936f-b1cd-4ac9-aab3-f63b0f33d55e.
Node.js will also start listening for debugging messages if it receives a
SIGUSR1 signal. (SIGUSR1 is not available on Windows.) In Node.js 7 and
earlier, this activates the legacy Debugger API. In Node.js 8 and later, it will
activate the Inspector API.
Security Implications
Since the debugger has full access to the Node.js execution environment, a
malicious actor able to connect to this port may be able to execute arbitrary
code on behalf of the Node.js process. It is important to understand the security
implications of exposing the debugger port on public and private networks.
Exposing the debug port publicly is unsafe
If the debugger is bound to a public IP address, or to 0.0.0.0, any clients that
can reach your IP address will be able to connect to the debugger without any
restriction and will be able to run arbitrary code.
By default node --inspect binds to 127.0.0.1. You explicitly need to provide a
public IP address or 0.0.0.0, etc., if you intend to allow external connections
to the debugger. Doing so may expose you to a potentially significant security
threat. We suggest you ensure appropriate firewalls and access controls in place
to prevent a security exposure.
See the section on 'Enabling remote debugging scenarios' on some advice on how
to safely allow remote debugger clients to connect.
Local applications have full access to the inspector
Even if you bind the inspector port to 127.0.0.1 (the default), any applications
running locally on your machine will have unrestricted access. This is by design
to allow local debuggers to be able to attach conveniently.
Browsers, WebSockets and same-origin policy
Websites open in a web-browser can make WebSocket and HTTP requests under the
browser security model. An initial HTTP connection is necessary to obtain a
unique debugger session id. The same-origin-policy prevents websites from being
able to make this HTTP connection. For additional security against
DNS rebinding attacks, Node.js
verifies that the 'Host' headers for the connection either
specify an IP address or localhost precisely.
These security policies disallow connecting to a remote debug server by
specifying the hostname. You can work-around this restriction by specifying
either the IP address or by using ssh tunnels as described below.
Inspector Clients
A minimal CLI debugger is available with node inspect myscript.js.
Several commercial and open source tools can also connect to the Node.js Inspector.
Chrome DevTools 55+, Microsoft Edge

Option 1: Open chrome://inspect in a Chromium-based
browser or edge://inspect in Edge. Click the Configure button and ensure your target host and port
are listed.
Option 2: Copy the devtoolsFrontendUrl from the output of /json/list
(see above) or the --inspect hint text and paste into Chrome.

See https://github.com/ChromeDevTools/devtools-frontend, https://www.microsoftedgeinsider.com for more information.
Visual Studio Code 1.10+

In the Debug panel, click the settings icon to open .vscode/launch.json.
Select "Node.js" for initial setup.

See https://github.com/microsoft/vscode for more information.
Visual Studio 2017+

Choose "Debug > Start Debugging" from the menu or hit F5.
Detailed instructions.

JetBrains WebStorm and other JetBrains IDEs

Create a new Node.js debug configuration and hit Debug. --inspect will be used
by default for Node.js 7+. To disable uncheck js.debugger.node.use.inspect in
the IDE Registry. To learn more about running and debugging Node.js in WebStorm and other JetBrains IDEs,
check out WebStorm online help.

chrome-remote-interface

Library to ease connections to Inspector Protocol endpoints.

See https://github.com/cyrus-and/chrome-remote-interface for more information.
Gitpod

Start a Node.js debug configuration from the Debug view or hit F5. Detailed instructions

See https://www.gitpod.io for more information.
Eclipse IDE with Eclipse Wild Web Developer extension

From a .js file, choose "Debug As... > Node program", or
Create a Debug Configuration to attach debugger to running Node.js application (already started with --inspect).

See https://eclipse.org/eclipseide for more information.
Command-line options
The following table lists the impact of various runtime flags on debugging:
FlagMeaning--inspectEnable inspector agent; Listen on default address and port (127.0.0.1:9229)--inspect=[host:port]Enable inspector agent; Bind to address or hostname host (default: 127.0.0.1); Listen on port port (default: 9229)--inspect-brkEnable inspector agent; Listen on default address and port (127.0.0.1:9229); Break before user code starts--inspect-brk=[host:port]Enable inspector agent; Bind to address or hostname host (default: 127.0.0.1); Listen on port port (default: 9229); Break before user code starts--inspect-waitEnable inspector agent; Listen on default address and port (127.0.0.1:9229); Wait for debugger to be attached.--inspect-wait=[host:port]Enable inspector agent; Bind to address or hostname host (default: 127.0.0.1); Listen on port port (default: 9229); Wait for debugger to be attached.node inspect script.jsSpawn child process to run user's script under --inspect flag; and use main process to run CLI debugger.node inspect --port=xxxx script.jsSpawn child process to run user's script under --inspect flag; and use main process to run CLI debugger. Listen on port port (default: 9229)
Enabling remote debugging scenarios
We recommend that you never have the debugger listen on a public IP address. If
you need to allow remote debugging connections we recommend the use of ssh
tunnels instead. We provide the following example for illustrative purposes only.
Please understand the security risk of allowing remote access to a privileged
service before proceeding.
Let's say you are running Node.js on a remote machine, remote.example.com, that
you want to be able to debug. On that machine, you should start the node process
with the inspector listening only to localhost (the default).
node --inspect server.js
ShellCopy to clipboard
Now, on your local machine from where you want to initiate a debug client
connection, you can setup an ssh tunnel:
ssh -L 9221:localhost:9229 [email protected]
ShellCopy to clipboard
This starts a ssh tunnel session where a connection to port 9221 on your local
machine will be forwarded to port 9229 on remote.example.com. You can now attach
a debugger such as Chrome DevTools or Visual Studio Code to localhost:9221,
which should be able to debug as if the Node.js application was running locally.
Legacy Debugger
The legacy debugger has been deprecated as of Node.js 7.7.0. Please use
--inspect and Inspector instead.
When started with the --debug or --debug-brk switches in version 7 and
earlier, Node.js listens for debugging commands defined by the discontinued
V8 Debugging Protocol on a TCP port, by default 5858. Any debugger client
which speaks this protocol can connect to and debug the running process; a
couple popular ones are listed below.
The V8 Debugging Protocol is no longer maintained or documented.
Built-in Debugger
Start node debug script_name.js to start your script under the builtin
command-line debugger. Your script starts in another Node.js process started with
the --debug-brk option, and the initial Node.js process runs the _debugger.js
script and connects to your target. See docs for more information.
node-inspector
Debug your Node.js app with Chrome DevTools by using an intermediary process
which translates the Inspector Protocol used in Chromium to the V8 Debugger
protocol used in Node.js. See https://github.com/node-inspector/node-inspector for more information.PrevNode.js with WebAssemblyNextProfiling Node.js Applications\n\n\n\nProfiling Node.js Applications
Profiling a Node.js application involves measuring its performance by analyzing
the CPU, memory, and other runtime metrics while the application is running.
This helps in identifying bottlenecks, high CPU usage, memory leaks, or slow
function calls that may impact the application's efficiency, responsiveness
and scalability.
There are many third party tools available for profiling Node.js applications
but, in many cases, the easiest option is to use the Node.js built-in profiler.
The built-in profiler uses the profiler inside V8 which samples the stack at
regular intervals during program execution. It records the results of these
samples, along with important optimization events such as jit compiles, as a
series of ticks:
code-creation,LazyCompile,0,0x2d5000a337a0,396,"bp native array.js:1153:16",0x289f644df68,~
code-creation,LazyCompile,0,0x2d5000a33940,716,"hasOwnProperty native v8natives.js:198:30",0x289f64438d0,~
code-creation,LazyCompile,0,0x2d5000a33c20,284,"ToName native runtime.js:549:16",0x289f643bb28,~
code-creation,Stub,2,0x2d5000a33d40,182,"DoubleToIStub"
code-creation,Stub,2,0x2d5000a33e00,507,"NumberToStringStub"

In the past, you needed the V8 source code to be able to interpret the ticks.
Luckily, tools have been introduced since Node.js 4.4.0 that facilitate the
consumption of this information without separately building V8 from source.
Let's see how the built-in profiler can help provide insight into application
performance.
To illustrate the use of the tick profiler, we will work with a simple Express
application. Our application will have two handlers, one for adding new users to
our system:
app.get('/newUser', (req, res) => {
  let username = req.query.username || '';
  const password = req.query.password || '';

  username = username.replace(/[!@#$%^&*]/g, '');

  if (!username || !password || users[username]) {
    return res.sendStatus(400);
  }

  const salt = crypto.randomBytes(128).toString('base64');
  const hash = crypto.pbkdf2Sync(password, salt, 10000, 512, 'sha512');

  users[username] = { salt, hash };

  res.sendStatus(200);
});
JavaScriptCopy to clipboard
and another for validating user authentication attempts:
app.get('/auth', (req, res) => {
  let username = req.query.username || '';
  const password = req.query.password || '';

  username = username.replace(/[!@#$%^&*]/g, '');

  if (!username || !password || !users[username]) {
    return res.sendStatus(400);
  }

  const { salt, hash } = users[username];
  const encryptHash = crypto.pbkdf2Sync(password, salt, 10000, 512, 'sha512');

  if (crypto.timingSafeEqual(hash, encryptHash)) {
    res.sendStatus(200);
  } else {
    res.sendStatus(401);
  }
});
JavaScriptCopy to clipboard
Please note that these are NOT recommended handlers for authenticating users in
your Node.js applications and are used purely for illustration purposes. You
should not be trying to design your own cryptographic authentication mechanisms
in general. It is much better to use existing, proven authentication solutions.
Now assume that we've deployed our application and users are complaining about
high latency on requests. We can easily run the app with the built-in profiler:
NODE_ENV=production node --prof app.js

and put some load on the server using ab (ApacheBench):
curl -X GET "http://localhost:8080/newUser?username=matt&password=password"
ab -k -c 20 -n 250 "http://localhost:8080/auth?username=matt&password=password"

and get an ab output of:
Concurrency Level:      20
Time taken for tests:   46.932 seconds
Complete requests:      250
Failed requests:        0
Keep-Alive requests:    250
Total transferred:      50250 bytes
HTML transferred:       500 bytes
Requests per second:    5.33 [#/sec] (mean)
Time per request:       3754.556 [ms] (mean)
Time per request:       187.728 [ms] (mean, across all concurrent requests)
Transfer rate:          1.05 [Kbytes/sec] received

...

Percentage of the requests served within a certain time (ms)
  50%   3755
  66%   3804
  75%   3818
  80%   3825
  90%   3845
  95%   3858
  98%   3874
  99%   3875
 100%   4225 (longest request)

From this output, we see that we're only managing to serve about 5 requests per
second and that the average request takes just under 4 seconds round trip. In a
real-world example, we could be doing lots of work in many functions on behalf
of a user request but even in our simple example, time could be lost compiling
regular expressions, generating random salts, generating unique hashes from user
passwords, or inside the Express framework itself.
Since we ran our application using the --prof option, a tick file was generated
in the same directory as your local run of the application. It should have the
form isolate-0xnnnnnnnnnnnn-v8.log (where n is a digit).
In order to make sense of this file, we need to use the tick processor bundled
with the Node.js binary. To run the processor, use the --prof-process flag:
node --prof-process isolate-0xnnnnnnnnnnnn-v8.log > processed.txt

Opening processed.txt in your favorite text editor will give you a few different
types of information. The file is broken up into sections which are again broken
up by language. First, we look at the summary section and see:
 [Summary]:
   ticks  total  nonlib   name
     79    0.2%    0.2%  JavaScript
  36703   97.2%   99.2%  C++
      7    0.0%    0.0%  GC
    767    2.0%          Shared libraries
    215    0.6%          Unaccounted

This tells us that 97% of all samples gathered occurred in C++ code and that
when viewing other sections of the processed output we should pay most attention
to work being done in C++ (as opposed to JavaScript). With this in mind, we next
find the [C++] section which contains information about which C++ functions are
taking the most CPU time and see:
 [C++]:
   ticks  total  nonlib   name
  19557   51.8%   52.9%  node::crypto::PBKDF2(v8::FunctionCallbackInfo<v8::Value> const&)
   4510   11.9%   12.2%  _sha1_block_data_order
   3165    8.4%    8.6%  _malloc_zone_malloc

We see that the top 3 entries account for 72.1% of CPU time taken by the
program. From this output, we immediately see that at least 51.8% of CPU time is
taken up by a function called PBKDF2 which corresponds to our hash generation
from a user's password. However, it may not be immediately obvious how the lower
two entries factor into our application (or if it is we will pretend otherwise
for the sake of example). To better understand the relationship between these
functions, we will next look at the [Bottom up (heavy) profile] section which
provides information about the primary callers of each function. Examining this
section, we find:
   ticks parent  name
  19557   51.8%  node::crypto::PBKDF2(v8::FunctionCallbackInfo<v8::Value> const&)
  19557  100.0%    v8::internal::Builtins::~Builtins()
  19557  100.0%      LazyCompile: ~pbkdf2 crypto.js:557:16

   4510   11.9%  _sha1_block_data_order
   4510  100.0%    LazyCompile: *pbkdf2 crypto.js:557:16
   4510  100.0%      LazyCompile: *exports.pbkdf2Sync crypto.js:552:30

   3165    8.4%  _malloc_zone_malloc
   3161   99.9%    LazyCompile: *pbkdf2 crypto.js:557:16
   3161  100.0%      LazyCompile: *exports.pbkdf2Sync crypto.js:552:30

Parsing this section takes a little more work than the raw tick counts above.
Within each of the "call stacks" above, the percentage in the parent column
tells you the percentage of samples for which the function in the row above was
called by the function in the current row. For example, in the middle "call
stack" above for _sha1_block_data_order, we see that _sha1_block_data_order occurred
in 11.9% of samples, which we knew from the raw counts above. However, here, we
can also tell that it was always called by the pbkdf2 function inside the
Node.js crypto module. We see that similarly, _malloc_zone_malloc was called
almost exclusively by the same pbkdf2 function. Thus, using the information in
this view, we can tell that our hash computation from the user's password
accounts not only for the 51.8% from above but also for all CPU time in the top
3 most sampled functions since the calls to _sha1_block_data_order and
_malloc_zone_malloc were made on behalf of the pbkdf2 function.
At this point, it is very clear that the password-based hash generation should
be the target of our optimization. Thankfully, you've fully internalized the
benefits of asynchronous programming and you realize that the work to
generate a hash from the user's password is being done in a synchronous way and
thus tying down the event loop. This prevents us from working on other incoming
requests while computing a hash.
To remedy this issue, you make a small modification to the above handlers to use
the asynchronous version of the pbkdf2 function:
app.get('/auth', (req, res) => {
  let username = req.query.username || '';
  const password = req.query.password || '';

  username = username.replace(/[!@#$%^&*]/g, '');

  if (!username || !password || !users[username]) {
    return res.sendStatus(400);
  }

  crypto.pbkdf2(
    password,
    users[username].salt,
    10000,
    512,
    'sha512',
    (err, hash) => {
      if (users[username].hash.toString() === hash.toString()) {
        res.sendStatus(200);
      } else {
        res.sendStatus(401);
      }
    }
  );
});
JavaScriptCopy to clipboard
A new run of the ab benchmark above with the asynchronous version of your app
yields:
Concurrency Level:      20
Time taken for tests:   12.846 seconds
Complete requests:      250
Failed requests:        0
Keep-Alive requests:    250
Total transferred:      50250 bytes
HTML transferred:       500 bytes
Requests per second:    19.46 [#/sec] (mean)
Time per request:       1027.689 [ms] (mean)
Time per request:       51.384 [ms] (mean, across all concurrent requests)
Transfer rate:          3.82 [Kbytes/sec] received

...

Percentage of the requests served within a certain time (ms)
  50%   1018
  66%   1035
  75%   1041
  80%   1043
  90%   1049
  95%   1063
  98%   1070
  99%   1071
 100%   1079 (longest request)

Yay! Your app is now serving about 20 requests per second, roughly 4 times more
than it was with the synchronous hash generation. Additionally, the average
latency is down from the 4 seconds before to just over 1 second.
Hopefully, through the performance investigation of this (admittedly contrived)
example, you've seen how the V8 tick processor can help you gain a better
understanding of the performance of your Node.js applications.
You may also find how to create a flame graph helpful.PrevDebugging Node.jsNextFetching data with Node.js\n\n\n\nUsing the Fetch API with Undici in Node.js
Introduction
Undici is an HTTP client library that powers the fetch API in Node.js. It was written from scratch and does not rely on the built-in HTTP client in Node.js. It includes a number of features that make it a good choice for high-performance applications.
Basic GET Usage
async function main() {
  // Like the browser fetch API, the default method is GET
  const response = await fetch('https://jsonplaceholder.typicode.com/posts');
  const data = await response.json();
  console.log(data);
  // returns something like:
  //   {
  //   userId: 1,
  //   id: 1,
  //   title: 'sunt aut facere repellat provident occaecati excepturi optio reprehenderit',
  //   body: 'quia et suscipit\n' +
  //     'suscipit recusandae consequuntur expedita et cum\n' +
  //     'reprehenderit molestiae ut ut quas totam\n' +
  //     'nostrum rerum est autem sunt rem eveniet architecto'
  // }
}

main().catch(console.error);
JavaScriptCopy to clipboard
Basic POST Usage
// Data sent from the client to the server
const body = {
  title: 'foo',
  body: 'bar',
  userId: 1,
};

async function main() {
  const response = await fetch('https://jsonplaceholder.typicode.com/posts', {
    method: 'POST',
    headers: {
      'User-Agent': 'undici-stream-example',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  });
  const data = await response.json();
  console.log(data);
  // returns something like:
  // { title: 'foo', body: 'bar', userId: 1, id: 101 }
}

main().catch(console.error);
JavaScriptCopy to clipboard
Customizing the Fetch API with Undici
Undici allows you to customize the Fetch API by providing options to the fetch function. For example, you can set custom headers, set the request method, and set the request body. Here is an example of how you can customize the Fetch API with Undici:
The fetch function takes two arguments: the URL to fetch and an options object. The options object is the Request object that you can use to customize the request. The function returns a Promises that resolves to a Response object. One difference between the Fetch API in the browser and the Fetch API in Node.js is that the Node.js version does not support
In the following example, we are sending a POST request to the Ollama API with a JSON payload. Ollama is a cli tool that allows you to run LLM's (Large Language Models) on your local machine. You can download it here
ollama run mistral
ShellCopy to clipboard
This will download the mistral model and run it on your local machine.
With a pool, you can reuse connections to the same server, which can improve performance. Here is an example of how you can use a pool with Undici:
import { Pool } from 'undici';

const ollamaPool = new Pool('http://localhost:11434', {
  connections: 10,
});

/**
 * Stream the completion of a prompt using the Ollama API.
 * @param {string} prompt - The prompt to complete.
 * @link https://github.com/ollama/ollama/blob/main/docs/api.md
 **/
async function streamOllamaCompletion(prompt) {
  const { statusCode, body } = await ollamaPool.request({
    path: '/api/generate',
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ prompt, model: 'mistral' }),
  });

  // You can read about HTTP status codes here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status
  // 200 means the request was successful.
  if (statusCode !== 200) {
    throw new Error(`Ollama request failed with status ${statusCode}`);
  }

  let partial = '';

  const decoder = new TextDecoder();
  for await (const chunk of body) {
    partial += decoder.decode(chunk, { stream: true });
    console.log(partial);
  }

  console.log('Streaming complete.');
}

try {
  await streamOllamaCompletion('What is recursion?');
} catch (error) {
  console.error('Error calling Ollama:', error);
} finally {
  console.log('Closing Ollama pool.');
  ollamaPool.close();
}
JavaScriptCopy to clipboard
Streaming Responses with Undici
Streams is a feature in Node.js that allows you to read and write chunks of data.
import { stream } from 'undici';
import { Writable } from 'stream';

async function fetchGitHubRepos() {
  const url = 'https://api.github.com/users/nodejs/repos';

  const { statusCode } = await stream(
    url,
    {
      method: 'GET',
      headers: {
        'User-Agent': 'undici-stream-example',
        Accept: 'application/json',
      },
    },
    () => {
      let buffer = '';

      return new Writable({
        write(chunk, encoding, callback) {
          buffer += chunk.toString();

          try {
            const json = JSON.parse(buffer);
            console.log(
              'Repository Names:',
              json.map(repo => repo.name)
            );
            buffer = '';
          } catch (error) {
            console.error('Error parsing JSON:', error);
          }

          callback();
        },
        final(callback) {
          console.log('Stream processing completed.');
          callback();
        },
      });
    }
  );

  console.log(`Response status: ${statusCode}`);
}

fetchGitHubRepos().catch(console.error);
JavaScriptCopy to clipboardPrevProfiling Node.js ApplicationsNextWebSocket client with Node.js\n\n\n\nNative WebSocket Client in Node.js
Introduction
Since Node.js v21, the WebSocket API has been enhanced using the Undici library, introducing a built-in WebSocket client. This simplifies real-time communication for Node.js applications. In Node.js v22.4.0 release, the WebSocket API was marked as stable, indicating it's ready for production use.
What is a WebSocket
WebSocket is a standardized communication protocol that enables simultaneous two-way communication over a single TCP connection. It has full-duplex or bi-directional capabilities that distinguishes it from HTTP. WebSocket achieves HTTP compatibility by using the HTTP Upgrade header to transition protocols. It allows servers to push content to clients without initial requests and maintains open connections for continuous message exchange, making it ideal for real-time data transfer with lower overhead than alternatives like HTTP polling. WebSocket communications typically occur over TCP ports 443 (secured) or 80 (unsecured), helping bypass firewall restrictions on non-web connections. The protocol defines its own URI schemes (ws:// and wss://) for unencrypted and encrypted connections respectively and supported by all major browsers.
Native WebSocket Client
Node.js can now act as a WebSocket client without relying on external libraries like ws or socket.io for client connections. This allows Node.js applications to initiate and manage outgoing WebSocket connections directly, streamlining tasks such as connecting to real-time data feeds or interacting with other WebSocket servers. Users can now create a websocket client connection with the standard new WebSocket() constructor.
Building on the above, let's add more practical examples to demonstrate the new WebSocket client functionality that demonstrates basic use-cases.
Basic Connection and Message Handling
// Creates a new WebSocket connection to the specified URL.
const socket = new WebSocket('ws://localhost:8080');

// Executes when the connection is successfully established.
socket.addEventListener('open', event => {
  console.log('WebSocket connection established!');
  // Sends a message to the WebSocket server.
  socket.send('Hello Server!');
});

// Listen for messages and executes when a message is received from the server.
socket.addEventListener('message', event => {
  console.log('Message from server: ', event.data);
});

// Executes when the connection is closed, providing the close code and reason.
socket.addEventListener('close', event => {
  console.log('WebSocket connection closed:', event.code, event.reason);
});

// Executes if an error occurs during the WebSocket communication.
socket.addEventListener('error', error => {
  console.error('WebSocket error:', error);
});
JavaScriptCopy to clipboard
Sending and Receiving JSON Data
const socket = new WebSocket('ws://localhost:8080');

socket.addEventListener('open', () => {
  const data = { type: 'message', content: 'Hello from Node.js!' };
  socket.send(JSON.stringify(data));
});

socket.addEventListener('message', event => {
  try {
    const receivedData = JSON.parse(event.data);
    console.log('Received JSON:', receivedData);
  } catch (error) {
    console.error('Error parsing JSON:', error);
    console.log('Received data was:', event.data);
  }
});
JavaScriptCopy to clipboard
The json code above demonstrates sending and receiving JSON data, which is common in WebSocket applications. It uses JSON.stringify() to convert JavaScript objects to JSON strings before sending. And converts the received string back to a JavaScript object with JSON.parse(). Finally, it includes error handling for JSON parsing.
This offers reduced dependency management and improved compatibility. Developers can avoid installing and maintaining additional WebSocket client libraries. The built-in implementation aligns with modern web standards, ensuring better interoperability. The enhancement focuses on the client-side of WebSocket communication, enabling Node.js to act as a WebSocket client.
Important to Understand
Node.js v22 does not provide a built-in native WebSocket server implementation. To create a WebSocket server that accepts incoming connections from web browsers or other clients, one still need to use libraries like ws or socket.io. This means that while Node.js can now easily connect to WebSocket servers, it still requires external tools to become a WebSocket server.
In Summary
Node.js v22 empowers applications to seamlessly interact with WebSocket servers as clients, but the creation of WebSocket servers within Node.js remains dependent on established libraries. This distinction is crucial for developers to understand when implementing real-time communication in their Node.js projects.PrevFetching data with Node.jsNextSecurity Best Practices\n\n\n\nSecurity Best Practices
Intent
This document intends to extend the current threat model and provide extensive
guidelines on how to secure a Node.js application.
Document Content

Best practices: A simplified condensed way to see the best practices. We can
use this issue or this guideline
as the starting point. It is important to note that this document is specific
to Node.js, if you are looking for something broad, consider
OSSF Best Practices.
Attacks explained: illustrate and document in plain English with some code
examples (if possible) of the attacks that we are mentioning in the threat model.
Third-Party Libraries: define threats
(typosquatting attacks, malicious packages...) and best practices regarding
node modules dependencies, etc...

Threat List
Denial of Service of HTTP server (CWE-400)
This is an attack where the application becomes unavailable for the purpose it
was designed due to the way it processes incoming HTTP requests. These requests
need not be deliberately crafted by a malicious actor: a misconfigured or buggy
client can also send a pattern of requests to the server that result in a denial
of service.
HTTP requests are received by the Node.js HTTP server and handed over to the
application code via the registered request handler. The server does not parse
the content of the request body. Therefore any DoS caused by the contents of the
body after they are handed over to the request handler is not a vulnerability in
Node.js itself, since it's the responsibility of the application code to handle
it correctly.
Ensure that the WebServer handles socket errors properly, for instance, when a
server is created without an error handler, it will be vulnerable to DoS
CJSMJSconst net = require('node:net');

const server = net.createServer(function (socket) {
  // socket.on('error', console.error) // this prevents the server to crash
  socket.write('Echo server\r\n');
  socket.pipe(socket);
});

server.listen(5000, '0.0.0.0');
JavaScriptCopy to clipboardIf a bad request is performed the server could crash.
An example of a DoS attack that is not caused by the request's contents is
Slowloris. In this attack, HTTP requests are sent slowly and fragmented,
one fragment at a time. Until the full request is delivered, the server will
keep resources dedicated to the ongoing request. If enough of these requests
are sent at the same time, the amount of concurrent connections will soon reach
its maximum resulting in a denial of service. This is how the attack depends
not on the request's contents but on the timing and pattern of the requests
being sent to the server.
Mitigations

Use a reverse proxy to receive and forward requests to the Node.js application.
Reverse proxies can provide caching, load balancing, IP blacklisting, etc. which
reduce the probability of a DoS attack being effective.
Correctly configure the server timeouts, so that connections that are idle or
where requests are arriving too slowly can be dropped. See the different timeouts
in http.Server, particularly headersTimeout, requestTimeout, timeout,
and keepAliveTimeout.
Limit the number of open sockets per host and in total. See the http docs,
particularly agent.maxSockets, agent.maxTotalSockets, agent.maxFreeSockets
and server.maxRequestsPerSocket.

DNS Rebinding (CWE-346)
This is an attack that can target Node.js applications being run with the
debugging inspector enabled using the --inspect switch.
Since websites opened in a web browser can make WebSocket and HTTP requests,
they can target the debugging inspector running locally.
This is usually prevented by the same-origin policy implemented by modern
browsers, which forbids scripts from reaching resources from different origins
(meaning a malicious website cannot read data requested from a local IP address).
However, through DNS rebinding, an attacker can temporarily control the origin
for their requests so that they seem to originate from a local IP address.
This is done by controlling both a website and the DNS server used to resolve
its IP address. See DNS Rebinding wiki for more details.
Mitigations

Disable inspector on SIGUSR1 signal by attaching a process.on(‘SIGUSR1’, …)
listener to it.
Do not run the inspector protocol in production.

Exposure of Sensitive Information to an Unauthorized Actor (CWE-552)
All the files and folders included in the current directory are pushed to the
npm registry during the package publication.
There are some mechanisms to control this behavior by defining a blocklist with
.npmignore and .gitignore or by defining an allowlist in the package.json
Mitigations

Using npm publish --dry-run to list all the files to publish. Ensure to review the
content before publishing the package.
It’s also important to create and maintain ignore files such as .gitignore and
.npmignore.
Throughout these files, you can specify which files/folders should not be published.
The files property in package.json allows the inverse operation
-- allowed list.
In case of an exposure, make sure to unpublish the package.

HTTP Request Smuggling (CWE-444)
This is an attack that involves two HTTP servers (usually a proxy and a Node.js
application). A client sends an HTTP request that goes first through the
front-end server (the proxy) and then is redirected to the back-end server (the application).
When the front-end and back-end interpret ambiguous HTTP requests differently,
there is potential for an attacker to send a malicious message that won't be
seen by the front-end but will be seen by the back-end, effectively "smuggling"
it past the proxy server.
See the CWE-444 for a more detailed description and examples.
Since this attack depends on Node.js interpreting HTTP requests
differently from an (arbitrary) HTTP server, a successful attack can be due to
a vulnerability in Node.js, the front-end server, or both.
If the way the request is interpreted by Node.js is consistent with the
HTTP specification (see RFC7230), then it is not considered a vulnerability
in Node.js.
Mitigations

Do not use the insecureHTTPParser option when creating a HTTP Server.
Configure the front-end server to normalize ambiguous requests.
Continuously monitor for new HTTP request smuggling vulnerabilities in both
Node.js and the front-end server of choice.
Use HTTP/2 end to end and disable HTTP downgrading if possible.

Information Exposure through Timing Attacks (CWE-208)
This is an attack that allows the attacker to learn potentially sensitive information by, for example, measuring how long
it takes for the application to respond to a request. This attack is not specific to Node.js and can target almost all runtimes.
The attack is possible whenever the application uses a secret in a timing-sensitive operation (e.g., branch). Consider handling authentication in a typical application. Here, a basic authentication method includes email and password as credentials.
User information is retrieved from the input the user has supplied from ideally a
DBMS.
Upon retrieving user information, the password is compared with the user
information retrieved from the database. Using the built-in string comparison takes a longer
time for the same-length values.
This comparison, when run for an acceptable amount unwillingly increases the
response time of the request. By comparing the request response times, an
attacker can guess the length and the value of the password in a large quantity
of requests.
Mitigations


The crypto API exposes a function timingSafeEqual to compare actual and
expected sensitive values using a constant-time algorithm.


For password comparison, you can use the scrypt available also on the
native crypto module.


More generally, avoid using secrets in variable-time operations. This includes branching on secrets and, when the attacker could be co-located on the same infrastructure (e.g., same cloud machine), using a secret as an index into memory. Writing constant-time code in JavaScript is hard (partly because of the JIT). For crypto applications, use the built-in crypto APIs or WebAssembly (for algorithms not implemented in natively).


Malicious Third-Party Modules (CWE-1357)
Currently, in Node.js, any package can access powerful resources such as
network access.
Furthermore, because they also have access to the file system, they can send
any data anywhere.
All code running into a node process has the ability to load and run additional
arbitrary code by using eval()(or its equivalents).
All code with file system write access may achieve the same thing by writing to
new or existing files that are loaded.
Node.js has an experimental¹
policy mechanism to declare the loaded resource as untrusted or trusted.
However, this policy is not enabled by default.
Be sure to pin dependency versions and run automatic checks for vulnerabilities
using common workflows or npm scripts.
Before installing a package make sure that this package is maintained and
includes all the content you expected.
Be careful, the GitHub source code is not always the same as the published one,
validate it in the node_modules.
Supply chain attacks
A supply chain attack on a Node.js application happens when one of its
dependencies (either direct or transitive) are compromised.
This can happen either due to the application being too lax on the specification
of the dependencies (allowing for unwanted updates) and/or common typos in the
specification (vulnerable to typosquatting).
An attacker who takes control of an upstream package can publish a new version
with malicious code in it. If a Node.js application depends on that package
without being strict on which version is safe to use, the package can be
automatically updated to the latest malicious version, compromising the application.
Dependencies specified in the package.json file can have an exact version number
or a range. However, when pinning a dependency to an exact version, its
transitive dependencies are not themselves pinned.
This still leaves the application vulnerable to unwanted/unexpected updates.
Possible attack vectors:

Typosquatting attacks
Lockfile poisoning
Compromised maintainers
Malicious Packages
Dependency Confusions

Mitigations

Prevent npm from executing arbitrary scripts with --ignore-scripts

Additionally, you can disable it globally with npm config set ignore-scripts true


Pin dependency versions to a specific immutable version,
not a version that is a range or from a mutable source.
Use lockfiles, which pin every dependency (direct and transitive).

Use Mitigations for lockfile poisoning.


Automate checks for new vulnerabilities using CI, with tools like npm-audit.

Tools such as Socket can be used to analyze packages with static analysis
to find risky behaviors such as network or filesystem access.


Use npm ci instead of npm install.
This enforces the lockfile so that inconsistencies between it and the
package.json file causes an error (instead of silently ignoring the lockfile
in favor of package.json).
Carefully check the package.json file for errors/typos in the names of the
dependencies.

Memory Access Violation (CWE-284)
Memory-based or heap-based attacks depend on a combination of memory management
errors and an exploitable memory allocator.
Like all runtimes, Node.js is vulnerable to these attacks if your projects run
on a shared machine.
Using a secure heap is useful for preventing sensitive information from leaking
due to pointer overruns and underruns.
Unfortunately, a secure heap is not available on Windows.
More information can be found on Node.js secure-heap documentation.
Mitigations

Use --secure-heap=n depending on your application where n is the allocated
maximum byte size.
Do not run your production app on a shared machine.

Monkey Patching (CWE-349)
Monkey patching refers to the modification of properties in runtime aiming to
change the existing behavior. Example:
// eslint-disable-next-line no-extend-native
Array.prototype.push = function (item) {
  // overriding the global [].push
};
JavaScriptCopy to clipboard
Mitigations
The --frozen-intrinsics flag enables experimental¹
frozen intrinsics, which means all the built-in JavaScript objects and functions
are recursively frozen.
Therefore, the following snippet will not override the default behavior of
Array.prototype.push
// eslint-disable-next-line no-extend-native
Array.prototype.push = function (item) {
  // overriding the global [].push
};

// Uncaught:
// TypeError <Object <Object <[Object: null prototype] {}>>>:
// Cannot assign to read only property 'push' of object ''
JavaScriptCopy to clipboard
However, it’s important to mention you can still define new globals and replace
existing globals using globalThis
> globalThis.foo = 3; foo; // you can still define new globals
3
> globalThis.Array = 4; Array; // However, you can also replace existing globals
4
Shell SessionCopy to clipboard
Therefore, Object.freeze(globalThis) can be used to guarantee no globals will
be replaced.
Prototype Pollution Attacks (CWE-1321)
Prototype pollution refers to the possibility of modifying or injecting properties
into Javascript language items by abusing the usage of __proto_,
_constructor, prototype, and other properties inherited from built-in
prototypes.

const a = { a: 1, b: 2 };
const data = JSON.parse('{"__proto__": { "polluted": true}}');

const c = Object.assign({}, a, data);
console.log(c.polluted); // true

// Potential DoS
const data2 = JSON.parse('{"__proto__": null}');
const d = Object.assign(a, data2);
d.hasOwnProperty('b'); // Uncaught TypeError: d.hasOwnProperty is not a function
JavaScriptCopy to clipboard
This is a potential vulnerability inherited from the JavaScript
language.
Examples:

CVE-2022-21824 (Node.js)
CVE-2018-3721 (3rd Party library: Lodash)

Mitigations

Avoid insecure recursive merges, see CVE-2018-16487.
Implement JSON Schema validations for external/untrusted requests.
Create Objects without prototype by using Object.create(null).
Freezing the prototype: Object.freeze(MyObject.prototype).
Disable the Object.prototype.__proto__ property using --disable-proto flag.
Check that the property exists directly on the object, not from the prototype
using Object.hasOwn(obj, keyFromObj).
Avoid using methods from Object.prototype.

Uncontrolled Search Path Element (CWE-427)
Node.js loads modules following the Module Resolution Algorithm.
Therefore, it assumes the directory in which a module is requested
(require) is trusted.
By that, it means the following application behavior is expected.
Assuming the following directory structure:

app/

server.js
auth.js
auth



If server.js uses require('./auth') it will follow the module resolution
algorithm and load auth instead of auth.js.
Mitigations
Using the experimental¹
policy mechanism with integrity checking can avoid the above threat.
For the directory described above, one can use the following policy.json
{
  "resources": {
    "./app/auth.js": {
      "integrity": "sha256-iuGZ6SFVFpMuHUcJciQTIKpIyaQVigMZlvg9Lx66HV8="
    },
    "./app/server.js": {
      "dependencies": {
        "./auth": "./app/auth.js"
      },
      "integrity": "sha256-NPtLCQ0ntPPWgfVEgX46ryTNpdvTWdQPoZO3kHo0bKI="
    }
  }
}
JSONCopy to clipboard
Therefore, when requiring the auth module, the system will validate the
integrity and throw an error if doesn’t match the expected one.
» node --experimental-policy=policy.json app/server.js
node:internal/policy/sri:65
      throw new ERR_SRI_PARSE(str, str[prevIndex], prevIndex);
      ^

SyntaxError [ERR_SRI_PARSE]: Subresource Integrity string "sha256-iuGZ6SFVFpMuHUcJciQTIKpIyaQVigMZlvg9Lx66HV8=%" had an unexpected "%" at position 51
    at new NodeError (node:internal/errors:393:5)
    at Object.parse (node:internal/policy/sri:65:13)
    at processEntry (node:internal/policy/manifest:581:38)
    at Manifest.assertIntegrity (node:internal/policy/manifest:588:32)
    at Module._compile (node:internal/modules/cjs/loader:1119:21)
    at Module._extensions..js (node:internal/modules/cjs/loader:1213:10)
    at Module.load (node:internal/modules/cjs/loader:1037:32)
    at Module._load (node:internal/modules/cjs/loader:878:12)
    at Module.require (node:internal/modules/cjs/loader:1061:19)
    at require (node:internal/modules/cjs/helpers:99:18) {
  code: 'ERR_SRI_PARSE'
}
Shell SessionCopy to clipboard
Note, it's always recommended the use of --policy-integrity to avoid policy mutations.
Experimental Features in Production
The use of experimental features in production isn't recommended.
Experimental features can suffer breaking changes if needed, and their
functionality isn't securely stable. Although, feedback is highly appreciated.
OpenSSF Tools
The OpenSSF is leading several initiatives that can be very useful, especially if you plan to publish an npm package. These initiatives include:

OpenSSF Scorecard Scorecard evaluates open source projects using a series of automated security risk checks. You can use it to proactively assess vulnerabilities and dependencies in your code base and make informed decisions about accepting vulnerabilities.
OpenSSF Best Practices Badge Program Projects can voluntarily self-certify by describing how they comply with each best practice. This will generate a badge that can be added to the project.
PrevWebSocket client with Node.jsNextIntroduction to TypeScript\n\n\n\nIntroduction to TypeScript
What is TypeScript
TypeScript is an open-source language maintained and developed by Microsoft.
Basically, TypeScript adds additional syntax to JavaScript to support a tighter integration with your editor. Catch errors early in your editor or in your CI/CD pipeline, and write more maintainable code.
We can talk about other TypeScript benefits later, let's see some examples now!
First TypeScript code
Take a look at this code snippet and then we can unpack it together:

type User = {
  name: string;
  age: number;
};

function isAdult(user: User): boolean {
  return user.age >= 18;
}

const justine = {
  name: 'Justine',
  age: 23,
} satisfies User;

const isJustineAnAdult = isAdult(justine);
TypeScriptCopy to clipboard
The first part (with the type keyword) is responsible for declaring our custom object type representing users. Later we utilize this newly created type to create function isAdult that accepts one argument of type User and returns boolean. After this, we create justine, our example data that can be used for calling the previously defined function. Finally, we create a new variable with information on whether justine is an adult.
There are additional things about this example that you should know. Firstly, if we do not comply with the declared types, TypeScript will inform us that something is wrong and prevent misuse. Secondly, not everything must be typed explicitly—TypeScript infers types for us. For example, the variable isJustineAnAdult is of type boolean even if we didn't type it explicitly, and justine would be a valid argument for our function even though we didn't declare this variable as of User type.
What does TypeScript consist of?
TypeScript consists of two main components: the code itself and type definitions.
TypeScript Code
The code part is regular JavaScript with additional TypeScript-specific syntax for type annotations. When TypeScript code is compiled, all the TypeScript-specific parts are removed, resulting in clean JavaScript that can run in any environment. For example:
function greet(name: string) {
  console.log(`Hello, ${name}!`);
}
TypeScriptCopy to clipboard
Type Definitions
Type definitions describe the shape of existing JavaScript code. They are usually stored in .d.ts files and don't contain any actual implementation—they only describe the types. These definitions are essential for interoperability with JavaScript: code is not usually distributed as TypeScript, but instead transpiled to JavaScript that includes sidecar type definition files.
For example, when you use Node.js with TypeScript, you'll need type definitions for Node.js APIs. This is available via @types/node. Install it using:
npm add --save-dev @types/node
ShellCopy to clipboard
These type definitions allow TypeScript to understand Node.js APIs and provide proper type checking and autocompletion when you use functions like fs.readFile or http.createServer. For example:
import * as fs from 'fs';

fs.readFile('example.txt', 'foo', (err, data) => {
  //                          ^^^ Argument of type '"foo"' is not assignable to parameter of type …
  if (err) throw err;
  console.log(data);
});
JavaScriptCopy to clipboard
Many popular JavaScript libraries have their type definitions available under the @types namespace, maintained by the DefinitelyTyped community. This enables seamless integration of existing JavaScript libraries with TypeScript projects.
Transform Capabilities
TypeScript also includes powerful transformation capabilities, particularly for JSX (used in React and similar frameworks). The TypeScript compiler can transform JSX syntax into regular JavaScript, similar to how Babel works. While we won't cover these transformation features in these articles, it's worth noting that TypeScript isn't only a tool for type checking—it's also a build tool for transforming modern JavaScript syntax into compatible versions for different environments.
How to run TypeScript code
Okay, so we have some TypeScript code. Now how do we run it?
There are few possible ways to run TypeScript code, we will cover all of them in the next articles.PrevSecurity Best PracticesNextRunning TypeScript Natively\n\n\n\nRunning TypeScript Natively
Since v23.6.0, Node.js enables "type stripping" by default. If you are using v23.6.0 or later and your source code contains only erasable typescript syntax, you do not need this article.
Running TypeScript code with Node.js
Since V22.6.0, Node.js has experimental support for some TypeScript syntax via "type stripping". You can write code that's valid TypeScript directly in Node.js without the need to transpile it first.
The --experimental-strip-types flag tells Node.js to strip the type annotations from the TypeScript code before running it.
node --experimental-strip-types example.ts
ShellCopy to clipboard
And that's it! You can now run TypeScript code directly in Node.js without the need to transpile it first, and use TypeScript to catch type-related errors.
In V22.7.0 this experimental support was extended to transform TypeScript-only syntax, like enums and namespace, with the addition of the --experimental-transform-types flag. Enabling --experimental-transform-types automatically implies that --experimental-strip-types is enabled, so there's no need to use both flags in the same command:
node --experimental-transform-types another-example.ts
ShellCopy to clipboard
From v23.6.0 onwards, type stripping is enabled by default (you can disable it via --no-experimental-strip-types), enabling you to run any supported syntax, so running files like the one below with node file.ts is supported:
function foo(bar: number): string {
  return 'hello';
}
TypeScriptCopy to clipboard
However, running any code that requires transformations, like the code below still needs the use of --experimental-transform-types:
enum MyEnum {
  A,
  B,
}

console.log(MyEnum.A);
TypeScriptCopy to clipboard
Future versions of Node.js will include support for TypeScript without the need for a command line flag.
Limitations
At the time of writing, the experimental support for TypeScript in Node.js has some limitations.
You can get more information on the API docs.
Configuration
The Node.js TypeScript loader (Amaro) does not need or use tsconfig.json to run TypeScript code.
We recommend configuring your editor and tsc to reflect Node.js behavior by creating a tsconfig.json using the compilerOptions listed here, as well as using TypeScript version 5.7 or higher.
Important notes
Thanks to all the contributors who have made this feature possible. We hope that this feature will be stable and available in the LTS version of Node.js soon.
We can understand that this feature is experimental and has some limitations; if that doesn't suit your use-case, please use something else, or contribute a fix. Bug reports are also welcome, please keep in mind the project is run by volunteers, without warranty of any kind, so please be patient if you can't contribute the fix yourself.PrevIntroduction to TypeScriptNextRunning TypeScript with a runner\n\n\n\nRunning TypeScript with a runner
If you want more advanced processing of TypeScript than the built-in support (or you're using Node.js prior to v22.7.0), you have 2 options: use a runner (which handles much of the complexity for you), or handle it all yourself via transpilation.
Running TypeScript code with ts-node
ts-node is a TypeScript execution environment for Node.js. It allows you to run TypeScript code directly in Node.js without the need to compile it first. By default, ts-node performs type checking unless transpileOnly is enabled. While ts-node can catch type errors at runtime, we still recommend type-checking your code first with tsc before shipping it.
To use ts-node, you need to install it first:
npm i -D ts-node
ShellCopy to clipboard
Then you can run your TypeScript code like this:
npx ts-node example.ts
ShellCopy to clipboard
Running TypeScript code with tsx
tsx is another TypeScript execution environment for Node.js. It allows you to run TypeScript code directly in Node.js without the need to compile it first. Note, however, that it does not type check your code. So we recommend to type check your code first with tsc and then run it with tsx before shipping it.
To use tsx, you need to install it first:
npm i -D tsx
ShellCopy to clipboard
Then you can run your TypeScript code like this:
npx tsx example.ts
ShellCopy to clipboard
Registering tsx via node
If you want to use tsx via node, you can register tsx via --import:
node --import=tsx example.ts
ShellCopy to clipboardPrevRunning TypeScript NativelyNextRunning TypeScript code using transpilation\n\n\n\nRunning TypeScript code using transpilation
Transpilation is the process of converting source code from one language to another. In the case of TypeScript, it's the process of converting TypeScript code to JavaScript code. This is necessary because browsers and Node.js don't run TypeScript code directly.
Compiling TypeScript to JavaScript
The most common way to run TypeScript code is to compile it to JavaScript first. You can do this using the TypeScript compiler tsc.
Step 1: Write your TypeScript code in a file, for example example.ts.

type User = {
  name: string;
  age: number;
};

function isAdult(user: User): boolean {
  return user.age >= 18;
}

const justine = {
  name: 'Justine',
  age: 23,
} satisfies User;

const isJustineAnAdult = isAdult(justine);
TypeScriptCopy to clipboard
Step 2: Install TypeScript locally using a package manager:
In this example we're going to use npm, you can check our introduction to the npm package manager for more information.
npm i -D typescript # -D is a shorthand for --save-dev
ShellCopy to clipboard
Step 3: Compile your TypeScript code to JavaScript using the tsc command:
npx tsc example.ts
ShellCopy to clipboard

NOTE: npx is a tool that allows you to run Node.js packages without installing them globally.

tsc is the TypeScript compiler which will take our TypeScript code and compile it to JavaScript.
This command will result in a new file named example.js that we can run using Node.js.
Now when we know how to compile and run TypeScript code let's see TypeScript bug-preventing capabilities in action!
Step 4: Run your JavaScript code using Node.js:
node example.js
ShellCopy to clipboard
You should see the output of your TypeScript code in the terminal
If there are type errors
If you have type errors in your TypeScript code, the TypeScript compiler will catch them and prevent you from running the code. For example, if you change the age property of justine to a string, TypeScript will throw an error:
We will modify our code like this, to voluntarily introduce a type error:
type User = {
  name: string;
  age: number;
};

function isAdult(user: User): boolean {
  return user.age >= 18;
}

const justine: User = {
  name: 'Justine',
  age: 'Secret!',
};

const isJustineAnAdult: string = isAdult(justine, "I shouldn't be here!");
TypeScriptCopy to clipboard
And this is what TypeScript has to say about this:
example.ts:12:5 - error TS2322: Type 'string' is not assignable to type 'number'.

12     age: 'Secret!',
       ~~~

  example.ts:3:5
    3     age: number;
          ~~~
    The expected type comes from property 'age' which is declared here on type 'User'

example.ts:15:7 - error TS2322: Type 'boolean' is not assignable to type 'string'.

15 const isJustineAnAdult: string = isAdult(justine, "I shouldn't be here!");
         ~~~~~~~~~~~~~~~~

example.ts:15:51 - error TS2554: Expected 1 arguments, but got 2.

15 const isJustineAnAdult: string = isAdult(justine, "I shouldn't be here!");
                                                     ~~~~~~~~~~~~~~~~~~~~~~


Found 3 errors in the same file, starting at: example.ts:12
Shell SessionCopy to clipboard
As you can see, TypeScript is very helpful in catching bugs before they even happen. This is one of the reasons why TypeScript is so popular among developers.PrevRunning TypeScript with a runnerNextPublishing a TypeScript package\n\n\n\nPublishing a TypeScript package
This article covers items regarding TypeScript publishing specifically. Publishing means distributed as a package via npm (or other package manager); this is not about compiling an app / server to be run in production (such as a PWA and/or endpoint server).
Some important things to note:


Everything from Publishing a package applies here.


Fields like main operate on published content, so when TypeScript source-code is transpiled to JavaScript, JavaScript is the published content and main would point to a JavaScript file with a JavaScript file extension (ex main.ts → "main": "main.js").


Fields like scripts.test operate on source-code, so they would use the file extensions of the source code (ex "test": "node --test './src/**/*.test.ts').




Node runs TypeScript code via a process called "type stripping", wherein node (via Amaro) removes TypeScript-specific syntax, leaving behind vanilla JavaScript (which node already understands). This behaviour is enabled by default as of node version 23.6.0.

Node does not strip types in node_modules because it can cause significant performance issues for the official TypeScript compiler (tsc) and parts of VS Code, so the TypeScript maintainers would like to discourage people publishing raw TypeScript, at least for now.



Consuming TypeScript-specific features like enum in node still requires a flag (--experimental-transform-types). There are often better alternatives for these anyway.

To ensure TypeScript-specific features are not present (so your code can just run in node), set the erasableSyntaxOnly config option in TypeScript version 5.8+.



Use dependabot to keep your dependencies current, including those in github actions. It's a very easy set-and-forget configuration.


.nvmrc comes from nvm, a multi-version manager for node. It allows you to specify the version of node the project should generally use.


A directory overview of a repository would look something like:
Files co-locatedFiles co-located but segregated'src' and 'test' fully segregatedexample-ts-pkg/
├ .github/
│ ├ workflows/
│ │ ├ ci.yml
│ │ └ publish.yml
│ └ dependabot.yml
├ src/
│ ├ foo.fixture.js
│ ├ main.ts
│ ├ main.test.ts
│ ├ some-util.ts
│ └ some-util.test.ts
├ LICENSE
├ package.json
├ README.md
└ tsconfig.json
textCopy to clipboardAnd a directory overview of its published package would look something like:
Fully flatWith 'dist'example-ts-pkg/
├ LICENSE
├ main.d.ts
├ main.d.ts.map
├ main.js
├ package.json
├ README.md
├ some-util.d.ts
├ some-util.d.ts.map
└ some-util.js
textCopy to clipboardA note about directory organisation: There are a few common practices for placing tests. Principle of least knowledge says to co-locate them (put them adjacent to implementation). Sometimes, that's in the same directory, or within a drawer like a __test__ (also adjacent to the implementation, "Files co-located but segregated"). Alternatively, some opt to create a test/ sibling to src/ ("'src' and 'test' fully segregated"), either with a mirrored structure or a "junk drawer".
What to do with your types
Treat types like a test
The purpose of types is to warn an implementation will not work:
const foo = 'a';
const bar: number = 1 + foo;
//    ^^^ Type 'string' is not assignable to type 'number'.
TypeScriptCopy to clipboard
TypeScript has warned that the above code will not behave as intended, just like a unit test warns that code does not behave as intended. They are complementary and verify different things—you should have both.
Your editor (eg VS Code) likely has built-in support for TypeScript, displaying errors as you work. If not, and/or you missed those, CI will have your back.
The following GitHub Action sets up a CI task to automatically check (and require) types pass inspection for a PR into the main branch.
.github/workflows/ci.ymlpackage.jsontsconfig.json (flat output)tsconfig.json ('dist' output)# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json

name: Tests

on:
  pull_request:
    branches: ['*']

jobs:
  check-types:
    # Separate these from tests because
    # they are platform and node-version independent
    # and need be run only once.

    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'npm'
      - name: npm clean install
        run: npm ci
      # You may want to run a lint check here too
      - run: node --run types:check

  get-matrix:
    # Automatically pick active LTS versions
    runs-on: ubuntu-latest
    outputs:
      latest: ${{ steps.set-matrix.outputs.requireds }}
    steps:
      - uses: ljharb/actions/node/matrix@main
        id: set-matrix
        with:
          versionsAsRoot: true
          type: majors
          preset: '>= 22' # glob is not backported below 22.x

  test:
    needs: [get-matrix]
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        node-version: ${{ fromJson(needs.get-matrix.outputs.latest) }}
        os:
          - macos-latest
          - ubuntu-latest
          - windows-latest

    steps:
      - uses: actions/checkout@v4
      - name: Use node ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
      - name: npm clean install
        run: npm ci
      - run: node --run test
YAMLCopy to clipboardNote that test files may well have a different tsconfig.json applied (hence why they are excluded in the above sample).
Generate type declarations
Type declarations (.d.ts and friends) provide type information as a sidecar file, allowing the execution code to be vanilla JavaScript whilst still having types.
Since these are generated based on source code, they can be built as part of your publication process and do not need to be checked into your repository.
Take the following example, where the type declarations are generated just before publishing to the npm registry.
.github/workflows/publish.ymlpackage.json.npmignore.npmignore ('dist' output)# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json

# This is mostly boilerplate.

name: Publish to npm
on:
  push:
    tags:
      - '**@*'

jobs:
  build:
    runs-on: ubuntu-latest

    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          registry-url: 'https://registry.npmjs.org'
      - run: npm ci

      # - name: Publish to npm
      #   run: … npm publish …
YAMLCopy to clipboardYou'll want to publish a package compiled to support all Node.js LTS versions since you don't know which version the consumer will be running; the tsconfigs in this article support node 18.x and later.
npm publish will automatically run prepack beforehand. npm will also run prepack automatically before npm pack --dry-run (so you can easily see what your published package will be without actually publishing it). Beware, node --run does not do that. You can't use node --run for this step, so that caveat does not apply here, but it can for other steps.
The steps to actually publish to npm will be included in a separate article (there are several pros and cons beyond the scope of this article).
Breaking this down
Generating type declarations is deterministic: you'll get the same output from the same input, every time. So there is no need to commit these to git.
npm publish grabs everything applicable and available at the moment the command is run; so generating type declarations immediately before means those are available and will get picked up.
By default, npm publish grabs (almost) everything (see Files included in package). In order to keep your published package minimal (see the "Heaviest Objects in the Universe" meme about node_modules), you want to exclude certain files (like tests and test fixtures) from from packaging. Add these to the opt-out list specified in .npmignore; ensure the !*.d.ts exception is listed, or the generated type declartions will not be published! Alternatively, you can use package.json "files" to create an opt-in (if a mistake is made accidentally omitting a file, your package may be broken for downstream users, so this is a less safe option).PrevRunning TypeScript code using transpilationNextAsynchronous flow control\n\n\n\nAsynchronous flow control

The material in this post is heavily inspired by Mixu's Node.js Book.

At its core, JavaScript is designed to be non-blocking on the "main" thread, this is where views are rendered. You can imagine the importance of this in the browser. When the main thread becomes blocked it results in the infamous "freezing" that end users dread, and no other events can be dispatched resulting in the loss of data acquisition, for example.
This creates some unique constraints that only a functional style of programming can cure. This is where callbacks come in to the picture.
However, callbacks can become challenging to handle in more complicated procedures. This often results in "callback hell" where multiple nested functions with callbacks make the code more challenging to read, debug, organize, etc.
async1(function (input, result1) {
  async2(function (result2) {
    async3(function (result3) {
      async4(function (result4) {
        async5(function (output) {
          // do something with output
        });
      });
    });
  });
});
JavaScriptCopy to clipboard
Of course, in real life there would most likely be additional lines of code to handle result1, result2, etc., thus, the length and complexity of this issue usually results in code that looks much more messy than the example above.
This is where functions come in to great use. More complex operations are made up of many functions:

initiator style / input
middleware
terminator

The "initiator style / input" is the first function in the sequence. This function will accept the original input, if any, for the operation. The operation is an executable series of functions, and the original input will primarily be:

variables in a global environment
direct invocation with or without arguments
values obtained by file system or network requests

Network requests can be incoming requests initiated by a foreign network, by another application on the same network, or by the app itself on the same or foreign network.
A middleware function will return another function, and a terminator function will invoke the callback. The following illustrates the flow to network or file system requests. Here the latency is 0 because all these values are available in memory.
function final(someInput, callback) {
  callback(`${someInput} and terminated by executing callback `);
}

function middleware(someInput, callback) {
  return final(`${someInput} touched by middleware `, callback);
}

function initiate() {
  const someInput = 'hello this is a function ';
  middleware(someInput, function (result) {
    console.log(result);
    // requires callback to `return` result
  });
}

initiate();
JavaScriptCopy to clipboard
State management
Functions may or may not be state dependent. State dependency arises when the input or other variable of a function relies on an outside function.
In this way there are two primary strategies for state management:

passing in variables directly to a function, and
acquiring a variable value from a cache, session, file, database, network, or other outside source.

Note, I did not mention global variable. Managing state with global variables is often a sloppy anti-pattern that makes it difficult or impossible to guarantee state. Global variables in complex programs should be avoided when possible.
Control flow
If an object is available in memory, iteration is possible, and there will not be a change to control flow:
function getSong() {
  let _song = '';
  let i = 100;
  for (i; i > 0; i -= 1) {
    _song += `${i} beers on the wall, you take one down and pass it around, ${
      i - 1
    } bottles of beer on the wall\n`;
    if (i === 1) {
      _song += "Hey let's get some more beer";
    }
  }

  return _song;
}

function singSong(_song) {
  if (!_song) throw new Error("song is '' empty, FEED ME A SONG!");
  console.log(_song);
}

const song = getSong();
// this will work
singSong(song);
JavaScriptCopy to clipboard
However, if the data exists outside of memory the iteration will no longer work:
function getSong() {
  let _song = '';
  let i = 100;
  for (i; i > 0; i -= 1) {
    /* eslint-disable no-loop-func */
    setTimeout(function () {
      _song += `${i} beers on the wall, you take one down and pass it around, ${
        i - 1
      } bottles of beer on the wall\n`;
      if (i === 1) {
        _song += "Hey let's get some more beer";
      }
    }, 0);
    /* eslint-enable no-loop-func */
  }

  return _song;
}

function singSong(_song) {
  if (!_song) throw new Error("song is '' empty, FEED ME A SONG!");
  console.log(_song);
}

const song = getSong('beer');
// this will not work
singSong(song);
// Uncaught Error: song is '' empty, FEED ME A SONG!
JavaScriptCopy to clipboard
Why did this happen? setTimeout instructs the CPU to store the instructions elsewhere on the bus, and instructs that the data is scheduled for pickup at a later time. Thousands of CPU cycles pass before the function hits again at the 0 millisecond mark, the CPU fetches the instructions from the bus and executes them. The only problem is that song ('') was returned thousands of cycles prior.
The same situation arises in dealing with file systems and network requests. The main thread simply cannot be blocked for an indeterminate period of time-- therefore, we use callbacks to schedule the execution of code in time in a controlled manner.
You will be able to perform almost all of your operations with the following 3 patterns:

In series: functions will be executed in a strict sequential order, this one is most similar to for loops.

// operations defined elsewhere and ready to execute
const operations = [
  { func: function1, args: args1 },
  { func: function2, args: args2 },
  { func: function3, args: args3 },
];

function executeFunctionWithArgs(operation, callback) {
  // executes function
  const { args, func } = operation;
  func(args, callback);
}

function serialProcedure(operation) {
  if (!operation) process.exit(0); // finished
  executeFunctionWithArgs(operation, function (result) {
    // continue AFTER callback
    serialProcedure(operations.shift());
  });
}

serialProcedure(operations.shift());
JavaScriptCopy to clipboard

Limited in series: functions will be executed in a strict sequential order, but with a limit on the number of executions. Useful when you need to process a large list but with a cap on the number of items successfully processed.

let successCount = 0;

function final() {
  console.log(`dispatched ${successCount} emails`);
  console.log('finished');
}

function dispatch(recipient, callback) {
  // `sendEmail` is a hypothetical SMTP client
  sendMail(
    {
      subject: 'Dinner tonight',
      message: 'We have lots of cabbage on the plate. You coming?',
      smtp: recipient.email,
    },
    callback
  );
}

function sendOneMillionEmailsOnly() {
  getListOfTenMillionGreatEmails(function (err, bigList) {
    if (err) throw err;

    function serial(recipient) {
      if (!recipient || successCount >= 1000000) return final();
      dispatch(recipient, function (_err) {
        if (!_err) successCount += 1;
        serial(bigList.pop());
      });
    }

    serial(bigList.pop());
  });
}

sendOneMillionEmailsOnly();
JavaScriptCopy to clipboard

Full parallel: when ordering is not an issue, such as emailing a list of 1,000,000 email recipients.

let count = 0;
let success = 0;
const failed = [];
const recipients = [
  { name: 'Bart', email: 'bart@tld' },
  { name: 'Marge', email: 'marge@tld' },
  { name: 'Homer', email: 'homer@tld' },
  { name: 'Lisa', email: 'lisa@tld' },
  { name: 'Maggie', email: 'maggie@tld' },
];

function dispatch(recipient, callback) {
  // `sendEmail` is a hypothetical SMTP client
  sendMail(
    {
      subject: 'Dinner tonight',
      message: 'We have lots of cabbage on the plate. You coming?',
      smtp: recipient.email,
    },
    callback
  );
}

function final(result) {
  console.log(`Result: ${result.count} attempts \
      & ${result.success} succeeded emails`);
  if (result.failed.length)
    console.log(`Failed to send to: \
        \n${result.failed.join('\n')}\n`);
}

recipients.forEach(function (recipient) {
  dispatch(recipient, function (err) {
    if (!err) {
      success += 1;
    } else {
      failed.push(recipient.name);
    }
    count += 1;

    if (count === recipients.length) {
      final({
        count,
        success,
        failed,
      });
    }
  });
});
JavaScriptCopy to clipboard
Each has its own use cases, benefits, and issues you can experiment and read about in more detail. Most importantly, remember to modularize your operations and use callbacks! If you feel any doubt, treat everything as if it were middleware!PrevPublishing a TypeScript packageNextOverview of Blocking vs Non-Blocking\n\n\n\nOverview of Blocking vs Non-Blocking
This overview covers the difference between blocking and non-blocking
calls in Node.js. This overview will refer to the event loop and libuv but no
prior knowledge of those topics is required. Readers are assumed to have a
basic understanding of the JavaScript language and Node.js callback pattern.

"I/O" refers primarily to interaction with the system's disk and
network supported by libuv.

Blocking
Blocking is when the execution of additional JavaScript in the Node.js
process must wait until a non-JavaScript operation completes. This happens
because the event loop is unable to continue running JavaScript while a
blocking operation is occurring.
In Node.js, JavaScript that exhibits poor performance due to being CPU intensive
rather than waiting on a non-JavaScript operation, such as I/O, isn't typically
referred to as blocking. Synchronous methods in the Node.js standard library
that use libuv are the most commonly used blocking operations. Native
modules may also have blocking methods.
All of the I/O methods in the Node.js standard library provide asynchronous
versions, which are non-blocking, and accept callback functions. Some
methods also have blocking counterparts, which have names that end with
Sync.
Comparing Code
Blocking methods execute synchronously and non-blocking methods
execute asynchronously.
Using the File System module as an example, this is a synchronous file read:
const fs = require('node:fs');

const data = fs.readFileSync('/file.md'); // blocks here until file is read
JavaScriptCopy to clipboard
And here is an equivalent asynchronous example:
const fs = require('node:fs');

fs.readFile('/file.md', (err, data) => {
  if (err) throw err;
});
JavaScriptCopy to clipboard
The first example appears simpler than the second but has the disadvantage of
the second line blocking the execution of any additional JavaScript until
the entire file is read. Note that in the synchronous version if an error is
thrown it will need to be caught or the process will crash. In the asynchronous
version, it is up to the author to decide whether an error should throw as
shown.
Let's expand our example a little bit:
const fs = require('node:fs');

const data = fs.readFileSync('/file.md'); // blocks here until file is read
console.log(data);
moreWork(); // will run after console.log
JavaScriptCopy to clipboard
And here is a similar, but not equivalent asynchronous example:
const fs = require('node:fs');

fs.readFile('/file.md', (err, data) => {
  if (err) throw err;
  console.log(data);
});
moreWork(); // will run before console.log
JavaScriptCopy to clipboard
In the first example above, console.log will be called before moreWork(). In
the second example fs.readFile() is non-blocking so JavaScript execution
can continue and moreWork() will be called first. The ability to run
moreWork() without waiting for the file read to complete is a key design
choice that allows for higher throughput.
Concurrency and Throughput
JavaScript execution in Node.js is single threaded, so concurrency refers to the
event loop's capacity to execute JavaScript callback functions after completing
other work. Any code that is expected to run in a concurrent manner must allow
the event loop to continue running as non-JavaScript operations, like I/O, are
occurring.
As an example, let's consider a case where each request to a web server takes
50ms to complete and 45ms of that 50ms is database I/O that can be done
asynchronously. Choosing non-blocking asynchronous operations frees up that
45ms per request to handle other requests. This is a significant difference in
capacity just by choosing to use non-blocking methods instead of
blocking methods.
The event loop is different than models in many other languages where additional
threads may be created to handle concurrent work.
Dangers of Mixing Blocking and Non-Blocking Code
There are some patterns that should be avoided when dealing with I/O. Let's look
at an example:
const fs = require('node:fs');

fs.readFile('/file.md', (err, data) => {
  if (err) throw err;
  console.log(data);
});
fs.unlinkSync('/file.md');
JavaScriptCopy to clipboard
In the above example, fs.unlinkSync() is likely to be run before
fs.readFile(), which would delete file.md before it is actually read. A
better way to write this, which is completely non-blocking and guaranteed to
execute in the correct order is:
const fs = require('node:fs');

fs.readFile('/file.md', (readFileErr, data) => {
  if (readFileErr) throw readFileErr;
  console.log(data);
  fs.unlink('/file.md', unlinkErr => {
    if (unlinkErr) throw unlinkErr;
  });
});
JavaScriptCopy to clipboard
The above places a non-blocking call to fs.unlink() within the callback of
fs.readFile() which guarantees the correct order of operations.
Additional Resources

libuv
PrevAsynchronous flow controlNextJavaScript Asynchronous Programming and Callbacks\n\n\n\nJavaScript Asynchronous Programming and Callbacks
Asynchronicity in Programming Languages
Computers are asynchronous by design.
Asynchronous means that things can happen independently of the main program flow.
In the current consumer computers, every program runs for a specific time slot and then it stops its execution to let another program continue their execution. This thing runs in a cycle so fast that it's impossible to notice. We think our computers run many programs simultaneously, but this is an illusion (except on multiprocessor machines).
Programs internally use interrupts, a signal that's emitted to the processor to gain the attention of the system.
Let's not go into the internals of this now, but just keep in mind that it's normal for programs to be asynchronous and halt their execution until they need attention, allowing the computer to execute other things in the meantime. When a program is waiting for a response from the network, it cannot halt the processor until the request finishes.
Normally, programming languages are synchronous and some provide a way to manage asynchronicity in the language or through libraries. C, Java, C#, PHP, Go, Ruby, Swift, and Python are all synchronous by default. Some of them handle async operations by using threads, spawning a new process.
JavaScript
JavaScript is synchronous by default and is single threaded. This means that code cannot create new threads and run in parallel.
Lines of code are executed in series, one after another, for example:
const a = 1;
const b = 2;
const c = a * b;
console.log(c);
doSomething();
JavaScriptCopy to clipboard
But JavaScript was born inside the browser, its main job, in the beginning, was to respond to user actions, like onClick, onMouseOver, onChange, onSubmit and so on. How could it do this with a synchronous programming model?
The answer was in its environment. The browser provides a way to do it by providing a set of APIs that can handle this kind of functionality.
More recently, Node.js introduced a non-blocking I/O environment to extend this concept to file access, network calls and so on.
Callbacks
You can't know when a user is going to click a button. So, you define an event handler for the click event. This event handler accepts a function, which will be called when the event is triggered:
document.getElementById('button').addEventListener('click', () => {
  // item clicked
});
JavaScriptCopy to clipboard
This is the so-called callback.
A callback is a simple function that's passed as a value to another function, and will only be executed when the event happens. We can do this because JavaScript has first-class functions, which can be assigned to variables and passed around to other functions (called higher-order functions)
It's common to wrap all your client code in a load event listener on the window object, which runs the callback function only when the page is ready:
window.addEventListener('load', () => {
  // window loaded
  // do what you want
});
JavaScriptCopy to clipboard
Callbacks are used everywhere, not just in DOM events.
One common example is by using timers:
setTimeout(() => {
  // runs after 2 seconds
}, 2000);
JavaScriptCopy to clipboard
XHR requests also accept a callback, in this example by assigning a function to a property that will be called when a particular event occurs (in this case, the state of the request changes):
const xhr = new XMLHttpRequest();
xhr.onreadystatechange = () => {
  if (xhr.readyState === 4) {
    xhr.status === 200 ? console.log(xhr.responseText) : console.error('error');
  }
};
xhr.open('GET', 'https://yoursite.com');
xhr.send();
JavaScriptCopy to clipboard
Handling errors in callbacks
How do you handle errors with callbacks? One very common strategy is to use what Node.js adopted: the first parameter in any callback function is the error object: error-first callbacks
If there is no error, the object is null. If there is an error, it contains some description of the error and other information.
const fs = require('node:fs');

fs.readFile('/file.json', (err, data) => {
  if (err) {
    // handle error
    console.log(err);
    return;
  }

  // no errors, process data
  console.log(data);
});
JavaScriptCopy to clipboard
The problem with callbacks
Callbacks are great for simple cases!
However every callback adds a level of nesting, and when you have lots of callbacks, the code starts to be complicated very quickly:
window.addEventListener('load', () => {
  document.getElementById('button').addEventListener('click', () => {
    setTimeout(() => {
      items.forEach(item => {
        // your code here
      });
    }, 2000);
  });
});
JavaScriptCopy to clipboard
This is just a simple 4-levels code, but I've seen much more levels of nesting and it's not fun.
How do we solve this?
Alternatives to callbacks
Starting with ES6, JavaScript introduced several features that help us with asynchronous code that do not involve using callbacks: Promises (ES6) and Async/Await (ES2017).PrevOverview of Blocking vs Non-BlockingNextDiscover JavaScript Timers\n\n\n\nDiscover JavaScript Timers
setTimeout()
When writing JavaScript code, you might want to delay the execution of a function.
This is the job of setTimeout. You specify a callback function to execute later, and a value expressing how later you want it to run, in milliseconds:
setTimeout(() => {
  // runs after 2 seconds
}, 2000);

setTimeout(() => {
  // runs after 50 milliseconds
}, 50);
JavaScriptCopy to clipboard
This syntax defines a new function. You can call whatever other function you want in there, or you can pass an existing function name, and a set of parameters:
const myFunction = (firstParam, secondParam) => {
  // do something
};

// runs after 2 seconds
setTimeout(myFunction, 2000, firstParam, secondParam);
JavaScriptCopy to clipboard
setTimeout returns the timer id. This is generally not used, but you can store this id, and clear it if you want to delete this scheduled function execution:
const id = setTimeout(() => {
  // should run after 2 seconds
}, 2000);

// I changed my mind
clearTimeout(id);
JavaScriptCopy to clipboard
Zero delay
If you specify the timeout delay to 0, the callback function will be executed as soon as possible, but after the current function execution:
setTimeout(() => {
  console.log('after ');
}, 0);

console.log(' before ');
JavaScriptCopy to clipboard
This code will print
before
after
ShellCopy to clipboard
This is especially useful to avoid blocking the CPU on intensive tasks and let other functions be executed while performing a heavy calculation, by queuing functions in the scheduler.

Some browsers (IE and Edge) implement a setImmediate() method that does this same exact functionality, but it's not standard and unavailable on other browsers. But it's a standard function in Node.js.

setInterval()
setInterval is a function similar to setTimeout, with a difference: instead of running the callback function once, it will run it forever, at the specific time interval you specify (in milliseconds):
setInterval(() => {
  // runs every 2 seconds
}, 2000);
JavaScriptCopy to clipboard
The function above runs every 2 seconds unless you tell it to stop, using clearInterval, passing it the interval id that setInterval returned:
const id = setInterval(() => {
  // runs every 2 seconds
}, 2000);

clearInterval(id);
JavaScriptCopy to clipboard
It's common to call clearInterval inside the setInterval callback function, to let it auto-determine if it should run again or stop. For example this code runs something unless App.somethingIWait has the value arrived:
const interval = setInterval(() => {
  if (App.somethingIWait === 'arrived') {
    clearInterval(interval);
  }
  // otherwise do things
}, 100);
JavaScriptCopy to clipboard
Recursive setTimeout
setInterval starts a function every n milliseconds, without any consideration about when a function finished its execution.
If a function always takes the same amount of time, it's all fine:

Maybe the function takes different execution times, depending on network conditions for example:

And maybe one long execution overlaps the next one:

To avoid this, you can schedule a recursive setTimeout to be called when the callback function finishes:
const myFunction = () => {
  // do something

  setTimeout(myFunction, 1000);
};

setTimeout(myFunction, 1000);
JavaScriptCopy to clipboard
to achieve this scenario:

setTimeout and setInterval are available in Node.js, through the Timers module.
Node.js also provides setImmediate(), which is equivalent to using setTimeout(() => {}, 0), mostly used to work with the Node.js Event Loop.PrevJavaScript Asynchronous Programming and CallbacksNextThe Node.js Event Loop\n\n\n\nThe Node.js Event Loop
What is the Event Loop?
The event loop is what allows Node.js to perform non-blocking I/O
operations — despite the fact that a single JavaScript thread is used by default — by
offloading operations to the system kernel whenever possible.
Since most modern kernels are multi-threaded, they can handle multiple
operations executing in the background. When one of these operations
completes, the kernel tells Node.js so that the appropriate callback
may be added to the poll queue to eventually be executed. We'll explain
this in further detail later in this topic.
Event Loop Explained
When Node.js starts, it initializes the event loop, processes the
provided input script (or drops into the REPL, which is not covered in
this document) which may make async API calls, schedule timers, or call
process.nextTick(), then begins processing the event loop.
The following diagram shows a simplified overview of the event loop's
order of operations.
   ┌───────────────────────────┐
┌─>│           timers          │
│  └─────────────┬─────────────┘
│  ┌─────────────┴─────────────┐
│  │     pending callbacks     │
│  └─────────────┬─────────────┘
│  ┌─────────────┴─────────────┐
│  │       idle, prepare       │
│  └─────────────┬─────────────┘      ┌───────────────┐
│  ┌─────────────┴─────────────┐      │   incoming:   │
│  │           poll            │<─────┤  connections, │
│  └─────────────┬─────────────┘      │   data, etc.  │
│  ┌─────────────┴─────────────┐      └───────────────┘
│  │           check           │
│  └─────────────┬─────────────┘
│  ┌─────────────┴─────────────┐
└──┤      close callbacks      │
   └───────────────────────────┘


Each box will be referred to as a "phase" of the event loop.

Each phase has a FIFO queue of callbacks to execute. While each phase is
special in its own way, generally, when the event loop enters a given
phase, it will perform any operations specific to that phase, then
execute callbacks in that phase's queue until the queue has been
exhausted or the maximum number of callbacks has executed. When the
queue has been exhausted or the callback limit is reached, the event
loop will move to the next phase, and so on.
Since any of these operations may schedule more operations and new
events processed in the poll phase are queued by the kernel, poll
events can be queued while polling events are being processed. As a
result, long running callbacks can allow the poll phase to run much
longer than a timer's threshold. See the timers and
poll sections for more details.

There is a slight discrepancy between the Windows and the
Unix/Linux implementation, but that's not important for this
demonstration. The most important parts are here. There are actually
seven or eight steps, but the ones we care about — ones that Node.js
actually uses - are those above.

Phases Overview

timers: this phase executes callbacks scheduled by setTimeout()
and setInterval().
pending callbacks: executes I/O callbacks deferred to the next loop
iteration.
idle, prepare: only used internally.
poll: retrieve new I/O events; execute I/O related callbacks (almost
all with the exception of close callbacks, the ones scheduled by timers,
and setImmediate()); node will block here when appropriate.
check: setImmediate() callbacks are invoked here.
close callbacks: some close callbacks, e.g. socket.on('close', ...).

Between each run of the event loop, Node.js checks if it is waiting for
any asynchronous I/O or timers and shuts down cleanly if there are not
any.
Phases in Detail
timers
A timer specifies the threshold after which a provided callback
may be executed rather than the exact time a person wants it to
be executed. Timers callbacks will run as early as they can be
scheduled after the specified amount of time has passed; however,
Operating System scheduling or the running of other callbacks may delay
them.

Technically, the poll phase controls when timers are executed.

For example, say you schedule a timeout to execute after a 100 ms
threshold, then your script starts asynchronously reading a file which
takes 95 ms:
const fs = require('node:fs');

function someAsyncOperation(callback) {
  // Assume this takes 95ms to complete
  fs.readFile('/path/to/file', callback);
}

const timeoutScheduled = Date.now();

setTimeout(() => {
  const delay = Date.now() - timeoutScheduled;

  console.log(`${delay}ms have passed since I was scheduled`);
}, 100);

// do someAsyncOperation which takes 95 ms to complete
someAsyncOperation(() => {
  const startCallback = Date.now();

  // do something that will take 10ms...
  while (Date.now() - startCallback < 10) {
    // do nothing
  }
});
JavaScriptCopy to clipboard
When the event loop enters the poll phase, it has an empty queue
(fs.readFile() has not completed), so it will wait for the number of ms
remaining until the soonest timer's threshold is reached. While it is
waiting 95 ms pass, fs.readFile() finishes reading the file and its
callback which takes 10 ms to complete is added to the poll queue and
executed. When the callback finishes, there are no more callbacks in the
queue, so the event loop will see that the threshold of the soonest
timer has been reached then wrap back to the timers phase to execute
the timer's callback. In this example, you will see that the total delay
between the timer being scheduled and its callback being executed will
be 105ms.

To prevent the poll phase from starving the event loop, libuv
(the C library that implements the Node.js
event loop and all of the asynchronous behaviors of the platform)
also has a hard maximum (system dependent) before it stops polling for
more events.

pending callbacks
This phase executes callbacks for some system operations such as types
of TCP errors. For example if a TCP socket receives ECONNREFUSED when
attempting to connect, some *nix systems want to wait to report the
error. This will be queued to execute in the pending callbacks phase.
poll
The poll phase has two main functions:

Calculating how long it should block and poll for I/O, then
Processing events in the poll queue.

When the event loop enters the poll phase and there are no timers
scheduled, one of two things will happen:


If the poll queue is not empty, the event loop will iterate
through its queue of callbacks executing them synchronously until
either the queue has been exhausted, or the system-dependent hard limit
is reached.


If the poll queue is empty, one of two more things will
happen:


If scripts have been scheduled by setImmediate(), the event loop
will end the poll phase and continue to the check phase to
execute those scheduled scripts.


If scripts have not been scheduled by setImmediate(), the
event loop will wait for callbacks to be added to the queue, then
execute them immediately.




Once the poll queue is empty the event loop will check for timers
whose time thresholds have been reached. If one or more timers are
ready, the event loop will wrap back to the timers phase to execute
those timers' callbacks.
check
This phase allows the event loop to execute callbacks immediately after the
poll phase has completed. If the poll phase becomes idle and
scripts have been queued with setImmediate(), the event loop may
continue to the check phase rather than waiting.
setImmediate() is actually a special timer that runs in a separate
phase of the event loop. It uses a libuv API that schedules callbacks to
execute after the poll phase has completed.
Generally, as the code is executed, the event loop will eventually hit
the poll phase where it will wait for an incoming connection, request,
etc. However, if a callback has been scheduled with setImmediate()
and the poll phase becomes idle, it will end and continue to the
check phase rather than waiting for poll events.
close callbacks
If a socket or handle is closed abruptly (e.g. socket.destroy()), the
'close' event will be emitted in this phase. Otherwise it will be
emitted via process.nextTick().
setImmediate() vs setTimeout()
setImmediate() and setTimeout() are similar, but behave in different
ways depending on when they are called.

setImmediate() is designed to execute a script once the
current poll phase completes.
setTimeout() schedules a script to be run after a minimum threshold
in ms has elapsed.

The order in which the timers are executed will vary depending on the
context in which they are called. If both are called from within the
main module, then timing will be bound by the performance of the process
(which can be impacted by other applications running on the machine).
For example, if we run the following script which is not within an I/O
cycle (i.e. the main module), the order in which the two timers are
executed is non-deterministic, as it is bound by the performance of the
process:
JSBASH// timeout_vs_immediate.js
setTimeout(() => {
  console.log('timeout');
}, 0);

setImmediate(() => {
  console.log('immediate');
});
JavaScriptCopy to clipboardHowever, if you move the two calls within an I/O cycle, the immediate
callback is always executed first:
JSBASH// timeout_vs_immediate.js
const fs = require('node:fs');

fs.readFile(__filename, () => {
  setTimeout(() => {
    console.log('timeout');
  }, 0);
  setImmediate(() => {
    console.log('immediate');
  });
});
JavaScriptCopy to clipboardThe main advantage to using setImmediate() over setTimeout() is
setImmediate() will always be executed before any timers if scheduled
within an I/O cycle, independently of how many timers are present.
process.nextTick()
Understanding process.nextTick()
You may have noticed that process.nextTick() was not displayed in the
diagram, even though it's a part of the asynchronous API. This is because
process.nextTick() is not technically part of the event loop. Instead,
the nextTickQueue will be processed after the current operation is
completed, regardless of the current phase of the event loop. Here,
an operation is defined as a transition from the
underlying C/C++ handler, and handling the JavaScript that needs to be
executed.
Looking back at our diagram, any time you call process.nextTick() in a
given phase, all callbacks passed to process.nextTick() will be
resolved before the event loop continues. This can create some bad
situations because it allows you to "starve" your I/O by making
recursive process.nextTick() calls, which prevents the event loop
from reaching the poll phase.
Why would that be allowed?
Why would something like this be included in Node.js? Part of it is a
design philosophy where an API should always be asynchronous even where
it doesn't have to be. Take this code snippet for example:
function apiCall(arg, callback) {
  if (typeof arg !== 'string')
    return process.nextTick(
      callback,
      new TypeError('argument should be string')
    );
}
JavaScriptCopy to clipboard
The snippet does an argument check and if it's not correct, it will pass
the error to the callback. The API updated fairly recently to allow
passing arguments to process.nextTick() allowing it to take any
arguments passed after the callback to be propagated as the arguments to
the callback so you don't have to nest functions.
What we're doing is passing an error back to the user but only after
we have allowed the rest of the user's code to execute. By using
process.nextTick() we guarantee that apiCall() always runs its
callback after the rest of the user's code and before the event loop
is allowed to proceed. To achieve this, the JS call stack is allowed to
unwind then immediately execute the provided callback which allows a
person to make recursive calls to process.nextTick() without reaching a
RangeError: Maximum call stack size exceeded from v8.
This philosophy can lead to some potentially problematic situations.
Take this snippet for example:
let bar;

// this has an asynchronous signature, but calls callback synchronously
function someAsyncApiCall(callback) {
  callback();
}

// the callback is called before `someAsyncApiCall` completes.
someAsyncApiCall(() => {
  // since someAsyncApiCall hasn't completed, bar hasn't been assigned any value
  console.log('bar', bar); // undefined
});

bar = 1;
JavaScriptCopy to clipboard
The user defines someAsyncApiCall() to have an asynchronous signature,
but it actually operates synchronously. When it is called, the callback
provided to someAsyncApiCall() is called in the same phase of the
event loop because someAsyncApiCall() doesn't actually do anything
asynchronously. As a result, the callback tries to reference bar even
though it may not have that variable in scope yet, because the script has not
been able to run to completion.
By placing the callback in a process.nextTick(), the script still has the
ability to run to completion, allowing all the variables, functions,
etc., to be initialized prior to the callback being called. It also has
the advantage of not allowing the event loop to continue. It may be
useful for the user to be alerted to an error before the event loop is
allowed to continue. Here is the previous example using process.nextTick():
let bar;

function someAsyncApiCall(callback) {
  process.nextTick(callback);
}

someAsyncApiCall(() => {
  console.log('bar', bar); // 1
});

bar = 1;
JavaScriptCopy to clipboard
Here's another real world example:
const server = net.createServer(() => {}).listen(8080);

server.on('listening', () => {});
JavaScriptCopy to clipboard
When only a port is passed, the port is bound immediately. So, the
'listening' callback could be called immediately. The problem is that the
.on('listening') callback will not have been set by that time.
To get around this, the 'listening' event is queued in a nextTick()
to allow the script to run to completion. This allows the user to set
any event handlers they want.
process.nextTick() vs setImmediate()
We have two calls that are similar as far as users are concerned, but
their names are confusing.

process.nextTick() fires immediately on the same phase
setImmediate() fires on the following iteration or 'tick' of the
event loop

In essence, the names should be swapped. process.nextTick() fires more
immediately than setImmediate(), but this is an artifact of the past
which is unlikely to change. Making this switch would break a large
percentage of the packages on npm. Every day more new modules are being
added, which means every day we wait, more potential breakages occur.
While they are confusing, the names themselves won't change.

We recommend developers use setImmediate() in all cases because it's
easier to reason about.

Why use process.nextTick()?
There are two main reasons:


Allow users to handle errors, cleanup any then unneeded resources, or
perhaps try the request again before the event loop continues.


At times it's necessary to allow a callback to run after the call
stack has unwound but before the event loop continues.


One example is to match the user's expectations. Simple example:
const server = net.createServer();
server.on('connection', conn => {});

server.listen(8080);
server.on('listening', () => {});
JavaScriptCopy to clipboard
Say that listen() is run at the beginning of the event loop, but the
listening callback is placed in a setImmediate(). Unless a
hostname is passed, binding to the port will happen immediately. For
the event loop to proceed, it must hit the poll phase, which means
there is a non-zero chance that a connection could have been received
allowing the connection event to be fired before the listening event.
Another example is extending an EventEmitter and emitting an
event from within the constructor:
const EventEmitter = require('node:events');

class MyEmitter extends EventEmitter {
  constructor() {
    super();
    this.emit('event');
  }
}

const myEmitter = new MyEmitter();
myEmitter.on('event', () => {
  console.log('an event occurred!');
});
JavaScriptCopy to clipboard
You can't emit an event from the constructor immediately
because the script will not have processed to the point where the user
assigns a callback to that event. So, within the constructor itself,
you can use process.nextTick() to set a callback to emit the event
after the constructor has finished, which provides the expected results:
const EventEmitter = require('node:events');

class MyEmitter extends EventEmitter {
  constructor() {
    super();

    // use nextTick to emit the event once a handler is assigned
    process.nextTick(() => {
      this.emit('event');
    });
  }
}

const myEmitter = new MyEmitter();
myEmitter.on('event', () => {
  console.log('an event occurred!');
});
JavaScriptCopy to clipboardPrevDiscover JavaScript TimersNextThe Node.js Event Emitter\n\n\n\nThe Node.js Event emitter
If you worked with JavaScript in the browser, you know how much of the interaction of the user is handled through events: mouse clicks, keyboard button presses, reacting to mouse movements, and so on.
On the backend side, Node.js offers us the option to build a similar system using the events module.
This module, in particular, offers the EventEmitter class, which we'll use to handle our events.
You initialize that using
CJSMJSconst EventEmitter = require('node:events');

const eventEmitter = new EventEmitter();
JavaScriptCopy to clipboardThis object exposes, among many others, the on and emit methods.

emit is used to trigger an event
on is used to add a callback function that's going to be executed when the event is triggered

For example, let's create a start event, and as a matter of providing a sample, we react to that by just logging to the console:
eventEmitter.on('start', () => {
  console.log('started');
});
JavaScriptCopy to clipboard
When we run
eventEmitter.emit('start');
JavaScriptCopy to clipboard
the event handler function is triggered, and we get the console log.
You can pass arguments to the event handler by passing them as additional arguments to emit():
eventEmitter.on('start', number => {
  console.log(`started ${number}`);
});

eventEmitter.emit('start', 23);
JavaScriptCopy to clipboard
Multiple arguments:
eventEmitter.on('start', (start, end) => {
  console.log(`started from ${start} to ${end}`);
});

eventEmitter.emit('start', 1, 100);
JavaScriptCopy to clipboard
The EventEmitter object also exposes several other methods to interact with events, like

once(): add a one-time listener
removeListener() / off(): remove an event listener from an event
removeAllListeners(): remove all listeners for an event

You can read more about these methods in the official documentation.PrevThe Node.js Event LoopNextUnderstanding process.nextTick()\n\n\n\nUnderstanding process.nextTick()
As you try to understand the Node.js event loop, one important part of it is process.nextTick().
Every time the runtime calls back into JavaScript for an event, we call it a tick.
When we pass a function to process.nextTick(), we instruct the engine to invoke this function immediately after the current operation completes, before moving to the next phase in the event loop:
process.nextTick(() => {
  // do something
});
JavaScriptCopy to clipboard
The event loop is busy processing the current function code. When this operation ends, the JS engine runs all the functions passed to nextTick calls during that operation.
It's the way we can tell the JS engine to process a function asynchronously (after the current function), but as soon as possible, not queue it.
Calling setTimeout(() => {}, 0) will execute the function at the end of next tick, much later than when using nextTick() which prioritizes the call and executes it just before the beginning of the next tick.
Use nextTick() when you want to make sure that in the next event loop iteration that code is already executed.
An Example of the order of events:
console.log('Hello => number 1');

setImmediate(() => {
  console.log('Running before the timeout => number 3');
});

setTimeout(() => {
  console.log('The timeout running last => number 4');
}, 0);

process.nextTick(() => {
  console.log('Running at next tick => number 2');
});
JavaScriptCopy to clipboard
Example output:
Hello => number 1
Running at next tick => number 2
Running before the timeout => number 3
The timeout running last => number 4
ShellCopy to clipboard
The exact output may differ from run to run.PrevThe Node.js Event EmitterNextUnderstanding setImmediate()\n\n\n\nUnderstanding setImmediate()
When you want to execute some piece of code asynchronously, but as soon as possible, one option is to use the setImmediate() function provided by Node.js:
setImmediate(() => {
  // run something
});
JavaScriptCopy to clipboard
Any function passed as the setImmediate() argument is a callback that's executed in the next iteration of the event loop.
How is setImmediate() different from setTimeout(() => {}, 0) (passing a 0ms timeout), and from process.nextTick() and Promise.then()?
A function passed to process.nextTick() is going to be executed on the current iteration of the event loop, after the current operation ends. This means it will always execute before setTimeout and setImmediate.
A setTimeout() callback with a 0ms delay is very similar to setImmediate(). The execution order will depend on various factors, but they will be both run in the next iteration of the event loop.
A process.nextTick callback is added to process.nextTick queue. A Promise.then() callback is added to promises microtask queue. A setTimeout, setImmediate callback is added to macrotask queue.
Event loop executes tasks in process.nextTick queue first, and then executes promises microtask queue, and then executes macrotask queue.
Here is an example to show the order between setImmediate(), process.nextTick() and Promise.then():
const baz = () => console.log('baz');
const foo = () => console.log('foo');
const zoo = () => console.log('zoo');

const start = () => {
  console.log('start');
  setImmediate(baz);
  new Promise((resolve, reject) => {
    resolve('bar');
  }).then(resolve => {
    console.log(resolve);
    process.nextTick(zoo);
  });
  process.nextTick(foo);
};

start();

// start foo bar zoo baz
JavaScriptCopy to clipboard
This code will first call start(), then call foo() in process.nextTick queue. After that, it will handle promises microtask queue, which prints bar and adds zoo() in process.nextTick queue at the same time. Then it will call zoo() which has just been added. In the end, the baz() in macrotask queue is called.
The principle aforementioned holds true in CommonJS cases, but keep in mind in ES Modules, e.g. mjs files, the execution order will be different:
// start bar foo zoo baz
JavaScriptCopy to clipboard
This is because the ES Module being loaded is wrapped as an asynchronous operation, and thus the entire script is actually already in the promises microtask queue. So when the promise is immediately resolved, its callback is appended to the microtask queue. Node.js will attempt to clear the queue until moving to any other queue, and hence you will see it outputs bar first.PrevUnderstanding process.nextTick()NextDon't Block the Event Loop\n\n\n\nDon't Block the Event Loop (or the Worker Pool)
Should you read this guide?
If you're writing anything more complicated than a brief command-line script, reading this should help you write higher-performance, more-secure applications.
This document is written with Node.js servers in mind, but the concepts apply to complex Node.js applications as well.
Where OS-specific details vary, this document is Linux-centric.
Summary
Node.js runs JavaScript code in the Event Loop (initialization and callbacks), and offers a Worker Pool to handle expensive tasks like file I/O.
Node.js scales well, sometimes better than more heavyweight approaches like Apache.
The secret to the scalability of Node.js is that it uses a small number of threads to handle many clients.
If Node.js can make do with fewer threads, then it can spend more of your system's time and memory working on clients rather than on paying space and time overheads for threads (memory, context-switching).
But because Node.js has only a few threads, you must structure your application to use them wisely.
Here's a good rule of thumb for keeping your Node.js server speedy:
Node.js is fast when the work associated with each client at any given time is "small".
This applies to callbacks on the Event Loop and tasks on the Worker Pool.
Why should I avoid blocking the Event Loop and the Worker Pool?
Node.js uses a small number of threads to handle many clients.
In Node.js there are two types of threads: one Event Loop (aka the main loop, main thread, event thread, etc.), and a pool of k Workers in a Worker Pool (aka the threadpool).
If a thread is taking a long time to execute a callback (Event Loop) or a task (Worker), we call it "blocked".
While a thread is blocked working on behalf of one client, it cannot handle requests from any other clients.
This provides two motivations for blocking neither the Event Loop nor the Worker Pool:

Performance: If you regularly perform heavyweight activity on either type of thread, the throughput (requests/second) of your server will suffer.
Security: If it is possible that for certain input one of your threads might block, a malicious client could submit this "evil input", make your threads block, and keep them from working on other clients. This would be a Denial of Service attack.

A quick review of Node
Node.js uses the Event-Driven Architecture: it has an Event Loop for orchestration and a Worker Pool for expensive tasks.
What code runs on the Event Loop?
When they begin, Node.js applications first complete an initialization phase, require'ing modules and registering callbacks for events.
Node.js applications then enter the Event Loop, responding to incoming client requests by executing the appropriate callback.
This callback executes synchronously, and may register asynchronous requests to continue processing after it completes.
The callbacks for these asynchronous requests will also be executed on the Event Loop.
The Event Loop will also fulfill the non-blocking asynchronous requests made by its callbacks, e.g., network I/O.
In summary, the Event Loop executes the JavaScript callbacks registered for events, and is also responsible for fulfilling non-blocking asynchronous requests like network I/O.
What code runs on the Worker Pool?
The Worker Pool of Node.js is implemented in libuv (docs), which exposes a general task submission API.
Node.js uses the Worker Pool to handle "expensive" tasks.
This includes I/O for which an operating system does not provide a non-blocking version, as well as particularly CPU-intensive tasks.
These are the Node.js module APIs that make use of this Worker Pool:

I/O-intensive

DNS: dns.lookup(), dns.lookupService().
File System: All file system APIs except fs.FSWatcher() and those that are explicitly synchronous use libuv's threadpool.


CPU-intensive

Crypto: crypto.pbkdf2(), crypto.scrypt(), crypto.randomBytes(), crypto.randomFill(), crypto.generateKeyPair().
Zlib: All zlib APIs except those that are explicitly synchronous use libuv's threadpool.



In many Node.js applications, these APIs are the only sources of tasks for the Worker Pool. Applications and modules that use a C++ add-on can submit other tasks to the Worker Pool.
For the sake of completeness, we note that when you call one of these APIs from a callback on the Event Loop, the Event Loop pays some minor setup costs as it enters the Node.js C++ bindings for that API and submits a task to the Worker Pool.
These costs are negligible compared to the overall cost of the task, which is why the Event Loop is offloading it.
When submitting one of these tasks to the Worker Pool, Node.js provides a pointer to the corresponding C++ function in the Node.js C++ bindings.
How does Node.js decide what code to run next?
Abstractly, the Event Loop and the Worker Pool maintain queues for pending events and pending tasks, respectively.
In truth, the Event Loop does not actually maintain a queue.
Instead, it has a collection of file descriptors that it asks the operating system to monitor, using a mechanism like epoll (Linux), kqueue (OSX), event ports (Solaris), or IOCP (Windows).
These file descriptors correspond to network sockets, any files it is watching, and so on.
When the operating system says that one of these file descriptors is ready, the Event Loop translates it to the appropriate event and invokes the callback(s) associated with that event.
You can learn more about this process here.
In contrast, the Worker Pool uses a real queue whose entries are tasks to be processed.
A Worker pops a task from this queue and works on it, and when finished the Worker raises an "At least one task is finished" event for the Event Loop.
What does this mean for application design?
In a one-thread-per-client system like Apache, each pending client is assigned its own thread.
If a thread handling one client blocks, the operating system will interrupt it and give another client a turn.
The operating system thus ensures that clients that require a small amount of work are not penalized by clients that require more work.
Because Node.js handles many clients with few threads, if a thread blocks handling one client's request, then pending client requests may not get a turn until the thread finishes its callback or task.
The fair treatment of clients is thus the responsibility of your application.
This means that you shouldn't do too much work for any client in any single callback or task.
This is part of why Node.js can scale well, but it also means that you are responsible for ensuring fair scheduling.
The next sections talk about how to ensure fair scheduling for the Event Loop and for the Worker Pool.
Don't block the Event Loop
The Event Loop notices each new client connection and orchestrates the generation of a response.
All incoming requests and outgoing responses pass through the Event Loop.
This means that if the Event Loop spends too long at any point, all current and new clients will not get a turn.
You should make sure you never block the Event Loop.
In other words, each of your JavaScript callbacks should complete quickly.
This of course also applies to your await's, your Promise.then's, and so on.
A good way to ensure this is to reason about the "computational complexity" of your callbacks.
If your callback takes a constant number of steps no matter what its arguments are, then you'll always give every pending client a fair turn.
If your callback takes a different number of steps depending on its arguments, then you should think about how long the arguments might be.
Example 1: A constant-time callback.
app.get('/constant-time', (req, res) => {
  res.sendStatus(200);
});
JavaScriptCopy to clipboard
Example 2: An O(n) callback. This callback will run quickly for small n and more slowly for large n.
app.get('/countToN', (req, res) => {
  let n = req.query.n;

  // n iterations before giving someone else a turn
  for (let i = 0; i < n; i++) {
    console.log(`Iter ${i}`);
  }

  res.sendStatus(200);
});
JavaScriptCopy to clipboard
Example 3: An O(n^2) callback. This callback will still run quickly for small n, but for large n it will run much more slowly than the previous O(n) example.
app.get('/countToN2', (req, res) => {
  let n = req.query.n;

  // n^2 iterations before giving someone else a turn
  for (let i = 0; i < n; i++) {
    for (let j = 0; j < n; j++) {
      console.log(`Iter ${i}.${j}`);
    }
  }

  res.sendStatus(200);
});
JavaScriptCopy to clipboard
How careful should you be?
Node.js uses the Google V8 engine for JavaScript, which is quite fast for many common operations.
Exceptions to this rule are regexps and JSON operations, discussed below.
However, for complex tasks you should consider bounding the input and rejecting inputs that are too long.
That way, even if your callback has large complexity, by bounding the input you ensure the callback cannot take more than the worst-case time on the longest acceptable input.
You can then evaluate the worst-case cost of this callback and determine whether its running time is acceptable in your context.
Blocking the Event Loop: REDOS
One common way to block the Event Loop disastrously is by using a "vulnerable" regular expression.
Avoiding vulnerable regular expressions
A regular expression (regexp) matches an input string against a pattern.
We usually think of a regexp match as requiring a single pass through the input string --- O(n) time where n is the length of the input string.
In many cases, a single pass is indeed all it takes.
Unfortunately, in some cases the regexp match might require an exponential number of trips through the input string --- O(2^n) time.
An exponential number of trips means that if the engine requires x trips to determine a match, it will need 2*x trips if we add only one more character to the input string.
Since the number of trips is linearly related to the time required, the effect of this evaluation will be to block the Event Loop.
A vulnerable regular expression is one on which your regular expression engine might take exponential time, exposing you to REDOS on "evil input".
Whether or not your regular expression pattern is vulnerable (i.e. the regexp engine might take exponential time on it) is actually a difficult question to answer, and varies depending on whether you're using Perl, Python, Ruby, Java, JavaScript, etc., but here are some rules of thumb that apply across all of these languages:

Avoid nested quantifiers like (a+)*. V8's regexp engine can handle some of these quickly, but others are vulnerable.
Avoid OR's with overlapping clauses, like (a|a)*. Again, these are sometimes-fast.
Avoid using backreferences, like (a.*) \1. No regexp engine can guarantee evaluating these in linear time.
If you're doing a simple string match, use indexOf or the local equivalent. It will be cheaper and will never take more than O(n).

If you aren't sure whether your regular expression is vulnerable, remember that Node.js generally doesn't have trouble reporting a match even for a vulnerable regexp and a long input string.
The exponential behavior is triggered when there is a mismatch but Node.js can't be certain until it tries many paths through the input string.
A REDOS example
Here is an example vulnerable regexp exposing its server to REDOS:
app.get('/redos-me', (req, res) => {
  let filePath = req.query.filePath;

  // REDOS
  if (filePath.match(/(\/.+)+$/)) {
    console.log('valid path');
  } else {
    console.log('invalid path');
  }

  res.sendStatus(200);
});
JavaScriptCopy to clipboard
The vulnerable regexp in this example is a (bad!) way to check for a valid path on Linux.
It matches strings that are a sequence of "/"-delimited names, like "/a/b/c".
It is dangerous because it violates rule 1: it has a doubly-nested quantifier.
If a client queries with filePath ///.../\n (100 /'s followed by a newline character that the regexp's "." won't match), then the Event Loop will take effectively forever, blocking the Event Loop.
This client's REDOS attack causes all other clients not to get a turn until the regexp match finishes.
For this reason, you should be leery of using complex regular expressions to validate user input.
Anti-REDOS Resources
There are some tools to check your regexps for safety, like

safe-regex
rxxr2.

However, neither of these will catch all vulnerable regexps.
Another approach is to use a different regexp engine.
You could use the node-re2 module, which uses Google's blazing-fast RE2 regexp engine.
But be warned, RE2 is not 100% compatible with V8's regexps, so check for regressions if you swap in the node-re2 module to handle your regexps.
And particularly complicated regexps are not supported by node-re2.
If you're trying to match something "obvious", like a URL or a file path, find an example in a regexp library or use an npm module, e.g. ip-regex.
Blocking the Event Loop: Node.js core modules
Several Node.js core modules have synchronous expensive APIs, including:

Encryption
Compression
File system
Child process

These APIs are expensive, because they involve significant computation (encryption, compression), require I/O (file I/O), or potentially both (child process). These APIs are intended for scripting convenience, but are not intended for use in the server context. If you execute them on the Event Loop, they will take far longer to complete than a typical JavaScript instruction, blocking the Event Loop.
In a server, you should not use the following synchronous APIs from these modules:

Encryption:

crypto.randomBytes (synchronous version)
crypto.randomFillSync
crypto.pbkdf2Sync
You should also be careful about providing large input to the encryption and decryption routines.


Compression:

zlib.inflateSync
zlib.deflateSync


File system:

Do not use the synchronous file system APIs. For example, if the file you access is in a distributed file system like NFS, access times can vary widely.


Child process:

child_process.spawnSync
child_process.execSync
child_process.execFileSync



This list is reasonably complete as of Node.js v9.
Blocking the Event Loop: JSON DOS
JSON.parse and JSON.stringify are other potentially expensive operations.
While these are O(n) in the length of the input, for large n they can take surprisingly long.
If your server manipulates JSON objects, particularly those from a client, you should be cautious about the size of the objects or strings you work with on the Event Loop.
Example: JSON blocking. We create an object obj of size 2^21 and JSON.stringify it, run indexOf on the string, and then JSON.parse it. The JSON.stringify'd string is 50MB. It takes 0.7 seconds to stringify the object, 0.03 seconds to indexOf on the 50MB string, and 1.3 seconds to parse the string.
let obj = { a: 1 };
let niter = 20;

let before, str, pos, res, took;

for (let i = 0; i < niter; i++) {
  obj = { obj1: obj, obj2: obj }; // Doubles in size each iter
}

before = process.hrtime();
str = JSON.stringify(obj);
took = process.hrtime(before);
console.log('JSON.stringify took ' + took);

before = process.hrtime();
pos = str.indexOf('nomatch');
took = process.hrtime(before);
console.log('Pure indexof took ' + took);

before = process.hrtime();
res = JSON.parse(str);
took = process.hrtime(before);
console.log('JSON.parse took ' + took);
JavaScriptCopy to clipboard
There are npm modules that offer asynchronous JSON APIs. See for example:

JSONStream, which has stream APIs.
Big-Friendly JSON, which has stream APIs as well as asynchronous versions of the standard JSON APIs using the partitioning-on-the-Event-Loop paradigm outlined below.

Complex calculations without blocking the Event Loop
Suppose you want to do complex calculations in JavaScript without blocking the Event Loop.
You have two options: partitioning or offloading.
Partitioning
You could partition your calculations so that each runs on the Event Loop but regularly yields (gives turns to) other pending events.
In JavaScript it's easy to save the state of an ongoing task in a closure, as shown in example 2 below.
For a simple example, suppose you want to compute the average of the numbers 1 to n.
Example 1: Un-partitioned average, costs O(n)
for (let i = 0; i < n; i++) sum += i;
let avg = sum / n;
console.log('avg: ' + avg);
JavaScriptCopy to clipboard
Example 2: Partitioned average, each of the n asynchronous steps costs O(1).
function asyncAvg(n, avgCB) {
  // Save ongoing sum in JS closure.
  let sum = 0;
  function help(i, cb) {
    sum += i;
    if (i == n) {
      cb(sum);
      return;
    }

    // "Asynchronous recursion".
    // Schedule next operation asynchronously.
    setImmediate(help.bind(null, i + 1, cb));
  }

  // Start the helper, with CB to call avgCB.
  help(1, function (sum) {
    let avg = sum / n;
    avgCB(avg);
  });
}

asyncAvg(n, function (avg) {
  console.log('avg of 1-n: ' + avg);
});
JavaScriptCopy to clipboard
You can apply this principle to array iterations and so forth.
Offloading
If you need to do something more complex, partitioning is not a good option.
This is because partitioning uses only the Event Loop, and you won't benefit from multiple cores almost certainly available on your machine.
Remember, the Event Loop should orchestrate client requests, not fulfill them itself.
For a complicated task, move the work off of the Event Loop onto a Worker Pool.
How to offload
You have two options for a destination Worker Pool to which to offload work.

You can use the built-in Node.js Worker Pool by developing a C++ addon. On older versions of Node, build your C++ addon using NAN, and on newer versions use N-API. node-webworker-threads offers a JavaScript-only way to access the Node.js Worker Pool.
You can create and manage your own Worker Pool dedicated to computation rather than the Node.js I/O-themed Worker Pool. The most straightforward ways to do this is using Child Process or Cluster.

You should not simply create a Child Process for every client.
You can receive client requests more quickly than you can create and manage children, and your server might become a fork bomb.
Downside of offloading
The downside of the offloading approach is that it incurs overhead in the form of communication costs.
Only the Event Loop is allowed to see the "namespace" (JavaScript state) of your application.
From a Worker, you cannot manipulate a JavaScript object in the Event Loop's namespace.
Instead, you have to serialize and deserialize any objects you wish to share.
Then the Worker can operate on its own copy of these object(s) and return the modified object (or a "patch") to the Event Loop.
For serialization concerns, see the section on JSON DOS.
Some suggestions for offloading
You may wish to distinguish between CPU-intensive and I/O-intensive tasks because they have markedly different characteristics.
A CPU-intensive task only makes progress when its Worker is scheduled, and the Worker must be scheduled onto one of your machine's logical cores.
If you have 4 logical cores and 5 Workers, one of these Workers cannot make progress.
As a result, you are paying overhead (memory and scheduling costs) for this Worker and getting no return for it.
I/O-intensive tasks involve querying an external service provider (DNS, file system, etc.) and waiting for its response.
While a Worker with an I/O-intensive task is waiting for its response, it has nothing else to do and can be de-scheduled by the operating system, giving another Worker a chance to submit their request.
Thus, I/O-intensive tasks will be making progress even while the associated thread is not running.
External service providers like databases and file systems have been highly optimized to handle many pending requests concurrently.
For example, a file system will examine a large set of pending write and read requests to merge conflicting updates and to retrieve files in an optimal order.
If you rely on only one Worker Pool, e.g. the Node.js Worker Pool, then the differing characteristics of CPU-bound and I/O-bound work may harm your application's performance.
For this reason, you might wish to maintain a separate Computation Worker Pool.
Offloading: conclusions
For simple tasks, like iterating over the elements of an arbitrarily long array, partitioning might be a good option.
If your computation is more complex, offloading is a better approach: the communication costs, i.e. the overhead of passing serialized objects between the Event Loop and the Worker Pool, are offset by the benefit of using multiple cores.
However, if your server relies heavily on complex calculations, you should think about whether Node.js is really a good fit. Node.js excels for I/O-bound work, but for expensive computation it might not be the best option.
If you take the offloading approach, see the section on not blocking the Worker Pool.
Don't block the Worker Pool
Node.js has a Worker Pool composed of k Workers.
If you are using the Offloading paradigm discussed above, you might have a separate Computational Worker Pool, to which the same principles apply.
In either case, let us assume that k is much smaller than the number of clients you might be handling concurrently.
This is in keeping with the "one thread for many clients" philosophy of Node.js, the secret to its scalability.
As discussed above, each Worker completes its current Task before proceeding to the next one on the Worker Pool queue.
Now, there will be variation in the cost of the Tasks required to handle your clients' requests.
Some Tasks can be completed quickly (e.g. reading short or cached files, or producing a small number of random bytes), and others will take longer (e.g reading larger or uncached files, or generating more random bytes).
Your goal should be to minimize the variation in Task times, and you should use Task partitioning to accomplish this.
Minimizing the variation in Task times
If a Worker's current Task is much more expensive than other Tasks, then it will be unavailable to work on other pending Tasks.
In other words, each relatively long Task effectively decreases the size of the Worker Pool by one until it is completed.
This is undesirable because, up to a point, the more Workers in the Worker Pool, the greater the Worker Pool throughput (tasks/second) and thus the greater the server throughput (client requests/second).
One client with a relatively expensive Task will decrease the throughput of the Worker Pool, in turn decreasing the throughput of the server.
To avoid this, you should try to minimize variation in the length of Tasks you submit to the Worker Pool.
While it is appropriate to treat the external systems accessed by your I/O requests (DB, FS, etc.) as black boxes, you should be aware of the relative cost of these I/O requests, and should avoid submitting requests you can expect to be particularly long.
Two examples should illustrate the possible variation in task times.
Variation example: Long-running file system reads
Suppose your server must read files in order to handle some client requests.
After consulting the Node.js File system APIs, you opted to use fs.readFile() for simplicity.
However, fs.readFile() before v10 was not partitioned: it submitted a single fs.read() Task spanning the entire file.
If you read shorter files for some users and longer files for others, fs.readFile() may introduce significant variation in Task lengths, to the detriment of Worker Pool throughput.
For a worst-case scenario, suppose an attacker can convince your server to read an arbitrary file (this is a directory traversal vulnerability).
If your server is running Linux, the attacker can name an extremely slow file: /dev/random.
For all practical purposes, /dev/random is infinitely slow, and every Worker asked to read from /dev/random will never finish that Task.
An attacker then submits k requests, one for each Worker, and no other client requests that use the Worker Pool will make progress.
Variation example: Long-running crypto operations
Suppose your server generates cryptographically secure random bytes using crypto.randomBytes().
crypto.randomBytes() is not partitioned: it creates a single randomBytes() Task to generate as many bytes as you requested.
If you create fewer bytes for some users and more bytes for others, crypto.randomBytes() is another source of variation in Task lengths.
Task partitioning
Tasks with variable time costs can harm the throughput of the Worker Pool.
To minimize variation in Task times, as far as possible you should partition each Task into comparable-cost sub-Tasks.
When each sub-Task completes it should submit the next sub-Task, and when the final sub-Task completes it should notify the submitter.
To continue the fs.readFile() example, you should instead use fs.read() (manual partitioning) or ReadStream (automatically partitioned).
The same principle applies to CPU-bound tasks; the asyncAvg example might be inappropriate for the Event Loop, but it is well suited to the Worker Pool.
When you partition a Task into sub-Tasks, shorter Tasks expand into a small number of sub-Tasks, and longer Tasks expand into a larger number of sub-Tasks.
Between each sub-Task of a longer Task, the Worker to which it was assigned can work on a sub-Task from another, shorter, Task, thus improving the overall Task throughput of the Worker Pool.
Note that the number of sub-Tasks completed is not a useful metric for the throughput of the Worker Pool.
Instead, concern yourself with the number of Tasks completed.
Avoiding Task partitioning
Recall that the purpose of Task partitioning is to minimize the variation in Task times.
If you can distinguish between shorter Tasks and longer Tasks (e.g. summing an array vs. sorting an array), you could create one Worker Pool for each class of Task.
Routing shorter Tasks and longer Tasks to separate Worker Pools is another way to minimize Task time variation.
In favor of this approach, partitioning Tasks incurs overhead (the costs of creating a Worker Pool Task representation and of manipulating the Worker Pool queue), and avoiding partitioning saves you the costs of additional trips to the Worker Pool.
It also keeps you from making mistakes in partitioning your Tasks.
The downside of this approach is that Workers in all of these Worker Pools will incur space and time overheads and will compete with each other for CPU time.
Remember that each CPU-bound Task makes progress only while it is scheduled.
As a result, you should only consider this approach after careful analysis.
Worker Pool: conclusions
Whether you use only the Node.js Worker Pool or maintain separate Worker Pool(s), you should optimize the Task throughput of your Pool(s).
To do this, minimize the variation in Task times by using Task partitioning.
The risks of npm modules
While the Node.js core modules offer building blocks for a wide variety of applications, sometimes something more is needed. Node.js developers benefit tremendously from the npm ecosystem, with hundreds of thousands of modules offering functionality to accelerate your development process.
Remember, however, that the majority of these modules are written by third-party developers and are generally released with only best-effort guarantees. A developer using an npm module should be concerned about two things, though the latter is frequently forgotten.

Does it honor its APIs?
Might its APIs block the Event Loop or a Worker?
Many modules make no effort to indicate the cost of their APIs, to the detriment of the community.

For simple APIs you can estimate the cost of the APIs; the cost of string manipulation isn't hard to fathom.
But in many cases it's unclear how much an API might cost.
If you are calling an API that might do something expensive, double-check the cost. Ask the developers to document it, or examine the source code yourself (and submit a PR documenting the cost).
Remember, even if the API is asynchronous, you don't know how much time it might spend on a Worker or on the Event Loop in each of its partitions.
For example, suppose in the asyncAvg example given above, each call to the helper function summed half of the numbers rather than one of them.
Then this function would still be asynchronous, but the cost of each partition would be O(n), not O(1), making it much less safe to use for arbitrary values of n.
Conclusion
Node.js has two types of threads: one Event Loop and k Workers.
The Event Loop is responsible for JavaScript callbacks and non-blocking I/O, and a Worker executes tasks corresponding to C++ code that completes an asynchronous request, including blocking I/O and CPU-intensive work.
Both types of threads work on no more than one activity at a time.
If any callback or task takes a long time, the thread running it becomes blocked.
If your application makes blocking callbacks or tasks, this can lead to degraded throughput (clients/second) at best, and complete denial of service at worst.
To write a high-throughput, more DoS-proof web server, you must ensure that on benign and on malicious input, neither your Event Loop nor your Workers will block.PrevUnderstanding setImmediate()NextNode.js file stats\n\n\n\nNode.js file stats
Every file comes with a set of details that we can inspect using Node.js. In particular, using the stat() method provided by the fs module.
You call it passing a file path, and once Node.js gets the file details it will call the callback function you pass, with 2 parameters: an error message, and the file stats:
CJSMJSconst fs = require('node:fs');

fs.stat('/Users/joe/test.txt', (err, stats) => {
  if (err) {
    console.error(err);
  }
  // we have access to the file stats in `stats`
});
JavaScriptCopy to clipboardNode.js also provides a sync method, which blocks the thread until the file stats are ready:
CJSMJSconst fs = require('node:fs');

try {
  const stats = fs.statSync('/Users/joe/test.txt');
} catch (err) {
  console.error(err);
}
JavaScriptCopy to clipboardThe file information is included in the stats variable. What kind of information can we extract using the stats?
A lot, including:

if the file is a directory or a file, using stats.isFile() and stats.isDirectory()
if the file is a symbolic link using stats.isSymbolicLink()
the file size in bytes using stats.size.

There are other advanced methods, but the bulk of what you'll use in your day-to-day programming is this.
CJSMJSconst fs = require('node:fs');

fs.stat('/Users/joe/test.txt', (err, stats) => {
  if (err) {
    console.error(err);
    return;
  }

  stats.isFile(); // true
  stats.isDirectory(); // false
  stats.isSymbolicLink(); // false
  stats.size; // 1024000 //= 1MB
});
JavaScriptCopy to clipboardYou can also use promise-based fsPromises.stat() method offered by the fs/promises module if you like:
CJSMJSconst fs = require('node:fs/promises');

async function example() {
  try {
    const stats = await fs.stat('/Users/joe/test.txt');
    stats.isFile(); // true
    stats.isDirectory(); // false
    stats.isSymbolicLink(); // false
    stats.size; // 1024000 //= 1MB
  } catch (err) {
    console.log(err);
  }
}
example();
JavaScriptCopy to clipboardYou can read more about the fs module in the official documentation.PrevDon't Block the Event LoopNextNode.js File Paths\n\n\n\nNode.js File Paths
Every file in the system has a path. On Linux and macOS, a path might look like: /users/joe/file.txt while Windows computers are different, and have a structure such as: C:\users\joe\file.txt
You need to pay attention when using paths in your applications, as this difference must be taken into account.
You include this module in your files using const path = require('node:path'); and you can start using its methods.
Getting information out of a path
Given a path, you can extract information out of it using those methods:

dirname: gets the parent folder of a file
basename: gets the filename part
extname: gets the file extension

Example
CJSMJSconst path = require('node:path');

const notes = '/users/joe/notes.txt';

path.dirname(notes); // /users/joe
path.basename(notes); // notes.txt
path.extname(notes); // .txt
JavaScriptCopy to clipboardYou can get the file name without the extension by specifying a second argument to basename:
path.basename(notes, path.extname(notes)); // notes
JavaScriptCopy to clipboard
Working with paths
You can join two or more parts of a path by using path.join():
const name = 'joe';
path.join('/', 'users', name, 'notes.txt'); // '/users/joe/notes.txt'
JavaScriptCopy to clipboard
You can get the absolute path calculation of a relative path using path.resolve():
path.resolve('joe.txt'); // '/Users/joe/joe.txt' if run from my home folder
JavaScriptCopy to clipboard
In this case Node.js will simply append /joe.txt to the current working directory. If you specify a second parameter folder, resolve will use the first as a base for the second:
path.resolve('tmp', 'joe.txt'); // '/Users/joe/tmp/joe.txt' if run from my home folder
JavaScriptCopy to clipboard
If the first parameter starts with a slash, that means it's an absolute path:
path.resolve('/etc', 'joe.txt'); // '/etc/joe.txt'
JavaScriptCopy to clipboard
path.normalize() is another useful function, that will try and calculate the actual path, when it contains relative specifiers like . or .., or double slashes:
path.normalize('/users/joe/..//test.txt'); // '/users/test.txt'
JavaScriptCopy to clipboard
Neither resolve nor normalize will check if the path exists. They just calculate a path based on the information they got.PrevNode.js file statsNextWorking with file descriptors in Node.js\n\n\n\nWorking with file descriptors in Node.js
Before you're able to interact with a file that sits in your filesystem, you must get a file descriptor.
A file descriptor is a reference to an open file, a number (fd) returned by opening the file using the open() method offered by the fs module. This number (fd) uniquely identifies an open file in operating system:
CJSMJSconst fs = require('node:fs');

fs.open('/Users/joe/test.txt', 'r', (err, fd) => {
  // fd is our file descriptor
});
JavaScriptCopy to clipboardNotice the r we used as the second parameter to the fs.open() call.
That flag means we open the file for reading.
Other flags you'll commonly use are:
FlagDescriptionFile gets created if it doesn't existr+This flag opens the file for reading and writing❌w+This flag opens the file for reading and writing and it also positions the stream at the beginning of the file✅aThis flag opens the file for writing and it also positions the stream at the end of the file✅a+This flag opens the file for reading and writing and it also positions the stream at the end of the file✅
You can also open the file by using the fs.openSync method, which returns the file descriptor, instead of providing it in a callback:
CJSMJSconst fs = require('node:fs');

try {
  const fd = fs.openSync('/Users/joe/test.txt', 'r');
} catch (err) {
  console.error(err);
}
JavaScriptCopy to clipboardOnce you get the file descriptor, in whatever way you choose, you can perform all the operations that require it, like calling fs.close() and many other operations that interact with the filesystem.
You can also open the file by using the promise-based fsPromises.open method offered by the fs/promises module.
The fs/promises module is available starting only from Node.js v14. Before v14, after v10, you can use require('fs').promises instead. Before v10, after v8, you can use util.promisify to convert fs methods into promise-based methods.
CJSMJSconst fs = require('node:fs/promises');
// Or const fs = require('fs').promises before v14.
async function example() {
  let filehandle;
  try {
    filehandle = await fs.open('/Users/joe/test.txt', 'r');
    console.log(filehandle.fd);
    console.log(await filehandle.readFile({ encoding: 'utf8' }));
  } finally {
    if (filehandle) await filehandle.close();
  }
}
example();
JavaScriptCopy to clipboardHere is an example of util.promisify:
CJSMJSconst fs = require('node:fs');
const util = require('node:util');

async function example() {
  const open = util.promisify(fs.open);
  const fd = await open('/Users/joe/test.txt', 'r');
}
example();
JavaScriptCopy to clipboardTo see more details about the fs/promises module, please check fs/promises API.PrevNode.js File PathsNextReading files with Node.js\n\n\n\nReading files with Node.js
The simplest way to read a file in Node.js is to use the fs.readFile() method, passing it the file path, encoding and a callback function that will be called with the file data (and the error):
CJSMJSconst fs = require('node:fs');

fs.readFile('/Users/joe/test.txt', 'utf8', (err, data) => {
  if (err) {
    console.error(err);
    return;
  }
  console.log(data);
});
JavaScriptCopy to clipboardAlternatively, you can use the synchronous version fs.readFileSync():
CJSMJSconst fs = require('node:fs');

try {
  const data = fs.readFileSync('/Users/joe/test.txt', 'utf8');
  console.log(data);
} catch (err) {
  console.error(err);
}
JavaScriptCopy to clipboardYou can also use the promise-based fsPromises.readFile() method offered by the fs/promises module:
CJSMJSconst fs = require('node:fs/promises');

async function example() {
  try {
    const data = await fs.readFile('/Users/joe/test.txt', { encoding: 'utf8' });
    console.log(data);
  } catch (err) {
    console.error(err);
  }
}
example();
JavaScriptCopy to clipboardAll three of fs.readFile(), fs.readFileSync() and fsPromises.readFile() read the full content of the file in memory before returning the data.
This means that big files are going to have a major impact on your memory consumption and speed of execution of the program.
In this case, a better option is to read the file content using streams.
import fs from 'fs';
import path from 'path';
import { pipeline } from 'node:stream/promises';

const fileUrl = 'https://www.gutenberg.org/files/2701/2701-0.txt';
const outputFilePath = path.join(process.cwd(), 'moby.md');

async function downloadFile(url, outputPath) {
  const response = await fetch(url);

  if (!response.ok || !response.body) {
    throw new Error(`Failed to fetch ${url}. Status: ${response.status}`);
  }

  const fileStream = fs.createWriteStream(outputPath);
  console.log(`Downloading file from ${url} to ${outputPath}`);

  await pipeline(response.body, fileStream);
  console.log('File downloaded successfully');
}

async function readFile(filePath) {
  const readStream = fs.createReadStream(filePath, { encoding: 'utf8' });

  try {
    for await (const chunk of readStream) {
      console.log('--- File chunk start ---');
      console.log(chunk);
      console.log('--- File chunk end ---');
    }
    console.log('Finished reading the file.');
  } catch (error) {
    console.error(`Error reading file: ${error.message}`);
  }
}

try {
  await downloadFile(fileUrl, outputFilePath);
  await readFile(outputFilePath);
} catch (error) {
  console.error(`Error: ${error.message}`);
}
JavaScriptCopy to clipboardPrevWorking with file descriptors in Node.jsNextWriting files with Node.js\n\n\n\nWriting files with Node.js
Writing a file
The easiest way to write to files in Node.js is to use the fs.writeFile() API.
const fs = require('node:fs');

const content = 'Some content!';

fs.writeFile('/Users/joe/test.txt', content, err => {
  if (err) {
    console.error(err);
  } else {
    // file written successfully
  }
});
JavaScriptCopy to clipboard
Writing a file synchronously
Alternatively, you can use the synchronous version fs.writeFileSync():
const fs = require('node:fs');

const content = 'Some content!';

try {
  fs.writeFileSync('/Users/joe/test.txt', content);
  // file written successfully
} catch (err) {
  console.error(err);
}
JavaScriptCopy to clipboard
You can also use the promise-based fsPromises.writeFile() method offered by the fs/promises module:
const fs = require('node:fs/promises');

async function example() {
  try {
    const content = 'Some content!';
    await fs.writeFile('/Users/joe/test.txt', content);
  } catch (err) {
    console.log(err);
  }
}

example();
JavaScriptCopy to clipboard
By default, this API will replace the contents of the file if it does already exist.
You can modify the default by specifying a flag:
fs.writeFile('/Users/joe/test.txt', content, { flag: 'a+' }, err => {});
JavaScriptCopy to clipboard
The flags you'll likely use are
FlagDescriptionFile gets created if it doesn't existr+This flag opens the file for reading and writing❌w+This flag opens the file for reading and writing and it also positions the stream at the beginning of the file✅aThis flag opens the file for writing and it also positions the stream at the end of the file✅a+This flag opens the file for reading and writing and it also positions the stream at the end of the file✅

You can find more information about the flags in the fs documentation.

Appending content to a file
Appending to files is handy when you don't want to overwrite a file with new content, but rather add to it.
Examples
A handy method to append content to the end of a file is fs.appendFile() (and its fs.appendFileSync() counterpart):
const fs = require('node:fs');

const content = 'Some content!';

fs.appendFile('file.log', content, err => {
  if (err) {
    console.error(err);
  } else {
    // done!
  }
});
JavaScriptCopy to clipboard
Example with Promises
Here is a fsPromises.appendFile() example:
const fs = require('node:fs/promises');

async function example() {
  try {
    const content = 'Some content!';
    await fs.appendFile('/Users/joe/test.txt', content);
  } catch (err) {
    console.log(err);
  }
}

example();
JavaScriptCopy to clipboardPrevReading files with Node.jsNextWorking with folders in Node.js\n\n\n\nWorking with folders in Node.js
The Node.js fs core module provides many handy methods you can use to work with folders.
Check if a folder exists
Use fs.access() (and its promise-based fsPromises.access() counterpart) to check if the folder exists and Node.js can access it with its permissions.
Create a new folder
Use fs.mkdir() or fs.mkdirSync() or fsPromises.mkdir() to create a new folder.
CJSMJSconst fs = require('node:fs');

const folderName = '/Users/joe/test';

try {
  if (!fs.existsSync(folderName)) {
    fs.mkdirSync(folderName);
  }
} catch (err) {
  console.error(err);
}
JavaScriptCopy to clipboardRead the content of a directory
Use fs.readdir() or fs.readdirSync() or fsPromises.readdir() to read the contents of a directory.
This piece of code reads the content of a folder, both files and subfolders, and returns their relative path:
CJSMJSconst fs = require('node:fs');

const folderPath = '/Users/joe';

fs.readdirSync(folderPath);
JavaScriptCopy to clipboardYou can get the full path:
fs.readdirSync(folderPath).map(fileName => {
  return path.join(folderPath, fileName);
});
JavaScriptCopy to clipboard
You can also filter the results to only return the files, and exclude the folders:
CJSMJSconst fs = require('node:fs');

const isFile = fileName => {
  return fs.lstatSync(fileName).isFile();
};

fs.readdirSync(folderPath)
  .map(fileName => {
    return path.join(folderPath, fileName);
  })
  .filter(isFile);
JavaScriptCopy to clipboardRename a folder
Use fs.rename() or fs.renameSync() or fsPromises.rename() to rename folder. The first parameter is the current path, the second the new path:
CJSMJSconst fs = require('node:fs');

fs.rename('/Users/joe', '/Users/roger', err => {
  if (err) {
    console.error(err);
  }
  // done
});
JavaScriptCopy to clipboardfs.renameSync() is the synchronous version:
CJSMJSconst fs = require('node:fs');

try {
  fs.renameSync('/Users/joe', '/Users/roger');
} catch (err) {
  console.error(err);
}
JavaScriptCopy to clipboardfsPromises.rename() is the promise-based version:
CJSMJSconst fs = require('node:fs/promises');

async function example() {
  try {
    await fs.rename('/Users/joe', '/Users/roger');
  } catch (err) {
    console.log(err);
  }
}
example();
JavaScriptCopy to clipboardRemove a folder
Use fs.rmdir() or fs.rmdirSync() or fsPromises.rmdir() to remove a folder.
CJSMJSconst fs = require('node:fs');

fs.rmdir(dir, err => {
  if (err) {
    throw err;
  }

  console.log(`${dir} is deleted!`);
});
JavaScriptCopy to clipboardTo remove a folder that has contents use fs.rm() with the option { recursive: true } to recursively remove the contents.
{ recursive: true, force: true } makes it so that exceptions will be ignored if the folder does not exist.
CJSMJSconst fs = require('node:fs');

fs.rm(dir, { recursive: true, force: true }, err => {
  if (err) {
    throw err;
  }

  console.log(`${dir} is deleted!`);
});
JavaScriptCopy to clipboardPrevWriting files with Node.jsNextHow to work with Different Filesystems\n\n\n\nHow to Work with Different Filesystems
Node.js exposes many features of the filesystem. But not all filesystems are alike.
The following are suggested best practices to keep your code simple and safe
when working with different filesystems.
Filesystem Behavior
Before you can work with a filesystem, you need to know how it behaves.
Different filesystems behave differently and have more or less features than
others: case sensitivity, case insensitivity, case preservation, Unicode form
preservation, timestamp resolution, extended attributes, inodes, Unix
permissions, alternate data streams etc.
Be wary of inferring filesystem behavior from process.platform. For example,
do not assume that because your program is running on Darwin that you are
therefore working on a case-insensitive filesystem (HFS+), as the user may be
using a case-sensitive filesystem (HFSX). Similarly, do not assume that because
your program is running on Linux that you are therefore working on a filesystem
which supports Unix permissions and inodes, as you may be on a particular
external drive, USB or network drive which does not.
The operating system may not make it easy to infer filesystem behavior, but all
is not lost. Instead of keeping a list of every known filesystem and behavior
(which is always going to be incomplete), you can probe the filesystem to see
how it actually behaves. The presence or absence of certain features which are
easy to probe, are often enough to infer the behavior of other features which
are more difficult to probe.
Remember that some users may have different filesystems mounted at various paths
in the working tree.
Avoid a Lowest Common Denominator Approach
You might be tempted to make your program act like a lowest common denominator
filesystem, by normalizing all filenames to uppercase, normalizing all filenames
to NFC Unicode form, and normalizing all file timestamps to say 1-second
resolution. This would be the lowest common denominator approach.
Do not do this. You would only be able to interact safely with a filesystem
which has the exact same lowest common denominator characteristics in every
respect. You would be unable to work with more advanced filesystems in the way
that users expect, and you would run into filename or timestamp collisions. You
would most certainly lose and corrupt user data through a series of complicated
dependent events, and you would create bugs that would be difficult if not
impossible to solve.
What happens when you later need to support a filesystem that only has 2-second
or 24-hour timestamp resolution? What happens when the Unicode standard advances
to include a slightly different normalization algorithm (as has happened in the
past)?
A lowest common denominator approach would tend to try to create a portable
program by using only "portable" system calls. This leads to programs that are
leaky and not in fact portable.
Adopt a Superset Approach
Make the best use of each platform you support by adopting a superset approach.
For example, a portable backup program should sync btimes (the created time of a
file or folder) correctly between Windows systems, and should not destroy or
alter btimes, even though btimes are not supported on Linux systems. The same
portable backup program should sync Unix permissions correctly between Linux
systems, and should not destroy or alter Unix permissions, even though Unix
permissions are not supported on Windows systems.
Handle different filesystems by making your program act like a more advanced
filesystem. Support a superset of all possible features: case-sensitivity,
case-preservation, Unicode form sensitivity, Unicode form preservation, Unix
permissions, high-resolution nanosecond timestamps, extended attributes etc.
Once you have case-preservation in your program, you can always implement
case-insensitivity if you need to interact with a case-insensitive filesystem.
But if you forego case-preservation in your program, you cannot interact safely
with a case-preserving filesystem. The same is true for Unicode form
preservation and timestamp resolution preservation.
If a filesystem provides you with a filename in a mix of lowercase and
uppercase, then keep the filename in the exact case given. If a filesystem
provides you with a filename in mixed Unicode form or NFC or NFD (or NFKC or
NFKD), then keep the filename in the exact byte sequence given. If a filesystem
provides you with a millisecond timestamp, then keep the timestamp in
millisecond resolution.
When you work with a lesser filesystem, you can always downsample appropriately,
with comparison functions as required by the behavior of the filesystem on which
your program is running. If you know that the filesystem does not support Unix
permissions, then you should not expect to read the same Unix permissions you
write. If you know that the filesystem does not preserve case, then you should
be prepared to see ABC in a directory listing when your program creates abc.
But if you know that the filesystem does preserve case, then you should consider
ABC to be a different filename to abc, when detecting file renames or if the
filesystem is case-sensitive.
Case Preservation
You may create a directory called test/abc and be surprised to see sometimes
that fs.readdir('test') returns ['ABC']. This is not a bug in Node. Node
returns the filename as the filesystem stores it, and not all filesystems
support case-preservation. Some filesystems convert all filenames to uppercase
(or lowercase).
Unicode Form Preservation
Case preservation and Unicode form preservation are similar concepts. To
understand why Unicode form should be preserved , make sure that you first
understand why case should be preserved. Unicode form preservation is just as
simple when understood correctly.
Unicode can encode the same characters using several different byte sequences.
Several strings may look the same, but have different byte sequences. When
working with UTF-8 strings, be careful that your expectations are in line with
how Unicode works. Just as you would not expect all UTF-8 characters to encode
to a single byte, you should not expect several UTF-8 strings that look the same
to the human eye to have the same byte representation. This may be an
expectation that you can have of ASCII, but not of UTF-8.
You may create a directory called test/café (NFC Unicode form with byte
sequence <63 61 66 c3 a9> and string.length === 5) and be surprised to see
sometimes that fs.readdir('test') returns ['café'] (NFD Unicode form with
byte sequence <63 61 66 65 cc 81> and string.length === 6). This is not a
bug in Node. Node.js returns the filename as the filesystem stores it, and not
all filesystems support Unicode form preservation.
HFS+, for example, will normalize all filenames to a form almost always the same
as NFD form. Do not expect HFS+ to behave the same as NTFS or EXT4 and
vice-versa. Do not try to change data permanently through normalization as a
leaky abstraction to paper over Unicode differences between filesystems. This
would create problems without solving any. Rather, preserve Unicode form and use
normalization as a comparison function only.
Unicode Form Insensitivity
Unicode form insensitivity and Unicode form preservation are two different
filesystem behaviors often mistaken for each other. Just as case-insensitivity
has sometimes been incorrectly implemented by permanently normalizing filenames
to uppercase when storing and transmitting filenames, so Unicode form
insensitivity has sometimes been incorrectly implemented by permanently
normalizing filenames to a certain Unicode form (NFD in the case of HFS+) when
storing and transmitting filenames. It is possible and much better to implement
Unicode form insensitivity without sacrificing Unicode form preservation, by
using Unicode normalization for comparison only.
Comparing Different Unicode Forms
Node.js provides string.normalize('NFC' / 'NFD') which you can use to normalize a
UTF-8 string to either NFC or NFD. You should never store the output from this
function but only use it as part of a comparison function to test whether two
UTF-8 strings would look the same to the user.
You can use string1.normalize('NFC') === string2.normalize('NFC') or
string1.normalize('NFD') === string2.normalize('NFD') as your comparison
function. Which form you use does not matter.
Normalization is fast but you may want to use a cache as input to your
comparison function to avoid normalizing the same string many times over. If the
string is not present in the cache then normalize it and cache it. Be careful
not to store or persist the cache, use it only as a cache.
Note that using normalize() requires that your version of Node.js include ICU
(otherwise normalize() will just return the original string). If you download
the latest version of Node.js from the website then it will include ICU.
Timestamp Resolution
You may set the mtime (the modified time) of a file to 1444291759414
(millisecond resolution) and be surprised to see sometimes that fs.stat
returns the new mtime as 1444291759000 (1-second resolution) or
1444291758000 (2-second resolution). This is not a bug in Node. Node.js returns
the timestamp as the filesystem stores it, and not all filesystems support
nanosecond, millisecond or 1-second timestamp resolution. Some filesystems even
have very coarse resolution for the atime timestamp in particular, e.g. 24 hours
for some FAT filesystems.
Do Not Corrupt Filenames and Timestamps Through Normalization
Filenames and timestamps are user data. Just as you would never automatically
rewrite user file data to uppercase the data or normalize CRLF to LF
line-endings, so you should never change, interfere or corrupt filenames or
timestamps through case / Unicode form / timestamp normalization. Normalization
should only ever be used for comparison, never for altering data.
Normalization is effectively a lossy hash code. You can use it to test for
certain kinds of equivalence (e.g. do several strings look the same even though
they have different byte sequences) but you can never use it as a substitute for
the actual data. Your program should pass on filename and timestamp data as is.
Your program can create new data in NFC (or in any combination of Unicode form
it prefers) or with a lowercase or uppercase filename, or with a 2-second
resolution timestamp, but your program should not corrupt existing user data by
imposing case / Unicode form / timestamp normalization. Rather, adopt a superset
approach and preserve case, Unicode form and timestamp resolution in your
program. That way, you will be able to interact safely with filesystems which do
the same.
Use Normalization Comparison Functions Appropriately
Make sure that you use case / Unicode form / timestamp comparison functions
appropriately. Do not use a case-insensitive filename comparison function if you
are working on a case-sensitive filesystem. Do not use a Unicode form
insensitive comparison function if you are working on a Unicode form sensitive
filesystem (e.g. NTFS and most Linux filesystems which preserve both NFC and NFD
or mixed Unicode forms). Do not compare timestamps at 2-second resolution if you
are working on a nanosecond timestamp resolution filesystem.
Be Prepared for Slight Differences in Comparison Functions
Be careful that your comparison functions match those of the filesystem (or
probe the filesystem if possible to see how it would actually compare).
Case-insensitivity for example is more complex than a simple toLowerCase()
comparison. In fact, toUpperCase() is usually better than toLowerCase()
(since it handles certain foreign language characters differently). But better
still would be to probe the filesystem since every filesystem has its own case
comparison table baked in.
As an example, Apple's HFS+ normalizes filenames to NFD form but this NFD form
is actually an older version of the current NFD form and may sometimes be
slightly different from the latest Unicode standard's NFD form. Do not expect
HFS+ NFD to be exactly the same as Unicode NFD all the time.PrevWorking with folders in Node.jsNextRun Node.js scripts from the command line\n\n\n\nRun Node.js scripts from the command line
The usual way to run a Node.js program is to run the globally available node command (once you install Node.js) and pass the name of the file you want to execute.
If your main Node.js application file is app.js, you can call it by typing:
node app.js
ShellCopy to clipboard
Above, you are explicitly telling the shell to run your script with node. You can also embed this information into your JavaScript file with a "shebang" line. The "shebang" is the first line in the file, and tells the OS which interpreter to use for running the script. Below is the first line of JavaScript:
#!/usr/bin/node
JavaScriptCopy to clipboard
Above, we are explicitly giving the absolute path of interpreter. Not all operating systems have node in the bin folder, but all should have env. You can tell the OS to run env with node as parameter:
#!/usr/bin/env node

// your javascript code
JavaScriptCopy to clipboard
To use a shebang, your file should have executable permission. You can give app.js the executable permission by running:
chmod u+x app.js
ShellCopy to clipboard
While running the command, make sure you are in the same directory which contains the app.js file.
Pass string as argument to node instead of file path
To execute a string as argument you can use -e, --eval "script". Evaluate the following argument as JavaScript. The modules which are predefined in the REPL can also be used in script.
On Windows, using cmd.exe a single quote will not work correctly because it only recognizes double " for quoting. In Powershell or Git bash, both ' and " are usable.
node -e "console.log(123)"
ShellCopy to clipboard
Restart the application automatically
As of Node.js V16, there is a built-in option to automatically restart the application when a file changes. This is useful for development purposes.
To use this feature, you need to pass the --watch flag to Node.js.
node --watch app.js
ShellCopy to clipboard
So when you change the file, the application will restart automatically.
Read the --watch flag documentation.
Run a task with Node.js
Node.js provides a built-in task runner that allows you to execute specific commands defined in your package.json file. This can be particularly useful for automating repetitive tasks such as running tests, building your project, or linting your code.
Using the --run flag
The --run flag allows you to run a specified command from the scripts section of your package.json file. For example, if you have the following package.json:
{
  "type": "module",
  "scripts": {
    "start": "node app.js",
    "dev": "node --run -- --watch",
    "test": "node --test"
  }
}
JSONCopy to clipboard
You can run the test script using the --run flag:
node --run test
ShellCopy to clipboard
Passing arguments to the command
Let's explain the dev key in the scripts object of the package.json file.
The syntax -- --another-argument is used to pass arguments to the command. In this case, the --watch argument is passed to the dev script.
node --run dev
ShellCopy to clipboard
Environment variables
The --run flag sets specific environment variables that can be useful for your scripts:

NODE_RUN_SCRIPT_NAME: The name of the script being run.
NODE_RUN_PACKAGE_JSON_PATH: The path to the package.json file being processed.

Intentional limitations
The Node.js task runner is intentionally more limited compared to other task runners like npm run or yarn run. It focuses on performance and simplicity, omitting features like running pre or post scripts. This makes it suitable for straightforward tasks but may not cover all use cases.PrevHow to work with Different FilesystemsNextHow to read environment variables from Node.js\n\n\n\nHow to read environment variables from Node.js
The process core module of Node.js provides the env property which hosts all the environment variables that were set at the moment the process was started.
The below code runs app.js and set USER_ID and USER_KEY.
USER_ID=239482 USER_KEY=foobar node app.js
ShellCopy to clipboard
That will pass the user USER_ID as 239482 and the USER_KEY as foobar. This is suitable for testing, however for production, you will probably be configuring some bash scripts to export variables.

Note: process does not need to be imported, it is a global object in Node.js.

Here is an example that accesses the USER_ID and USER_KEY environment variables, which we set in above code.
process.env.USER_ID; // "239482"
process.env.USER_KEY; // "foobar"
JavaScriptCopy to clipboard
In the same way you can access any custom environment variable you set.
Node.js 20 introduced experimental support for .env files.
Now, you can use the --env-file flag to specify an environment file when running your Node.js application. Here's an example .env file and how to access its variables using process.env.
# .env file
PORT=3000
ShellCopy to clipboard
In your js file
process.env.PORT; // "3000"
JavaScriptCopy to clipboard
Run app.js file with environment variables set in .env file.
node --env-file=.env app.js
ShellCopy to clipboard
This command loads all the environment variables from the .env file, making them available to the application on process.env
Also, you can pass multiple --env-file arguments. Subsequent files override pre-existing variables defined in previous files.
node --env-file=.env --env-file=.development.env app.js
ShellCopy to clipboard

Note: if the same variable is defined in the environment and in the file, the value from the environment takes precedence.

In case you want to optionally read from a .env file, it's possible to avoid
throwing an error if the file is missing using the --env-file-if-exists flag.
node --env-file-if-exists=.env app.js
ShellCopy to clipboardPrevRun Node.js scripts from the command lineNextHow to use the Node.js REPL\n\n\n\nHow to use the Node.js REPL
What is the Node.js REPL?
Node.js comes with a built-in REPL (Read-Eval-Print Loop) environment that allows you to execute JavaScript code interactively. The REPL is accessible through the terminal and is a great way to test out small pieces of code.
How to use the Node.js REPL
The node command is the one we use to run our Node.js scripts:
node script.js
ShellCopy to clipboard
If we run the node command without any script to execute or without any arguments, we start a REPL session:
node
ShellCopy to clipboard

Note: REPL stands for Read Evaluate Print Loop, and it is a programming language environment (basically a console window) that takes single expression as user input and returns the result back to the console after execution. The REPL session provides a convenient way to quickly test simple JavaScript code.

If you try it now in your terminal, this is what happens:
❯ node
>
ShellCopy to clipboard
The command stays in idle mode and waits for us to enter something.

Tip: if you are unsure how to open your terminal, google "How to open terminal on your-operating-system".

The REPL is waiting for us to enter some JavaScript code, to be more precise.
Start simple and enter
> console.log('test')
test
undefined
>
Shell SessionCopy to clipboard
The first value, test, is the output we told the console to print, then we get undefined which is the return value of running console.log().
Node read this line of code, evaluated it, printed the result, and then went back to waiting for more lines of code. Node will loop through these three steps for every piece of code we execute in the REPL until we exit the session. That is where the REPL got its name.
Node automatically prints the result of any line of JavaScript code without the need to instruct it to do so. For example, type in the following line and press enter:
> 5 === '5'
false
>
Shell SessionCopy to clipboard
Note the difference in the outputs of the above two lines. The Node REPL printed undefined after executing console.log(), while on the other hand, it just printed the result of 5 === '5'. You need to keep in mind that the former is just a statement in JavaScript, and the latter is an expression.
In some cases, the code you want to test might need multiple lines. For example, say you want to define a function that generates a random number, in the REPL session type in the following line and press enter:
function generateRandom() {
...
Shell SessionCopy to clipboard
The Node REPL is smart enough to determine that you are not done writing your code yet, and it will go into a multi-line mode for you to type in more code. Now finish your function definition and press enter:
function generateRandom() {
...return Math.random()
}
undefined
Shell SessionCopy to clipboard
The _ special variable
If after some code you type _, that is going to print the result of the last operation.
The Up arrow key
If you press the up arrow key, you will get access to the history of the previous lines of code executed in the current, and even previous REPL sessions.
Dot commands
The REPL has some special commands, all starting with a dot .. They are

.help: shows the dot commands help
.editor: enables editor mode, to write multiline JavaScript code with ease. Once you are in this mode, enter ctrl-D to run the code you wrote.
.break: when inputting a multi-line expression, entering the .break command will abort further input. Same as pressing ctrl-C.
.clear: resets the REPL context to an empty object and clears any multi-line expression currently being input.
.load: loads a JavaScript file, relative to the current working directory
.save: saves all you entered in the REPL session to a file (specify the filename)
.exit: exits the repl (same as pressing ctrl-C two times)

The REPL knows when you are typing a multi-line statement without the need to invoke .editor.
For example if you start typing an iteration like this:
[1, 2, 3].forEach(num => {
Shell SessionCopy to clipboard
and you press enter, the REPL will go to a new line that starts with 3 dots, indicating you can now continue to work on that block.
... console.log(num)
... })
Shell SessionCopy to clipboard
If you type .break at the end of a line, the multiline mode will stop and the statement will not be executed.
Run REPL from JavaScript file
We can import the REPL in a JavaScript file using repl.
CJSMJSconst repl = require('node:repl');
JavaScriptCopy to clipboardUsing the repl variable we can perform various operations.
To start the REPL command prompt, type in the following line
repl.start();
JavaScriptCopy to clipboard
Run the file in the command line.
BASHCONSOLEnode repl.js
ShellCopy to clipboardYou can pass a string which shows when the REPL starts. The default is '> ' (with a trailing space), but we can define custom prompt.
// a Unix style prompt
const local = repl.start('$ ');
JavaScriptCopy to clipboard
You can display a message while exiting the REPL
local.on('exit', () => {
  console.log('exiting repl');
  process.exit();
});
JavaScriptCopy to clipboard
You can read more about the REPL module in the repl documentation.PrevHow to read environment variables from Node.jsNextOutput to the command line using Node.js\n\n\n\nOutput to the command line using Node.js
Basic output using the console module
Node.js provides a console module which provides tons of very useful ways to interact with the command line.
It is basically the same as the console object you find in the browser.
The most basic and most used method is console.log(), which prints the string you pass to it to the console.
If you pass an object, it will render it as a string.
You can pass multiple variables to console.log, for example:
const x = 'x';
const y = 'y';

console.log(x, y);
JavaScriptCopy to clipboard
and Node.js will print both.
We can also format pretty phrases by passing variables and a format specifier.
For example:
console.log('My %s has %d ears', 'cat', 2);
JavaScriptCopy to clipboard

%s format a variable as a string
%d format a variable as a number
%i format a variable as its integer part only
%o format a variable as an object

Example:
console.log('%o', Number);
JavaScriptCopy to clipboard
Clear the console
console.clear() clears the console (the behavior might depend on the console used)
Counting elements
console.count() is a handy method.
Take this code:
const x = 1;
const y = 2;
const z = 3;

console.count(
  'The value of x is ' + x + ' and has been checked .. how many times?'
);

console.count(
  'The value of x is ' + x + ' and has been checked .. how many times?'
);

console.count(
  'The value of y is ' + y + ' and has been checked .. how many times?'
);
JavaScriptCopy to clipboard
What happens is that console.count() will count the number of times a string is printed, and print the count next to it:
You can just count apples and oranges:
const oranges = ['orange', 'orange'];
const apples = ['just one apple'];

oranges.forEach(fruit => {
  console.count(fruit);
});
apples.forEach(fruit => {
  console.count(fruit);
});
JavaScriptCopy to clipboard
Reset counting
The console.countReset() method resets counter used with console.count().
We will use the apples and orange example to demonstrate this.
const oranges = ['orange', 'orange'];
const apples = ['just one apple'];

oranges.forEach(fruit => {
  console.count(fruit);
});
apples.forEach(fruit => {
  console.count(fruit);
});

console.countReset('orange');

oranges.forEach(fruit => {
  console.count(fruit);
});
JavaScriptCopy to clipboard
Notice how the call to console.countReset('orange') resets the value counter to zero.
Print the stack trace
There might be cases where it's useful to print the call stack trace of a function, maybe to answer the question how did you reach that part of the code?
You can do so using console.trace():
const function2 = () => console.trace();
const function1 = () => function2();
function1();
JavaScriptCopy to clipboard
This will print the stack trace. This is what's printed if we try this in the Node.js REPL:
Trace
    at function2 (repl:1:33)
    at function1 (repl:1:25)
    at repl:1:1
    at ContextifyScript.Script.runInThisContext (vm.js:44:33)
    at REPLServer.defaultEval (repl.js:239:29)
    at bound (domain.js:301:14)
    at REPLServer.runBound [as eval] (domain.js:314:12)
    at REPLServer.onLine (repl.js:440:10)
    at emitOne (events.js:120:20)
    at REPLServer.emit (events.js:210:7)
ShellCopy to clipboard
Calculate the time spent
You can easily calculate how much time a function takes to run, using time() and timeEnd()
const doSomething = () => console.log('test');
const measureDoingSomething = () => {
  console.time('doSomething()');
  // do something, and measure the time it takes
  doSomething();
  console.timeEnd('doSomething()');
};
measureDoingSomething();
JavaScriptCopy to clipboard
stdout and stderr
As we saw console.log is great for printing messages in the Console. This is what's called the standard output, or stdout.
console.error prints to the stderr stream.
It will not appear in the console, but it will appear in the error log.
Color the output

NOTE
This part of the resource is designed with version 22.11 which notes styleText as ‘Active development’.

In many cases, you will be tempted to paste certain text to get a nice output at the terminal.
There is a styleText function provided by the node:util module. Let's discover how to use it.
First of all, you need to import the styleText function from the node:util module:
MJSCJSimport { styleText } from 'node:util';
JavaScriptCopy to clipboardThen, you can use it to style your text:
console.log(
  styleText(['red'], 'This is red text ') +
    styleText(['green', 'bold'], 'and this is green bold text ') +
    'this is normal text'
);
JavaScriptCopy to clipboard
The first argument is an array of styles, and the second argument is the text you want to style. We invite you to read the docsPrevHow to use the Node.js REPLNextAccept input from the command line in Node.js\n\n\n\nAccept input from the command line in Node.js
How to make a Node.js CLI program interactive?
Node.js since version 7 provides the readline module to perform exactly this: get input from a readable stream such as the process.stdin stream, which during the execution of a Node.js program is the terminal input, one line at a time.
CJSMJSconst readline = require('node:readline');

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

rl.question(`What's your name?`, name => {
  console.log(`Hi ${name}!`);
  rl.close();
});
JavaScriptCopy to clipboardThis piece of code asks the user's name, and once the text is entered and the user presses enter, we send a greeting.
The question() method shows the first parameter (a question) and waits for the user input. It calls the callback function once enter is pressed.
In this callback function, we close the readline interface.
readline offers several other methods, please check them out on the package documentation linked above.
If you need to require a password, it's best not to echo it back, but instead show a * symbol.PrevOutput to the command line using Node.jsNextPublishing a package\n\n\n\nPublishing a package
All the provided package.json configurations (not specifically marked “does not work”) work in Node.js 12.22.x (v12 latest, the oldest supported line) and 17.2.0 (current latest at the time)1, and for grins, with webpack 5.53.0 and 5.63.0 respectively. These are available: JakobJingleheimer/nodejs-module-config-examples.
For curious cats, How did we get here and Down the rabbit-hole provide background and deeper explanations.
Pick your fix
There are 2 main options, which cover almost all use-cases:

Write source code and publish in CJS (you use require()); CJS is consumable by both CJS and ESM (in all versions of node). Skip to CJS source and distribution.
Write source code and publish in ESM (you use import, and don't use top-level await); ESM is consumable by both ESM and CJS (in node 22.x and 23.x; see require() an ES Module). Skip to ESM source and distribution.

It's generally best to publish only 1 format, either CJS or ESM. Not both. Publishing multiple formats can result in the dual-package hazard, as well as other drawbacks.
There are other options available, mostly for historical purposes.
You as a package author writeConsumers of your package write their code inYour optionsCJS source code using require()ESM: consumers import your packageCJS source and only ESM distributionCJS & ESM: consumers either require() or import your packageCJS source and both CJS & ESM distributionESM source code using importCJS: consumers require() your package (and you use top-level await)ESM source with only CJS distributionCJS & ESM: consumers either require() or import your packageESM source and both CJS & ESM distribution
CJS source and distribution
The most minimal configuration may be only "name". But the less arcane, the better: Essentially just declare the package’s exports via the "exports" field/field-set.
Working example: cjs-with-cjs-distro
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "cjs-source-and-distribution"
  // "main": "./index.js"
}
JSONCopy to clipboardNote that packageJson.exports["."] = filepath is shorthand for packageJson.exports["."].default = filepath
ESM source and distribution
Simple, tried, and true.
Note that since Node.js v23.0.0, it is possible to require static ESM (code that does not use top-level await). See Loading ECMAScript modules using require() for details.
This is almost exactly the same as the CJS-CJS configuration above with 1 small difference: the "type" field.
Working example: esm-with-esm-distro
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "esm-source-and-distribution",
  "type": "module"
  // "main": "./index.js"
}
JSONCopy to clipboardNote that ESM now is “backwards” compatible with CJS: a CJS module now can require() an ES Module without a flag as of 23.0.0 and 22.12.0.
CJS source and only ESM distribution
This takes a small bit of finesse but is also pretty straight-forward. This may be the choice pick of older projects targetting newer standards, or authors who merely prefer CJS but are publishing for a different environment.
Working example: cjs-with-esm-distro
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "cjs-source-with-esm-distribution",
  "main": "./dist/index.mjs"
}
JSONCopy to clipboardThe .mjs file extension is a trump-card: it will override any other configuration and the file will be treated as ESM. Using this file extension is necessary because packageJson.exports.import does NOT signify that the file is ESM (contrary to common, if not universal, misperception), only that it is the file to be used when the package is imported (ESM can import CJS. See Gotchas below).
CJS source and both CJS & ESM distribution
In order to directly supply both audiences (so that your distribution works "natively" in either), you have a few options:
Attach named exports directly onto exports
Classic but takes some sophistication and finesse. This means adding properties onto the existing module.exports (instead of re-assigning module.exports as a whole).
Working example: cjs-with-dual-distro (properties)
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "cjs-source-with-esm-via-properties-distribution",
  "main": "./dist/cjs/index.js"
}
JSONCopy to clipboardPros:

Smaller package weight
Easy and simple (probably least effort if you don't mind keeping to a minor syntax stipulation)
Precludes the Dual-Package Hazard

Cons:

Requires very specific syntax (either in source code and/or bundler gymnastics).

Sometimes, a CJS module may re-assign module.exports to something else (be it an object or a function) like this:
const someObject = {
  foo() {},
  bar() {},
  qux() {},
};

module.exports = someObject;
JavaScriptCopy to clipboard
Node.js detects the named exports in CJS via static analysis that look for certain patterns, which the example above evades. To make the named exports detectable, do this:
module.exports.foo = function foo() {};
module.exports.bar = function bar() {};
module.exports.qux = function qux() {};
JavaScriptCopy to clipboard
Use a simple ESM wrapper
Complicated setup and difficult to get the balance right.
Working example: cjs-with-dual-distro (wrapper)
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "cjs-with-wrapper-dual-distro",
  "exports": {
    ".": {
      "import": "./dist/esm/wrapper.mjs",
      "require": "./dist/cjs/index.js",
      "default": "./dist/cjs/index.js"
    }
  }
}
JSONCopy to clipboardPros:

Smaller package weight

Cons:

Likely requires complicated bundler gymnastics (we could not find any existing option to automate this in Webpack).

When the CJS output from the bundler evades the named exports detection in Node.js, a ESM wrapper can be used to explicitly re-export the known named exports for ESM consumers.
When CJS exports an object (which gets aliased to ESM's default), you can save references to all the members of the object locally in the wrapper, and then re-export them so the ESM consumer can access all of them by name.
import cjs from '../cjs/index.js';

const { a, b, c /* … */ } = cjs;

export { a, b, c /* … */ };
JavaScriptCopy to clipboard
However, this does break live bindings: a reassignment to cjs.a will not reflect in esmWrapper.a.
Two full distributions
Chuck in a bunch of stuff and hope for the best. This is probably the most common and easiest of the CJS to CJS & ESM options, but you pay for it. This is rarely a good idea.
Working example: cjs-with-dual-distro (double)
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "cjs-with-full-dual-distro",
  "exports": {
    ".": {
      "import": "./dist/esm/index.mjs",
      "require": "./dist/cjs/index.js",
      "default": "./dist/cjs/index.js"
    }
  }
}
JSONCopy to clipboardPros:

Simple bundler configuration

Cons:

Larger package weight (basically double)
Vulnerable to the Dual-Package Hazard

Alternatively, you can use "default" and "node" keys, which are less counter-intuitive: Node.js will always choose the "node" option (which always works), and non-Node.js tooling will choose "default" when configured to target something other than node. This precludes the dual-package hazard.
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "cjs-with-alt-full-dual-distro",
  "exports": {
    ".": {
      "node": "./dist/cjs/index.js",
      "default": "./dist/esm/index.mjs"
    }
  }
}
JSONCopy to clipboardESM source with only CJS distribution
We're not in Kansas anymore, Toto.
The configurations (there are 2 options) are nearly the same as ESM source and both CJS & ESM distribution, just exclude packageJson.exports.import.
💡 Using "type": "module"2 paired with the .cjs file extension (for commonjs files) yields best results. For more information on why, see Down the rabbit-hole and Gotchas below.
Working example: esm-with-cjs-distro
ESM source and both CJS & ESM distribution
When source code is written in non-JavaScript (ex TypeScript), options can be limited due to needing to use file extension(s) specific to that language (ex .ts) and there may be no .mjs equivalent.
Similar to CJS source and both CJS & ESM distribution, you have the same options.
Publish only a CJS distribution with property exports
Tricky to make and needs good ingredients.
This option is almost identical to the CJS source with CJS & ESM distribution's property exports above. The only difference is in package.json: "type": "module".
Only some build tools support generating this output. Rollup produces compatible output out of the box when targetting commonjs. Webpack as of v5.66.0+ does with the new commonjs-static output type, (prior to this, no commonjs options produces compatible output). It is not currently possible with esbuild (which produces a non-static exports).
The working example below was created prior to Webpack's recent release, so it uses Rollup (I'll get around to adding a Webpack option too).
These examples assume javascript files within use the extension .js; "type" in package.json controls how those are interpreted:
"type":"commonjs" + .js → cjs
"type":"module" + .js → mjs
If your files explicitly all use .cjs and/or .mjs file extensions (none use .js), "type" is superfluous.
Working example: esm-with-cjs-distro
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "esm-with-cjs-distribution",
  "type": "module",
  "main": "./dist/index.cjs"
}
JSONCopy to clipboard💡 Using "type": "module"2 paired with the .cjs file extension (for commonjs files) yields best results. For more information on why, see Down the rabbit-hole and Gotchas below.
Publish a CJS distribution with an ESM wrapper
There's a lot going on here, and this is usually not the best.
This is also almost identical to the CJS source and dual distribution using an ESM wrapper, but with subtle differences "type": "module" and some .cjs file extenions in package.json.
Working example: esm-with-dual-distro (wrapper)
Minimal package.jsonAdvanced (verbose) package.json{
  "name": "esm-with-cjs-and-esm-wrapper-distribution",
  "type": "module",
  "exports": {
    ".": {
      "import": "./dist/esm/wrapper.js",
      "require": "./dist/cjs/index.cjs",
      "default": "./dist/cjs/index.cjs"
    }
  }
}
JSONCopy to clipboard💡 Using "type": "module"2 paired with the .cjs file extension (for commonjs files) yields best results. For more information on why, see Down the rabbit-hole and Gotchas below.
Publish both full CJS & ESM distributions
Chuck in a bunch of stuff (with a surprise) and hope for the best. This is probably the most common and easiest of the ESM to CJS & ESM options, but you pay for it. This is rarely a good idea.
In terms of package configuration, there are a few options that differ mostly in personal preference.
Mark the whole package as ESM and specifically mark the CJS exports as CJS via the .cjs file extension
This option has the least burden on development/developer experience.
This also means that whatever build tooling must produce the distribution file with a .cjs file extension. This might necessitate chaining multiple build tools or adding a subsequent step to move/rename the file to have the .cjs file extension (ex mv ./dist/index.js ./dist/index.cjs). This can be worked around by adding a subsequent step to move/rename those outputted files (ex Rollup or a simple shell script).
Support for the .cjs file extension was added in 12.0.0, and using it will cause ESM to properly recognised a file as commonjs (import { foo } from './foo.cjs' works). However, require() does not auto-resolve .cjs like it does for .js, so file extension cannot be omitted as is commonplace in commonjs: require('./foo') will fail, but require('./foo.cjs') works. Using it in your package's exports has no drawbacks: packageJson.exports (and packageJson.main) requires a file extension regardless, and consumers reference your package by the "name" field of your package.json (so they're blissfully unaware).
Working example: esm-with-dual-distro
Minimal import & require package.jsonAdvanced (verbose) import & require package.json{
  "type": "module",
  "exports": {
    ".": {
      "import": "./dist/esm/index.js",
      "require": "./dist/index.cjs"
    }
  }
}
JSONCopy to clipboardAlternatively, you can use "default" and "node" keys, which are less counter-intuitive: Node.js will always choose the "node" option (which always works), and non-Node.js tooling will choose "default" when configured to target something other than node. This precludes the dual-package hazard.
Minimal default & node package.jsonAdvanced (verbose) default & node package.json{
  "type": "module",
  "exports": {
    ".": {
      "node": "./dist/index.cjs",
      "default": "./dist/esm/index.js"
    }
  }
}
JSONCopy to clipboard💡 Using "type": "module"2 paired with the .cjs file extension (for commonjs files) yields best results. For more information on why, see Down the rabbit-hole and Gotchas below.
Use the .mjs (or equivalent) file extension for all source code files
The configuration for this is the same as CJS source and both CJS & ESM distribution.
Non-JavaScript source code: The non-JavaScript language’s own configuration needs to recognise/specify that the input files are ESM.
Node.js before 12.22.x
🛑 You should not do this: Versions of Node.js prior to 12.x are End of Life and are now vulnerable to serious security exploits.
If you're a security researcher needing to investigate Node.js prior to v12.22.x, feel free to contact us for help configuring.
General notes
Syntax detection is not a replacement for proper package configuration; syntax detection is not fool-proof and it has significant performance cost.
When using "exports" in package.json, it is generally a good idea to include "./package.json": "./package.json" so that it can be imported (module.findPackageJSON is not affected by this limitation, but import may be more convenient).
"exports" can be advisable over "main" because it prevents external access to internal code (so you can be relatively sure users are not depending on things they shouldn't). If you don't need that, "main" is simpler and may be a better option for you.
The "engines" field provides both a human-friendly and a machine-friendly indication of which version(s) of Node.js the package is compatible. Depending on the package manager used, an exception may be thrown causing the installation to fail when the consumer is using an incompatible version of Node.js (which can be very helpful to consumers). Including this field will save a lot of headache for consumers with an older version of Node.js who cannot use the package.
Down the rabbit-hole
Specifically in relation to Node.js, there are 4 problems to solve:


Determining format of source code files (author running her/his own code)


Determining format of distribution files (code consumers will receive)


Publicising distribution code for when it is require()’d (consumer expects CJS)


Publicising distribution code for when it is import’d (consumer probably wants ESM)


⚠️ The first 2 are independent of the last 2.
The method of loading does NOT determine the format the file is interpreted as:

package.json’s exports.require ≠ CJS. require() does NOT and cannot blindly interpret the file as CJS; for instance, require('foo.json') correctly interprets the file as JSON, not CJS. The module containing the require() call of course must be CJS, but what it is loading is not necessarily also CJS.
package.json’s exports.import ≠ ESM. import similarly does NOT and cannot blindly interpret the file as ESM; import can load CJS, JSON, and WASM, as well as ESM. The module containing the import statement of course must be ESM, but what it is loading is not necessarily also ESM.

So when you see configuration options citing or named with require or import, resist the urge to assume they are for determining CJS vs ES Modules.
⚠️ Adding an "exports" field/field-set to a package’s configuration effectively blocks deep pathing into the package for anything not explicitly listed in the exports’ subpathing. This means it can be a breaking change.
⚠️ Consider carefully whether to distribute both CJS and ESM: It creates the potential for the Dual Package Hazard (especially if misconfigured and the consumer tries to get clever). This can lead to an extremely confusing bug in consuming projects, especially when your package is not perfectly configured. Consumers can even be blind-sided by an intermediary package that uses the "other" format of your package (eg consumer uses the ESM distribution, and some other package the consumer is also using itself uses the CJS distribution). If your package is in any way stateful, consuming both the CJS and ESM distributions will result in parallel states (which is almost surely unintentional).
The dual-package hazard
When an application is using a package that provides both CommonJS and ES module sources, there is a risk of certain bugs if both instances of the package get loaded. This potential comes from the fact that the pkgInstance created by const pkgInstance = require('pkg') is not the same as the pkgInstance created by import pkgInstance from 'pkg' (or an alternative main path like 'pkg/module'). This is the “dual package hazard”, where two instances of the same package can be loaded within the same runtime environment. While it is unlikely that an application or package would intentionally load both instances directly, it is common for an application to load one copy while a dependency of the application loads the other copy. This hazard can happen because Node.js supports intermixing CommonJS and ES modules, and can lead to unexpected and confusing behavior.
If the package main export is a constructor, an instanceof comparison of instances created by the two copies returns false, and if the export is an object, properties added to one (like pkgInstance.foo = 3) are not present on the other. This differs from how import and require statements work in all-CommonJS or all-ES module environments, respectively, and therefore is surprising to users. It also differs from the behavior users are familiar with when using transpilation via tools like Babel or esm.
How did we get here
CommonJS (CJS) was created long before ECMAScript Modules (ESM), back when JavaScript was still adolescent—CJS and jQuery were created just 3 years apart. CJS is not an official (TC39) standard and is supported by a limited few platforms (most notably, Node.js). ESM as a standard has been incoming for several years; it is currently supported by all major platforms (browsers, Deno, Node.js, etc), meaning it will run pretty much everywhere. As it became clear ESM would effectively succeed CJS (which is still very popular and widespread), many attempted to adopt early on, often before a particular aspect of the ESM specification was finalised. Because of this, those changed over time as better information became available (often informed by learnings/experiences of those eager beavers), going from best-guess to the aligning with the specification.
An additional complication is bundlers, which historically managed much of this territory. However, much of what we previously needed bundle(r)s to manage is now native functionality; yet bundlers are still (and likely always will be) necessary for some things. Unfortunately, functionality bundlers no-longer need to provide is deeply ingrained in older bundlers’ implementations, so they can at times be too helpful, and in some cases, anti-pattern (bundling a library is often not recommended by bundler authors themselves). The hows and whys of that are an article unto itself.
Gotchas
The package.json's "type" field changes the .js file extension to mean either commonjs or ES module respectively. It is very common in dual/mixed packages (that contain both CJS and ESM) to use this field incorrectly.
{
  "type": "module",
  "main": "./dist/CJS/index.js",
  "exports": {
    ".": {
      "import": "./dist/esm/index.js",
      "require": "./dist/cjs/index.js",
      "default": "./dist/cjs/index.js"
    },
    "./package.json": "./package.json"
  }
}
JSONCopy to clipboard
This does not work because "type": "module" causes packageJson.main, packageJson.exports["."].require, and packageJson.exports["."].default to get interpreted as ESM (but they’re actually CJS).
Excluding "type": "module" produces the opposite problem:
{
  "main": "./dist/CJS/index.js",
  "exports": {
    ".": {
      "import": "./dist/esm/index.js",
      "require": "./dist/cjs/index.js",
      "default": "./dist/cjs/index.js"
    },
    "./package.json": "./package.json"
  }
}
JSONCopy to clipboard
This does not work because packageJson.exports["."].import will get interpreted as CJS (but it’s actually ESM).
Footnotes


There was a bug in Node.js v13.0–13.6 where packageJson.exports["."] had to be an array with verbose config options as the first item (as an object) and the “default” as the second item (as a string). See nodejs/modules#446. ↩


The "type" field in package.json changes what the .js file extension means, similar to to an HTML script element’s type attribute. ↩ ↩2 ↩3 ↩4


PrevAccept input from the command line in Node.jsNextHow to publish a Node-API package\n\n\n\nHow to publish a Node-API version of a package alongside a non-Node-API version
The following steps are illustrated using the package iotivity-node:

First, publish the non-Node-API version:

Update the version in package.json. For iotivity-node, the version
becomes 1.2.0-2.
Go through the release checklist (ensure tests/demos/docs are OK)
npm publish


Then, publish the Node-API version:

Update the version in package.json. In the case of iotivity-node,
the version becomes 1.2.0-3. For versioning, we recommend following
the pre-release version scheme as described by
semver.org e.g. 1.2.0-napi.
Go through the release checklist (ensure tests/demos/docs are OK)
npm publish --tag n-api



In this example, tagging the release with n-api has ensured that, although
version 1.2.0-3 is later than the non-Node-API published version (1.2.0-2), it
will not be installed if someone chooses to install iotivity-node by simply
running npm install iotivity-node. This will install the non-Node-API version
by default. The user will have to run npm install iotivity-node@n-api to
receive the Node-API version. For more information on using tags with npm check
out "Using dist-tags".
How to introduce a dependency on a Node-API version of a package
To add the Node-API version of iotivity-node as a dependency, the package.json
will look like this:
"dependencies": {
  "iotivity-node": "n-api"
}
JSONCopy to clipboard

As explained in
"Using dist-tags", unlike regular versions, tagged versions cannot be
addressed by version ranges such as "^2.0.0" inside package.json. The
reason for this is that the tag refers to exactly one version. So, if the
package maintainer chooses to tag a later version of the package using the
same tag, npm update will receive the later version. This should be acceptable
version other than the latest published, the package.json dependency will
have to refer to the exact version like the following:

"dependencies": {
  "iotivity-node": "1.2.0-3"
}
JSONCopy to clipboardPrevPublishing a packageNextAnatomy of an HTTP Transaction\n\n\n\nAnatomy of an HTTP Transaction
The purpose of this guide is to impart a solid understanding of the process of
Node.js HTTP handling. We'll assume that you know, in a general sense, how HTTP
requests work, regardless of language or programming environment. We'll also
assume a bit of familiarity with Node.js EventEmitters and Streams.
If you're not quite familiar with them, it's worth taking a quick read through
the API docs for each of those.
Create the Server
Any node web server application will at some point have to create a web server
object. This is done by using createServer.
CJSMJSconst http = require('node:http');

const server = http.createServer((request, response) => {
  // magic happens here!
});
JavaScriptCopy to clipboardThe function that's passed in to createServer is called once for every
HTTP request that's made against that server, so it's called the request
handler. In fact, the Server object returned by createServer is an
EventEmitter, and what we have here is just shorthand for creating a
server object and then adding the listener later.
const server = http.createServer();
server.on('request', (request, response) => {
  // the same kind of magic happens here!
});
JavaScriptCopy to clipboard
When an HTTP request hits the server, Node calls the request handler function
with a few handy objects for dealing with the transaction, request and
response. We'll get to those shortly.
In order to actually serve requests, the listen method needs to be called
on the server object. In most cases, all you'll need to pass to listen is
the port number you want the server to listen on. There are some other options
too, so consult the API reference.
Method, URL and Headers
When handling a request, the first thing you'll probably want to do is look at
the method and URL, so that appropriate actions can be taken. Node.js makes this
relatively painless by putting handy properties onto the request object.
const { method, url } = request;
JavaScriptCopy to clipboard

The request object is an instance of IncomingMessage.

The method here will always be a normal HTTP method/verb. The url is the
full URL without the server, protocol or port. For a typical URL, this means
everything after and including the third forward slash.
Headers are also not far away. They're in their own object on request called
headers.
const { headers } = request;
const userAgent = headers['user-agent'];
JavaScriptCopy to clipboard
It's important to note here that all headers are represented in lower-case only,
regardless of how the client actually sent them. This simplifies the task of
parsing headers for whatever purpose.
If some headers are repeated, then their values are overwritten or joined
together as comma-separated strings, depending on the header. In some cases,
this can be problematic, so rawHeaders is also available.
Request Body
When receiving a POST or PUT request, the request body might be important to
your application. Getting at the body data is a little more involved than
accessing request headers. The request object that's passed in to a handler
implements the ReadableStream interface. This stream can be listened to or
piped elsewhere just like any other stream. We can grab the data right out of
the stream by listening to the stream's 'data' and 'end' events.
The chunk emitted in each 'data' event is a Buffer. If you know it's
going to be string data, the best thing to do is collect the data in an array,
then at the 'end', concatenate and stringify it.
let body = [];
request
  .on('data', chunk => {
    body.push(chunk);
  })
  .on('end', () => {
    body = Buffer.concat(body).toString();
    // at this point, `body` has the entire request body stored in it as a string
  });
JavaScriptCopy to clipboard

This may seem a tad tedious, and in many cases, it is. Luckily,
there are modules like concat-stream and body on npm which can
help hide away some of this logic. It's important to have a good understanding
of what's going on before going down that road, and that's why you're here!

A Quick Thing About Errors
Since the request object is a ReadableStream, it's also an
EventEmitter and behaves like one when an error happens.
An error in the request stream presents itself by emitting an 'error' event
on the stream. If you don't have a listener for that event, the error will be
thrown, which could crash your Node.js program. You should therefore add an
'error' listener on your request streams, even if you just log it and
continue on your way. (Though it's probably best to send some kind of HTTP error
response. More on that later.)
request.on('error', err => {
  // This prints the error message and stack trace to `stderr`.
  console.error(err.stack);
});
JavaScriptCopy to clipboard
There are other ways of handling these errors such as
other abstractions and tools, but always be aware that errors can and do happen,
and you're going to have to deal with them.
What We've Got so Far
At this point, we've covered creating a server, and grabbing the method, URL,
headers and body out of requests. When we put that all together, it might look
something like this:
CJSMJSconst http = require('node:http');

http
  .createServer((request, response) => {
    const { headers, method, url } = request;
    let body = [];
    request
      .on('error', err => {
        console.error(err);
      })
      .on('data', chunk => {
        body.push(chunk);
      })
      .on('end', () => {
        body = Buffer.concat(body).toString();
        // At this point, we have the headers, method, url and body, and can now
        // do whatever we need to in order to respond to this request.
      });
  })
  .listen(8080); // Activates this server, listening on port 8080.
JavaScriptCopy to clipboardIf we run this example, we'll be able to receive requests, but not respond
to them. In fact, if you hit this example in a web browser, your request would
time out, as nothing is being sent back to the client.
So far we haven't touched on the response object at all, which is an instance
of ServerResponse, which is a WritableStream. It contains many
useful methods for sending data back to the client. We'll cover that next.
HTTP Status Code
If you don't bother setting it, the HTTP status code on a response will always
be 200. Of course, not every HTTP response warrants this, and at some point
you'll definitely want to send a different status code. To do that, you can set
the statusCode property.
response.statusCode = 404; // Tell the client that the resource wasn't found.
JavaScriptCopy to clipboard
There are some other shortcuts to this, as we'll see soon.
Setting Response Headers
Headers are set through a convenient method called setHeader.
response.setHeader('Content-Type', 'application/json');
response.setHeader('X-Powered-By', 'bacon');
JavaScriptCopy to clipboard
When setting the headers on a response, the case is insensitive on their names.
If you set a header repeatedly, the last value you set is the value that gets
sent.
Explicitly Sending Header Data
The methods of setting the headers and status code that we've already discussed
assume that you're using "implicit headers". This means you're counting on node
to send the headers for you at the correct time before you start sending body
data.
If you want, you can explicitly write the headers to the response stream.
To do this, there's a method called writeHead, which writes the status
code and the headers to the stream.
response.writeHead(200, {
  'Content-Type': 'application/json',
  'X-Powered-By': 'bacon',
});
JavaScriptCopy to clipboard
Once you've set the headers (either implicitly or explicitly), you're ready to
start sending response data.
Sending Response Body
Since the response object is a WritableStream, writing a response body
out to the client is just a matter of using the usual stream methods.
response.write('<html>');
response.write('<body>');
response.write('<h1>Hello, World!</h1>');
response.write('</body>');
response.write('</html>');
response.end();
JavaScriptCopy to clipboard
The end function on streams can also take in some optional data to send as the
last bit of data on the stream, so we can simplify the example above as follows.
response.end('<html><body><h1>Hello, World!</h1></body></html>');
JavaScriptCopy to clipboard

It's important to set the status and headers before you start
writing chunks of data to the body. This makes sense, since headers come before
the body in HTTP responses.

Another Quick Thing About Errors
The response stream can also emit 'error' events, and at some point you're
going to have to deal with that as well. All of the advice for request stream
errors still applies here.
Put It All Together
Now that we've learned about making HTTP responses, let's put it all together.
Building on the earlier example, we're going to make a server that sends back
all of the data that was sent to us by the user. We'll format that data as JSON
using JSON.stringify.
CJSMJSconst http = require('node:http');

http
  .createServer((request, response) => {
    const { headers, method, url } = request;
    let body = [];
    request
      .on('error', err => {
        console.error(err);
      })
      .on('data', chunk => {
        body.push(chunk);
      })
      .on('end', () => {
        body = Buffer.concat(body).toString();
        // BEGINNING OF NEW STUFF

        response.on('error', err => {
          console.error(err);
        });

        response.statusCode = 200;
        response.setHeader('Content-Type', 'application/json');
        // Note: the 2 lines above could be replaced with this next one:
        // response.writeHead(200, {'Content-Type': 'application/json'})

        const responseBody = { headers, method, url, body };

        response.write(JSON.stringify(responseBody));
        response.end();
        // Note: the 2 lines above could be replaced with this next one:
        // response.end(JSON.stringify(responseBody))

        // END OF NEW STUFF
      });
  })
  .listen(8080);
JavaScriptCopy to clipboardEcho Server Example
Let's simplify the previous example to make a simple echo server, which just
sends whatever data is received in the request right back in the response. All
we need to do is grab the data from the request stream and write that data to
the response stream, similar to what we did previously.
CJSMJSconst http = require('node:http');

http
  .createServer((request, response) => {
    let body = [];
    request
      .on('data', chunk => {
        body.push(chunk);
      })
      .on('end', () => {
        body = Buffer.concat(body).toString();
        response.end(body);
      });
  })
  .listen(8080);
JavaScriptCopy to clipboardNow let's tweak this. We want to only send an echo under the following
conditions:

The request method is POST.
The URL is /echo.

In any other case, we want to simply respond with a 404.
CJSMJSconst http = require('node:http');

http
  .createServer((request, response) => {
    if (request.method === 'POST' && request.url === '/echo') {
      let body = [];
      request
        .on('data', chunk => {
          body.push(chunk);
        })
        .on('end', () => {
          body = Buffer.concat(body).toString();
          response.end(body);
        });
    } else {
      response.statusCode = 404;
      response.end();
    }
  })
  .listen(8080);
JavaScriptCopy to clipboard
By checking the URL in this way, we're doing a form of "routing".
Other forms of routing can be as simple as switch statements or as complex as
whole frameworks like express. If you're looking for something that does
routing and nothing else, try router.

Great! Now let's take a stab at simplifying this. Remember, the request object
is a ReadableStream and the response object is a WritableStream.
That means we can use pipe to direct data from one to the other. That's
exactly what we want for an echo server!
CJSMJSconst http = require('node:http');

http
  .createServer((request, response) => {
    if (request.method === 'POST' && request.url === '/echo') {
      request.pipe(response);
    } else {
      response.statusCode = 404;
      response.end();
    }
  })
  .listen(8080);
JavaScriptCopy to clipboardYay streams!
We're not quite done yet though. As mentioned multiple times in this guide,
errors can and do happen, and we need to deal with them.
To handle errors on the request stream, we'll log the error to stderr and send
a 400 status code to indicate a Bad Request. In a real-world application,
though, we'd want to inspect the error to figure out what the correct status code
and message would be. As usual with errors, you should consult the
Error documentation.
On the response, we'll just log the error to stderr.
CJSMJSconst http = require('node:http');

http
  .createServer((request, response) => {
    request.on('error', err => {
      console.error(err);
      response.statusCode = 400;
      response.end();
    });
    response.on('error', err => {
      console.error(err);
    });
    if (request.method === 'POST' && request.url === '/echo') {
      request.pipe(response);
    } else {
      response.statusCode = 404;
      response.end();
    }
  })
  .listen(8080);
JavaScriptCopy to clipboardWe've now covered most of the basics of handling HTTP requests. At this point,
you should be able to:

Instantiate an HTTP server with a request handler function, and have it listen
on a port.
Get headers, URL, method and body data from request objects.
Make routing decisions based on URL and/or other data in request objects.
Send headers, HTTP status codes and body data via response objects.
Pipe data from request objects and to response objects.
Handle stream errors in both the request and response streams.

From these basics, Node.js HTTP servers for many typical use cases can be
constructed. There are plenty of other things these APIs provide, so be sure to
read through the API docs for EventEmitters, Streams, and HTTP.PrevHow to publish a Node-API packageNextABI Stability\n\n\n\nABI Stability
Introduction
An Application Binary Interface (ABI) is a way for programs to call functions
and use data structures from other compiled programs. It is the compiled version
of an Application Programming Interface (API). In other words, the headers files
describing the classes, functions, data structures, enumerations, and constants
which enable an application to perform a desired task correspond by way of
compilation to a set of addresses and expected parameter values and memory
structure sizes and layouts with which the provider of the ABI was compiled.
The application using the ABI must be compiled such that the available
addresses, expected parameter values, and memory structure sizes and layouts
agree with those with which the ABI provider was compiled. This is usually
accomplished by compiling against the headers provided by the ABI provider.
Since the provider of the ABI and the user of the ABI may be compiled at
different times with different versions of the compiler, a portion of the
responsibility for ensuring ABI compatibility lies with the compiler. Different
versions of the compiler, perhaps provided by different vendors, must all
produce the same ABI from a header file with a certain content, and must produce
code for the application using the ABI that accesses the API described in a
given header according to the conventions of the ABI resulting from the
description in the header. Modern compilers have a fairly good track record of
not breaking the ABI compatibility of the applications they compile.
The remaining responsibility for ensuring ABI compatibility lies with the team
maintaining the header files which provide the API that results, upon
compilation, in the ABI that is to remain stable. Changes to the header files
can be made, but the nature of the changes has to be closely tracked to ensure
that, upon compilation, the ABI does not change in a way that will render
existing users of the ABI incompatible with the new version.
ABI Stability in Node.js
Node.js provides header files maintained by several independent teams. For
example, header files such as node.h and node_buffer.h are maintained by
the Node.js team. v8.h is maintained by the V8 team, which, although in close
co-operation with the Node.js team, is independent, and with its own schedule
and priorities. Thus, the Node.js team has only partial control over the
changes that are introduced in the headers the project provides. As a result,
the Node.js project has adopted semantic versioning.
This ensures that the APIs provided by the project will result in a stable ABI
for all minor and patch versions of Node.js released within one major version.
In practice, this means that the Node.js project has committed itself to
ensuring that a Node.js native addon compiled against a given major version of
Node.js will load successfully when loaded by any Node.js minor or patch version
within the major version against which it was compiled.
N-API
Demand has arisen for equipping Node.js with an API that results in an ABI that
remains stable across multiple Node.js major versions. The motivation for
creating such an API is as follows:


The JavaScript language has remained compatible with itself since its very
early days, whereas the ABI of the engine executing the JavaScript code changes
with every major version of Node.js. This means that applications consisting of
Node.js packages written entirely in JavaScript need not be recompiled,
reinstalled, or redeployed as a new major version of Node.js is dropped into
the production environment in which such applications run. In contrast, if an
application depends on a package that contains a native addon, the application
has to be recompiled, reinstalled, and redeployed whenever a new major version
of Node.js is introduced into the production environment. This disparity
between Node.js packages containing native addons and those that are written
entirely in JavaScript has added to the maintenance burden of production
systems which rely on native addons.


Other projects have started to produce JavaScript interfaces that are
essentially alternative implementations of Node.js. Since these projects are
usually built on a different JavaScript engine than V8, their native addons
necessarily take on a different structure and use a different API. Nevertheless,
using a single API for a native addon across different implementations of the
Node.js JavaScript API would allow these projects to take advantage of the
ecosystem of JavaScript packages that has accrued around Node.js.


Node.js may contain a different JavaScript engine in the future. This means
that, externally, all Node.js interfaces would remain the same, but the V8
header file would be absent. Such a step would cause the disruption of the
Node.js ecosystem in general, and that of the native addons in particular, if
an API that is JavaScript engine agnostic is not first provided by Node.js and
adopted by native addons.


To these ends Node.js has introduced N-API in version 8.6.0 and marked it as a
stable component of the project as of Node.js 8.12.0. The API is defined in the
headers node_api.h and node_api_types.h, and provides a forward-
compatibility guarantee that crosses the Node.js major version boundary. The
guarantee can be stated as follows:
A given version n of N-API will be available in the major version of
Node.js in which it was published, and in all subsequent versions of Node.js,
including subsequent major versions.
A native addon author can take advantage of the N-API forward compatibility
guarantee by ensuring that the addon makes use only of APIs defined in
node_api.h and data structures and constants defined in node_api_types.h.
By doing so, the author facilitates adoption of their addon by indicating to
production users that the maintenance burden for their application will increase
no more by the addition of the native addon to their project than it would by
the addition of a package written purely in JavaScript.
N-API is versioned because new APIs are added from time to time. Unlike
semantic versioning, N-API versioning is cumulative. That is, each version of
N-API conveys the same meaning as a minor version in the semver system, meaning
that all changes made to N-API will be backwards compatible. Additionally, new
N-APIs are added under an experimental flag to give the community an opportunity
to vet them in a production environment. Experimental status means that,
although care has been taken to ensure that the new API will not have to be
modified in an ABI-incompatible way in the future, it has not yet been
sufficiently proven in production to be correct and useful as designed and, as
such, may undergo ABI-incompatible changes before it is finally incorporated
into a forthcoming version of N-API. That is, an experimental N-API is not yet
covered by the forward compatibility guarantee.PrevAnatomy of an HTTP TransactionNextHow to use streams\n\n\n\nHow To Use Streams
Working with large amounts of data in Node.js applications can be a double-edged sword. The ability to handle massive amounts of data is extremely handy but can also lead to performance bottlenecks and memory exhaustion. Traditionally, developers tackled this challenge by reading the entire dataset into memory at once. This approach, while intuitive for smaller datasets, becomes inefficient and resource-intensive for large data (e.g., files, network requests…).
This is where Node.js streams come in. Streams offer a fundamentally different approach, allowing you to process data incrementally and optimize memory usage. By handling data in manageable chunks, streams empower you to build scalable applications that can efficiently tackle even the most daunting datasets. As popularly quoted, “streams are arrays over time.”
In this guide, we give an overview of the Stream concept, history, and API as well as some recommendations on how to use and operate them.
What are Node.js Streams?
Node.js streams offer a powerful abstraction for managing data flow in your applications. They excel at processing large datasets, such as reading or writing from files and network requests, without compromising performance.
This approach differs from loading the entire dataset into memory at once. Streams process data in chunks, significantly reducing memory usage. All streams in Node.js inherit from the EventEmitter class, allowing them to emit events at various stages of data processing. These streams can be readable, writable, or both, providing flexibility for different data-handling scenarios.
Event-Driven Architecture
Node.js thrives on an event-driven architecture, making it ideal for real-time I/O. This means consuming input as soon as it's available and sending output as soon as the application generates it. Streams seamlessly integrate with this approach, enabling continuous data processing.
They achieve this by emitting events at key stages. These events include signals for received data (data event) and the stream's completion (end event). Developers can listen to these events and execute custom logic accordingly. This event-driven nature makes streams highly efficient for the processing of data from external sources.
Why use Streams?
Streams provide three key advantages over other data-handling methods:

Memory Efficiency: Streams process data incrementally, consuming and processing data in chunks rather than loading the entire dataset into memory. This is a major advantage when dealing with large datasets, as it significantly reduces memory usage and prevents memory-related performance issues.
Improved Response Time: Streams allow for immediate data processing. When a chunk of data arrives, it can be processed without waiting for the entire payload or dataset to be received. This reduces latency and improves your application's overall responsiveness.
Scalability for Real-Time Processing: By handling data in chunks, Node.js streams can efficiently handle large amounts of data with limited resources. This scalability makes streams ideal for applications that process high volumes of data in real time.

These advantages make streams a powerful tool for building high-performance, scalable Node.js applications, particularly when working with large datasets or real-time data processing.
Note on performance
If your application already has all the data readily available in memory, using streams might add unnecessary overhead, complexity, and slow down your application.
Stream history
This section is a reference of the history of streams in Node.js. Unless you’re working with a codebase written for a Node.js version prior to 0.11.5 (2013), you will rarely encounter older versions of the streams API, but the terms might still be in use.
Streams 0
The first version of streams was released at the same time as Node.js. Although there wasn't a Stream class yet, different modules used the concept and implemented the read/write functions. The util.pump() function was available to control the flow of data between streams.
Streams 1 (Classic)
With the release of Node v0.4.0 in 2011, the Stream class was introduced, as well as the pipe() method.
Streams 2
In 2012, with the release of Node v0.10.0, Streams 2 were unveiled. This update brought new stream subclasses, including Readable, Writable, Duplex, and Transform. Additionally, the readable event was added. To maintain backwards compatibility, streams could be switched to the old mode by adding a data event listener or calling pause() or resume() methods.
Streams 3
In 2013, Streams 3 were released with Node v0.11.5, to address the problem of a stream having both a data and readable event handlers. This removed the need to choose between 'current' and 'old' modes. Streams 3 is the current version of streams in Node.js.
Stream types
Readable
Readable is the class that we use to sequentially read a source of data. Typical examples of Readable streams in Node.js API are fs.ReadStream  when reading files, http.IncomingMessage  when reading HTTP requests, and process.stdin  when reading from the standard input.
Key Methods and Events
A readable stream operates with several core methods and events that allow fine control over data handling:

on('data'): This event is triggered whenever data is available from the stream. It is very fast, as the stream pushes data as quickly as it can handle, making it suitable for high-throughput scenarios.
on('end'): Emitted when there is no more data to read from the stream. It signifies the completion of data delivery. This event is only fired when all the data from the stream has been consumed.
on('readable'): This event is triggered when there is data available to read from the stream or when the end of the stream has been reached. It allows for more controlled data reading when needed.
on('close'): This event is emitted when the stream and its underlying resources have been closed and indicates that no more events will be emitted.
on('error'): This event can be emitted at any point, signaling that there was an error processing. A handler for this event can be used to avoid uncaught exceptions.

A demonstration of the use of these events can be seen in the following sections.
Basic Readable Stream
Here's an example of a simple readable stream implementation that generates data dynamically:
CJSMJSconst { Readable } = require('node:stream');

class MyStream extends Readable {
  #count = 0;
  _read(size) {
    this.push(':-)');
    if (++this.#count === 5) {
      this.push(null);
    }
  }
}

const stream = new MyStream();

stream.on('data', chunk => {
  console.log(chunk.toString());
});
JavaScriptCopy to clipboardIn this code, the MyStream class extends Readable and overrides the [_read][] method to push a string ":-)" to the internal buffer. After pushing the string five times, it signals the end of the stream by pushing null. The on('data') event handler logs each chunk to the console as it is received.
Advanced Control with the readable Event
For even finer control over data flow, the readable event can be used. This event is more complex but provides better performance for certain applications by allowing explicit control over when data is read from the stream:
CJSMJSconst stream = new MyStream({
  highWaterMark: 1,
});

stream.on('readable', () => {
  console.count('>> readable event');
  let chunk;
  while ((chunk = stream.read()) !== null) {
    console.log(chunk.toString()); // Process the chunk
  }
});
stream.on('end', () => console.log('>> end event'));
JavaScriptCopy to clipboardHere, the readable event is used to pull data from the stream as needed manually. The loop inside the readable event handler continues to read data from the stream buffer until it returns null, indicating that the buffer is temporarily empty or the stream has ended. Setting highWaterMark to 1 keeps the buffer size small, triggering the readable event more frequently and allowing more granular control over the data flow.
With the previous code, you’ll get an output like
>> readable event: 1
:-):-)
:-)
:-)
:-)
>> readable event: 2
>> readable event: 3
>> readable event: 4
>> end event
ShellCopy to clipboard
Let’s try to digest that. When we attach the on('readable') event, it makes a first call to read() because that is what might trigger the emission of a readable event. After the emission of said event, we call read on the first iteration of the while loop. That’s why we get the first two smileys in one row. After that, we keep calling read until null is pushed. Each call to read programs the emission of a new readable event, but as we are in “flow” mode (i.e., using the readable event), the emission is scheduled for the nextTick. That’s why we get them all at the end, when the synchronous code of the loop is finished.
NOTE: You can try to run the code with NODE_DEBUG=stream to see that emitReadable is triggered after each push.
If we want to see readable events called before each smiley, we can wrap push into a setImmediate or process.nextTick like this:
CJSMJSclass MyStream extends Readable {
  #count = 0;
  _read(size) {
    setImmediate(() => {
      this.push(':-)');
      if (++this.#count === 5) {
        return this.push(null);
      }
    });
  }
}
JavaScriptCopy to clipboardAnd we’ll get:
>> readable event: 1
:-)
>> readable event: 2
:-)
>> readable event: 3
:-)
>> readable event: 4
:-)
>> readable event: 5
:-)
>> readable event: 6
>> end event
ShellCopy to clipboard
Writable
Writable streams are useful for creating files, uploading data, or any task that involves sequentially outputting data. While readable streams provide the source of data, writable streams in Node.js act as the destination for your data. Typical examples of writable streams in the Node.js API are fs.WriteStream , process.stdout , and process.stderr .
Key Methods and Events in Writable Streams

.write(): This method is used to write a chunk of data to the stream. It handles the data by buffering it up to a defined limit (highWaterMark), and returns a boolean indicating whether more data can be written immediately.
.end(): This method signals the end of the data writing process. It signals the stream to complete the write operation and potentially perform any necessary cleanup.

Creating a Writable
Here's an example of creating a writable stream that converts all incoming data to uppercase before writing it to the standard output:
CJSMJSconst { Writable } = require('node:stream');
const { once } = require('node:events');

class MyStream extends Writable {
  constructor() {
    super({ highWaterMark: 10 /* 10 bytes */ });
  }
  _write(data, encode, cb) {
    process.stdout.write(data.toString().toUpperCase() + '\n', cb);
  }
}

async function main() {
  const stream = new MyStream();

  for (let i = 0; i < 10; i++) {
    const waitDrain = !stream.write('hello');

    if (waitDrain) {
      console.log('>> wait drain');
      await once(stream, 'drain');
    }
  }

  stream.end('world');
}

// Call the async function
main().catch(console.error);
JavaScriptCopy to clipboardIn this code, MyStream is a custom Writable stream with a buffer capacity (highWaterMark) of 10 bytes. It overrides the _write method to convert data to uppercase before writing it out.
The loop attempts to write hello ten times to the stream. If the buffer fills up (waitDrain becomes true), it waits for a drain event before continuing, ensuring we do not overwhelm the stream's buffer.
The output will be:
HELLO
>> wait drain
HELLO
HELLO
>> wait drain
HELLO
HELLO
>> wait drain
HELLO
HELLO
>> wait drain
HELLO
HELLO
>> wait drain
HELLO
WORLD
ShellCopy to clipboard
Duplex
Duplex streams implement both the readable and writable interfaces.
Key Methods and Events in Duplex Streams
Duplex streams implement all the methods and events described in Readable and Writable Streams.
A good example of a duplex stream is the Socket class in the net module:
CJSMJSconst net = require('node:net');

// Create a TCP server
const server = net.createServer(socket => {
  socket.write('Hello from server!\n');

  socket.on('data', data => {
    console.log(`Client says: ${data.toString()}`);
  });

  // Handle client disconnection
  socket.on('end', () => {
    console.log('Client disconnected');
  });
});

// Start the server on port 8080
server.listen(8080, () => {
  console.log('Server listening on port 8080');
});
JavaScriptCopy to clipboardThe previous code will open a TCP socket on port 8080, send Hello from server! to any connecting client, and log any data received.
CJSMJSconst net = require('node:net');

// Connect to the server at localhost:8080
const client = net.createConnection({ port: 8080 }, () => {
  client.write('Hello from client!\n');
});

client.on('data', data => {
  console.log(`Server says: ${data.toString()}`);
});

// Handle the server closing the connection
client.on('end', () => {
  console.log('Disconnected from server');
});
JavaScriptCopy to clipboardThe previous code will connect to the TCP socket, send a Hello from client message, and log any received data.
Transform
Transform streams are duplex streams, where the output is computed based on the input. As the name suggests, they are usually used between a readable and a writable stream to transform the data as it passes through.
Key Methods and Events in Transform Streams
Apart from all the methods and events in Duplex Streams, there is:

_transform: This function is called internally to handle the flow of data between the readable and writable parts. This MUST NOT be called by application code.

Creating a Transform Stream
To create a new transform stream, we can pass an options object to the Transform constructor, including a transform function that handles how the output data is computed from the input data using the push method.
CJSMJSconst { Transform } = require('node:stream');

const upper = new Transform({
  transform: function (data, enc, cb) {
    this.push(data.toString().toUpperCase());
    cb();
  },
});
JavaScriptCopy to clipboardThis stream will take any input and output it in uppercase.
How to operate with streams
When working with streams, we usually want to read from a source and write to a destination, possibly needing some transformation of the data in between. The following sections will cover different ways to do so.
.pipe()
The .pipe() method concatenates one readable stream to a writable (or transform) stream. Although this seems like a simple way to achieve our goal, it delegates all error handling to the programmer, making it difficult to get it right.
The following example shows a pipe trying to output the current file in uppercase to the console.
CJSMJSconst fs = require('node:fs');
const { Transform } = require('node:stream');

let errorCount = 0;
const upper = new Transform({
  transform: function (data, enc, cb) {
    if (errorCount === 10) {
      return cb(new Error('BOOM!'));
    }
    errorCount++;
    this.push(data.toString().toUpperCase());
    cb();
  },
});

const readStream = fs.createReadStream(__filename, { highWaterMark: 1 });
const writeStream = process.stdout;

readStream.pipe(upper).pipe(writeStream);

readStream.on('close', () => {
  console.log('Readable stream closed');
});

upper.on('close', () => {
  console.log('Transform stream closed');
});

upper.on('error', err => {
  console.error('\nError in transform stream:', err.message);
});

writeStream.on('close', () => {
  console.log('Writable stream closed');
});
JavaScriptCopy to clipboardAfter writing 10 characters, upper will return an error in the callback, which will cause the stream to close. However, the other streams won’t be notified, resulting in memory leaks. The output will be:
CONST FS =
Error in transform stream: BOOM!
Transform stream closed
ShellCopy to clipboard
pipeline()
To avoid the pitfalls and low-level complexity of the .pipe() method, in most cases, it is recommended to use the pipeline() method. This method is a safer and more robust way to pipe streams together, handling errors and cleanup automatically.
The following example demonstrates how using pipeline() prevents the pitfalls of the previous example:
CJSMJSconst fs = require('node:fs');
const { Transform, pipeline } = require('node:stream');

let errorCount = 0;
const upper = new Transform({
  transform: function (data, enc, cb) {
    if (errorCount === 10) {
      return cb(new Error('BOOM!'));
    }
    errorCount++;
    this.push(data.toString().toUpperCase());
    cb();
  },
});

const readStream = fs.createReadStream(__filename, { highWaterMark: 1 });
const writeStream = process.stdout;

readStream.on('close', () => {
  console.log('Readable stream closed');
});

upper.on('close', () => {
  console.log('\nTransform stream closed');
});

writeStream.on('close', () => {
  console.log('Writable stream closed');
});

pipeline(readStream, upper, writeStream, err => {
  if (err) {
    return console.error('Pipeline error:', err.message);
  }
  console.log('Pipeline succeeded');
});
JavaScriptCopy to clipboardIn this case, all streams will be closed with the following output:
CONST FS =
Transform stream closed
Writable stream closed
Pipeline error: BOOM!
Readable stream closed
ShellCopy to clipboard
The pipeline() method also has an async pipeline() version, which doesn’t accept a callback but instead returns a promise that is rejected if the pipeline fails.
Async Iterators
Async iterators are recommended as the standard way of interfacing with the Streams API. Compared to all the stream primitives in both the Web and Node.js, async iterators are easier to understand and use, contributing to fewer bugs and more maintainable code. In recent versions of Node.js, async iterators have emerged as a more elegant and readable way to interact with streams. Building upon the foundation of events, async iterators provide a higher-level abstraction that simplifies stream consumption.
In Node.js, all readable streams are asynchronous iterables. This means you can use the for await...of syntax to loop through the stream's data as it becomes available, handling each piece of data with the efficiency and simplicity of asynchronous code.
Benefits of Using Async Iterators with Streams
Using async iterators with streams simplifies the handling of asynchronous data flows in several ways:

Enhanced Readability: The code structure is cleaner and more readable, particularly when dealing with multiple asynchronous data sources.
Error Handling: Async iterators allow straightforward error handling using try/catch blocks, akin to regular asynchronous functions.
Flow Control: They inherently manage backpressure, as the consumer controls the flow by awaiting the next piece of data, allowing for more efficient memory usage and processing.

Async iterators offer a more modern and often more readable way to work with readable streams, especially when dealing with asynchronous data sources or when you prefer a more sequential, loop-based approach to data processing.
Here's an example demonstrating the use of async iterators with a readable stream:
CJSMJSconst fs = require('node:fs');
const { pipeline } = require('node:stream/promises');

async function main() {
  await pipeline(
    fs.createReadStream(__filename),
    async function* (source) {
      for await (let chunk of source) {
        yield chunk.toString().toUpperCase();
      }
    },
    process.stdout
  );
}

main().catch(console.error);
JavaScriptCopy to clipboardThis code achieves the same result as the previous examples, without the need to define a new transform stream. The error from the previous examples has been removed for the sake of brevity. The async version of the pipeline has been used, and it should be wrapped in a try...catch block to handle possible errors.
Object mode
By default, streams can work with strings, Buffer, TypedArray, or DataView. If an arbitrary value different from these (e.g., an object) is pushed into a stream, a TypeError will be thrown. However, it is possible to work with objects by setting the objectMode option to true. This allows the stream to work with any JavaScript value, except for null, which is used to signal the end of the stream. This means you can push and read any value in a readable stream, and write any value in a writable stream.
CJSMJSconst { Readable } = require('node:stream');

const readable = Readable({
  objectMode: true,
  read() {
    this.push({ hello: 'world' });
    this.push(null);
  },
});
JavaScriptCopy to clipboardWhen working in object mode, it is important to remember that the highWaterMark option refers to the number of objects, not bytes.
Backpressure
When using streams, it is important to make sure the producer doesn't overwhelm the consumer. For this, the backpressure mechanism is used in all streams in the Node.js API, and implementors are responsible for maintaining that behavior.
In any scenario where the data buffer has exceeded the highWaterMark or the write queue is currently busy, .write() will return false.
When a false value is returned, the backpressure system kicks in. It will pause the incoming Readable stream from sending any data and wait until the consumer is ready again. Once the data buffer is emptied, a 'drain' event will be emitted to resume the incoming data flow.
For a deeper understanding of backpressure, check the backpressure guide.
Streams vs Web streams
The stream concept is not exclusive to Node.js. In fact, Node.js has a different implementation of the stream concept called Web Streams, which implements the WHATWG Streams Standard. Although the concepts behind them are similar, it is important to be aware that they have different APIs and are not directly compatible.
Web Streams implement the ReadableStream, WritableStream, and TransformStream classes, which are homologous to Node.js's Readable, Writable, and Transform streams.
Interoperability of streams and Web Streams
Node.js provides utility functions to convert to/from Web Streams and Node.js streams. These functions are implemented as toWeb and fromWeb methods in each stream class.
The following example in the Duplex class demonstrates how to work with both readable and writable streams converted to Web Streams:
CJSMJSconst { Duplex } = require('node:stream');

const duplex = Duplex({
  read() {
    this.push('world');
    this.push(null);
  },
  write(chunk, encoding, callback) {
    console.log('writable', chunk);
    callback();
  },
});

const { readable, writable } = Duplex.toWeb(duplex);
writable.getWriter().write('hello');

readable
  .getReader()
  .read()
  .then(result => {
    console.log('readable', result.value);
  });
JavaScriptCopy to clipboardThe helper functions are useful if you need to return a Web Stream from a Node.js module or vice versa. For regular consumption of streams, async iterators enable seamless interaction with both Node.js and Web Streams.
CJSMJSconst { pipeline } = require('node:stream/promises');

async function main() {
  const { body } = await fetch('https://nodejs.org/api/stream.html');

  await pipeline(
    body,
    new TextDecoderStream(),
    async function* (source) {
      for await (const chunk of source) {
        yield chunk.toString().toUpperCase();
      }
    },
    process.stdout
  );
}

main().catch(console.error);
JavaScriptCopy to clipboardBe aware that the fetch body is a ReadableStream<Uint8Array>, and therefore a TextDecoderStream is needed to work with chunks as strings.
This work is derived from content published by Matteo Collina in Platformatic's Blog.PrevABI StabilityNextBackpressuring in Streams\n\n\n\nBackpressuring in Streams
There is a general problem that occurs during data handling called
backpressure and describes a buildup of data behind a buffer during data
transfer. When the receiving end of the transfer has complex operations, or is
slower for whatever reason, there is a tendency for data from the incoming
source to accumulate, like a clog.
To solve this problem, there must be a delegation system in place to ensure a
smooth flow of data from one source to another. Different communities have
resolved this issue uniquely to their programs, Unix pipes and TCP sockets are
good examples of this, and are often referred to as flow control. In
Node.js, streams have been the adopted solution.
The purpose of this guide is to further detail what backpressure is, and how
exactly streams address this in Node.js' source code. The second part of
the guide will introduce suggested best practices to ensure your application's
code is safe and optimized when implementing streams.
We assume a little familiarity with the general definition of
backpressure, Buffer, and EventEmitters in Node.js, as well as
some experience with Stream. If you haven't read through those docs,
it's not a bad idea to take a look at the API documentation first, as it will
help expand your understanding while reading this guide.
The Problem with Data Handling
In a computer system, data is transferred from one process to another through
pipes, sockets, and signals. In Node.js, we find a similar mechanism called
Stream. Streams are great! They do so much for Node.js and almost every
part of the internal codebase utilizes that module. As a developer, you
are more than encouraged to use them too!
CJSMJSconst readline = require('node:readline');

// process.stdin and process.stdout are both instances of Streams.
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

rl.question('Why should you use streams? ', answer => {
  console.log(`Maybe it's ${answer}, maybe it's because they are awesome! :)`);

  rl.close();
});
JavaScriptCopy to clipboardA good example of why the backpressure mechanism implemented through streams is
a great optimization can be demonstrated by comparing the internal system tools
from Node.js' Stream implementation.
In one scenario, we will take a large file (approximately ~9 GB) and compress it
using the familiar zip(1) tool.
zip The.Matrix.1080p.mkv

While that will take a few minutes to complete, in another shell we may run
a script that takes Node.js' module zlib, that wraps around another
compression tool, gzip(1).
CJSMJSconst gzip = require('node:zlib').createGzip();
const fs = require('node:fs');

const inp = fs.createReadStream('The.Matrix.1080p.mkv');
const out = fs.createWriteStream('The.Matrix.1080p.mkv.gz');

inp.pipe(gzip).pipe(out);
JavaScriptCopy to clipboardTo test the results, try opening each compressed file. The file compressed by
the zip(1) tool will notify you the file is corrupt, whereas the
compression finished by Stream will decompress without error.

In this example, we use .pipe() to get the data source from one end
to the other. However, notice there are no proper error handlers attached. If
a chunk of data were to fail to be properly received, the Readable source or
gzip stream will not be destroyed. pump is a utility tool that would
properly destroy all the streams in a pipeline if one of them fails or closes,
and is a must-have in this case!

pump is only necessary for Node.js 8.x or earlier, as for Node.js 10.x
or later version, pipeline is introduced to replace for pump.
This is a module method to pipe between streams forwarding errors and properly
cleaning up and providing a callback when the pipeline is complete.
Here is an example of using pipeline:
CJSMJSconst { pipeline } = require('node:stream');
const fs = require('node:fs');
const zlib = require('node:zlib');

// Use the pipeline API to easily pipe a series of streams
// together and get notified when the pipeline is fully done.
// A pipeline to gzip a potentially huge video file efficiently:

pipeline(
  fs.createReadStream('The.Matrix.1080p.mkv'),
  zlib.createGzip(),
  fs.createWriteStream('The.Matrix.1080p.mkv.gz'),
  err => {
    if (err) {
      console.error('Pipeline failed', err);
    } else {
      console.log('Pipeline succeeded');
    }
  }
);
JavaScriptCopy to clipboardYou can also use the stream/promises module to use pipeline with async / await:
CJSMJSconst { pipeline } = require('node:stream/promises');
const fs = require('node:fs');
const zlib = require('node:zlib');

async function run() {
  try {
    await pipeline(
      fs.createReadStream('The.Matrix.1080p.mkv'),
      zlib.createGzip(),
      fs.createWriteStream('The.Matrix.1080p.mkv.gz')
    );
    console.log('Pipeline succeeded');
  } catch (err) {
    console.error('Pipeline failed', err);
  }
}
JavaScriptCopy to clipboardToo Much Data, Too Quickly
There are instances where a Readable stream might give data to the
Writable much too quickly — much more than the consumer can handle!
When that occurs, the consumer will begin to queue all the chunks of data for
later consumption. The write queue will get longer and longer, and because of
this more data must be kept in memory until the entire process has been completed.
Writing to a disk is a lot slower than reading from a disk, thus, when we are
trying to compress a file and write it to our hard disk, backpressure will
occur because the write disk will not be able to keep up with the speed from
the read.
// Secretly the stream is saying: "whoa, whoa! hang on, this is way too much!"
// Data will begin to build up on the read side of the data buffer as
// `write` tries to keep up with the incoming data flow.
inp.pipe(gzip).pipe(outputFile);
JavaScriptCopy to clipboard
This is why a backpressure mechanism is important. If a backpressure system was
not present, the process would use up your system's memory, effectively slowing
down other processes, and monopolizing a large part of your system until
completion.
This results in a few things:

Slowing down all other current processes
A very overworked garbage collector
Memory exhaustion

In the following examples, we will take out the return value of the
.write() function and change it to true, which effectively disables
backpressure support in Node.js core. In any reference to 'modified' binary,
we are talking about running the node binary without the return ret; line,
and instead with the replaced return true;.
Excess Drag on Garbage Collection
Let's take a look at a quick benchmark. Using the same example from above, we
ran a few time trials to get a median time for both binaries.
   trial (#)  | `node` binary (ms) | modified `node` binary (ms)
=================================================================
      1       |      56924         |           55011
      2       |      52686         |           55869
      3       |      59479         |           54043
      4       |      54473         |           55229
      5       |      52933         |           59723
=================================================================
average time: |      55299         |           55975

Both take around a minute to run, so there's not much of a difference at all,
but let's take a closer look to confirm whether our suspicions are correct. We
use the Linux tool dtrace to evaluate what's happening with the V8 garbage
collector.
The GC (garbage collector) measured time indicates the intervals of a full cycle
of a single sweep done by the garbage collector:
approx. time (ms) | GC (ms) | modified GC (ms)
=================================================
          0       |    0    |      0
          1       |    0    |      0
         40       |    0    |      2
        170       |    3    |      1
        300       |    3    |      1

         *             *           *
         *             *           *
         *             *           *

      39000       |    6    |     26
      42000       |    6    |     21
      47000       |    5    |     32
      50000       |    8    |     28
      54000       |    6    |     35

While the two processes start the same and seem to work the GC at the same
rate, it becomes evident that after a few seconds with a properly working
backpressure system in place, it spreads the GC load across consistent
intervals of 4-8 milliseconds until the end of the data transfer.
However, when a backpressure system is not in place, the V8 garbage collection
starts to drag out. The normal binary called the GC fires approximately 75
times in a minute, whereas, the modified binary fires only 36 times.
This is the slow and gradual debt accumulating from growing memory usage. As
data gets transferred, without a backpressure system in place, more memory is
being used for each chunk transfer.
The more memory that is being allocated, the more the GC has to take care of in
one sweep. The bigger the sweep, the more the GC needs to decide what can be
freed up, and scanning for detached pointers in a larger memory space will
consume more computing power.
Memory Exhaustion
To determine the memory consumption of each binary, we've clocked each process
with /usr/bin/time -lp sudo ./node ./backpressure-example/zlib.js
individually.
This is the output on the normal binary:
Respecting the return value of .write()
=============================================
real        58.88
user        56.79
sys          8.79
  87810048  maximum resident set size
         0  average shared memory size
         0  average unshared data size
         0  average unshared stack size
     19427  page reclaims
      3134  page faults
         0  swaps
         5  block input operations
       194  block output operations
         0  messages sent
         0  messages received
         1  signals received
        12  voluntary context switches
    666037  involuntary context switches

The maximum byte size occupied by virtual memory turns out to be approximately
87.81 mb.
And now changing the return value of the .write() function, we get:
Without respecting the return value of .write():
==================================================
real        54.48
user        53.15
sys          7.43
1524965376  maximum resident set size
         0  average shared memory size
         0  average unshared data size
         0  average unshared stack size
    373617  page reclaims
      3139  page faults
         0  swaps
        18  block input operations
       199  block output operations
         0  messages sent
         0  messages received
         1  signals received
        25  voluntary context switches
    629566  involuntary context switches

The maximum byte size occupied by virtual memory turns out to be approximately
1.52 gb.
Without streams in place to delegate the backpressure, there is an order of
magnitude greater of memory space being allocated - a huge margin of
difference between the same process!
This experiment shows how optimized and cost-effective Node.js' backpressure
mechanism is for your computing system. Now, let's do a breakdown of how it
works!
How Does Backpressure Resolve These Issues?
There are different functions to transfer data from one process to another. In
Node.js, there is an internal built-in function called .pipe(). There are
other packages out there you can use too! Ultimately though, at the basic
level of this process, we have two separate components: the source of the
data and the consumer.
When .pipe() is called from the source, it signals to the consumer that
there is data to be transferred. The pipe function helps to set up the
appropriate backpressure closures for the event triggers.
In Node.js the source is a Readable stream and the consumer is the
Writable stream (both of these may be interchanged with a Duplex or
a Transform stream, but that is out-of-scope for this guide).
The moment that backpressure is triggered can be narrowed exactly to the return
value of a Writable's .write() function. This return value is
determined by a few conditions, of course.
In any scenario where the data buffer has exceeded the highWaterMark or
the write queue is currently busy, .write() will return false.
When a false value is returned, the backpressure system kicks in. It will
pause the incoming Readable stream from sending any data and wait until
the consumer is ready again. Once the data buffer is emptied, a 'drain'
event will be emitted and resume the incoming data flow.
Once the queue is finished, backpressure will allow data to be sent again.
The space in memory that was being used will free itself up and prepare for the
next batch of data.
This effectively allows a fixed amount of memory to be used at any given
time for a .pipe() function. There will be no memory leakage, and no
infinite buffering, and the garbage collector will only have to deal with
one area in memory!
So, if backpressure is so important, why have you (probably) not heard of it?
Well, the answer is simple: Node.js does all of this automatically for you.
That's so great! But also not so great when we are trying to understand how to
implement our custom streams.

In most machines, there is a byte size that determines when a buffer
is full (which will vary across different machines). Node.js allows you to set
your custom highWaterMark, but commonly, the default is set to 16kb
(16384, or 16 for objectMode streams). In instances where you might
want to raise that value, go for it, but do so with caution!

Lifecycle of .pipe()
To achieve a better understanding of backpressure, here is a flow-chart on the
lifecycle of a Readable stream being piped into a Writable
stream:
                                                     +===================+
                         x-->  Piping functions   +-->   src.pipe(dest)  |
                         x     are set up during     |===================|
                         x     the .pipe method.     |  Event callbacks  |
  +===============+      x                           |-------------------|
  |   Your Data   |      x     They exist outside    | .on('close', cb)  |
  +=======+=======+      x     the data flow, but    | .on('data', cb)   |
          |              x     importantly attach    | .on('drain', cb)  |
          |              x     events, and their     | .on('unpipe', cb) |
+---------v---------+    x     respective callbacks. | .on('error', cb)  |
|  Readable Stream  +----+                           | .on('finish', cb) |
+-^-------^-------^-+    |                           | .on('end', cb)    |
  ^       |       ^      |                           +-------------------+
  |       |       |      |
  |       ^       |      |
  ^       ^       ^      |    +-------------------+         +=================+
  ^       |       ^      +---->  Writable Stream  +--------->  .write(chunk)  |
  |       |       |           +-------------------+         +=======+=========+
  |       |       |                                                 |
  |       ^       |                              +------------------v---------+
  ^       |       +-> if (!chunk)                |    Is this chunk too big?  |
  ^       |       |     emit .end();             |    Is the queue busy?      |
  |       |       +-> else                       +-------+----------------+---+
  |       ^       |     emit .write();                   |                |
  |       ^       ^                                   +--v---+        +---v---+
  |       |       ^-----------------------------------<  No  |        |  Yes  |
  ^       |                                           +------+        +---v---+
  ^       |                                                               |
  |       ^               emit .pause();          +=================+     |
  |       ^---------------^-----------------------+  return false;  <-----+---+
  |                                               +=================+         |
  |                                                                           |
  ^            when queue is empty     +============+                         |
  ^------------^-----------------------<  Buffering |                         |
               |                       |============|                         |
               +> emit .drain();       |  ^Buffer^  |                         |
               +> emit .resume();      +------------+                         |
                                       |  ^Buffer^  |                         |
                                       +------------+   add chunk to queue    |
                                       |            <---^---------------------<
                                       +============+


If you are setting up a pipeline to chain together a few streams to
manipulate your data, you will most likely be implementing Transform
stream.

In this case, your output from your Readable stream will enter in the
Transform and will pipe into the Writable.
Readable.pipe(Transformable).pipe(Writable);
JavaScriptCopy to clipboard
Backpressure will be automatically applied, but note that both the incoming and
outgoing highWaterMark of the Transform stream may be manipulated and
will affect the backpressure system.
Backpressure Guidelines
Since Node.js v0.10, the Stream class has offered the ability to
modify the behavior of the .read() or .write() by using the
underscore version of these respective functions (._read() and
._write()).
There are guidelines documented for implementing Readable streams and
implementing Writable streams. We will assume you've read these over, and
the next section will go a little bit more in-depth.
Rules to Abide By When Implementing Custom Streams
The golden rule of streams is to always respect backpressure. What
constitutes as best practice is non-contradictory practice. So long as you are
careful to avoid behaviors that conflict with internal backpressure support,
you can be sure you're following good practice.
In general,

Never .push() if you are not asked.
Never call .write() after it returns false but wait for 'drain' instead.
Streams changes between different Node.js versions, and the library you use.
Be careful and test things.


In regards to point 3, an incredibly useful package for building
browser streams is readable-stream. Rodd Vagg has written a
great blog post describing the utility of this library. In short, it
provides a type of automated graceful degradation for Readable streams,
and supports older versions of browsers and Node.js.

Rules specific to Readable Streams
So far, we have taken a look at how .write() affects backpressure and have
focused much on the Writable stream. Because of Node.js' functionality,
data is technically flowing downstream from Readable to Writable.
However, as we can observe in any transmission of data, matter, or energy, the
source is just as important as the destination, and the Readable stream
is vital to how backpressure is handled.
Both these processes rely on one another to communicate effectively, if
the Readable ignores when the Writable stream asks for it to stop
sending in data, it can be just as problematic as when the .write()'s return
value is incorrect.
So, as well as respecting the .write() return, we must also respect the
return value of .push() used in the ._read() method. If
.push() returns a false value, the stream will stop reading from the
source. Otherwise, it will continue without pause.
Here is an example of bad practice using .push():
// This is problematic as it completely ignores the return value from the push
// which may be a signal for backpressure from the destination stream!
class MyReadable extends Readable {
  _read(size) {
    let chunk;
    while (null !== (chunk = getNextChunk())) {
      this.push(chunk);
    }
  }
}
JavaScriptCopy to clipboard
Additionally, from outside the custom stream, there are pitfalls to ignoring
backpressure. In this counter-example of good practice, the application's code
forces data through whenever it is available (signaled by the
'data' event):
// This ignores the backpressure mechanisms Node.js has set in place,
// and unconditionally pushes through data, regardless if the
// destination stream is ready for it or not.
readable.on('data', data => writable.write(data));
JavaScriptCopy to clipboard
Here's an example of using .push() with a Readable stream.
CJSMJSconst { Readable } = require('node:stream');

// Create a custom Readable stream
const myReadableStream = new Readable({
  objectMode: true,
  read(size) {
    // Push some data onto the stream
    this.push({ message: 'Hello, world!' });
    this.push(null); // Mark the end of the stream
  },
});

// Consume the stream
myReadableStream.on('data', chunk => {
  console.log(chunk);
});

// Output:
// { message: 'Hello, world!' }
JavaScriptCopy to clipboardIn this example, we create a custom Readable stream that pushes a single object
onto the stream using .push(). The ._read() method is called when the stream is ready
to consume data, and in this case, we immediately push some data onto the stream and
mark the end of the stream by pushing null.
We then consume the stream by listening for the 'data' event and logging each chunk of
data that is pushed onto the stream. In this case, we only push a single chunk of data
onto the stream, so we only see one log message.
Rules specific to Writable Streams
Recall that a .write() may return true or false dependent on some
conditions. Luckily for us, when building our own Writable stream,
the stream state machine will handle our callbacks and determine when to
handle backpressure and optimize the flow of data for us.
However, when we want to use a Writable directly, we must respect the
.write() return value and pay close attention to these conditions:

If the write queue is busy, .write() will return false.
If the data chunk is too large, .write() will return false (the limit
is indicated by the variable, highWaterMark).

// This writable is invalid because of the async nature of JavaScript callbacks.
// Without a return statement for each callback prior to the last,
// there is a great chance multiple callbacks will be called.
class MyWritable extends Writable {
  _write(chunk, encoding, callback) {
    if (chunk.toString().indexOf('a') >= 0) callback();
    else if (chunk.toString().indexOf('b') >= 0) callback();
    callback();
  }
}

// The proper way to write this would be:
if (chunk.contains('a')) return callback();
if (chunk.contains('b')) return callback();
callback();
JavaScriptCopy to clipboard
There are also some things to look out for when implementing ._writev().
The function is coupled with .cork(), but there is a common mistake when
writing:
// Using .uncork() twice here makes two calls on the C++ layer, rendering the
// cork/uncork technique useless.
ws.cork();
ws.write('hello ');
ws.write('world ');
ws.uncork();

ws.cork();
ws.write('from ');
ws.write('Matteo');
ws.uncork();

// The correct way to write this is to utilize process.nextTick(), which fires
// on the next event loop.
ws.cork();
ws.write('hello ');
ws.write('world ');
process.nextTick(doUncork, ws);

ws.cork();
ws.write('from ');
ws.write('Matteo');
process.nextTick(doUncork, ws);

// As a global function.
function doUncork(stream) {
  stream.uncork();
}
JavaScriptCopy to clipboard
.cork() can be called as many times as we want, we just need to be careful to
call .uncork() the same amount of times to make it flow again.
Conclusion
Streams are an often-used module in Node.js. They are important to the internal
structure, and for developers, to expand and connect across the Node.js modules
ecosystem.
Hopefully, you will now be able to troubleshoot and safely code your own
Writable and Readable streams with backpressure in mind, and share
your knowledge with colleagues and friends.
Be sure to read up more on Stream for other API functions to help
improve and unleash your streaming capabilities when building an application with
Node.js.PrevHow to use streamsNextUser Journey\n\n\n\nUser Journey
These diagnostics guides were created by the Diagnostics Working Group
and the Node.js Website Team with the objective of providing guidance
when diagnosing an issue in a user's application.
The documentation project is organized based on user journey. Those journeys
are a coherent set of step-by-step procedures that a user can follow to
root-cause their issues.PrevBackpressuring in StreamsNextMemory\n\n\n\nMemory
In this document you can learn about how to debug memory related issues.
My process runs out of memory
Node.js (JavaScript) is a garbage collected language, so having memory
leaks is possible through retainers. As Node.js applications are usually
multi-tenant, business critical, and long-running, providing an accessible and
efficient way of finding a memory leak is essential.
You can also fine-tune memory to get specific results. Check out
Understanding and Tuning Memory for more details.
Symptoms
The user observes continuously increasing memory usage (can be fast or slow,
over days or even weeks) then sees the process crashing and restarting by the
process manager. The process is maybe running slower than before and the
restarts cause some requests to fail (load balancer responds with 502).
Side Effects

Process restarts due to the memory exhaustion and requests are dropped
on the floor
Increased GC activity leads to higher CPU usage and slower response time

GC blocking the Event Loop causing slowness


Increased memory swapping slows down the process (GC activity)
May not have enough available memory to get a Heap Snapshot

My process utilizes memory inefficiently
Symptoms
The application uses an unexpected amount of memory and/or we observe elevated
garbage collector activity.
Side Effects

An elevated number of page faults
Higher GC activity and CPU usage

Debugging
Most memory issues can be solved by determining how much space our specific
type of objects take and what variables are preventing them from being garbage
collected. It can also help to know the allocation pattern of our program over
time.

Using Heap Profiler
Using Heap Snapshot
GC Traces
PrevUser JourneyNextLive Debugging\n\n\n\nLive Debugging
In this document you can learn about how to live debug a Node.js process.
My application doesn’t behave as expected
Symptoms
The user may observe that the application doesn’t provide the expected output
for certain inputs, for example, an HTTP server returns a JSON response where
certain fields are empty. Various things can go wrong in the process but in this
use case, we are mainly focused on the application logic and its correctness.
Debugging
In this use case, the user would like to understand the code path that our
application executes for a certain trigger like an incoming HTTP request. They
may also want to step through the code and control the execution as well as
inspect what values variables hold in memory.

Using Inspector
PrevMemoryNextPoor Performance\n\n\n\nPoor Performance
In this document you can learn about how to profile a Node.js process.
My application has a poor performance
Symptoms
My applications latency is high and I have already confirmed that the bottleneck
is not my dependencies like databases and downstream services. So I suspect that
my application spends significant time to run code or process information.
You are satisfied with your application performance in general but would like to
understand which part of our application can be improved to run faster or more
efficient. It can be useful when we want to improve the user experience or save
computation cost.
Debugging
In this use-case, we are interested in code pieces that use more CPU cycles than
the others. When we do this locally, we usually try to optimize our code.
This document provides two simple ways to profile a Node.js application:

Using V8 Sampling Profiler
Using Linux Perf
PrevLive DebuggingNextFlame Graphs\n\n\n\nFlame Graphs
What's a flame graph useful for?
Flame graphs are a way of visualizing CPU time spent in functions. They can help you pin down where you spend too much time doing synchronous operations.
How to create a flame graph
You might have heard creating a flame graph for Node.js is difficult, but that's not true (anymore).
Solaris vms are no longer needed for flame graphs!
Flame graphs are generated from perf output, which is not a node-specific tool. While it's the most powerful way to visualize CPU time spent, it may have issues with how JavaScript code is optimized in Node.js 8 and above. See perf output issues section below.
Use a pre-packaged tool
If you want a single step that produces a flame graph locally, try 0x
For diagnosing production deployments, read these notes: 0x production servers.
Create a flame graph with system perf tools
The purpose of this guide is to show the steps involved in creating a flame graph and keep you in control of each step.
If you want to understand each step better, take a look at the sections that follow where we go into more detail.
Now let's get to work.


Install perf (usually available through the linux-tools-common package if not already installed)


Try running perf - it might complain about missing kernel modules, install them too


Run node with perf enabled (see perf output issues for tips specific to Node.js versions)
perf record -e cycles:u -g -- node --perf-basic-prof --interpreted-frames-native-stack app.js



Disregard warnings unless they're saying you can't run perf due to missing packages; you may get some warnings about not being able to access kernel module samples which you're not after anyway.


Run perf script > perfs.out to generate the data file you'll visualize in a moment. It's useful to apply some cleanup for a more readable graph


Clone Brendan Gregg's FlameGraph tools: https://github.com/brendangregg/FlameGraph


Run cat perfs.out | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl --colors=js > profile.svg


Now open the flame graph file in your favorite browser and watch it burn. It's color-coded so you can focus on the most saturated orange bars first. They're likely to represent CPU heavy functions.
Worth mentioning - if you click an element of a flame graph a it will zoom-in on the section you clicked.
Using perf to sample a running process
This is great for recording flame graph data from an already running process that you don't want to interrupt. Imagine a production process with a hard to reproduce issue.
perf record -F99 -p `pgrep -n node` -g -- sleep 3
ShellCopy to clipboard
Wait, what is that sleep 3 for? It's there to keep the perf running - despite -p option pointing to a different pid, the command needs to be executed on a process and end with it.
perf runs for the life of the command you pass to it, whether or not you're actually profiling that command. sleep 3 ensures that perf runs for 3 seconds.
Why is -F (profiling frequency) set to 99? It's a reasonable default. You can adjust if you want.
-F99 tells perf to take 99 samples per second, for more precision increase the value. Lower values should produce less output with less precise results. The precision you need depends on how long your CPU intensive functions really run. If you're looking for the reason for a noticeable slowdown, 99 frames per second should be more than enough.
After you get that 3 second perf record, proceed with generating the flame graph with the last two steps from above.
Filtering out Node.js internal functions
Usually, you just want to look at the performance of your calls, so filtering out Node.js and V8 internal functions can make the graph much easier to read. You can clean up your perf file with:
sed -i -r \
  -e "/( __libc_start| LazyCompile | v8::internal::| Builtin:| Stub:| LoadIC:|\[unknown\]| LoadPolymorphicIC:)/d" \
  -e 's/ LazyCompile:[*~]?/ /' \
  perfs.out
ShellCopy to clipboard
If you read your flame graph and it seems odd, as if something is missing in the key function taking up most time, try generating your flame graph without the filters - maybe you got a rare case of an issue with Node.js itself.
Node.js's profiling options
--perf-basic-prof-only-functions and --perf-basic-prof are the two that are useful for debugging your JavaScript code. Other options are used for profiling Node.js itself, which is outside the scope of this guide.
--perf-basic-prof-only-functions produces less output, so it's the option with the least overhead.
Why do I need them at all?
Well, without these options, you'll still get a flame graph, but with most bars labeled v8::Function::Call.
perf output issues
Node.js 8.x V8 pipeline changes
Node.js 8.x and above ships with new optimizations to the JavaScript compilation pipeline in the V8 engine which makes function names/references unreachable for perf sometimes. (It's called Turbofan)
The result is you might not get your function names right in the flame graph.
You'll notice ByteCodeHandler: where you'd expect function names.
0x has some mitigations for that built in.
For details see:

https://github.com/nodejs/benchmarking/issues/168
https://github.com/nodejs/diagnostics/issues/148#issuecomment-369348961

Node.js 10+
Node.js 10.x addresses the issue with Turbofan using the --interpreted-frames-native-stack flag.
Run node --interpreted-frames-native-stack --perf-basic-prof-only-functions to get function names in the flame graph regardless of which pipeline V8 used to compile your JavaScript.
Broken labels in the flame graph
If you're seeing labels looking like this
node`_ZN2v88internal11interpreter17BytecodeGenerator15VisitStatementsEPNS0_8ZoneListIPNS0_9StatementEEE

it means the Linux perf you're using was not compiled with demangle support, see https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1396654 for example
Examples
Practice capturing flame graphs yourself with a flame graph exercise!PrevPoor PerformanceNextDiscovering Node.js's test runner\n\n\n\nDiscovering Node.js's test runner
In this series of resources, we will discover how to use Node.js's test runner to test our code.
What is a test runner?
A test runner is a tool that allows you to run tests on your code. It will execute your tests and give you feedback on whether they pass or fail. It can also provide you with additional information such as code coverage.
There are many test runners available for Node.js, but we will focus on the built-in test runner that comes with Node.js. It's cool because you don't need to install any additional dependencies to use it.
Why test your code?
Testing your code is important because it allows you to verify that your code works as expected. It helps you catch bugs early in the development process and ensures that your code is reliable and maintainable.
Running tests with Node.js
To run tests with Node.js, we recommend you to read the test runner documentation.PrevFlame GraphsNextUsing Node.js's test runner\n\n\n\nUsing Node.js's test runner
Node.js has a flexible and robust built-in test runner. This guide will show you how to set up and use it.
Architecture overviewInstall dependenciespackage.jsonexample/
  ├ …
  ├ src/
    ├ app/…
    └ sw/…
  └ test/
    ├ globals/
      ├ …
      ├ IndexedDb.js
      └ ServiceWorkerGlobalScope.js
    ├ setup.mjs
    ├ setup.units.mjs
    └ setup.ui.mjs
textCopy to clipboard
Note: globs require node v21+, and the globs must themselves be wrapped in quotes (without, you'll get different behaviour than expected, wherein it may first appear to be working but isn't).

There are some things you always want, so put them in a base setup file like the following. This file will get imported by other, more bespoke setups.
General setup

import { register } from 'node:module';

register('some-typescript-loader');
// TypeScript is supported hereafter
// BUT other test/setup.*.mjs files still must be plain JavaScript!
JavaScriptCopy to clipboard

Then for each setup, create a dedicated setup file (ensuring the base setup.mjs file is imported within each). There are a number of reasons to isolate the setups, but the most obvious reason is YAGNI + performance: much of what you may be setting up are environment-specific mocks/stubs, which can be quite expensive and will slow down test runs. You want to avoid those costs (literal money you pay to CI, time waiting for tests to finish, etc) when you don't need them.
Each example below was taken from real-world projects; they may not be appropriate/applicable to yours, but each demonstrate general concepts that are broadly applicable.
Dynamically generating test cases
Some times, you may want to dynamically generate test-cases. For instance, you want to test the same thing across a bunch of files. This is possible, albeit slightly arcane. You must use test (you cannot use describe) + testContext.test:
Simple example
23.8.0 and laterprior to 23.8.0import assert from 'node:assert/strict';
import { test } from 'node:test';

import { detectOsInUserAgent } from '…';

const userAgents = [
  { ua: /* … */, os: 'WIN' },
  // …
];

test('Detect OS via user-agent', { concurrency: true }, t => {
  for (const { os, ua } from userAgents) {
    t.test(ua, () => assert.equal(detectOsInUserAgent(ua), os));
  }
});
JavaScriptCopy to clipboardAdvanced example
23.8.0 and laterprior to 23.8.0./getWorkspacePJSONs.mjsimport assert from 'node:assert/strict';
import { test } from 'node:test';

import { getWorkspacePJSONs } from './getWorkspacePJSONs.mjs';

const requiredKeywords = ['node.js', 'sliced bread'];

test('Check package.jsons', { concurrency: true }, async t => {
  const pjsons = await getWorkspacePJSONs();

  for (const pjson of pjsons) {
    // ⚠️ `t.test`, NOT `test`
    t.test(`Ensure fields are properly set: ${pjson.name}`, () => {
      assert.partialDeepStrictEqual(pjson.keywords, requiredKeywords);
    });
  }
});
JavaScriptCopy to clipboard
Note: Prior to version 23.8.0, the setup is quite different because testContext.test was not automatically awaited.

ServiceWorker tests
ServiceWorkerGlobalScope contains very specific APIs that don't exist in other environments, and some of its APIs are seemingly similar to others (ex fetch) but have augmented behaviour. You do not want these to spill into unrelated tests.

import { beforeEach } from 'node:test';

import { ServiceWorkerGlobalScope } from './globals/ServiceWorkerGlobalScope.js';

import './setup.mjs'; // 💡

beforeEach(globalSWBeforeEach);
function globalSWBeforeEach() {
  globalThis.self = new ServiceWorkerGlobalScope();
}
JavaScriptCopy to clipboard

import assert from 'node:assert/strict';
import { describe, mock, it } from 'node:test';

import { onActivate } from './onActivate.js';

describe('ServiceWorker::onActivate()', () => {
  const globalSelf = globalThis.self;
  const claim = mock.fn(async function mock__claim() {});
  const matchAll = mock.fn(async function mock__matchAll() {});

  class ActivateEvent extends Event {
    constructor(...args) {
      super('activate', ...args);
    }
  }

  before(() => {
    globalThis.self = {
      clients: { claim, matchAll },
    };
  });
  after(() => {
    global.self = globalSelf;
  });

  it('should claim all clients', async () => {
    await onActivate(new ActivateEvent());

    assert.equal(claim.mock.callCount(), 1);
    assert.equal(matchAll.mock.callCount(), 1);
  });
});
JavaScriptCopy to clipboard
Snapshot tests
These were popularised by Jest; now, many libraries implement such functionality, including Node.js as of v22.3.0. There are several use-cases such as verifying component rendering output and Infrastructure as Code config. The concept is the same regardless of use-case.
There is no specific configuration required except enabling the feature via --experimental-test-snapshots. But to demonstrate the optional configuration, you would probably add something like the following to one of your existing test config files.

By default, node generates a filename that is incompatible with syntax highlighting detection: .js.snapshot. The generated file is actually a CJS file, so a more appropriate file name would end with .snapshot.cjs (or more succinctly .snap.cjs as below); this will also handle better in ESM projects.
import { basename, dirname, extname, join } from 'node:path';
import { snapshot } from 'node:test';

snapshot.setResolveSnapshotPath(generateSnapshotPath);
/**
 * @param {string} testFilePath '/tmp/foo.test.js'
 * @returns {string} '/tmp/foo.test.snap.cjs'
 */
function generateSnapshotPath(testFilePath) {
  const ext = extname(testFilePath);
  const filename = basename(testFilePath, ext);
  const base = dirname(testFilePath);

  return join(base, `${filename}.snap.cjs`);
}
JavaScriptCopy to clipboard

The example below demonstrates snapshot testing with testing library for UI components; note the two different ways of accessing assert.snapshot):
import { describe, it } from 'node:test';

import { prettyDOM } from '@testing-library/dom';
import { render } from '@testing-library/react'; // Any framework (ex svelte)

import { SomeComponent } from './SomeComponent.jsx';


describe('<SomeComponent>', () => {
  // For people preferring "fat-arrow" syntax, the following is probably better for consistency
  it('should render defaults when no props are provided', (t) => {
    const component = render(<SomeComponent />).container.firstChild;

    t.assert.snapshot(prettyDOM(component));
  });

  it('should consume `foo` when provided', function() {
    const component = render(<SomeComponent foo="bar" />).container.firstChild;

    this.assert.snapshot(prettyDOM(component));
    // `this` works only when `function` is used (not "fat arrow").
  });
});
TypeScriptCopy to clipboard

⚠️ assert.snapshot comes from the test's context (t or this), not node:assert. This is necessary because the test context has access to scope that is impossible for node:assert (you would have to manually provide it every time assert.snapshot is used, like snapshot(this, value), which would be rather tedious).

Unit tests
Unit tests are the simplest tests and generally require relatively nothing special. The vast majority of your tests will likely be unit tests, so it is important to keep this setup minimal because a small decrease to setup performance will magnify and cascade.

import { register } from 'node:module';

import './setup.mjs'; // 💡

register('some-plaintext-loader');
// plain-text files like graphql can now be imported:
// import GET_ME from 'get-me.gql'; GET_ME = '
JavaScriptCopy to clipboard

import assert from 'node:assert/strict';
import { describe, it } from 'node:test';

import { Cat } from './Cat.js';
import { Fish } from './Fish.js';
import { Plastic } from './Plastic.js';

describe('Cat', () => {
  it('should eat fish', () => {
    const cat = new Cat();
    const fish = new Fish();

    assert.doesNotThrow(() => cat.eat(fish));
  });

  it('should NOT eat plastic', () => {
    const cat = new Cat();
    const plastic = new Plastic();

    assert.throws(() => cat.eat(plastic));
  });
});
JavaScriptCopy to clipboard
User Interface tests
UI tests generally require a DOM, and possibly other browser-specific APIs (such as IndexedDb used below). These tend to be very complicated and expensive to setup.

If you use an API like IndexedDb but it's very isolated, a global mock like below is perhaps not the way to go. Instead, perhaps move this beforeEach into the specific test where IndexedDb will be accessed. Note that if the module accessing IndexedDb (or whatever) is itself widely accessed, either mock that module (probably the better option), or do keep this here.
import { register } from 'node:module';

// ⚠️ Ensure only 1 instance of JSDom is instantiated; multiples will lead to many 🤬
import jsdom from 'global-jsdom';

import './setup.units.mjs'; // 💡

import { IndexedDb } from './globals/IndexedDb.js';

register('some-css-modules-loader');

jsdom(undefined, {
  url: 'https://test.example.com', // ⚠️ Failing to specify this will likely lead to many 🤬
});

// Example of how to decorate a global.
// JSDOM's `history` does not handle navigation; the following handles most cases.
const pushState = globalThis.history.pushState.bind(globalThis.history);
globalThis.history.pushState = function mock_pushState(data, unused, url) {
  pushState(data, unused, url);
  globalThis.location.assign(url);
};

beforeEach(globalUIBeforeEach);
function globalUIBeforeEach() {
  globalThis.indexedDb = new IndexedDb();
}
JavaScriptCopy to clipboard

You can have 2 different levels of UI tests: a unit-like (wherein externals & dependencies are mocked) and a more end-to-end (where only externals like IndexedDb are mocked but the rest of the chain is real). The former is generally the purer option, and the latter is generally deferred to a fully end-to-end automated usability test via something like Playwright or Puppeteer. Below is an example of the former.
import { before, describe, mock, it } from 'node:test';

import { screen } from '@testing-library/dom';
import { render } from '@testing-library/react'; // Any framework (ex svelte)

// ⚠️ Note that SomeOtherComponent is NOT a static import;
// this is necessary in order to facilitate mocking its own imports.


describe('<SomeOtherComponent>', () => {
  let SomeOtherComponent;
  let calcSomeValue;

  before(async () => {
    // ⚠️ Sequence matters: the mock must be set up BEFORE its consumer is imported.

    // Requires the `--experimental-test-module-mocks` be set.
    calcSomeValue = mock.module('./calcSomeValue.js', { calcSomeValue: mock.fn() });

    ({ SomeOtherComponent } = await import('./SomeOtherComponent.jsx'));
  });

  describe('when calcSomeValue fails', () => {
    // This you would not want to handle with a snapshot because that would be brittle:
    // When inconsequential updates are made to the error message,
    // the snapshot test would erroneously fail
    // (and the snapshot would need to be updated for no real value).

    it('should fail gracefully by displaying a pretty error', () => {
      calcSomeValue.mockImplementation(function mock__calcSomeValue() { return null });

      render(<SomeOtherComponent>);

      const errorMessage = screen.queryByText('unable');

      assert.ok(errorMessage);
    });
  });
});
TypeScriptCopy to clipboardPrevDiscovering Node.js's test runnerNextMocking in tests\n\n\n\nMocking in tests
Mocking is a means of creating a facsimile, a puppet. This is generally done in a when 'a', do 'b' manner of puppeteering. The idea is to limit the number of moving pieces and control things that "don't matter". "mocks" and "stubs" are technically different kinds of "test doubles". For the curious mind, a stub is a replacement that does nothing (a no-op) but track its invocation. A mock is a stub that also has a fake implementation (the when 'a', do 'b'). Within this doc, the difference is unimportant, and stubs are referred to as mocks.
Tests should be deterministic: runnable in any order, any number of times, and always produce the same result. Proper setup and mocking make this possible.
Node.js provides many ways to mock various pieces of code.
This articles deals with the following types of tests:
typedescriptionexamplemock candidatesunitthe smallest bit of code you can isolateconst sum = (a, b) => a + bown code, external code, external systemcomponenta unit + dependenciesconst arithmetic = (op = sum, a, b) => ops[op](a, b)external code, external systemintegrationcomponents fitting together-external code, external systemend-to-end (e2e)app + external data stores, delivery, etcA fake user (ex a Playwright agent) literally using an app connected to real external systems.none (do not mock)
There are different schools of thought about when to mock and when not to mock, the broad strokes of which are outlined below.
When and not to mock
There are 3 main mock candidates:

Own code
External code
External system

Own code
This is what your project controls.
import foo from './foo.mjs';

export function main() {
  const f = foo();
}
JavaScriptCopy to clipboard
Here, foo is an "own code" dependency of main.
Why
For a true unit test of main, foo should be mocked: you're testing that main works, not that main + foo work (that's a different test).
Why not
Mocking foo can be more trouble than worth, especially when foo is simple, well-tested, and rarely updated.
Not mocking foo can be better because it's more authentic and increases coverage of foo (because main's tests will also verify foo). This can, however, create noise: when foo breaks, a bunch of other tests will also break, so tracking down the problem is more tedious: if only the 1 test for the item ultimately responsible for the issue is failing, that's very easy to spot; whereas 100 tests failing creates a needle-in-a-haystack to find the real problem.
External code
This is what your project does not control.
import bar from 'bar';

export function main() {
  const f = bar();
}
JavaScriptCopy to clipboard
Here, bar is an external package, e.g. an npm dependency.
Uncontroversially, for unit tests, this should always be mocked. For component and integration tests, whether to mock depends on what this is.
Why
Verifying that code that your project does not maintain works is not the goal of a unit test (and that code should have its own tests).
Why not
Sometimes, it's just not realistic to mock. For example, you would almost never mock a large framework such as react or angular (the medicine would be worse than the ailment).
External system
These are things like databases, environments (Chromium or Firefox for a web app, an operating system for a node app, etc), file systems, memory store, etc.
Ideally, mocking these would not be necessary. Aside from somehow creating isolated copies for each case (usually very impractical due to cost, additional execution time, etc), the next best option is to mock. Without mocking, tests sabotage each other:
storage.mjsstorage.test.mjsimport { db } from 'db';

export function read(key, all = false) {
  validate(key, val);

  if (all) return db.getAll(key);

  return db.getOne(key);
}

export function save(key, val) {
  validate(key, val);

  return db.upsert(key, val);
}
JavaScriptCopy to clipboardIn the above, the first and second cases (the it() statements) can sabotage each other because they are run concurrently and mutate the same store (a race condition): save()'s insertion can cause the otherwise valid read()'s test to fail its assertion on items found (and read()'s can do the same thing to save()'s).
What to mock
Modules + units
This leverages mock from the Node.js test runner.
import assert from 'node:assert/strict';
import { before, describe, it, mock } from 'node:test';

describe('foo', { concurrency: true }, () => {
  let barMock = mock.fn();
  let foo;

  before(async () => {
    const barNamedExports = await import('./bar.mjs')
      // discard the original default export
      .then(({ default: _, ...rest }) => rest);

    // It's usually not necessary to manually call restore() after each
    // nor reset() after all (node does this automatically).
    mock.module('./bar.mjs', {
      defaultExport: barMock,
      // Keep the other exports that you don't want to mock.
      namedExports: barNamedExports,
    });

    // This MUST be a dynamic import because that is the only way to ensure the
    // import starts after the mock has been set up.
    ({ foo } = await import('./foo.mjs'));
  });

  it('should do the thing', () => {
    barMock.mockImplementationOnce(function bar_mock() {
      /* … */
    });

    assert.equal(foo(), 42);
  });
});
JavaScriptCopy to clipboard
APIs
A little-known fact is that there is a builtin way to mock fetch. undici is the Node.js implementation of fetch. It's shipped with node, but not currently exposed by node itself, so it must be installed (ex npm install undici).
import assert from 'node:assert/strict';
import { beforeEach, describe, it } from 'node:test';
import { MockAgent, setGlobalDispatcher } from 'undici';

import endpoints from './endpoints.mjs';

describe('endpoints', { concurrency: true }, () => {
  let agent;
  beforeEach(() => {
    agent = new MockAgent();
    setGlobalDispatcher(agent);
  });

  it('should retrieve data', async () => {
    const endpoint = 'foo';
    const code = 200;
    const data = {
      key: 'good',
      val: 'item',
    };

    agent
      .get('https://example.com')
      .intercept({
        path: endpoint,
        method: 'GET',
      })
      .reply(code, data);

    assert.deepEqual(await endpoints.get(endpoint), {
      code,
      data,
    });
  });

  it('should save data', async () => {
    const endpoint = 'foo/1';
    const code = 201;
    const data = {
      key: 'good',
      val: 'item',
    };

    agent
      .get('https://example.com')
      .intercept({
        path: endpoint,
        method: 'PUT',
      })
      .reply(code, data);

    assert.deepEqual(await endpoints.save(endpoint), {
      code,
      data,
    });
  });
});
JavaScriptCopy to clipboard
Time
Like Doctor Strange, you too can control time. You would usually do this just for convenience to avoid artificially protracted test runs (do you really want to wait 3 minutes for that setTimeout() to trigger?). You may also want to travel through time. This leverages mock.timers from the Node.js test runner.
Note the use of time-zone here (Z in the time-stamps). Neglecting to include a consistent time-zone will likely lead to unexpected restults.
import assert from 'node:assert/strict';
import { describe, it, mock } from 'node:test';

import ago from './ago.mjs';

describe('whatever', { concurrency: true }, () => {
  it('should choose "minutes" when that\'s the closet unit', () => {
    mock.timers.enable({ now: new Date('2000-01-01T00:02:02Z') });

    const t = ago('1999-12-01T23:59:59Z');

    assert.equal(t, '2 minutes ago');
  });
});
JavaScriptCopy to clipboard
This is especially useful when comparing against a static fixture (that is checked into a repository), such as in snapshot testing.PrevUsing Node.js's test runnerNextCollecting code coverage in Node.js\n\n\n\nCollecting code coverage in Node.js
Node.js provides built-in support for code coverage through its test runner, which can be enabled using the --experimental-code-coverage flag.
If using the run() API, the coverage option must be set to true. For more information on the run() API, see the node:test documentation.
What is code coverage?
Code coverage is a metric for test runners that gauges how much of a program’s source code is executed during testing. It reveals which portions of the codebase are tested and which are not, helping to pinpoint gaps in the test suite. This ensures more comprehensive testing of the software and minimizes the risk of undetected bugs. Typically expressed as a percentage, higher code coverage percentages indicate more thorough test coverage. For a more detailed explanation of code coverage, you can refer to the "Code coverage" Wikipedia article.
Basic coverage reporting
Let's walk through a simple example to demonstrate how code coverage works in Node.js.

Note: This example, and all other ones in this file, are written using CommonJS. If you are unfamiliar with this concept, please read the CommonJS Modules documentation.

main.jsmain.test.jsfunction add(a, b) {
  return a + b;
}

function isEven(num) {
  return num % 2 === 0;
}

function multiply(a, b) {
  return a * b;
}

module.exports = { add, isEven, multiply };
JavaScriptCopy to clipboardIn the module, we have three functions: add, isEven, and multiply.
In the test file, we are testing the add() and isEven() functions. Notice that the multiply() function is not covered by any tests.
To collect code coverage while running your tests, see the following snippets:
CLIrun()node --experimental-test-coverage --test main.test.js
ShellCopy to clipboardAfter running the tests, you'll receive a report that looks something like this:
✔ add() should add two numbers (1.505987ms)
✔ isEven() should report whether a number is even (0.175859ms)
ℹ tests 2
ℹ suites 0
ℹ pass 2
ℹ fail 0
ℹ cancelled 0
ℹ skipped 0
ℹ todo 0
ℹ duration_ms 59.480373
ℹ start of coverage report
ℹ -------------------------------------------------------------
ℹ file         | line % | branch % | funcs % | uncovered lines
ℹ -------------------------------------------------------------
ℹ main.js      |  76.92 |   100.00 |   66.67 | 9-11
ℹ main.test.js | 100.00 |   100.00 |  100.00 |
ℹ -------------------------------------------------------------
ℹ all files    |  86.96 |   100.00 |   80.00 |
ℹ -------------------------------------------------------------
ℹ end of coverage report
textCopy to clipboard
The coverage report provides a breakdown of how much of your code is covered by tests:

Line Coverage: The percentage of lines executed during the tests.
Branch Coverage: The percentage of code branches (like if-else statements) tested.
Function Coverage: The percentage of functions that have been invoked during testing.

In this example:

main.js shows 76.92% line coverage and 66.67% function coverage because the multiply() function was not tested. The uncovered lines (9-11) correspond to this function.
main.test.js shows 100% coverage across all metrics, indicating that the tests themselves were fully executed.

Including and excluding
When working on applications, you might encounter situations where certain files or lines of code need to be excluded.
Node.js provides mechanisms to handle this, including the use of comments to ignore specific code sections and the CLI to exclude entire patterns.
Using comments
main.jsCoverage Reportfunction add(a, b) {
  return a + b;
}

function isEven(num) {
  return num % 2 === 0;
}

/* node:coverage ignore next 3 */
function multiply(a, b) {
  return a * b;
}

module.exports = { add, isEven, multiply };
JavaScriptCopy to clipboardWhen reporting coverage with this modified main.js file, the report will now show 100% coverage across all metrics. This is because the uncovered lines (9-11) have been ignored.
There are multiple ways to ignore sections of code using comments.
ignore nextignore nextdisablefunction add(a, b) {
  return a + b;
}

function isEven(num) {
  return num % 2 === 0;
}

/* node:coverage ignore next 3 */
function multiply(a, b) {
  return a * b;
}

module.exports = { add, isEven, multiply };
JavaScriptCopy to clipboardEach of these different methods will produce the same report, with 100% code coverage across all metrics.
Using the CLI
Node.js offers two CLI arguments for managing the inclusion or exclusion of specific files in a coverage report.
The --test-coverage-include flag (coverageIncludeGlobs in the run() API) restricts the coverage to files that match the provided glob pattern. By default, files in the /node_modules/ directory are excluded, but this flag allows you to explicitly include them.
The --test-coverage-exclude flag (coverageExcludeGlobs in the run() API) omits files that match the given glob pattern from the coverage report.
These flags can be used multiple times, and when both are used together, files must adhere to the inclusion rules, while also avoiding the exclusion rules.
Directory StructureCoverage Report.
├── main.test.js
├── src
│   ├── age.js
│   └── name.js
textCopy to clipboardsrc/age.js has less-than-optimal coverage in the report above, but with the --test-coverage-exclude flag (coverageExcludeGlobs in the run() API), it can be excluded from the report entirely.
CLIrun()New coverage reportnode --experimental-test-coverage --test-coverage-exclude=src/age.js --test main.test.js
ShellCopy to clipboardOur test file is also included in this coverage report, but we only want JavaScript files in the src/ directory. The --test-coverage-include flag (coverageIncludeGlobs in the run() API) can be used in this case.
CLIrun()New coverage reportnode --experimental-test-coverage --test-coverage-include=src/*.js --test main.test.js
ShellCopy to clipboardThresholds
By default, when all tests pass, Node.js exits with code 0, which indicates a successful execution. However, the coverage report can be configured to exit with code 1 when coverage is failing.
Node.js currently supports thresholds for all three of the coverages supported:

--test-coverage-lines (lineCoverage in the run() API) for line coverage.
--test-coverage-branches (branchCoverage in the run() API) for branch coverage.
--test-coverage-functions (functionCoverage in the run() API) for function coverage.

If you wanted to require the previous example to have line coverage >= 90%, you could use the --test-coverage-lines=90 flag (lineCoverage: 90 in the run() API).
CLIrun()Coverage Reportnode --experimental-test-coverage --test-coverage-lines=90 --test main.test.js
ShellCopy to clipboardPrevMocking in tests\n\n\n\nIntroduction to Node.js
Node.js is an open-source and cross-platform JavaScript runtime environment. It is a popular tool for almost any kind of project!
Node.js runs the V8 JavaScript engine, the core of Google Chrome, outside of the browser. This allows Node.js to be very performant.
A Node.js app runs in a single process, without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in its standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are written using non-blocking paradigms, making blocking behavior the exception rather than the norm.
When Node.js performs an I/O operation, like reading from the network, accessing a database or the filesystem, instead of blocking the thread and wasting CPU cycles waiting, Node.js will resume the operations when the response comes back.
This allows Node.js to handle thousands of concurrent connections with a single server without introducing the burden of managing thread concurrency, which could be a significant source of bugs.
Node.js has a unique advantage because millions of frontend developers that write JavaScript for the browser are now able to write the server-side code in addition to the client-side code without the need to learn a completely different language.
In Node.js the new ECMAScript standards can be used without problems, as you don't have to wait for all your users to update their browsers - you are in charge of deciding which ECMAScript version to use by changing the Node.js version, and you can also enable specific experimental features by running Node.js with flags.
An Example Node.js Application
The most common example Hello World of Node.js is a web server:
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardTo run this snippet, save it as a server.js file and run node server.js in your terminal.
If you use mjs version of the code, you should save it as a server.mjs file and run node server.mjs in your terminal.
This code first includes the Node.js http module.
Node.js has a fantastic standard library, including first-class support for networking.
The createServer() method of http creates a new HTTP server and returns it.
The server is set to listen on the specified port and host name. When the server is ready, the callback function is called, in this case informing us that the server is running.
Whenever a new request is received, the request event is called, providing two objects: a request (an http.IncomingMessage object) and a response (an http.ServerResponse object).
Those 2 objects are essential to handle the HTTP call.
The first provides the request details. In this simple example, this is not used, but you could access the request headers and request data.
The second is used to return data to the caller.
In this case with:
res.statusCode = 200;
JavaScriptCopy to clipboard
we set the statusCode property to 200, to indicate a successful response.
We set the Content-Type header:
res.setHeader('Content-Type', 'text/plain');
JavaScriptCopy to clipboard
and we close the response, adding the content as an argument to end():
res.end('Hello World\n');
JavaScriptCopy to clipboard
If you haven't already done so, download Node.js.NextHow much JavaScript do you need to know to use Node.js?\n\n\n\nIntroduction to Node.js
Node.js is an open-source and cross-platform JavaScript runtime environment. It is a popular tool for almost any kind of project!
Node.js runs the V8 JavaScript engine, the core of Google Chrome, outside of the browser. This allows Node.js to be very performant.
A Node.js app runs in a single process, without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in its standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are written using non-blocking paradigms, making blocking behavior the exception rather than the norm.
When Node.js performs an I/O operation, like reading from the network, accessing a database or the filesystem, instead of blocking the thread and wasting CPU cycles waiting, Node.js will resume the operations when the response comes back.
This allows Node.js to handle thousands of concurrent connections with a single server without introducing the burden of managing thread concurrency, which could be a significant source of bugs.
Node.js has a unique advantage because millions of frontend developers that write JavaScript for the browser are now able to write the server-side code in addition to the client-side code without the need to learn a completely different language.
In Node.js the new ECMAScript standards can be used without problems, as you don't have to wait for all your users to update their browsers - you are in charge of deciding which ECMAScript version to use by changing the Node.js version, and you can also enable specific experimental features by running Node.js with flags.
An Example Node.js Application
The most common example Hello World of Node.js is a web server:
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardTo run this snippet, save it as a server.js file and run node server.js in your terminal.
If you use mjs version of the code, you should save it as a server.mjs file and run node server.mjs in your terminal.
This code first includes the Node.js http module.
Node.js has a fantastic standard library, including first-class support for networking.
The createServer() method of http creates a new HTTP server and returns it.
The server is set to listen on the specified port and host name. When the server is ready, the callback function is called, in this case informing us that the server is running.
Whenever a new request is received, the request event is called, providing two objects: a request (an http.IncomingMessage object) and a response (an http.ServerResponse object).
Those 2 objects are essential to handle the HTTP call.
The first provides the request details. In this simple example, this is not used, but you could access the request headers and request data.
The second is used to return data to the caller.
In this case with:
res.statusCode = 200;
JavaScriptCopy to clipboard
we set the statusCode property to 200, to indicate a successful response.
We set the Content-Type header:
res.setHeader('Content-Type', 'text/plain');
JavaScriptCopy to clipboard
and we close the response, adding the content as an argument to end():
res.end('Hello World\n');
JavaScriptCopy to clipboard
If you haven't already done so, download Node.js.NextHow much JavaScript do you need to know to use Node.js?\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket




      
        HTTP#

Stability: 2 - Stable
Source Code: lib/http.js
This module, containing both a client and server, can be imported via
require('node:http') (CommonJS) or import * as http from 'node:http' (ES module).
The HTTP interfaces in Node.js are designed to support many features
of the protocol which have been traditionally difficult to use.
In particular, large, possibly chunk-encoded, messages. The interface is
careful to never buffer entire requests or responses, so the
user is able to stream data.
HTTP message headers are represented by an object like this:
{ "content-length": "123",
  "content-type": "text/plain",
  "connection": "keep-alive",
  "host": "example.com",
  "accept": "*/*" } copy
Keys are lowercased. Values are not modified.
In order to support the full spectrum of possible HTTP applications, the Node.js
HTTP API is very low-level. It deals with stream handling and message
parsing only. It parses a message into headers and body but it does not
parse the actual headers or the body.
See message.headers for details on how duplicate headers are handled.
The raw headers as they were received are retained in the rawHeaders
property, which is an array of [key, value, key2, value2, ...]. For
example, the previous message header object might have a rawHeaders
list like the following:

[ 'ConTent-Length', '123456',
  'content-LENGTH', '123',
  'content-type', 'text/plain',
  'CONNECTION', 'keep-alive',
  'Host', 'example.com',
  'accepT', '*/*' ] copy
Class: http.Agent#

Added in: v0.3.4

An Agent is responsible for managing connection persistence
and reuse for HTTP clients. It maintains a queue of pending requests
for a given host and port, reusing a single socket connection for each
until the queue is empty, at which time the socket is either destroyed
or put into a pool where it is kept to be used again for requests to the
same host and port. Whether it is destroyed or pooled depends on the
keepAlive option.
Pooled connections have TCP Keep-Alive enabled for them, but servers may
still close idle connections, in which case they will be removed from the
pool and a new connection will be made when a new HTTP request is made for
that host and port. Servers may also refuse to allow multiple requests
over the same connection, in which case the connection will have to be
remade for every request and cannot be pooled. The Agent will still make
the requests to that server, but each one will occur over a new connection.
When a connection is closed by the client or the server, it is removed
from the pool. Any unused sockets in the pool will be unrefed so as not
to keep the Node.js process running when there are no outstanding requests.
(see socket.unref()).
It is good practice, to destroy() an Agent instance when it is no
longer in use, because unused sockets consume OS resources.
Sockets are removed from an agent when the socket emits either
a 'close' event or an 'agentRemove' event. When intending to keep one
HTTP request open for a long time without keeping it in the agent, something
like the following may be done:
http.get(options, (res) => {
  // Do stuff
}).on('socket', (socket) => {
  socket.emit('agentRemove');
}); copy
An agent may also be used for an individual request. By providing
{agent: false} as an option to the http.get() or http.request()
functions, a one-time use Agent with default options will be used
for the client connection.
agent:false:
http.get({
  hostname: 'localhost',
  port: 80,
  path: '/',
  agent: false,  // Create a new agent just for this one request
}, (res) => {
  // Do stuff with response
}); copy

new Agent([options])#

History

VersionChanges
v15.6.0, v14.17.0
Change the default scheduling from 'fifo' to 'lifo'.
v14.5.0, v12.20.0
Add scheduling option to specify the free socket scheduling strategy.
v14.5.0, v12.19.0
Add maxTotalSockets option to agent constructor.
v0.3.4
Added in: v0.3.4




options <Object> Set of configurable options to set on the agent.
Can have the following fields:

keepAlive <boolean> Keep sockets around even when there are no
outstanding requests, so they can be used for future requests without
having to reestablish a TCP connection. Not to be confused with the
keep-alive value of the Connection header. The Connection: keep-alive
header is always sent when using an agent except when the Connection
header is explicitly specified or when the keepAlive and maxSockets
options are respectively set to false and Infinity, in which case
Connection: close will be used. Default: false.
keepAliveMsecs <number> When using the keepAlive option, specifies
the initial delay
for TCP Keep-Alive packets. Ignored when the
keepAlive option is false or undefined. Default: 1000.
maxSockets <number> Maximum number of sockets to allow per host.
If the same host opens multiple concurrent connections, each request
will use new socket until the maxSockets value is reached.
If the host attempts to open more connections than maxSockets,
the additional requests will enter into a pending request queue, and
will enter active connection state when an existing connection terminates.
This makes sure there are at most maxSockets active connections at
any point in time, from a given host.
Default: Infinity.
maxTotalSockets <number> Maximum number of sockets allowed for
all hosts in total. Each request will use a new socket
until the maximum is reached.
Default: Infinity.
maxFreeSockets <number> Maximum number of sockets per host to leave open
in a free state. Only relevant if keepAlive is set to true.
Default: 256.
scheduling <string> Scheduling strategy to apply when picking
the next free socket to use. It can be 'fifo' or 'lifo'.
The main difference between the two scheduling strategies is that 'lifo'
selects the most recently used socket, while 'fifo' selects
the least recently used socket.
In case of a low rate of request per second, the 'lifo' scheduling
will lower the risk of picking a socket that might have been closed
by the server due to inactivity.
In case of a high rate of request per second,
the 'fifo' scheduling will maximize the number of open sockets,
while the 'lifo' scheduling will keep it as low as possible.
Default: 'lifo'.
timeout <number> Socket timeout in milliseconds.
This will set the timeout when the socket is created.



options in socket.connect() are also supported.
To configure any of them, a custom http.Agent instance must be created.

import { Agent, request } from 'node:http';
const keepAliveAgent = new Agent({ keepAlive: true });
options.agent = keepAliveAgent;
request(options, onResponseCallback);const http = require('node:http');
const keepAliveAgent = new http.Agent({ keepAlive: true });
options.agent = keepAliveAgent;
http.request(options, onResponseCallback);copy

agent.createConnection(options[, callback])#

Added in: v0.11.4


options <Object> Options containing connection details. Check
net.createConnection() for the format of the options
callback <Function> Callback function that receives the created socket
Returns: <stream.Duplex>

Produces a socket/stream to be used for HTTP requests.
By default, this function is the same as net.createConnection(). However,
custom agents may override this method in case greater flexibility is desired.
A socket/stream can be supplied in one of two ways: by returning the
socket/stream from this function, or by passing the socket/stream to callback.
This method is guaranteed to return an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
callback has a signature of (err, stream).

agent.keepSocketAlive(socket)#

Added in: v8.1.0


socket <stream.Duplex>

Called when socket is detached from a request and could be persisted by the
Agent. Default behavior is to:
socket.setKeepAlive(true, this.keepAliveMsecs);
socket.unref();
return true; copy
This method can be overridden by a particular Agent subclass. If this
method returns a falsy value, the socket will be destroyed instead of persisting
it for use with the next request.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.reuseSocket(socket, request)#

Added in: v8.1.0


socket <stream.Duplex>
request <http.ClientRequest>

Called when socket is attached to request after being persisted because of
the keep-alive options. Default behavior is to:
socket.ref(); copy
This method can be overridden by a particular Agent subclass.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.destroy()#

Added in: v0.11.4

Destroy any sockets that are currently in use by the agent.
It is usually not necessary to do this. However, if using an
agent with keepAlive enabled, then it is best to explicitly shut down
the agent when it is no longer needed. Otherwise,
sockets might stay open for quite a long time before the server
terminates them.

agent.freeSockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.11.4
Added in: v0.11.4




<Object>

An object which contains arrays of sockets currently awaiting use by
the agent when keepAlive is enabled. Do not modify.
Sockets in the freeSockets list will be automatically destroyed and
removed from the array on 'timeout'.

agent.getName([options])#

History

VersionChanges
v17.7.0, v16.15.0
The options parameter is now optional.
v0.11.4
Added in: v0.11.4




options <Object> A set of options providing information for name generation

host <string> A domain name or IP address of the server to issue the
request to
port <number> Port of remote server
localAddress <string> Local interface to bind for network connections
when issuing the request
family <integer> Must be 4 or 6 if this doesn't equal undefined.


Returns: <string>

Get a unique name for a set of request options, to determine whether a
connection can be reused. For an HTTP agent, this returns
host:port:localAddress or host:port:localAddress:family. For an HTTPS agent,
the name includes the CA, cert, ciphers, and other HTTPS/TLS-specific options
that determine socket reusability.

agent.maxFreeSockets#

Added in: v0.11.7


<number>

By default set to 256. For agents with keepAlive enabled, this
sets the maximum number of sockets that will be left open in the free
state.

agent.maxSockets#

Added in: v0.3.6


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open per origin. Origin is the returned value of agent.getName().

agent.maxTotalSockets#

Added in: v14.5.0, v12.19.0


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open. Unlike maxSockets, this parameter applies across all origins.

agent.requests#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.5.9
Added in: v0.5.9




<Object>

An object which contains queues of requests that have not yet been assigned to
sockets. Do not modify.

agent.sockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.3.6
Added in: v0.3.6




<Object>

An object which contains arrays of sockets currently in use by the
agent. Do not modify.

Class: http.ClientRequest#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally and returned from http.request(). It
represents an in-progress request whose header has already been queued. The
header is still mutable using the setHeader(name, value),
getHeader(name), removeHeader(name) API. The actual header will
be sent along with the first data chunk or when calling request.end().
To get the response, add a listener for 'response' to the request object.
'response' will be emitted from the request object when the response
headers have been received. The 'response' event is executed with one
argument which is an instance of http.IncomingMessage.
During the 'response' event, one can add listeners to the
response object; particularly to listen for the 'data' event.
If no 'response' handler is added, then the response will be
entirely discarded. However, if a 'response' event handler is added,
then the data from the response object must be consumed, either by
calling response.read() whenever there is a 'readable' event, or
by adding a 'data' handler, or by calling the .resume() method.
Until the data is consumed, the 'end' event will not fire. Also, until
the data is read it will consume memory that can eventually lead to a
'process out of memory' error.
For backward compatibility, res will only emit 'error' if there is an
'error' listener registered.
Set Content-Length header to limit the response body size.
If response.strictContentLength is set to true, mismatching the
Content-Length header value will result in an Error being thrown,
identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.
Content-Length value should be in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes.

Event: 'abort'#

Added in: v1.4.1Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for the 'close' event instead.
Emitted when the request has been aborted by the client. This event is only
emitted on the first call to abort().

Event: 'close'#

Added in: v0.5.4

Indicates that the request is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'connect'#

Added in: v0.7.0


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with a CONNECT method. If
this event is not being listened for, clients receiving a CONNECT method will
have their connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client and server pair demonstrating how to listen for the 'connect' event:

import { createServer, request } from 'node:http';
import { connect } from 'node:net';
import { URL } from 'node:url';

// Create an HTTP tunneling proxy
const proxy = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});const http = require('node:http');
const net = require('node:net');
const { URL } = require('node:url');

// Create an HTTP tunneling proxy
const proxy = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = net.connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = http.request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});copy

Event: 'continue'#

Added in: v0.3.2

Emitted when the server sends a '100 Continue' HTTP response, usually because
the request contained 'Expect: 100-continue'. This is an instruction that
the client should send the request body.

Event: 'finish'#

Added in: v0.3.6

Emitted when the request has been sent. More specifically, this event is emitted
when the last segment of the response headers and body have been handed off to
the operating system for transmission over the network. It does not imply that
the server has received anything yet.

Event: 'information'#

Added in: v10.0.0


info <Object>

httpVersion <string>
httpVersionMajor <integer>
httpVersionMinor <integer>
statusCode <integer>
statusMessage <string>
headers <Object>
rawHeaders <string[]>



Emitted when the server sends a 1xx intermediate response (excluding 101
Upgrade). The listeners of this event will receive an object containing the
HTTP version, status code, status message, key-value headers object,
and array with the raw header names followed by their respective values.

import { request } from 'node:http';

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});const http = require('node:http');

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = http.request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});copy
101 Upgrade statuses do not fire this event due to their break from the
traditional HTTP request/response chain, such as web sockets, in-place TLS
upgrades, or HTTP 2.0. To be notified of 101 Upgrade notices, listen for the
'upgrade' event instead.

Event: 'response'#

Added in: v0.1.0


response <http.IncomingMessage>

Emitted when a response is received to this request. This event is emitted only
once.

Event: 'socket'#

Added in: v0.5.3


socket <stream.Duplex>

This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'timeout'#

Added in: v0.7.8

Emitted when the underlying socket times out from inactivity. This only notifies
that the socket has been idle. The request must be destroyed manually.
See also: request.setTimeout().

Event: 'upgrade'#

Added in: v0.1.94


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with an upgrade. If this
event is not being listened for and the response status code is 101 Switching
Protocols, clients receiving an upgrade header will have their connections
closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client server pair demonstrating how to listen for the 'upgrade' event.

import http from 'node:http';
import process from 'node:process';

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});const http = require('node:http');

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});copy

request.abort()#

Added in: v0.3.8Deprecated since: v14.1.0, v13.14.0

Stability: 0 - Deprecated: Use request.destroy() instead.
Marks the request as aborting. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.

request.aborted#

History

VersionChanges
v17.0.0, v16.12.0
Deprecated since: v17.0.0, v16.12.0
v11.0.0
The aborted property is no longer a timestamp number.
v0.11.14
Added in: v0.11.14



Stability: 0 - Deprecated. Check request.destroyed instead.

<boolean>

The request.aborted property will be true if the request has
been aborted.

request.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use request.socket.

<stream.Duplex>

See request.socket.

request.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

request.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ClientRequest.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

Finishes sending the request. If any parts of the body are
unsent, it will flush them to the stream. If the request is
chunked, this will send the terminating '0\r\n\r\n'.
If data is specified, it is equivalent to calling
request.write(data, encoding) followed by request.end(callback).
If callback is specified, it will be called when the request stream
is finished.

request.destroy([error])#

History

VersionChanges
v14.5.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error> Optional, an error to emit with 'error' event.
Returns: <this>

Destroy the request. Optionally emit an 'error' event,
and emit a 'close' event. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.
See writable.destroy() for further details.

request.destroyed#

Added in: v14.1.0, v13.14.0


<boolean>

Is true after request.destroy() has been called.
See writable.destroyed for further details.

request.finished#

Added in: v0.0.1Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use request.writableEnded.

<boolean>

The request.finished property will be true if request.end()
has been called. request.end() will automatically be called if the
request was initiated via http.get().

request.flushHeaders()#

Added in: v1.6.0

Flushes the request headers.
For efficiency reasons, Node.js normally buffers the request headers until
request.end() is called or the first chunk of request data is written. It
then tries to pack the request headers and data into a single TCP packet.
That's usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. request.flushHeaders() bypasses
the optimization and kickstarts the request.

request.getHeader(name)#

Added in: v1.6.0


name <string>
Returns: <any>

Reads out a header on the request. The name is case-insensitive.
The type of the return value depends on the arguments provided to
request.setHeader().
request.setHeader('content-type', 'text/html');
request.setHeader('Content-Length', Buffer.byteLength(body));
request.setHeader('Cookie', ['type=ninja', 'language=javascript']);
const contentType = request.getHeader('Content-Type');
// 'contentType' is 'text/html'
const contentLength = request.getHeader('Content-Length');
// 'contentLength' is of type number
const cookie = request.getHeader('Cookie');
// 'cookie' is of type string[] copy

request.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getHeaderNames();
// headerNames === ['foo', 'cookie'] copy

request.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the request.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headers = request.getHeaders();
// headers === { foo: 'bar', 'cookie': ['foo=bar', 'bar=baz'] } copy

request.getRawHeaderNames()#

Added in: v15.13.0, v14.17.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing raw
headers. Header names are returned with their exact casing being set.
request.setHeader('Foo', 'bar');
request.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getRawHeaderNames();
// headerNames === ['Foo', 'Set-Cookie'] copy

request.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = request.hasHeader('content-type'); copy

request.maxHeadersCount#

<number> Default: 2000

Limits maximum response headers count. If set to 0, no limit will be applied.

request.path#

Added in: v0.4.0


<string> The request path.


request.method#

Added in: v0.1.97


<string> The request method.


request.host#

Added in: v14.5.0, v12.19.0


<string> The request host.


request.protocol#

Added in: v14.5.0, v12.19.0


<string> The request protocol.


request.removeHeader(name)#

Added in: v1.6.0


name <string>

Removes a header that's already defined into headers object.
request.removeHeader('Content-Type'); copy

request.reusedSocket#

Added in: v13.0.0, v12.16.0


<boolean> Whether the request is send through a reused socket.

When sending request through a keep-alive enabled agent, the underlying socket
might be reused. But if server closes connection at unfortunate time, client
may run into a 'ECONNRESET' error.

import http from 'node:http';

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutconst http = require('node:http');

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutcopy
By marking a request whether it reused socket or not, we can do
automatic error retry base on it.

import http from 'node:http';
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();const http = require('node:http');
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();copy

request.setHeader(name, value)#

Added in: v1.6.0


name <string>
value <any>

Sets a single header value for headers object. If this header already exists in
the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, request.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission.
request.setHeader('Content-Type', 'application/json'); copy
or
request.setHeader('Cookie', ['type=ninja', 'language=javascript']); copy
When the value is a string an exception will be thrown if it contains
characters outside the latin1 encoding.
If you need to pass UTF-8 characters in the value please encode the value
using the RFC 8187 standard.
const filename = 'Rock 🎵.txt';
request.setHeader('Content-Disposition', `attachment; filename*=utf-8''${encodeURIComponent(filename)}`); copy

request.setNoDelay([noDelay])#

Added in: v0.5.9


noDelay <boolean>

Once a socket is assigned to this request and is connected
socket.setNoDelay() will be called.

request.setSocketKeepAlive([enable][, initialDelay])#

Added in: v0.5.9


enable <boolean>
initialDelay <number>

Once a socket is assigned to this request and is connected
socket.setKeepAlive() will be called.

request.setTimeout(timeout[, callback])#

History

VersionChanges
v9.0.0
Consistently set socket timeout only when the socket connects.
v0.5.9
Added in: v0.5.9




timeout <number> Milliseconds before a request times out.
callback <Function> Optional function to be called when a timeout occurs.
Same as binding to the 'timeout' event.
Returns: <http.ClientRequest>

Once a socket is assigned to this request and is connected
socket.setTimeout() will be called.

request.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket.

import http from 'node:http';
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});const http = require('node:http');
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

request.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

request.writableEnded#

Added in: v12.9.0


<boolean>

Is true after request.end() has been called. This property
does not indicate whether the data has been flushed, for this use
request.writableFinished instead.

request.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

request.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times. If no
Content-Length is set, data will automatically be encoded in HTTP Chunked
transfer encoding, so that server knows when the data ends. The
Transfer-Encoding: chunked header is added. Calling request.end()
is necessary to finish sending the request.
The encoding argument is optional and only applies when chunk is a string.
Defaults to 'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed, but only if the chunk is non-empty.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.
When write function is called with empty string or buffer, it does
nothing and waits for more input.

Class: http.Server#

Added in: v0.1.17


Extends: <net.Server>


Event: 'checkContinue'#

Added in: v0.3.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect: 100-continue is received.
If this event is not listened for, the server will automatically respond
with a 100 Continue as appropriate.
Handling this event involves calling response.writeContinue() if the
client should continue to send the request body, or generating an appropriate
HTTP response (e.g. 400 Bad Request) if the client should not continue to send
the request body.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'checkExpectation'#

Added in: v5.5.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect header is received, where the
value is not 100-continue. If this event is not listened for, the server will
automatically respond with a 417 Expectation Failed as appropriate.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'clientError'#

History

VersionChanges
v12.0.0
The default behavior will return a 431 Request Header Fields Too Large if a HPE_HEADER_OVERFLOW error occurs.
v9.4.0
The rawPacket is the current buffer that just parsed. Adding this buffer to the error object of 'clientError' event is to make it possible that developers can log the broken packet.
v6.0.0
The default action of calling .destroy() on the socket will no longer take place if there are listeners attached for 'clientError'.
v0.1.94
Added in: v0.1.94




exception <Error>
socket <stream.Duplex>

If a client connection emits an 'error' event, it will be forwarded here.
Listener of this event is responsible for closing/destroying the underlying
socket. For example, one may wish to more gracefully close the socket with a
custom HTTP response instead of abruptly severing the connection. The socket
must be closed or destroyed before the listener ends.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
Default behavior is to try close the socket with a HTTP '400 Bad Request',
or a HTTP '431 Request Header Fields Too Large' in the case of a
HPE_HEADER_OVERFLOW error. If the socket is not writable or headers
of the current attached http.ServerResponse has been sent, it is
immediately destroyed.
socket is the net.Socket object that the error originated from.

import http from 'node:http';

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);const http = require('node:http');

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);copy
When the 'clientError' event occurs, there is no request or response
object, so any HTTP response sent, including response headers and payload,
must be written directly to the socket object. Care must be taken to
ensure the response is a properly formatted HTTP response message.
err is an instance of Error with two extra columns:

bytesParsed: the bytes count of request packet that Node.js may have parsed
correctly;
rawPacket: the raw packet of current request.

In some cases, the client has already received the response and/or the socket
has already been destroyed, like in case of ECONNRESET errors. Before
trying to send data to the socket, it is better to check that it is still
writable.
server.on('clientError', (err, socket) => {
  if (err.code === 'ECONNRESET' || !socket.writable) {
    return;
  }

  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
}); copy

Event: 'close'#

Added in: v0.1.4

Emitted when the server closes.

Event: 'connect'#

Added in: v0.7.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the tunneling stream (may be empty)

Emitted each time a client requests an HTTP CONNECT method. If this event is
not listened for, then clients requesting a CONNECT method will have their
connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.

Event: 'connection'#

Added in: v0.1.0


socket <stream.Duplex>

This event is emitted when a new TCP stream is established. socket is
typically an object of type net.Socket. Usually users will not want to
access this event. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. The socket can
also be accessed at request.socket.
This event can also be explicitly emitted by users to inject connections
into the HTTP server. In that case, any Duplex stream can be passed.
If socket.setTimeout() is called here, the timeout will be replaced with
server.keepAliveTimeout when the socket has served a request (if
server.keepAliveTimeout is non-zero).
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'dropRequest'#

Added in: v18.7.0, v16.17.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client

When the number of requests on a socket reaches the threshold of
server.maxRequestsPerSocket, the server will drop new requests
and emit 'dropRequest' event instead, then send 503 to client.

Event: 'request'#

Added in: v0.1.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time there is a request. There may be multiple requests
per connection (in the case of HTTP Keep-Alive connections).

Event: 'upgrade'#

History

VersionChanges
v10.0.0
Not listening to this event no longer causes the socket to be destroyed if a client sends an Upgrade header.
v0.1.94
Added in: v0.1.94




request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the upgraded stream (may be empty)

Emitted each time a client requests an HTTP upgrade. Listening to this event
is optional and clients cannot insist on a protocol change.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

server.close([callback])#

History

VersionChanges
v19.0.0
The method closes idle connections before returning.
v0.1.90
Added in: v0.1.90




callback <Function>

Stops the server from accepting new connections and closes all connections
connected to this server which are not sending a request or waiting for
a response.
See net.Server.close().
const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
}, 10000); copy

server.closeAllConnections()#

Added in: v18.2.0

Closes all established HTTP(S) connections connected to this server, including
active connections connected to this server which are sending a request or
waiting for a response. This does not destroy sockets upgraded to a different
protocol, such as WebSocket or HTTP/2.

This is a forceful way of closing all connections and should be used with
caution. Whenever using this in conjunction with server.close, calling this
after server.close is recommended as to avoid race conditions where new
connections are created between a call to this and a call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes all connections, ensuring the server closes successfully
  server.closeAllConnections();
}, 10000); copy

server.closeIdleConnections()#

Added in: v18.2.0

Closes all connections connected to this server which are not sending a request
or waiting for a response.

Starting with Node.js 19.0.0, there's no need for calling this method in
conjunction with server.close to reap keep-alive connections. Using it
won't cause any harm though, and it can be useful to ensure backwards
compatibility for libraries and applications that need to support versions
older than 19.0.0. Whenever using this in conjunction with server.close,
calling this after server.close is recommended as to avoid race
conditions where new connections are created between a call to this and a
call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes idle connections, such as keep-alive connections. Server will close
  // once remaining active connections are terminated
  server.closeIdleConnections();
}, 10000); copy

server.headersTimeout#

History

VersionChanges
v19.4.0, v18.14.0
The default is now set to the minimum between 60000 (60 seconds) or requestTimeout.
v11.3.0, v10.14.0
Added in: v11.3.0, v10.14.0




<number> Default: The minimum between server.requestTimeout or 60000.

Limit the amount of time the parser will wait to receive the complete HTTP
headers.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.listen()#
Starts the HTTP server listening for connections.
This method is identical to server.listen() from net.Server.

server.listening#

Added in: v5.7.0


<boolean> Indicates whether or not the server is listening for connections.


server.maxHeadersCount#

Added in: v0.7.0


<number> Default: 2000

Limits maximum incoming headers count. If set to 0, no limit will be applied.

server.requestTimeout#

History

VersionChanges
v18.0.0
The default request timeout changed from no timeout to 300s (5 minutes).
v14.11.0
Added in: v14.11.0




<number> Default: 300000

Sets the timeout value in milliseconds for receiving the entire request from
the client.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.setTimeout([msecs][, callback])#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




msecs <number> Default: 0 (no timeout)
callback <Function>
Returns: <http.Server>

Sets the timeout value for sockets, and emits a 'timeout' event on
the Server object, passing the socket as an argument, if a timeout
occurs.
If there is a 'timeout' event listener on the Server object, then it
will be called with the timed-out socket as an argument.
By default, the Server does not timeout sockets. However, if a callback
is assigned to the Server's 'timeout' event, timeouts must be handled
explicitly.

server.maxRequestsPerSocket#

Added in: v16.10.0


<number> Requests per socket. Default: 0 (no limit)

The maximum number of requests socket can handle
before closing keep alive connection.
A value of 0 will disable the limit.
When the limit is reached it will set the Connection header value to close,
but will not actually close the connection, subsequent requests sent
after the limit is reached will get 503 Service Unavailable as a response.

server.timeout#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




<number> Timeout in milliseconds. Default: 0 (no timeout)

The number of milliseconds of inactivity before a socket is presumed
to have timed out.
A value of 0 will disable the timeout behavior on incoming connections.
The socket timeout logic is set up on connection, so changing this
value only affects new connections to the server, not any existing connections.

server.keepAliveTimeout#

Added in: v8.0.0


<number> Timeout in milliseconds. Default: 5000 (5 seconds).

The number of milliseconds of inactivity a server needs to wait for additional
incoming data, after it has finished writing the last response, before a socket
will be destroyed. If the server receives new data before the keep-alive
timeout has fired, it will reset the regular inactivity timeout, i.e.,
server.timeout.
A value of 0 will disable the keep-alive timeout behavior on incoming
connections.
A value of 0 makes the http server behave similarly to Node.js versions prior
to 8.0.0, which did not have a keep-alive timeout.
The socket timeout logic is set up on connection, so changing this value only
affects new connections to the server, not any existing connections.

server[Symbol.asyncDispose]()#

Added in: v20.4.0

Stability: 1 - Experimental
Calls server.close() and returns a promise that fulfills when the
server has closed.

Class: http.ServerResponse#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally by an HTTP server, not by the user. It is
passed as the second parameter to the 'request' event.

Event: 'close'#

Added in: v0.6.7

Indicates that the response is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'finish'#

Added in: v0.3.6

Emitted when the response has been sent. More specifically, this event is
emitted when the last segment of the response headers and body have been
handed off to the operating system for transmission over the network. It
does not imply that the client has received anything yet.

response.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

This method adds HTTP trailing headers (a header but at the end of the
message) to the response.
Trailers will only be emitted if chunked encoding is used for the
response; if it is not (e.g. if the request was HTTP/1.0), they will
be silently discarded.
HTTP requires the Trailer header to be sent in order to
emit trailers, with a list of the header fields in its value. E.g.,
response.writeHead(200, { 'Content-Type': 'text/plain',
                          'Trailer': 'Content-MD5' });
response.write(fileData);
response.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
response.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

response.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use response.socket.

<stream.Duplex>

See response.socket.

response.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

response.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ServerResponse.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

This method signals to the server that all of the response headers and body
have been sent; that server should consider this message complete.
The method, response.end(), MUST be called on each response.
If data is specified, it is similar in effect to calling
response.write(data, encoding) followed by response.end(callback).
If callback is specified, it will be called when the response stream
is finished.

response.finished#

Added in: v0.0.2Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use response.writableEnded.

<boolean>

The response.finished property will be true if response.end()
has been called.

response.flushHeaders()#

Added in: v1.6.0

Flushes the response headers. See also: request.flushHeaders().

response.getHeader(name)#

Added in: v0.4.0


name <string>
Returns: <any>

Reads out a header that's already been queued but not sent to the client.
The name is case-insensitive. The type of the return value depends
on the arguments provided to response.setHeader().
response.setHeader('Content-Type', 'text/html');
response.setHeader('Content-Length', Buffer.byteLength(body));
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']);
const contentType = response.getHeader('content-type');
// contentType is 'text/html'
const contentLength = response.getHeader('Content-Length');
// contentLength is of type number
const setCookie = response.getHeader('set-cookie');
// setCookie is of type string[] copy

response.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = response.getHeaderNames();
// headerNames === ['foo', 'set-cookie'] copy

response.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the response.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = response.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

response.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = response.hasHeader('content-type'); copy

response.headersSent#

Added in: v0.9.3


<boolean>

Boolean (read-only). True if headers were sent, false otherwise.

response.removeHeader(name)#

Added in: v0.4.0


name <string>

Removes a header that's queued for implicit sending.
response.removeHeader('Content-Encoding'); copy

response.req#

Added in: v15.7.0


<http.IncomingMessage>

A reference to the original HTTP request object.

response.sendDate#

Added in: v0.7.5


<boolean>

When true, the Date header will be automatically generated and sent in
the response if it is not already present in the headers. Defaults to true.
This should only be disabled for testing; HTTP requires the Date header
in responses.

response.setHeader(name, value)#

Added in: v0.4.0


name <string>
value <any>
Returns: <http.ServerResponse>

Returns the response object.
Sets a single header value for implicit headers. If this header already exists
in the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, response.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission. The same response object is returned to the caller,
to enable call chaining.
response.setHeader('Content-Type', 'text/html'); copy
or
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
If response.writeHead() method is called and this method has not been
called, it will directly write the supplied header values onto the network
channel without caching internally, and the response.getHeader() on the
header will not yield the expected result. If progressive population of headers
is desired with potential future retrieval and modification, use
response.setHeader() instead of response.writeHead().

response.setTimeout(msecs[, callback])#

Added in: v0.9.12


msecs <number>
callback <Function>
Returns: <http.ServerResponse>

Sets the Socket's timeout value to msecs. If a callback is
provided, then it is added as a listener on the 'timeout' event on
the response object.
If no 'timeout' listener is added to the request, the response, or
the server, then sockets are destroyed when they time out. If a handler is
assigned to the request, the response, or the server's 'timeout' events,
timed out sockets must be handled explicitly.

response.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. After
response.end(), the property is nulled.

import http from 'node:http';
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);const http = require('node:http');
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

response.statusCode#

Added in: v0.4.0


<number> Default: 200

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status code that will be sent to the client when
the headers get flushed.
response.statusCode = 404; copy
After response header was sent to the client, this property indicates the
status code which was sent out.

response.statusMessage#

Added in: v0.11.8


<string>

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status message that will be sent to the client when
the headers get flushed. If this is left as undefined then the standard
message for the status code will be used.
response.statusMessage = 'Not found'; copy
After response header was sent to the client, this property indicates the
status message which was sent out.

response.strictContentLength#

Added in: v18.10.0, v16.18.0


<boolean> Default: false

If set to true, Node.js will check whether the Content-Length
header value and the size of the body, in bytes, are equal.
Mismatching the Content-Length header value will result
in an Error being thrown, identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.

response.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

response.writableEnded#

Added in: v12.9.0


<boolean>

Is true after response.end() has been called. This property
does not indicate whether the data has been flushed, for this use
response.writableFinished instead.

response.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

response.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: 'utf8'
callback <Function>
Returns: <boolean>

If this method is called and response.writeHead() has not been called,
it will switch to implicit header mode and flush the implicit headers.
This sends a chunk of the response body. This method may
be called multiple times to provide successive parts of the body.
If rejectNonStandardBodyWrites is set to true in createServer
then writing to the body is not allowed when the request method or response
status do not support content. If an attempt is made to write to the body for a
HEAD request or as part of a 204 or 304response, a synchronous Error
with the code ERR_HTTP_BODY_NOT_ALLOWED is thrown.
chunk can be a string or a buffer. If chunk is a string,
the second parameter specifies how to encode it into a byte stream.
callback will be called when this chunk of data is flushed.
This is the raw HTTP body and has nothing to do with higher-level multi-part
body encodings that may be used.
The first time response.write() is called, it will send the buffered
header information and the first chunk of the body to the client. The second
time response.write() is called, Node.js assumes data will be streamed,
and sends the new data separately. That is, the response is buffered up to the
first chunk of the body.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.

response.writeContinue()#

Added in: v0.3.0

Sends an HTTP/1.1 100 Continue message to the client, indicating that
the request body should be sent. See the 'checkContinue' event on
Server.

response.writeEarlyHints(hints[, callback])#

History

VersionChanges
v18.11.0
Allow passing hints as an object.
v18.11.0
Added in: v18.11.0




hints <Object>
callback <Function>

Sends an HTTP/1.1 103 Early Hints message to the client with a Link header,
indicating that the user agent can preload/preconnect the linked resources.
The hints is an object containing the values of headers to be sent with
early hints message. The optional callback argument will be called when
the response message has been written.
Example
const earlyHintsLink = '</styles.css>; rel=preload; as=style';
response.writeEarlyHints({
  'link': earlyHintsLink,
});

const earlyHintsLinks = [
  '</styles.css>; rel=preload; as=style',
  '</scripts.js>; rel=preload; as=script',
];
response.writeEarlyHints({
  'link': earlyHintsLinks,
  'x-trace-id': 'id for diagnostics',
});

const earlyHintsCallback = () => console.log('early hints message sent');
response.writeEarlyHints({
  'link': earlyHintsLinks,
}, earlyHintsCallback); copy

response.writeHead(statusCode[, statusMessage][, headers])#

History

VersionChanges
v14.14.0
Allow passing headers as an array.
v11.10.0, v10.17.0
Return this from writeHead() to allow chaining with end().
v5.11.0, v4.4.5
A RangeError is thrown if statusCode is not a number in the range [100, 999].
v0.1.30
Added in: v0.1.30




statusCode <number>
statusMessage <string>
headers <Object> | <Array>
Returns: <http.ServerResponse>

Sends a response header to the request. The status code is a 3-digit HTTP
status code, like 404. The last argument, headers, are the response headers.
Optionally one can give a human-readable statusMessage as the second
argument.
headers may be an Array where the keys and values are in the same list.
It is not a list of tuples. So, the even-numbered offsets are key values,
and the odd-numbered offsets are the associated values. The array is in the same
format as request.rawHeaders.
Returns a reference to the ServerResponse, so that calls can be chained.
const body = 'hello world';
response
  .writeHead(200, {
    'Content-Length': Buffer.byteLength(body),
    'Content-Type': 'text/plain',
  })
  .end(body); copy
This method must only be called once on a message and it must
be called before response.end() is called.
If response.write() or response.end() are called before calling
this, the implicit/mutable headers will be calculated and call this function.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
If this method is called and response.setHeader() has not been called,
it will directly write the supplied header values onto the network channel
without caching internally, and the response.getHeader() on the header
will not yield the expected result. If progressive population of headers is
desired with potential future retrieval and modification, use
response.setHeader() instead.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
Content-Length is read in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes. Node.js
will check whether Content-Length and the length of the body which has
been transmitted are equal or not.
Attempting to set a header field name or value that contains invalid characters
will result in a [Error][] being thrown.

response.writeProcessing()#

Added in: v10.0.0

Sends a HTTP/1.1 102 Processing message to the client, indicating that
the request body should be sent.

Class: http.IncomingMessage#

History

VersionChanges
v15.5.0
The destroyed value returns true after the incoming data is consumed.
v13.1.0, v12.16.0
The readableHighWaterMark value mirrors that of the socket.
v0.1.17
Added in: v0.1.17




Extends: <stream.Readable>

An IncomingMessage object is created by http.Server or
http.ClientRequest and passed as the first argument to the 'request'
and 'response' event respectively. It may be used to access response
status, headers, and data.
Different from its socket value which is a subclass of <stream.Duplex>, the
IncomingMessage itself extends <stream.Readable> and is created separately to
parse and emit the incoming HTTP headers and payload, as the underlying socket
may be reused multiple times in case of keep-alive.

Event: 'aborted'#

Added in: v0.3.8Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for 'close' event instead.
Emitted when the request has been aborted.

Event: 'close'#

History

VersionChanges
v16.0.0
The close event is now emitted when the request has been completed and not when the underlying socket is closed.
v0.4.2
Added in: v0.4.2



Emitted when the request has been completed.

message.aborted#

Added in: v10.1.0Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Check message.destroyed from <stream.Readable>.

<boolean>

The message.aborted property will be true if the request has
been aborted.

message.complete#

Added in: v0.3.0


<boolean>

The message.complete property will be true if a complete HTTP message has
been received and successfully parsed.
This property is particularly useful as a means of determining if a client or
server fully transmitted a message before a connection was terminated:
const req = http.request({
  host: '127.0.0.1',
  port: 8080,
  method: 'POST',
}, (res) => {
  res.resume();
  res.on('end', () => {
    if (!res.complete)
      console.error(
        'The connection was terminated while the message was still being sent');
  });
}); copy

message.connection#

Added in: v0.1.90Deprecated since: v16.0.0

Stability: 0 - Deprecated. Use message.socket.
Alias for message.socket.

message.destroy([error])#

History

VersionChanges
v14.5.0, v12.19.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error>
Returns: <this>

Calls destroy() on the socket that received the IncomingMessage. If error
is provided, an 'error' event is emitted on the socket and error is passed
as an argument to any listeners on the event.

message.headers#

History

VersionChanges
v19.5.0, v18.14.0
The joinDuplicateHeaders option in the http.request() and http.createServer() functions ensures that duplicate headers are not discarded, but rather combined using a comma separator, in accordance with RFC 9110 Section 5.3.
v15.1.0
message.headers is now lazily computed using an accessor property on the prototype and is no longer enumerable.
v0.1.5
Added in: v0.1.5




<Object>

The request/response headers object.
Key-value pairs of header names and values. Header names are lower-cased.
// Prints something like:
//
// { 'user-agent': 'curl/7.22.0',
//   host: '127.0.0.1:8000',
//   accept: '*/*' }
console.log(request.headers); copy
Duplicates in raw headers are handled in the following ways, depending on the
header name:

Duplicates of age, authorization, content-length, content-type,
etag, expires, from, host, if-modified-since, if-unmodified-since,
last-modified, location, max-forwards, proxy-authorization, referer,
retry-after, server, or user-agent are discarded.
To allow duplicate values of the headers listed above to be joined,
use the option joinDuplicateHeaders in http.request()
and http.createServer(). See RFC 9110 Section 5.3 for more
information.
set-cookie is always an array. Duplicates are added to the array.
For duplicate cookie headers, the values are joined together with ; .
For all other headers, the values are joined together with , .


message.headersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.headers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
// Prints something like:
//
// { 'user-agent': ['curl/7.22.0'],
//   host: ['127.0.0.1:8000'],
//   accept: ['*/*'] }
console.log(request.headersDistinct); copy

message.httpVersion#

Added in: v0.1.1


<string>

In case of server request, the HTTP version sent by the client. In the case of
client response, the HTTP version of the connected-to server.
Probably either '1.1' or '1.0'.
Also message.httpVersionMajor is the first integer and
message.httpVersionMinor is the second.

message.method#

Added in: v0.1.1


<string>

Only valid for request obtained from http.Server.
The request method as a string. Read only. Examples: 'GET', 'DELETE'.

message.rawHeaders#

Added in: v0.11.6


<string[]>

The raw request/response headers list exactly as they were received.
The keys and values are in the same list. It is not a
list of tuples. So, the even-numbered offsets are key values, and the
odd-numbered offsets are the associated values.
Header names are not lowercased, and duplicates are not merged.
// Prints something like:
//
// [ 'user-agent',
//   'this is invalid because there can be only one',
//   'User-Agent',
//   'curl/7.22.0',
//   'Host',
//   '127.0.0.1:8000',
//   'ACCEPT',
//   '*/*' ]
console.log(request.rawHeaders); copy

message.rawTrailers#

Added in: v0.11.6


<string[]>

The raw request/response trailer keys and values exactly as they were
received. Only populated at the 'end' event.

message.setTimeout(msecs[, callback])#

Added in: v0.5.9


msecs <number>
callback <Function>
Returns: <http.IncomingMessage>

Calls message.socket.setTimeout(msecs, callback).

message.socket#

Added in: v0.3.0


<stream.Duplex>

The net.Socket object associated with the connection.
With HTTPS support, use request.socket.getPeerCertificate() to obtain the
client's authentication details.
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket> or internally nulled.

message.statusCode#

Added in: v0.1.1


<number>

Only valid for response obtained from http.ClientRequest.
The 3-digit HTTP response status code. E.G. 404.

message.statusMessage#

Added in: v0.11.10


<string>

Only valid for response obtained from http.ClientRequest.
The HTTP response status message (reason phrase). E.G. OK or Internal Server Error.

message.trailers#

Added in: v0.3.0


<Object>

The request/response trailers object. Only populated at the 'end' event.

message.trailersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.trailers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
Only populated at the 'end' event.

message.url#

Added in: v0.1.90


<string>

Only valid for request obtained from http.Server.
Request URL string. This contains only the URL that is present in the actual
HTTP request. Take the following request:
GET /status?name=ryan HTTP/1.1
Accept: text/plain copy
To parse the URL into its parts:
new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`); copy
When request.url is '/status?name=ryan' and process.env.HOST is undefined:
$ node
> new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`);
URL {
  href: 'http://localhost/status?name=ryan',
  origin: 'http://localhost',
  protocol: 'http:',
  username: '',
  password: '',
  host: 'localhost',
  hostname: 'localhost',
  port: '',
  pathname: '/status',
  search: '?name=ryan',
  searchParams: URLSearchParams { 'name' => 'ryan' },
  hash: ''
} copy
Ensure that you set process.env.HOST to the server's host name, or consider
replacing this part entirely. If using req.headers.host, ensure proper
validation is used, as clients may specify a custom Host header.

Class: http.OutgoingMessage#

Added in: v0.1.17


Extends: <Stream>

This class serves as the parent class of http.ClientRequest
and http.ServerResponse. It is an abstract outgoing message from
the perspective of the participants of an HTTP transaction.

Event: 'drain'#

Added in: v0.3.6

Emitted when the buffer of the message is free again.

Event: 'finish'#

Added in: v0.1.17

Emitted when the transmission is finished successfully.

Event: 'prefinish'#

Added in: v0.11.6

Emitted after outgoingMessage.end() is called.
When the event is emitted, all data has been processed but not necessarily
completely flushed.

outgoingMessage.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

Adds HTTP trailers (headers but at the end of the message) to the message.
Trailers will only be emitted if the message is chunked encoded. If not,
the trailers will be silently discarded.
HTTP requires the Trailer header to be sent to emit trailers,
with a list of header field names in its value, e.g.
message.writeHead(200, { 'Content-Type': 'text/plain',
                         'Trailer': 'Content-MD5' });
message.write(fileData);
message.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
message.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

outgoingMessage.appendHeader(name, value)#

Added in: v18.3.0, v16.17.0


name <string> Header name
value <string> | <string[]> Header value
Returns: <this>

Append a single header value to the header object.
If the value is an array, this is equivalent to calling this method multiple
times.
If there were no previous values for the header, this is equivalent to calling
outgoingMessage.setHeader(name, value).
Depending of the value of options.uniqueHeaders when the client request or the
server were created, this will end up in the header being sent multiple times or
a single time with values joined using ; .

outgoingMessage.connection#

Added in: v0.3.0Deprecated since: v15.12.0, v14.17.1

Stability: 0 - Deprecated: Use outgoingMessage.socket instead.
Alias of outgoingMessage.socket.

outgoingMessage.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

outgoingMessage.destroy([error])#

Added in: v0.3.0


error <Error> Optional, an error to emit with error event
Returns: <this>

Destroys the message. Once a socket is associated with the message
and is connected, that socket will be destroyed as well.

outgoingMessage.end(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
add callback argument.
v0.1.90
Added in: v0.1.90




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Optional, Default: utf8
callback <Function> Optional
Returns: <this>

Finishes the outgoing message. If any parts of the body are unsent, it will
flush them to the underlying system. If the message is chunked, it will
send the terminating chunk 0\r\n\r\n, and send the trailers (if any).
If chunk is specified, it is equivalent to calling
outgoingMessage.write(chunk, encoding), followed by
outgoingMessage.end(callback).
If callback is provided, it will be called when the message is finished
(equivalent to a listener of the 'finish' event).

outgoingMessage.flushHeaders()#

Added in: v1.6.0

Flushes the message headers.
For efficiency reason, Node.js normally buffers the message headers
until outgoingMessage.end() is called or the first chunk of message data
is written. It then tries to pack the headers and data into a single TCP
packet.
It is usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. outgoingMessage.flushHeaders()
bypasses the optimization and kickstarts the message.

outgoingMessage.getHeader(name)#

Added in: v0.4.0


name <string> Name of header
Returns: <string> | <undefined>

Gets the value of the HTTP header with the given name. If that header is not
set, the returned value will be undefined.

outgoingMessage.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All names are lowercase.

outgoingMessage.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow
copy is used, array values may be mutated without additional calls to
various header-related HTTP module methods. The keys of the returned
object are the header names and the values are the respective header
values. All header names are lowercase.
The object returned by the outgoingMessage.getHeaders() method does
not prototypically inherit from the JavaScript Object. This means that
typical Object methods such as obj.toString(), obj.hasOwnProperty(),
and others are not defined and will not work.
outgoingMessage.setHeader('Foo', 'bar');
outgoingMessage.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = outgoingMessage.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

outgoingMessage.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name is case-insensitive.
const hasContentType = outgoingMessage.hasHeader('content-type'); copy

outgoingMessage.headersSent#

Added in: v0.9.3


<boolean>

Read-only. true if the headers were sent, otherwise false.

outgoingMessage.pipe()#

Added in: v9.0.0

Overrides the stream.pipe() method inherited from the legacy Stream class
which is the parent class of http.OutgoingMessage.
Calling this method will throw an Error because outgoingMessage is a
write-only stream.

outgoingMessage.removeHeader(name)#

Added in: v0.4.0


name <string> Header name

Removes a header that is queued for implicit sending.
outgoingMessage.removeHeader('Content-Encoding'); copy

outgoingMessage.setHeader(name, value)#

Added in: v0.4.0


name <string> Header name
value <any> Header value
Returns: <this>

Sets a single header value. If the header already exists in the to-be-sent
headers, its value will be replaced. Use an array of strings to send multiple
headers with the same name.

outgoingMessage.setHeaders(headers)#

Added in: v19.6.0, v18.15.0


headers <Headers> | <Map>
Returns: <this>

Sets multiple header values for implicit headers.
headers must be an instance of Headers or Map,
if a header already exists in the to-be-sent headers,
its value will be replaced.
const headers = new Headers({ foo: 'bar' });
outgoingMessage.setHeaders(headers); copy
or
const headers = new Map([['foo', 'bar']]);
outgoingMessage.setHeaders(headers); copy
When headers have been set with outgoingMessage.setHeaders(),
they will be merged with any headers passed to response.writeHead(),
with the headers passed to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  const headers = new Headers({ 'Content-Type': 'text/html' });
  res.setHeaders(headers);
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy

outgoingMessage.setTimeout(msesc[, callback])#

Added in: v0.9.12


msesc <number>
callback <Function> Optional function to be called when a timeout
occurs. Same as binding to the timeout event.
Returns: <this>

Once a socket is associated with the message and is connected,
socket.setTimeout() will be called with msecs as the first parameter.

outgoingMessage.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually, users will not want to access
this property.
After calling outgoingMessage.end(), this property will be nulled.

outgoingMessage.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork()

outgoingMessage.writableCorked#

Added in: v13.2.0, v12.16.0


<number>

The number of times outgoingMessage.cork() has been called.

outgoingMessage.writableEnded#

Added in: v12.9.0


<boolean>

Is true if outgoingMessage.end() has been called. This property does
not indicate whether the data has been flushed. For that purpose, use
message.writableFinished instead.

outgoingMessage.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system.

outgoingMessage.writableHighWaterMark#

Added in: v12.9.0


<number>

The highWaterMark of the underlying socket if assigned. Otherwise, the default
buffer level when writable.write() starts returning false (16384).

outgoingMessage.writableLength#

Added in: v12.9.0


<number>

The number of buffered bytes.

outgoingMessage.writableObjectMode#

Added in: v12.9.0


<boolean>

Always false.

outgoingMessage.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
The callback argument was added.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: utf8
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times.
The encoding argument is only relevant when chunk is a string. Defaults to
'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in the user
memory. The 'drain' event will be emitted when the buffer is free again.

http.METHODS#

Added in: v0.11.8


<string[]>

A list of the HTTP methods that are supported by the parser.
http.STATUS_CODES#

Added in: v0.1.22


<Object>

A collection of all the standard HTTP response status codes, and the
short description of each. For example, http.STATUS_CODES[404] === 'Not Found'.
http.createServer([options][, requestListener])#

History

VersionChanges
v20.1.0, v18.17.0
The highWaterMark option is supported now.
v18.0.0
The requestTimeout, headersTimeout, keepAliveTimeout, and connectionsCheckingInterval options are supported now.
v18.0.0
The noDelay option now defaults to true.
v17.7.0, v16.15.0
The noDelay, keepAlive and keepAliveInitialDelay options are supported now.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v9.6.0, v8.12.0
The options argument is supported now.
v0.1.13
Added in: v0.1.13





options <Object>

connectionsCheckingInterval: Sets the interval value in milliseconds to
check for request and headers timeout in incomplete requests.
Default: 30000.
headersTimeout: Sets the timeout value in milliseconds for receiving
the complete HTTP headers from the client.
See server.headersTimeout for more information.
Default: 60000.
highWaterMark <number> Optionally overrides all sockets'
readableHighWaterMark and writableHighWaterMark. This affects
highWaterMark property of both IncomingMessage and ServerResponse.
Default: See stream.getDefaultHighWaterMark().
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false.
IncomingMessage <http.IncomingMessage> Specifies the IncomingMessage
class to be used. Useful for extending the original IncomingMessage.
Default: IncomingMessage.
joinDuplicateHeaders <boolean> If set to true, this option allows
joining the field line values of multiple headers in a request with
a comma (, ) instead of discarding the duplicates.
For more information, refer to message.headers.
Default: false.
keepAlive <boolean> If set to true, it enables keep-alive functionality
on the socket immediately after a new incoming connection is received,
similarly on what is done in [socket.setKeepAlive([enable][, initialDelay])][socket.setKeepAlive(enable, initialDelay)].
Default: false.
keepAliveInitialDelay <number> If set to a positive number, it sets the
initial delay before the first keepalive probe is sent on an idle socket.
Default: 0.
keepAliveTimeout: The number of milliseconds of inactivity a server
needs to wait for additional incoming data, after it has finished writing
the last response, before a socket will be destroyed.
See server.keepAliveTimeout for more information.
Default: 5000.
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size for requests received by this server, i.e.
the maximum length of request headers in bytes.
Default: 16384 (16 KiB).
noDelay <boolean> If set to true, it disables the use of Nagle's
algorithm immediately after a new incoming connection is received.
Default: true.
requestTimeout: Sets the timeout value in milliseconds for receiving
the entire request from the client.
See server.requestTimeout for more information.
Default: 300000.
requireHostHeader <boolean> If set to true, it forces the server to
respond with a 400 (Bad Request) status code to any HTTP/1.1
request message that lacks a Host header
(as mandated by the specification).
Default: true.
ServerResponse <http.ServerResponse> Specifies the ServerResponse class
to be used. Useful for extending the original ServerResponse. Default:
ServerResponse.
uniqueHeaders <Array> A list of response headers that should be sent only
once. If the header's value is an array, the items will be joined
using ; .
rejectNonStandardBodyWrites <boolean> If set to true, an error is thrown
when writing to an HTTP response which does not have a body.
Default: false.



requestListener <Function>


Returns: <http.Server>


Returns a new instance of http.Server.
The requestListener is a function which is automatically
added to the 'request' event.

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy
http.get(options[, callback])#
http.get(url[, options][, callback])#

History

VersionChanges
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object> Accepts the same options as
http.request(), with the method set to GET by default.
callback <Function>
Returns: <http.ClientRequest>

Since most requests are GET requests without bodies, Node.js provides this
convenience method. The only difference between this method and
http.request() is that it sets the method to GET by default and calls req.end()
automatically. The callback must take care to consume the response
data for reasons stated in http.ClientRequest section.
The callback is invoked with a single argument that is an instance of
http.IncomingMessage.
JSON fetching example:
http.get('http://localhost:8000/', (res) => {
  const { statusCode } = res;
  const contentType = res.headers['content-type'];

  let error;
  // Any 2xx status code signals a successful response but
  // here we're only checking for 200.
  if (statusCode !== 200) {
    error = new Error('Request Failed.\n' +
                      `Status Code: ${statusCode}`);
  } else if (!/^application\/json/.test(contentType)) {
    error = new Error('Invalid content-type.\n' +
                      `Expected application/json but received ${contentType}`);
  }
  if (error) {
    console.error(error.message);
    // Consume response data to free up memory
    res.resume();
    return;
  }

  res.setEncoding('utf8');
  let rawData = '';
  res.on('data', (chunk) => { rawData += chunk; });
  res.on('end', () => {
    try {
      const parsedData = JSON.parse(rawData);
      console.log(parsedData);
    } catch (e) {
      console.error(e.message);
    }
  });
}).on('error', (e) => {
  console.error(`Got error: ${e.message}`);
});

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000); copy
http.globalAgent#

History

VersionChanges
v19.0.0
The agent now uses HTTP Keep-Alive and a 5 second timeout by default.
v0.5.9
Added in: v0.5.9




<http.Agent>

Global instance of Agent which is used as the default for all HTTP client
requests. Diverges from a default Agent configuration by having keepAlive
enabled and a timeout of 5 seconds.
http.maxHeaderSize#

Added in: v11.6.0, v10.15.0


<number>

Read-only property specifying the maximum allowed size of HTTP headers in bytes.
Defaults to 16 KiB. Configurable using the --max-http-header-size CLI
option.
This can be overridden for servers and client requests by passing the
maxHeaderSize option.
http.request(options[, callback])#
http.request(url[, options][, callback])#

History

VersionChanges
v16.7.0, v14.18.0
When using a URL object parsed username and password will now be properly URI decoded.
v15.3.0, v14.17.0
It is possible to abort a request with an AbortSignal.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object>

agent <http.Agent> | <boolean> Controls Agent behavior. Possible
values:

undefined (default): use http.globalAgent for this host and port.
Agent object: explicitly use the passed in Agent.
false: causes a new Agent with default values to be used.


auth <string> Basic authentication ('user:password') to compute an
Authorization header.
createConnection <Function> A function that produces a socket/stream to
use for the request when the agent option is not used. This can be used to
avoid creating a custom Agent class just to override the default
createConnection function. See agent.createConnection() for more
details. Any Duplex stream is a valid return value.
defaultPort <number> Default port for the protocol. Default:
agent.defaultPort if an Agent is used, else undefined.
family <number> IP address family to use when resolving host or
hostname. Valid values are 4 or 6. When unspecified, both IP v4 and
v6 will be used.
headers <Object> An object containing request headers.
hints <number> Optional dns.lookup() hints.
host <string> A domain name or IP address of the server to issue the
request to. Default: 'localhost'.
hostname <string> Alias for host. To support url.parse(),
hostname will be used if both host and hostname are specified.
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false
joinDuplicateHeaders <boolean> It joins the field line values of
multiple headers in a request with ,  instead of discarding
the duplicates. See message.headers for more information.
Default: false.
localAddress <string> Local interface to bind for network connections.
localPort <number> Local port to connect from.
lookup <Function> Custom lookup function. Default: dns.lookup().
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size (the maximum length of response headers in
bytes) for responses received from the server.
Default: 16384 (16 KiB).
method <string> A string specifying the HTTP request method. Default:
'GET'.
path <string> Request path. Should include query string if any.
E.G. '/index.html?page=12'. An exception is thrown when the request path
contains illegal characters. Currently, only spaces are rejected but that
may change in the future. Default: '/'.
port <number> Port of remote server. Default: defaultPort if set,
else 80.
protocol <string> Protocol to use. Default: 'http:'.
setDefaultHeaders <boolean>: Specifies whether or not to automatically add
default headers such as Connection, Content-Length, Transfer-Encoding,
and Host. If set to false then all necessary headers must be added
manually. Defaults to true.
setHost <boolean>: Specifies whether or not to automatically add the
Host header. If provided, this overrides setDefaultHeaders. Defaults to
true.
signal <AbortSignal>: An AbortSignal that may be used to abort an ongoing
request.
socketPath <string> Unix domain socket. Cannot be used if one of host
or port is specified, as those specify a TCP Socket.
timeout <number>: A number specifying the socket timeout in milliseconds.
This will set the timeout before the socket is connected.
uniqueHeaders <Array> A list of request headers that should be sent
only once. If the header's value is an array, the items will be joined
using ; .


callback <Function>
Returns: <http.ClientRequest>

options in socket.connect() are also supported.
Node.js maintains several connections per server to make HTTP requests.
This function allows one to transparently issue requests.
url can be a string or a URL object. If url is a
string, it is automatically parsed with new URL(). If it is a URL
object, it will be automatically converted to an ordinary options object.
If both url and options are specified, the objects are merged, with the
options properties taking precedence.
The optional callback parameter will be added as a one-time listener for
the 'response' event.
http.request() returns an instance of the http.ClientRequest
class. The ClientRequest instance is a writable stream. If one needs to
upload a file with a POST request, then write to the ClientRequest object.

import http from 'node:http';
import { Buffer } from 'node:buffer';

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();const http = require('node:http');

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();copy
In the example req.end() was called. With http.request() one
must always call req.end() to signify the end of the request -
even if there is no data being written to the request body.
If any error is encountered during the request (be that with DNS resolution,
TCP level errors, or actual HTTP parse errors) an 'error' event is emitted
on the returned request object. As with all 'error' events, if no listeners
are registered the error will be thrown.
There are a few special headers that should be noted.


Sending a 'Connection: keep-alive' will notify Node.js that the connection to
the server should be persisted until the next request.


Sending a 'Content-Length' header will disable the default chunked encoding.


Sending an 'Expect' header will immediately send the request headers.
Usually, when sending 'Expect: 100-continue', both a timeout and a listener
for the 'continue' event should be set. See RFC 2616 Section 8.2.3 for more
information.


Sending an Authorization header will override using the auth option
to compute basic authentication.


Example using a URL as options:
const options = new URL('http://abc:xyz@example.com');

const req = http.request(options, (res) => {
  // ...
}); copy
In a successful request, the following events will be emitted in the following
order:

'socket'
'response'

'data' any number of times, on the res object
('data' will not be emitted at all if the response body is empty, for
instance, in most redirects)
'end' on the res object


'close'

In the case of a connection error, the following events will be emitted:

'socket'
'error'
'close'

In the case of a premature connection close before the response is received,
the following events will be emitted in the following order:

'socket'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

In the case of a premature connection close after the response is received,
the following events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(connection closed here)
'aborted' on the res object
'close'
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'
'close' on the res object

If req.destroy() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.destroy() called here)
'aborted' on the res object
'close'
'error' on the res object with an error with message 'Error: aborted'
and code 'ECONNRESET', or the error with which req.destroy() was called
'close' on the res object

If req.abort() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.abort() called here)
'abort'
'close'

If req.abort() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.abort() called here)
'abort'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

If req.abort() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.abort() called here)
'abort'
'aborted' on the res object
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'.
'close'
'close' on the res object

Setting the timeout option or using the setTimeout() function will
not abort the request or do anything besides add a 'timeout' event.
Passing an AbortSignal and then calling abort() on the corresponding
AbortController will behave the same way as calling .destroy() on the
request. Specifically, the 'error' event will be emitted with an error with
the message 'AbortError: The operation was aborted', the code 'ABORT_ERR'
and the cause, if one was provided.
http.validateHeaderName(name[, label])#

History

VersionChanges
v19.5.0, v18.14.0
The label parameter is added.
v14.3.0
Added in: v14.3.0




name <string>
label <string> Label for error message. Default: 'Header name'.

Performs the low-level validations on the provided name that are done when
res.setHeader(name, value) is called.
Passing illegal value as name will result in a TypeError being thrown,
identified by code: 'ERR_INVALID_HTTP_TOKEN'.
It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Example:

import { validateHeaderName } from 'node:http';

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}const { validateHeaderName } = require('node:http');

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}copy
http.validateHeaderValue(name, value)#

Added in: v14.3.0


name <string>
value <any>

Performs the low-level validations on the provided value that are done when
res.setHeader(name, value) is called.
Passing illegal value as value will result in a TypeError being thrown.

Undefined value error is identified by code: 'ERR_HTTP_INVALID_HEADER_VALUE'.
Invalid value character error is identified by code: 'ERR_INVALID_CHAR'.

It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Examples:

import { validateHeaderValue } from 'node:http';

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}const { validateHeaderValue } = require('node:http');

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}copy
http.setMaxIdleHTTPParsers(max)#

Added in: v18.8.0, v16.18.0


max <number> Default: 1000.

Set the maximum number of idle HTTP parsers.
WebSocket#

Added in: v22.5.0

A browser-compatible implementation of WebSocket.\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
            
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      

      
        


About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket




      
        HTTP#

Stability: 2 - Stable
Source Code: lib/http.js
This module, containing both a client and server, can be imported via
require('node:http') (CommonJS) or import * as http from 'node:http' (ES module).
The HTTP interfaces in Node.js are designed to support many features
of the protocol which have been traditionally difficult to use.
In particular, large, possibly chunk-encoded, messages. The interface is
careful to never buffer entire requests or responses, so the
user is able to stream data.
HTTP message headers are represented by an object like this:
{ "content-length": "123",
  "content-type": "text/plain",
  "connection": "keep-alive",
  "host": "example.com",
  "accept": "*/*" } copy
Keys are lowercased. Values are not modified.
In order to support the full spectrum of possible HTTP applications, the Node.js
HTTP API is very low-level. It deals with stream handling and message
parsing only. It parses a message into headers and body but it does not
parse the actual headers or the body.
See message.headers for details on how duplicate headers are handled.
The raw headers as they were received are retained in the rawHeaders
property, which is an array of [key, value, key2, value2, ...]. For
example, the previous message header object might have a rawHeaders
list like the following:

[ 'ConTent-Length', '123456',
  'content-LENGTH', '123',
  'content-type', 'text/plain',
  'CONNECTION', 'keep-alive',
  'Host', 'example.com',
  'accepT', '*/*' ] copy
Class: http.Agent#

Added in: v0.3.4

An Agent is responsible for managing connection persistence
and reuse for HTTP clients. It maintains a queue of pending requests
for a given host and port, reusing a single socket connection for each
until the queue is empty, at which time the socket is either destroyed
or put into a pool where it is kept to be used again for requests to the
same host and port. Whether it is destroyed or pooled depends on the
keepAlive option.
Pooled connections have TCP Keep-Alive enabled for them, but servers may
still close idle connections, in which case they will be removed from the
pool and a new connection will be made when a new HTTP request is made for
that host and port. Servers may also refuse to allow multiple requests
over the same connection, in which case the connection will have to be
remade for every request and cannot be pooled. The Agent will still make
the requests to that server, but each one will occur over a new connection.
When a connection is closed by the client or the server, it is removed
from the pool. Any unused sockets in the pool will be unrefed so as not
to keep the Node.js process running when there are no outstanding requests.
(see socket.unref()).
It is good practice, to destroy() an Agent instance when it is no
longer in use, because unused sockets consume OS resources.
Sockets are removed from an agent when the socket emits either
a 'close' event or an 'agentRemove' event. When intending to keep one
HTTP request open for a long time without keeping it in the agent, something
like the following may be done:
http.get(options, (res) => {
  // Do stuff
}).on('socket', (socket) => {
  socket.emit('agentRemove');
}); copy
An agent may also be used for an individual request. By providing
{agent: false} as an option to the http.get() or http.request()
functions, a one-time use Agent with default options will be used
for the client connection.
agent:false:
http.get({
  hostname: 'localhost',
  port: 80,
  path: '/',
  agent: false,  // Create a new agent just for this one request
}, (res) => {
  // Do stuff with response
}); copy

new Agent([options])#

History

VersionChanges
v15.6.0, v14.17.0
Change the default scheduling from 'fifo' to 'lifo'.
v14.5.0, v12.20.0
Add scheduling option to specify the free socket scheduling strategy.
v14.5.0, v12.19.0
Add maxTotalSockets option to agent constructor.
v0.3.4
Added in: v0.3.4




options <Object> Set of configurable options to set on the agent.
Can have the following fields:

keepAlive <boolean> Keep sockets around even when there are no
outstanding requests, so they can be used for future requests without
having to reestablish a TCP connection. Not to be confused with the
keep-alive value of the Connection header. The Connection: keep-alive
header is always sent when using an agent except when the Connection
header is explicitly specified or when the keepAlive and maxSockets
options are respectively set to false and Infinity, in which case
Connection: close will be used. Default: false.
keepAliveMsecs <number> When using the keepAlive option, specifies
the initial delay
for TCP Keep-Alive packets. Ignored when the
keepAlive option is false or undefined. Default: 1000.
maxSockets <number> Maximum number of sockets to allow per host.
If the same host opens multiple concurrent connections, each request
will use new socket until the maxSockets value is reached.
If the host attempts to open more connections than maxSockets,
the additional requests will enter into a pending request queue, and
will enter active connection state when an existing connection terminates.
This makes sure there are at most maxSockets active connections at
any point in time, from a given host.
Default: Infinity.
maxTotalSockets <number> Maximum number of sockets allowed for
all hosts in total. Each request will use a new socket
until the maximum is reached.
Default: Infinity.
maxFreeSockets <number> Maximum number of sockets per host to leave open
in a free state. Only relevant if keepAlive is set to true.
Default: 256.
scheduling <string> Scheduling strategy to apply when picking
the next free socket to use. It can be 'fifo' or 'lifo'.
The main difference between the two scheduling strategies is that 'lifo'
selects the most recently used socket, while 'fifo' selects
the least recently used socket.
In case of a low rate of request per second, the 'lifo' scheduling
will lower the risk of picking a socket that might have been closed
by the server due to inactivity.
In case of a high rate of request per second,
the 'fifo' scheduling will maximize the number of open sockets,
while the 'lifo' scheduling will keep it as low as possible.
Default: 'lifo'.
timeout <number> Socket timeout in milliseconds.
This will set the timeout when the socket is created.



options in socket.connect() are also supported.
To configure any of them, a custom http.Agent instance must be created.

import { Agent, request } from 'node:http';
const keepAliveAgent = new Agent({ keepAlive: true });
options.agent = keepAliveAgent;
request(options, onResponseCallback);const http = require('node:http');
const keepAliveAgent = new http.Agent({ keepAlive: true });
options.agent = keepAliveAgent;
http.request(options, onResponseCallback);copy

agent.createConnection(options[, callback])#

Added in: v0.11.4


options <Object> Options containing connection details. Check
net.createConnection() for the format of the options
callback <Function> Callback function that receives the created socket
Returns: <stream.Duplex>

Produces a socket/stream to be used for HTTP requests.
By default, this function is the same as net.createConnection(). However,
custom agents may override this method in case greater flexibility is desired.
A socket/stream can be supplied in one of two ways: by returning the
socket/stream from this function, or by passing the socket/stream to callback.
This method is guaranteed to return an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
callback has a signature of (err, stream).

agent.keepSocketAlive(socket)#

Added in: v8.1.0


socket <stream.Duplex>

Called when socket is detached from a request and could be persisted by the
Agent. Default behavior is to:
socket.setKeepAlive(true, this.keepAliveMsecs);
socket.unref();
return true; copy
This method can be overridden by a particular Agent subclass. If this
method returns a falsy value, the socket will be destroyed instead of persisting
it for use with the next request.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.reuseSocket(socket, request)#

Added in: v8.1.0


socket <stream.Duplex>
request <http.ClientRequest>

Called when socket is attached to request after being persisted because of
the keep-alive options. Default behavior is to:
socket.ref(); copy
This method can be overridden by a particular Agent subclass.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.destroy()#

Added in: v0.11.4

Destroy any sockets that are currently in use by the agent.
It is usually not necessary to do this. However, if using an
agent with keepAlive enabled, then it is best to explicitly shut down
the agent when it is no longer needed. Otherwise,
sockets might stay open for quite a long time before the server
terminates them.

agent.freeSockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.11.4
Added in: v0.11.4




<Object>

An object which contains arrays of sockets currently awaiting use by
the agent when keepAlive is enabled. Do not modify.
Sockets in the freeSockets list will be automatically destroyed and
removed from the array on 'timeout'.

agent.getName([options])#

History

VersionChanges
v17.7.0, v16.15.0
The options parameter is now optional.
v0.11.4
Added in: v0.11.4




options <Object> A set of options providing information for name generation

host <string> A domain name or IP address of the server to issue the
request to
port <number> Port of remote server
localAddress <string> Local interface to bind for network connections
when issuing the request
family <integer> Must be 4 or 6 if this doesn't equal undefined.


Returns: <string>

Get a unique name for a set of request options, to determine whether a
connection can be reused. For an HTTP agent, this returns
host:port:localAddress or host:port:localAddress:family. For an HTTPS agent,
the name includes the CA, cert, ciphers, and other HTTPS/TLS-specific options
that determine socket reusability.

agent.maxFreeSockets#

Added in: v0.11.7


<number>

By default set to 256. For agents with keepAlive enabled, this
sets the maximum number of sockets that will be left open in the free
state.

agent.maxSockets#

Added in: v0.3.6


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open per origin. Origin is the returned value of agent.getName().

agent.maxTotalSockets#

Added in: v14.5.0, v12.19.0


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open. Unlike maxSockets, this parameter applies across all origins.

agent.requests#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.5.9
Added in: v0.5.9




<Object>

An object which contains queues of requests that have not yet been assigned to
sockets. Do not modify.

agent.sockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.3.6
Added in: v0.3.6




<Object>

An object which contains arrays of sockets currently in use by the
agent. Do not modify.

Class: http.ClientRequest#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally and returned from http.request(). It
represents an in-progress request whose header has already been queued. The
header is still mutable using the setHeader(name, value),
getHeader(name), removeHeader(name) API. The actual header will
be sent along with the first data chunk or when calling request.end().
To get the response, add a listener for 'response' to the request object.
'response' will be emitted from the request object when the response
headers have been received. The 'response' event is executed with one
argument which is an instance of http.IncomingMessage.
During the 'response' event, one can add listeners to the
response object; particularly to listen for the 'data' event.
If no 'response' handler is added, then the response will be
entirely discarded. However, if a 'response' event handler is added,
then the data from the response object must be consumed, either by
calling response.read() whenever there is a 'readable' event, or
by adding a 'data' handler, or by calling the .resume() method.
Until the data is consumed, the 'end' event will not fire. Also, until
the data is read it will consume memory that can eventually lead to a
'process out of memory' error.
For backward compatibility, res will only emit 'error' if there is an
'error' listener registered.
Set Content-Length header to limit the response body size.
If response.strictContentLength is set to true, mismatching the
Content-Length header value will result in an Error being thrown,
identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.
Content-Length value should be in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes.

Event: 'abort'#

Added in: v1.4.1Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for the 'close' event instead.
Emitted when the request has been aborted by the client. This event is only
emitted on the first call to abort().

Event: 'close'#

Added in: v0.5.4

Indicates that the request is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'connect'#

Added in: v0.7.0


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with a CONNECT method. If
this event is not being listened for, clients receiving a CONNECT method will
have their connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client and server pair demonstrating how to listen for the 'connect' event:

import { createServer, request } from 'node:http';
import { connect } from 'node:net';
import { URL } from 'node:url';

// Create an HTTP tunneling proxy
const proxy = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});const http = require('node:http');
const net = require('node:net');
const { URL } = require('node:url');

// Create an HTTP tunneling proxy
const proxy = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = net.connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = http.request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});copy

Event: 'continue'#

Added in: v0.3.2

Emitted when the server sends a '100 Continue' HTTP response, usually because
the request contained 'Expect: 100-continue'. This is an instruction that
the client should send the request body.

Event: 'finish'#

Added in: v0.3.6

Emitted when the request has been sent. More specifically, this event is emitted
when the last segment of the response headers and body have been handed off to
the operating system for transmission over the network. It does not imply that
the server has received anything yet.

Event: 'information'#

Added in: v10.0.0


info <Object>

httpVersion <string>
httpVersionMajor <integer>
httpVersionMinor <integer>
statusCode <integer>
statusMessage <string>
headers <Object>
rawHeaders <string[]>



Emitted when the server sends a 1xx intermediate response (excluding 101
Upgrade). The listeners of this event will receive an object containing the
HTTP version, status code, status message, key-value headers object,
and array with the raw header names followed by their respective values.

import { request } from 'node:http';

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});const http = require('node:http');

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = http.request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});copy
101 Upgrade statuses do not fire this event due to their break from the
traditional HTTP request/response chain, such as web sockets, in-place TLS
upgrades, or HTTP 2.0. To be notified of 101 Upgrade notices, listen for the
'upgrade' event instead.

Event: 'response'#

Added in: v0.1.0


response <http.IncomingMessage>

Emitted when a response is received to this request. This event is emitted only
once.

Event: 'socket'#

Added in: v0.5.3


socket <stream.Duplex>

This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'timeout'#

Added in: v0.7.8

Emitted when the underlying socket times out from inactivity. This only notifies
that the socket has been idle. The request must be destroyed manually.
See also: request.setTimeout().

Event: 'upgrade'#

Added in: v0.1.94


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with an upgrade. If this
event is not being listened for and the response status code is 101 Switching
Protocols, clients receiving an upgrade header will have their connections
closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client server pair demonstrating how to listen for the 'upgrade' event.

import http from 'node:http';
import process from 'node:process';

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});const http = require('node:http');

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});copy

request.abort()#

Added in: v0.3.8Deprecated since: v14.1.0, v13.14.0

Stability: 0 - Deprecated: Use request.destroy() instead.
Marks the request as aborting. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.

request.aborted#

History

VersionChanges
v17.0.0, v16.12.0
Deprecated since: v17.0.0, v16.12.0
v11.0.0
The aborted property is no longer a timestamp number.
v0.11.14
Added in: v0.11.14



Stability: 0 - Deprecated. Check request.destroyed instead.

<boolean>

The request.aborted property will be true if the request has
been aborted.

request.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use request.socket.

<stream.Duplex>

See request.socket.

request.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

request.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ClientRequest.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

Finishes sending the request. If any parts of the body are
unsent, it will flush them to the stream. If the request is
chunked, this will send the terminating '0\r\n\r\n'.
If data is specified, it is equivalent to calling
request.write(data, encoding) followed by request.end(callback).
If callback is specified, it will be called when the request stream
is finished.

request.destroy([error])#

History

VersionChanges
v14.5.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error> Optional, an error to emit with 'error' event.
Returns: <this>

Destroy the request. Optionally emit an 'error' event,
and emit a 'close' event. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.
See writable.destroy() for further details.

request.destroyed#

Added in: v14.1.0, v13.14.0


<boolean>

Is true after request.destroy() has been called.
See writable.destroyed for further details.

request.finished#

Added in: v0.0.1Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use request.writableEnded.

<boolean>

The request.finished property will be true if request.end()
has been called. request.end() will automatically be called if the
request was initiated via http.get().

request.flushHeaders()#

Added in: v1.6.0

Flushes the request headers.
For efficiency reasons, Node.js normally buffers the request headers until
request.end() is called or the first chunk of request data is written. It
then tries to pack the request headers and data into a single TCP packet.
That's usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. request.flushHeaders() bypasses
the optimization and kickstarts the request.

request.getHeader(name)#

Added in: v1.6.0


name <string>
Returns: <any>

Reads out a header on the request. The name is case-insensitive.
The type of the return value depends on the arguments provided to
request.setHeader().
request.setHeader('content-type', 'text/html');
request.setHeader('Content-Length', Buffer.byteLength(body));
request.setHeader('Cookie', ['type=ninja', 'language=javascript']);
const contentType = request.getHeader('Content-Type');
// 'contentType' is 'text/html'
const contentLength = request.getHeader('Content-Length');
// 'contentLength' is of type number
const cookie = request.getHeader('Cookie');
// 'cookie' is of type string[] copy

request.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getHeaderNames();
// headerNames === ['foo', 'cookie'] copy

request.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the request.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headers = request.getHeaders();
// headers === { foo: 'bar', 'cookie': ['foo=bar', 'bar=baz'] } copy

request.getRawHeaderNames()#

Added in: v15.13.0, v14.17.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing raw
headers. Header names are returned with their exact casing being set.
request.setHeader('Foo', 'bar');
request.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getRawHeaderNames();
// headerNames === ['Foo', 'Set-Cookie'] copy

request.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = request.hasHeader('content-type'); copy

request.maxHeadersCount#

<number> Default: 2000

Limits maximum response headers count. If set to 0, no limit will be applied.

request.path#

Added in: v0.4.0


<string> The request path.


request.method#

Added in: v0.1.97


<string> The request method.


request.host#

Added in: v14.5.0, v12.19.0


<string> The request host.


request.protocol#

Added in: v14.5.0, v12.19.0


<string> The request protocol.


request.removeHeader(name)#

Added in: v1.6.0


name <string>

Removes a header that's already defined into headers object.
request.removeHeader('Content-Type'); copy

request.reusedSocket#

Added in: v13.0.0, v12.16.0


<boolean> Whether the request is send through a reused socket.

When sending request through a keep-alive enabled agent, the underlying socket
might be reused. But if server closes connection at unfortunate time, client
may run into a 'ECONNRESET' error.

import http from 'node:http';

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutconst http = require('node:http');

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutcopy
By marking a request whether it reused socket or not, we can do
automatic error retry base on it.

import http from 'node:http';
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();const http = require('node:http');
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();copy

request.setHeader(name, value)#

Added in: v1.6.0


name <string>
value <any>

Sets a single header value for headers object. If this header already exists in
the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, request.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission.
request.setHeader('Content-Type', 'application/json'); copy
or
request.setHeader('Cookie', ['type=ninja', 'language=javascript']); copy
When the value is a string an exception will be thrown if it contains
characters outside the latin1 encoding.
If you need to pass UTF-8 characters in the value please encode the value
using the RFC 8187 standard.
const filename = 'Rock 🎵.txt';
request.setHeader('Content-Disposition', `attachment; filename*=utf-8''${encodeURIComponent(filename)}`); copy

request.setNoDelay([noDelay])#

Added in: v0.5.9


noDelay <boolean>

Once a socket is assigned to this request and is connected
socket.setNoDelay() will be called.

request.setSocketKeepAlive([enable][, initialDelay])#

Added in: v0.5.9


enable <boolean>
initialDelay <number>

Once a socket is assigned to this request and is connected
socket.setKeepAlive() will be called.

request.setTimeout(timeout[, callback])#

History

VersionChanges
v9.0.0
Consistently set socket timeout only when the socket connects.
v0.5.9
Added in: v0.5.9




timeout <number> Milliseconds before a request times out.
callback <Function> Optional function to be called when a timeout occurs.
Same as binding to the 'timeout' event.
Returns: <http.ClientRequest>

Once a socket is assigned to this request and is connected
socket.setTimeout() will be called.

request.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket.

import http from 'node:http';
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});const http = require('node:http');
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

request.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

request.writableEnded#

Added in: v12.9.0


<boolean>

Is true after request.end() has been called. This property
does not indicate whether the data has been flushed, for this use
request.writableFinished instead.

request.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

request.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times. If no
Content-Length is set, data will automatically be encoded in HTTP Chunked
transfer encoding, so that server knows when the data ends. The
Transfer-Encoding: chunked header is added. Calling request.end()
is necessary to finish sending the request.
The encoding argument is optional and only applies when chunk is a string.
Defaults to 'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed, but only if the chunk is non-empty.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.
When write function is called with empty string or buffer, it does
nothing and waits for more input.

Class: http.Server#

Added in: v0.1.17


Extends: <net.Server>


Event: 'checkContinue'#

Added in: v0.3.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect: 100-continue is received.
If this event is not listened for, the server will automatically respond
with a 100 Continue as appropriate.
Handling this event involves calling response.writeContinue() if the
client should continue to send the request body, or generating an appropriate
HTTP response (e.g. 400 Bad Request) if the client should not continue to send
the request body.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'checkExpectation'#

Added in: v5.5.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect header is received, where the
value is not 100-continue. If this event is not listened for, the server will
automatically respond with a 417 Expectation Failed as appropriate.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'clientError'#

History

VersionChanges
v12.0.0
The default behavior will return a 431 Request Header Fields Too Large if a HPE_HEADER_OVERFLOW error occurs.
v9.4.0
The rawPacket is the current buffer that just parsed. Adding this buffer to the error object of 'clientError' event is to make it possible that developers can log the broken packet.
v6.0.0
The default action of calling .destroy() on the socket will no longer take place if there are listeners attached for 'clientError'.
v0.1.94
Added in: v0.1.94




exception <Error>
socket <stream.Duplex>

If a client connection emits an 'error' event, it will be forwarded here.
Listener of this event is responsible for closing/destroying the underlying
socket. For example, one may wish to more gracefully close the socket with a
custom HTTP response instead of abruptly severing the connection. The socket
must be closed or destroyed before the listener ends.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
Default behavior is to try close the socket with a HTTP '400 Bad Request',
or a HTTP '431 Request Header Fields Too Large' in the case of a
HPE_HEADER_OVERFLOW error. If the socket is not writable or headers
of the current attached http.ServerResponse has been sent, it is
immediately destroyed.
socket is the net.Socket object that the error originated from.

import http from 'node:http';

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);const http = require('node:http');

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);copy
When the 'clientError' event occurs, there is no request or response
object, so any HTTP response sent, including response headers and payload,
must be written directly to the socket object. Care must be taken to
ensure the response is a properly formatted HTTP response message.
err is an instance of Error with two extra columns:

bytesParsed: the bytes count of request packet that Node.js may have parsed
correctly;
rawPacket: the raw packet of current request.

In some cases, the client has already received the response and/or the socket
has already been destroyed, like in case of ECONNRESET errors. Before
trying to send data to the socket, it is better to check that it is still
writable.
server.on('clientError', (err, socket) => {
  if (err.code === 'ECONNRESET' || !socket.writable) {
    return;
  }

  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
}); copy

Event: 'close'#

Added in: v0.1.4

Emitted when the server closes.

Event: 'connect'#

Added in: v0.7.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the tunneling stream (may be empty)

Emitted each time a client requests an HTTP CONNECT method. If this event is
not listened for, then clients requesting a CONNECT method will have their
connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.

Event: 'connection'#

Added in: v0.1.0


socket <stream.Duplex>

This event is emitted when a new TCP stream is established. socket is
typically an object of type net.Socket. Usually users will not want to
access this event. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. The socket can
also be accessed at request.socket.
This event can also be explicitly emitted by users to inject connections
into the HTTP server. In that case, any Duplex stream can be passed.
If socket.setTimeout() is called here, the timeout will be replaced with
server.keepAliveTimeout when the socket has served a request (if
server.keepAliveTimeout is non-zero).
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'dropRequest'#

Added in: v18.7.0, v16.17.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client

When the number of requests on a socket reaches the threshold of
server.maxRequestsPerSocket, the server will drop new requests
and emit 'dropRequest' event instead, then send 503 to client.

Event: 'request'#

Added in: v0.1.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time there is a request. There may be multiple requests
per connection (in the case of HTTP Keep-Alive connections).

Event: 'upgrade'#

History

VersionChanges
v10.0.0
Not listening to this event no longer causes the socket to be destroyed if a client sends an Upgrade header.
v0.1.94
Added in: v0.1.94




request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the upgraded stream (may be empty)

Emitted each time a client requests an HTTP upgrade. Listening to this event
is optional and clients cannot insist on a protocol change.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

server.close([callback])#

History

VersionChanges
v19.0.0
The method closes idle connections before returning.
v0.1.90
Added in: v0.1.90




callback <Function>

Stops the server from accepting new connections and closes all connections
connected to this server which are not sending a request or waiting for
a response.
See net.Server.close().
const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
}, 10000); copy

server.closeAllConnections()#

Added in: v18.2.0

Closes all established HTTP(S) connections connected to this server, including
active connections connected to this server which are sending a request or
waiting for a response. This does not destroy sockets upgraded to a different
protocol, such as WebSocket or HTTP/2.

This is a forceful way of closing all connections and should be used with
caution. Whenever using this in conjunction with server.close, calling this
after server.close is recommended as to avoid race conditions where new
connections are created between a call to this and a call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes all connections, ensuring the server closes successfully
  server.closeAllConnections();
}, 10000); copy

server.closeIdleConnections()#

Added in: v18.2.0

Closes all connections connected to this server which are not sending a request
or waiting for a response.

Starting with Node.js 19.0.0, there's no need for calling this method in
conjunction with server.close to reap keep-alive connections. Using it
won't cause any harm though, and it can be useful to ensure backwards
compatibility for libraries and applications that need to support versions
older than 19.0.0. Whenever using this in conjunction with server.close,
calling this after server.close is recommended as to avoid race
conditions where new connections are created between a call to this and a
call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes idle connections, such as keep-alive connections. Server will close
  // once remaining active connections are terminated
  server.closeIdleConnections();
}, 10000); copy

server.headersTimeout#

History

VersionChanges
v19.4.0, v18.14.0
The default is now set to the minimum between 60000 (60 seconds) or requestTimeout.
v11.3.0, v10.14.0
Added in: v11.3.0, v10.14.0




<number> Default: The minimum between server.requestTimeout or 60000.

Limit the amount of time the parser will wait to receive the complete HTTP
headers.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.listen()#
Starts the HTTP server listening for connections.
This method is identical to server.listen() from net.Server.

server.listening#

Added in: v5.7.0


<boolean> Indicates whether or not the server is listening for connections.


server.maxHeadersCount#

Added in: v0.7.0


<number> Default: 2000

Limits maximum incoming headers count. If set to 0, no limit will be applied.

server.requestTimeout#

History

VersionChanges
v18.0.0
The default request timeout changed from no timeout to 300s (5 minutes).
v14.11.0
Added in: v14.11.0




<number> Default: 300000

Sets the timeout value in milliseconds for receiving the entire request from
the client.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.setTimeout([msecs][, callback])#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




msecs <number> Default: 0 (no timeout)
callback <Function>
Returns: <http.Server>

Sets the timeout value for sockets, and emits a 'timeout' event on
the Server object, passing the socket as an argument, if a timeout
occurs.
If there is a 'timeout' event listener on the Server object, then it
will be called with the timed-out socket as an argument.
By default, the Server does not timeout sockets. However, if a callback
is assigned to the Server's 'timeout' event, timeouts must be handled
explicitly.

server.maxRequestsPerSocket#

Added in: v16.10.0


<number> Requests per socket. Default: 0 (no limit)

The maximum number of requests socket can handle
before closing keep alive connection.
A value of 0 will disable the limit.
When the limit is reached it will set the Connection header value to close,
but will not actually close the connection, subsequent requests sent
after the limit is reached will get 503 Service Unavailable as a response.

server.timeout#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




<number> Timeout in milliseconds. Default: 0 (no timeout)

The number of milliseconds of inactivity before a socket is presumed
to have timed out.
A value of 0 will disable the timeout behavior on incoming connections.
The socket timeout logic is set up on connection, so changing this
value only affects new connections to the server, not any existing connections.

server.keepAliveTimeout#

Added in: v8.0.0


<number> Timeout in milliseconds. Default: 5000 (5 seconds).

The number of milliseconds of inactivity a server needs to wait for additional
incoming data, after it has finished writing the last response, before a socket
will be destroyed. If the server receives new data before the keep-alive
timeout has fired, it will reset the regular inactivity timeout, i.e.,
server.timeout.
A value of 0 will disable the keep-alive timeout behavior on incoming
connections.
A value of 0 makes the http server behave similarly to Node.js versions prior
to 8.0.0, which did not have a keep-alive timeout.
The socket timeout logic is set up on connection, so changing this value only
affects new connections to the server, not any existing connections.

server[Symbol.asyncDispose]()#

Added in: v20.4.0

Stability: 1 - Experimental
Calls server.close() and returns a promise that fulfills when the
server has closed.

Class: http.ServerResponse#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally by an HTTP server, not by the user. It is
passed as the second parameter to the 'request' event.

Event: 'close'#

Added in: v0.6.7

Indicates that the response is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'finish'#

Added in: v0.3.6

Emitted when the response has been sent. More specifically, this event is
emitted when the last segment of the response headers and body have been
handed off to the operating system for transmission over the network. It
does not imply that the client has received anything yet.

response.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

This method adds HTTP trailing headers (a header but at the end of the
message) to the response.
Trailers will only be emitted if chunked encoding is used for the
response; if it is not (e.g. if the request was HTTP/1.0), they will
be silently discarded.
HTTP requires the Trailer header to be sent in order to
emit trailers, with a list of the header fields in its value. E.g.,
response.writeHead(200, { 'Content-Type': 'text/plain',
                          'Trailer': 'Content-MD5' });
response.write(fileData);
response.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
response.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

response.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use response.socket.

<stream.Duplex>

See response.socket.

response.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

response.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ServerResponse.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

This method signals to the server that all of the response headers and body
have been sent; that server should consider this message complete.
The method, response.end(), MUST be called on each response.
If data is specified, it is similar in effect to calling
response.write(data, encoding) followed by response.end(callback).
If callback is specified, it will be called when the response stream
is finished.

response.finished#

Added in: v0.0.2Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use response.writableEnded.

<boolean>

The response.finished property will be true if response.end()
has been called.

response.flushHeaders()#

Added in: v1.6.0

Flushes the response headers. See also: request.flushHeaders().

response.getHeader(name)#

Added in: v0.4.0


name <string>
Returns: <any>

Reads out a header that's already been queued but not sent to the client.
The name is case-insensitive. The type of the return value depends
on the arguments provided to response.setHeader().
response.setHeader('Content-Type', 'text/html');
response.setHeader('Content-Length', Buffer.byteLength(body));
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']);
const contentType = response.getHeader('content-type');
// contentType is 'text/html'
const contentLength = response.getHeader('Content-Length');
// contentLength is of type number
const setCookie = response.getHeader('set-cookie');
// setCookie is of type string[] copy

response.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = response.getHeaderNames();
// headerNames === ['foo', 'set-cookie'] copy

response.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the response.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = response.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

response.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = response.hasHeader('content-type'); copy

response.headersSent#

Added in: v0.9.3


<boolean>

Boolean (read-only). True if headers were sent, false otherwise.

response.removeHeader(name)#

Added in: v0.4.0


name <string>

Removes a header that's queued for implicit sending.
response.removeHeader('Content-Encoding'); copy

response.req#

Added in: v15.7.0


<http.IncomingMessage>

A reference to the original HTTP request object.

response.sendDate#

Added in: v0.7.5


<boolean>

When true, the Date header will be automatically generated and sent in
the response if it is not already present in the headers. Defaults to true.
This should only be disabled for testing; HTTP requires the Date header
in responses.

response.setHeader(name, value)#

Added in: v0.4.0


name <string>
value <any>
Returns: <http.ServerResponse>

Returns the response object.
Sets a single header value for implicit headers. If this header already exists
in the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, response.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission. The same response object is returned to the caller,
to enable call chaining.
response.setHeader('Content-Type', 'text/html'); copy
or
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
If response.writeHead() method is called and this method has not been
called, it will directly write the supplied header values onto the network
channel without caching internally, and the response.getHeader() on the
header will not yield the expected result. If progressive population of headers
is desired with potential future retrieval and modification, use
response.setHeader() instead of response.writeHead().

response.setTimeout(msecs[, callback])#

Added in: v0.9.12


msecs <number>
callback <Function>
Returns: <http.ServerResponse>

Sets the Socket's timeout value to msecs. If a callback is
provided, then it is added as a listener on the 'timeout' event on
the response object.
If no 'timeout' listener is added to the request, the response, or
the server, then sockets are destroyed when they time out. If a handler is
assigned to the request, the response, or the server's 'timeout' events,
timed out sockets must be handled explicitly.

response.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. After
response.end(), the property is nulled.

import http from 'node:http';
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);const http = require('node:http');
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

response.statusCode#

Added in: v0.4.0


<number> Default: 200

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status code that will be sent to the client when
the headers get flushed.
response.statusCode = 404; copy
After response header was sent to the client, this property indicates the
status code which was sent out.

response.statusMessage#

Added in: v0.11.8


<string>

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status message that will be sent to the client when
the headers get flushed. If this is left as undefined then the standard
message for the status code will be used.
response.statusMessage = 'Not found'; copy
After response header was sent to the client, this property indicates the
status message which was sent out.

response.strictContentLength#

Added in: v18.10.0, v16.18.0


<boolean> Default: false

If set to true, Node.js will check whether the Content-Length
header value and the size of the body, in bytes, are equal.
Mismatching the Content-Length header value will result
in an Error being thrown, identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.

response.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

response.writableEnded#

Added in: v12.9.0


<boolean>

Is true after response.end() has been called. This property
does not indicate whether the data has been flushed, for this use
response.writableFinished instead.

response.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

response.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: 'utf8'
callback <Function>
Returns: <boolean>

If this method is called and response.writeHead() has not been called,
it will switch to implicit header mode and flush the implicit headers.
This sends a chunk of the response body. This method may
be called multiple times to provide successive parts of the body.
If rejectNonStandardBodyWrites is set to true in createServer
then writing to the body is not allowed when the request method or response
status do not support content. If an attempt is made to write to the body for a
HEAD request or as part of a 204 or 304response, a synchronous Error
with the code ERR_HTTP_BODY_NOT_ALLOWED is thrown.
chunk can be a string or a buffer. If chunk is a string,
the second parameter specifies how to encode it into a byte stream.
callback will be called when this chunk of data is flushed.
This is the raw HTTP body and has nothing to do with higher-level multi-part
body encodings that may be used.
The first time response.write() is called, it will send the buffered
header information and the first chunk of the body to the client. The second
time response.write() is called, Node.js assumes data will be streamed,
and sends the new data separately. That is, the response is buffered up to the
first chunk of the body.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.

response.writeContinue()#

Added in: v0.3.0

Sends an HTTP/1.1 100 Continue message to the client, indicating that
the request body should be sent. See the 'checkContinue' event on
Server.

response.writeEarlyHints(hints[, callback])#

History

VersionChanges
v18.11.0
Allow passing hints as an object.
v18.11.0
Added in: v18.11.0




hints <Object>
callback <Function>

Sends an HTTP/1.1 103 Early Hints message to the client with a Link header,
indicating that the user agent can preload/preconnect the linked resources.
The hints is an object containing the values of headers to be sent with
early hints message. The optional callback argument will be called when
the response message has been written.
Example
const earlyHintsLink = '</styles.css>; rel=preload; as=style';
response.writeEarlyHints({
  'link': earlyHintsLink,
});

const earlyHintsLinks = [
  '</styles.css>; rel=preload; as=style',
  '</scripts.js>; rel=preload; as=script',
];
response.writeEarlyHints({
  'link': earlyHintsLinks,
  'x-trace-id': 'id for diagnostics',
});

const earlyHintsCallback = () => console.log('early hints message sent');
response.writeEarlyHints({
  'link': earlyHintsLinks,
}, earlyHintsCallback); copy

response.writeHead(statusCode[, statusMessage][, headers])#

History

VersionChanges
v14.14.0
Allow passing headers as an array.
v11.10.0, v10.17.0
Return this from writeHead() to allow chaining with end().
v5.11.0, v4.4.5
A RangeError is thrown if statusCode is not a number in the range [100, 999].
v0.1.30
Added in: v0.1.30




statusCode <number>
statusMessage <string>
headers <Object> | <Array>
Returns: <http.ServerResponse>

Sends a response header to the request. The status code is a 3-digit HTTP
status code, like 404. The last argument, headers, are the response headers.
Optionally one can give a human-readable statusMessage as the second
argument.
headers may be an Array where the keys and values are in the same list.
It is not a list of tuples. So, the even-numbered offsets are key values,
and the odd-numbered offsets are the associated values. The array is in the same
format as request.rawHeaders.
Returns a reference to the ServerResponse, so that calls can be chained.
const body = 'hello world';
response
  .writeHead(200, {
    'Content-Length': Buffer.byteLength(body),
    'Content-Type': 'text/plain',
  })
  .end(body); copy
This method must only be called once on a message and it must
be called before response.end() is called.
If response.write() or response.end() are called before calling
this, the implicit/mutable headers will be calculated and call this function.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
If this method is called and response.setHeader() has not been called,
it will directly write the supplied header values onto the network channel
without caching internally, and the response.getHeader() on the header
will not yield the expected result. If progressive population of headers is
desired with potential future retrieval and modification, use
response.setHeader() instead.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
Content-Length is read in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes. Node.js
will check whether Content-Length and the length of the body which has
been transmitted are equal or not.
Attempting to set a header field name or value that contains invalid characters
will result in a [Error][] being thrown.

response.writeProcessing()#

Added in: v10.0.0

Sends a HTTP/1.1 102 Processing message to the client, indicating that
the request body should be sent.

Class: http.IncomingMessage#

History

VersionChanges
v15.5.0
The destroyed value returns true after the incoming data is consumed.
v13.1.0, v12.16.0
The readableHighWaterMark value mirrors that of the socket.
v0.1.17
Added in: v0.1.17




Extends: <stream.Readable>

An IncomingMessage object is created by http.Server or
http.ClientRequest and passed as the first argument to the 'request'
and 'response' event respectively. It may be used to access response
status, headers, and data.
Different from its socket value which is a subclass of <stream.Duplex>, the
IncomingMessage itself extends <stream.Readable> and is created separately to
parse and emit the incoming HTTP headers and payload, as the underlying socket
may be reused multiple times in case of keep-alive.

Event: 'aborted'#

Added in: v0.3.8Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for 'close' event instead.
Emitted when the request has been aborted.

Event: 'close'#

History

VersionChanges
v16.0.0
The close event is now emitted when the request has been completed and not when the underlying socket is closed.
v0.4.2
Added in: v0.4.2



Emitted when the request has been completed.

message.aborted#

Added in: v10.1.0Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Check message.destroyed from <stream.Readable>.

<boolean>

The message.aborted property will be true if the request has
been aborted.

message.complete#

Added in: v0.3.0


<boolean>

The message.complete property will be true if a complete HTTP message has
been received and successfully parsed.
This property is particularly useful as a means of determining if a client or
server fully transmitted a message before a connection was terminated:
const req = http.request({
  host: '127.0.0.1',
  port: 8080,
  method: 'POST',
}, (res) => {
  res.resume();
  res.on('end', () => {
    if (!res.complete)
      console.error(
        'The connection was terminated while the message was still being sent');
  });
}); copy

message.connection#

Added in: v0.1.90Deprecated since: v16.0.0

Stability: 0 - Deprecated. Use message.socket.
Alias for message.socket.

message.destroy([error])#

History

VersionChanges
v14.5.0, v12.19.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error>
Returns: <this>

Calls destroy() on the socket that received the IncomingMessage. If error
is provided, an 'error' event is emitted on the socket and error is passed
as an argument to any listeners on the event.

message.headers#

History

VersionChanges
v19.5.0, v18.14.0
The joinDuplicateHeaders option in the http.request() and http.createServer() functions ensures that duplicate headers are not discarded, but rather combined using a comma separator, in accordance with RFC 9110 Section 5.3.
v15.1.0
message.headers is now lazily computed using an accessor property on the prototype and is no longer enumerable.
v0.1.5
Added in: v0.1.5




<Object>

The request/response headers object.
Key-value pairs of header names and values. Header names are lower-cased.
// Prints something like:
//
// { 'user-agent': 'curl/7.22.0',
//   host: '127.0.0.1:8000',
//   accept: '*/*' }
console.log(request.headers); copy
Duplicates in raw headers are handled in the following ways, depending on the
header name:

Duplicates of age, authorization, content-length, content-type,
etag, expires, from, host, if-modified-since, if-unmodified-since,
last-modified, location, max-forwards, proxy-authorization, referer,
retry-after, server, or user-agent are discarded.
To allow duplicate values of the headers listed above to be joined,
use the option joinDuplicateHeaders in http.request()
and http.createServer(). See RFC 9110 Section 5.3 for more
information.
set-cookie is always an array. Duplicates are added to the array.
For duplicate cookie headers, the values are joined together with ; .
For all other headers, the values are joined together with , .


message.headersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.headers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
// Prints something like:
//
// { 'user-agent': ['curl/7.22.0'],
//   host: ['127.0.0.1:8000'],
//   accept: ['*/*'] }
console.log(request.headersDistinct); copy

message.httpVersion#

Added in: v0.1.1


<string>

In case of server request, the HTTP version sent by the client. In the case of
client response, the HTTP version of the connected-to server.
Probably either '1.1' or '1.0'.
Also message.httpVersionMajor is the first integer and
message.httpVersionMinor is the second.

message.method#

Added in: v0.1.1


<string>

Only valid for request obtained from http.Server.
The request method as a string. Read only. Examples: 'GET', 'DELETE'.

message.rawHeaders#

Added in: v0.11.6


<string[]>

The raw request/response headers list exactly as they were received.
The keys and values are in the same list. It is not a
list of tuples. So, the even-numbered offsets are key values, and the
odd-numbered offsets are the associated values.
Header names are not lowercased, and duplicates are not merged.
// Prints something like:
//
// [ 'user-agent',
//   'this is invalid because there can be only one',
//   'User-Agent',
//   'curl/7.22.0',
//   'Host',
//   '127.0.0.1:8000',
//   'ACCEPT',
//   '*/*' ]
console.log(request.rawHeaders); copy

message.rawTrailers#

Added in: v0.11.6


<string[]>

The raw request/response trailer keys and values exactly as they were
received. Only populated at the 'end' event.

message.setTimeout(msecs[, callback])#

Added in: v0.5.9


msecs <number>
callback <Function>
Returns: <http.IncomingMessage>

Calls message.socket.setTimeout(msecs, callback).

message.socket#

Added in: v0.3.0


<stream.Duplex>

The net.Socket object associated with the connection.
With HTTPS support, use request.socket.getPeerCertificate() to obtain the
client's authentication details.
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket> or internally nulled.

message.statusCode#

Added in: v0.1.1


<number>

Only valid for response obtained from http.ClientRequest.
The 3-digit HTTP response status code. E.G. 404.

message.statusMessage#

Added in: v0.11.10


<string>

Only valid for response obtained from http.ClientRequest.
The HTTP response status message (reason phrase). E.G. OK or Internal Server Error.

message.trailers#

Added in: v0.3.0


<Object>

The request/response trailers object. Only populated at the 'end' event.

message.trailersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.trailers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
Only populated at the 'end' event.

message.url#

Added in: v0.1.90


<string>

Only valid for request obtained from http.Server.
Request URL string. This contains only the URL that is present in the actual
HTTP request. Take the following request:
GET /status?name=ryan HTTP/1.1
Accept: text/plain copy
To parse the URL into its parts:
new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`); copy
When request.url is '/status?name=ryan' and process.env.HOST is undefined:
$ node
> new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`);
URL {
  href: 'http://localhost/status?name=ryan',
  origin: 'http://localhost',
  protocol: 'http:',
  username: '',
  password: '',
  host: 'localhost',
  hostname: 'localhost',
  port: '',
  pathname: '/status',
  search: '?name=ryan',
  searchParams: URLSearchParams { 'name' => 'ryan' },
  hash: ''
} copy
Ensure that you set process.env.HOST to the server's host name, or consider
replacing this part entirely. If using req.headers.host, ensure proper
validation is used, as clients may specify a custom Host header.

Class: http.OutgoingMessage#

Added in: v0.1.17


Extends: <Stream>

This class serves as the parent class of http.ClientRequest
and http.ServerResponse. It is an abstract outgoing message from
the perspective of the participants of an HTTP transaction.

Event: 'drain'#

Added in: v0.3.6

Emitted when the buffer of the message is free again.

Event: 'finish'#

Added in: v0.1.17

Emitted when the transmission is finished successfully.

Event: 'prefinish'#

Added in: v0.11.6

Emitted after outgoingMessage.end() is called.
When the event is emitted, all data has been processed but not necessarily
completely flushed.

outgoingMessage.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

Adds HTTP trailers (headers but at the end of the message) to the message.
Trailers will only be emitted if the message is chunked encoded. If not,
the trailers will be silently discarded.
HTTP requires the Trailer header to be sent to emit trailers,
with a list of header field names in its value, e.g.
message.writeHead(200, { 'Content-Type': 'text/plain',
                         'Trailer': 'Content-MD5' });
message.write(fileData);
message.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
message.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

outgoingMessage.appendHeader(name, value)#

Added in: v18.3.0, v16.17.0


name <string> Header name
value <string> | <string[]> Header value
Returns: <this>

Append a single header value to the header object.
If the value is an array, this is equivalent to calling this method multiple
times.
If there were no previous values for the header, this is equivalent to calling
outgoingMessage.setHeader(name, value).
Depending of the value of options.uniqueHeaders when the client request or the
server were created, this will end up in the header being sent multiple times or
a single time with values joined using ; .

outgoingMessage.connection#

Added in: v0.3.0Deprecated since: v15.12.0, v14.17.1

Stability: 0 - Deprecated: Use outgoingMessage.socket instead.
Alias of outgoingMessage.socket.

outgoingMessage.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

outgoingMessage.destroy([error])#

Added in: v0.3.0


error <Error> Optional, an error to emit with error event
Returns: <this>

Destroys the message. Once a socket is associated with the message
and is connected, that socket will be destroyed as well.

outgoingMessage.end(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
add callback argument.
v0.1.90
Added in: v0.1.90




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Optional, Default: utf8
callback <Function> Optional
Returns: <this>

Finishes the outgoing message. If any parts of the body are unsent, it will
flush them to the underlying system. If the message is chunked, it will
send the terminating chunk 0\r\n\r\n, and send the trailers (if any).
If chunk is specified, it is equivalent to calling
outgoingMessage.write(chunk, encoding), followed by
outgoingMessage.end(callback).
If callback is provided, it will be called when the message is finished
(equivalent to a listener of the 'finish' event).

outgoingMessage.flushHeaders()#

Added in: v1.6.0

Flushes the message headers.
For efficiency reason, Node.js normally buffers the message headers
until outgoingMessage.end() is called or the first chunk of message data
is written. It then tries to pack the headers and data into a single TCP
packet.
It is usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. outgoingMessage.flushHeaders()
bypasses the optimization and kickstarts the message.

outgoingMessage.getHeader(name)#

Added in: v0.4.0


name <string> Name of header
Returns: <string> | <undefined>

Gets the value of the HTTP header with the given name. If that header is not
set, the returned value will be undefined.

outgoingMessage.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All names are lowercase.

outgoingMessage.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow
copy is used, array values may be mutated without additional calls to
various header-related HTTP module methods. The keys of the returned
object are the header names and the values are the respective header
values. All header names are lowercase.
The object returned by the outgoingMessage.getHeaders() method does
not prototypically inherit from the JavaScript Object. This means that
typical Object methods such as obj.toString(), obj.hasOwnProperty(),
and others are not defined and will not work.
outgoingMessage.setHeader('Foo', 'bar');
outgoingMessage.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = outgoingMessage.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

outgoingMessage.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name is case-insensitive.
const hasContentType = outgoingMessage.hasHeader('content-type'); copy

outgoingMessage.headersSent#

Added in: v0.9.3


<boolean>

Read-only. true if the headers were sent, otherwise false.

outgoingMessage.pipe()#

Added in: v9.0.0

Overrides the stream.pipe() method inherited from the legacy Stream class
which is the parent class of http.OutgoingMessage.
Calling this method will throw an Error because outgoingMessage is a
write-only stream.

outgoingMessage.removeHeader(name)#

Added in: v0.4.0


name <string> Header name

Removes a header that is queued for implicit sending.
outgoingMessage.removeHeader('Content-Encoding'); copy

outgoingMessage.setHeader(name, value)#

Added in: v0.4.0


name <string> Header name
value <any> Header value
Returns: <this>

Sets a single header value. If the header already exists in the to-be-sent
headers, its value will be replaced. Use an array of strings to send multiple
headers with the same name.

outgoingMessage.setHeaders(headers)#

Added in: v19.6.0, v18.15.0


headers <Headers> | <Map>
Returns: <this>

Sets multiple header values for implicit headers.
headers must be an instance of Headers or Map,
if a header already exists in the to-be-sent headers,
its value will be replaced.
const headers = new Headers({ foo: 'bar' });
outgoingMessage.setHeaders(headers); copy
or
const headers = new Map([['foo', 'bar']]);
outgoingMessage.setHeaders(headers); copy
When headers have been set with outgoingMessage.setHeaders(),
they will be merged with any headers passed to response.writeHead(),
with the headers passed to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  const headers = new Headers({ 'Content-Type': 'text/html' });
  res.setHeaders(headers);
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy

outgoingMessage.setTimeout(msesc[, callback])#

Added in: v0.9.12


msesc <number>
callback <Function> Optional function to be called when a timeout
occurs. Same as binding to the timeout event.
Returns: <this>

Once a socket is associated with the message and is connected,
socket.setTimeout() will be called with msecs as the first parameter.

outgoingMessage.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually, users will not want to access
this property.
After calling outgoingMessage.end(), this property will be nulled.

outgoingMessage.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork()

outgoingMessage.writableCorked#

Added in: v13.2.0, v12.16.0


<number>

The number of times outgoingMessage.cork() has been called.

outgoingMessage.writableEnded#

Added in: v12.9.0


<boolean>

Is true if outgoingMessage.end() has been called. This property does
not indicate whether the data has been flushed. For that purpose, use
message.writableFinished instead.

outgoingMessage.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system.

outgoingMessage.writableHighWaterMark#

Added in: v12.9.0


<number>

The highWaterMark of the underlying socket if assigned. Otherwise, the default
buffer level when writable.write() starts returning false (16384).

outgoingMessage.writableLength#

Added in: v12.9.0


<number>

The number of buffered bytes.

outgoingMessage.writableObjectMode#

Added in: v12.9.0


<boolean>

Always false.

outgoingMessage.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
The callback argument was added.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: utf8
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times.
The encoding argument is only relevant when chunk is a string. Defaults to
'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in the user
memory. The 'drain' event will be emitted when the buffer is free again.

http.METHODS#

Added in: v0.11.8


<string[]>

A list of the HTTP methods that are supported by the parser.
http.STATUS_CODES#

Added in: v0.1.22


<Object>

A collection of all the standard HTTP response status codes, and the
short description of each. For example, http.STATUS_CODES[404] === 'Not Found'.
http.createServer([options][, requestListener])#

History

VersionChanges
v20.1.0, v18.17.0
The highWaterMark option is supported now.
v18.0.0
The requestTimeout, headersTimeout, keepAliveTimeout, and connectionsCheckingInterval options are supported now.
v18.0.0
The noDelay option now defaults to true.
v17.7.0, v16.15.0
The noDelay, keepAlive and keepAliveInitialDelay options are supported now.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v9.6.0, v8.12.0
The options argument is supported now.
v0.1.13
Added in: v0.1.13





options <Object>

connectionsCheckingInterval: Sets the interval value in milliseconds to
check for request and headers timeout in incomplete requests.
Default: 30000.
headersTimeout: Sets the timeout value in milliseconds for receiving
the complete HTTP headers from the client.
See server.headersTimeout for more information.
Default: 60000.
highWaterMark <number> Optionally overrides all sockets'
readableHighWaterMark and writableHighWaterMark. This affects
highWaterMark property of both IncomingMessage and ServerResponse.
Default: See stream.getDefaultHighWaterMark().
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false.
IncomingMessage <http.IncomingMessage> Specifies the IncomingMessage
class to be used. Useful for extending the original IncomingMessage.
Default: IncomingMessage.
joinDuplicateHeaders <boolean> If set to true, this option allows
joining the field line values of multiple headers in a request with
a comma (, ) instead of discarding the duplicates.
For more information, refer to message.headers.
Default: false.
keepAlive <boolean> If set to true, it enables keep-alive functionality
on the socket immediately after a new incoming connection is received,
similarly on what is done in [socket.setKeepAlive([enable][, initialDelay])][socket.setKeepAlive(enable, initialDelay)].
Default: false.
keepAliveInitialDelay <number> If set to a positive number, it sets the
initial delay before the first keepalive probe is sent on an idle socket.
Default: 0.
keepAliveTimeout: The number of milliseconds of inactivity a server
needs to wait for additional incoming data, after it has finished writing
the last response, before a socket will be destroyed.
See server.keepAliveTimeout for more information.
Default: 5000.
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size for requests received by this server, i.e.
the maximum length of request headers in bytes.
Default: 16384 (16 KiB).
noDelay <boolean> If set to true, it disables the use of Nagle's
algorithm immediately after a new incoming connection is received.
Default: true.
requestTimeout: Sets the timeout value in milliseconds for receiving
the entire request from the client.
See server.requestTimeout for more information.
Default: 300000.
requireHostHeader <boolean> If set to true, it forces the server to
respond with a 400 (Bad Request) status code to any HTTP/1.1
request message that lacks a Host header
(as mandated by the specification).
Default: true.
ServerResponse <http.ServerResponse> Specifies the ServerResponse class
to be used. Useful for extending the original ServerResponse. Default:
ServerResponse.
uniqueHeaders <Array> A list of response headers that should be sent only
once. If the header's value is an array, the items will be joined
using ; .
rejectNonStandardBodyWrites <boolean> If set to true, an error is thrown
when writing to an HTTP response which does not have a body.
Default: false.



requestListener <Function>


Returns: <http.Server>


Returns a new instance of http.Server.
The requestListener is a function which is automatically
added to the 'request' event.

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy
http.get(options[, callback])#
http.get(url[, options][, callback])#

History

VersionChanges
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object> Accepts the same options as
http.request(), with the method set to GET by default.
callback <Function>
Returns: <http.ClientRequest>

Since most requests are GET requests without bodies, Node.js provides this
convenience method. The only difference between this method and
http.request() is that it sets the method to GET by default and calls req.end()
automatically. The callback must take care to consume the response
data for reasons stated in http.ClientRequest section.
The callback is invoked with a single argument that is an instance of
http.IncomingMessage.
JSON fetching example:
http.get('http://localhost:8000/', (res) => {
  const { statusCode } = res;
  const contentType = res.headers['content-type'];

  let error;
  // Any 2xx status code signals a successful response but
  // here we're only checking for 200.
  if (statusCode !== 200) {
    error = new Error('Request Failed.\n' +
                      `Status Code: ${statusCode}`);
  } else if (!/^application\/json/.test(contentType)) {
    error = new Error('Invalid content-type.\n' +
                      `Expected application/json but received ${contentType}`);
  }
  if (error) {
    console.error(error.message);
    // Consume response data to free up memory
    res.resume();
    return;
  }

  res.setEncoding('utf8');
  let rawData = '';
  res.on('data', (chunk) => { rawData += chunk; });
  res.on('end', () => {
    try {
      const parsedData = JSON.parse(rawData);
      console.log(parsedData);
    } catch (e) {
      console.error(e.message);
    }
  });
}).on('error', (e) => {
  console.error(`Got error: ${e.message}`);
});

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000); copy
http.globalAgent#

History

VersionChanges
v19.0.0
The agent now uses HTTP Keep-Alive and a 5 second timeout by default.
v0.5.9
Added in: v0.5.9




<http.Agent>

Global instance of Agent which is used as the default for all HTTP client
requests. Diverges from a default Agent configuration by having keepAlive
enabled and a timeout of 5 seconds.
http.maxHeaderSize#

Added in: v11.6.0, v10.15.0


<number>

Read-only property specifying the maximum allowed size of HTTP headers in bytes.
Defaults to 16 KiB. Configurable using the --max-http-header-size CLI
option.
This can be overridden for servers and client requests by passing the
maxHeaderSize option.
http.request(options[, callback])#
http.request(url[, options][, callback])#

History

VersionChanges
v16.7.0, v14.18.0
When using a URL object parsed username and password will now be properly URI decoded.
v15.3.0, v14.17.0
It is possible to abort a request with an AbortSignal.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object>

agent <http.Agent> | <boolean> Controls Agent behavior. Possible
values:

undefined (default): use http.globalAgent for this host and port.
Agent object: explicitly use the passed in Agent.
false: causes a new Agent with default values to be used.


auth <string> Basic authentication ('user:password') to compute an
Authorization header.
createConnection <Function> A function that produces a socket/stream to
use for the request when the agent option is not used. This can be used to
avoid creating a custom Agent class just to override the default
createConnection function. See agent.createConnection() for more
details. Any Duplex stream is a valid return value.
defaultPort <number> Default port for the protocol. Default:
agent.defaultPort if an Agent is used, else undefined.
family <number> IP address family to use when resolving host or
hostname. Valid values are 4 or 6. When unspecified, both IP v4 and
v6 will be used.
headers <Object> An object containing request headers.
hints <number> Optional dns.lookup() hints.
host <string> A domain name or IP address of the server to issue the
request to. Default: 'localhost'.
hostname <string> Alias for host. To support url.parse(),
hostname will be used if both host and hostname are specified.
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false
joinDuplicateHeaders <boolean> It joins the field line values of
multiple headers in a request with ,  instead of discarding
the duplicates. See message.headers for more information.
Default: false.
localAddress <string> Local interface to bind for network connections.
localPort <number> Local port to connect from.
lookup <Function> Custom lookup function. Default: dns.lookup().
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size (the maximum length of response headers in
bytes) for responses received from the server.
Default: 16384 (16 KiB).
method <string> A string specifying the HTTP request method. Default:
'GET'.
path <string> Request path. Should include query string if any.
E.G. '/index.html?page=12'. An exception is thrown when the request path
contains illegal characters. Currently, only spaces are rejected but that
may change in the future. Default: '/'.
port <number> Port of remote server. Default: defaultPort if set,
else 80.
protocol <string> Protocol to use. Default: 'http:'.
setDefaultHeaders <boolean>: Specifies whether or not to automatically add
default headers such as Connection, Content-Length, Transfer-Encoding,
and Host. If set to false then all necessary headers must be added
manually. Defaults to true.
setHost <boolean>: Specifies whether or not to automatically add the
Host header. If provided, this overrides setDefaultHeaders. Defaults to
true.
signal <AbortSignal>: An AbortSignal that may be used to abort an ongoing
request.
socketPath <string> Unix domain socket. Cannot be used if one of host
or port is specified, as those specify a TCP Socket.
timeout <number>: A number specifying the socket timeout in milliseconds.
This will set the timeout before the socket is connected.
uniqueHeaders <Array> A list of request headers that should be sent
only once. If the header's value is an array, the items will be joined
using ; .


callback <Function>
Returns: <http.ClientRequest>

options in socket.connect() are also supported.
Node.js maintains several connections per server to make HTTP requests.
This function allows one to transparently issue requests.
url can be a string or a URL object. If url is a
string, it is automatically parsed with new URL(). If it is a URL
object, it will be automatically converted to an ordinary options object.
If both url and options are specified, the objects are merged, with the
options properties taking precedence.
The optional callback parameter will be added as a one-time listener for
the 'response' event.
http.request() returns an instance of the http.ClientRequest
class. The ClientRequest instance is a writable stream. If one needs to
upload a file with a POST request, then write to the ClientRequest object.

import http from 'node:http';
import { Buffer } from 'node:buffer';

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();const http = require('node:http');

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();copy
In the example req.end() was called. With http.request() one
must always call req.end() to signify the end of the request -
even if there is no data being written to the request body.
If any error is encountered during the request (be that with DNS resolution,
TCP level errors, or actual HTTP parse errors) an 'error' event is emitted
on the returned request object. As with all 'error' events, if no listeners
are registered the error will be thrown.
There are a few special headers that should be noted.


Sending a 'Connection: keep-alive' will notify Node.js that the connection to
the server should be persisted until the next request.


Sending a 'Content-Length' header will disable the default chunked encoding.


Sending an 'Expect' header will immediately send the request headers.
Usually, when sending 'Expect: 100-continue', both a timeout and a listener
for the 'continue' event should be set. See RFC 2616 Section 8.2.3 for more
information.


Sending an Authorization header will override using the auth option
to compute basic authentication.


Example using a URL as options:
const options = new URL('http://abc:xyz@example.com');

const req = http.request(options, (res) => {
  // ...
}); copy
In a successful request, the following events will be emitted in the following
order:

'socket'
'response'

'data' any number of times, on the res object
('data' will not be emitted at all if the response body is empty, for
instance, in most redirects)
'end' on the res object


'close'

In the case of a connection error, the following events will be emitted:

'socket'
'error'
'close'

In the case of a premature connection close before the response is received,
the following events will be emitted in the following order:

'socket'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

In the case of a premature connection close after the response is received,
the following events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(connection closed here)
'aborted' on the res object
'close'
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'
'close' on the res object

If req.destroy() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.destroy() called here)
'aborted' on the res object
'close'
'error' on the res object with an error with message 'Error: aborted'
and code 'ECONNRESET', or the error with which req.destroy() was called
'close' on the res object

If req.abort() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.abort() called here)
'abort'
'close'

If req.abort() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.abort() called here)
'abort'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

If req.abort() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.abort() called here)
'abort'
'aborted' on the res object
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'.
'close'
'close' on the res object

Setting the timeout option or using the setTimeout() function will
not abort the request or do anything besides add a 'timeout' event.
Passing an AbortSignal and then calling abort() on the corresponding
AbortController will behave the same way as calling .destroy() on the
request. Specifically, the 'error' event will be emitted with an error with
the message 'AbortError: The operation was aborted', the code 'ABORT_ERR'
and the cause, if one was provided.
http.validateHeaderName(name[, label])#

History

VersionChanges
v19.5.0, v18.14.0
The label parameter is added.
v14.3.0
Added in: v14.3.0




name <string>
label <string> Label for error message. Default: 'Header name'.

Performs the low-level validations on the provided name that are done when
res.setHeader(name, value) is called.
Passing illegal value as name will result in a TypeError being thrown,
identified by code: 'ERR_INVALID_HTTP_TOKEN'.
It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Example:

import { validateHeaderName } from 'node:http';

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}const { validateHeaderName } = require('node:http');

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}copy
http.validateHeaderValue(name, value)#

Added in: v14.3.0


name <string>
value <any>

Performs the low-level validations on the provided value that are done when
res.setHeader(name, value) is called.
Passing illegal value as value will result in a TypeError being thrown.

Undefined value error is identified by code: 'ERR_HTTP_INVALID_HEADER_VALUE'.
Invalid value character error is identified by code: 'ERR_INVALID_CHAR'.

It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Examples:

import { validateHeaderValue } from 'node:http';

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}const { validateHeaderValue } = require('node:http');

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}copy
http.setMaxIdleHTTPParsers(max)#

Added in: v18.8.0, v16.18.0


max <number> Default: 1000.

Set the maximum number of idle HTTP parsers.
WebSocket#

Added in: v22.5.0

A browser-compatible implementation of WebSocket.\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket




      
        HTTP#

Stability: 2 - Stable
Source Code: lib/http.js
This module, containing both a client and server, can be imported via
require('node:http') (CommonJS) or import * as http from 'node:http' (ES module).
The HTTP interfaces in Node.js are designed to support many features
of the protocol which have been traditionally difficult to use.
In particular, large, possibly chunk-encoded, messages. The interface is
careful to never buffer entire requests or responses, so the
user is able to stream data.
HTTP message headers are represented by an object like this:
{ "content-length": "123",
  "content-type": "text/plain",
  "connection": "keep-alive",
  "host": "example.com",
  "accept": "*/*" } copy
Keys are lowercased. Values are not modified.
In order to support the full spectrum of possible HTTP applications, the Node.js
HTTP API is very low-level. It deals with stream handling and message
parsing only. It parses a message into headers and body but it does not
parse the actual headers or the body.
See message.headers for details on how duplicate headers are handled.
The raw headers as they were received are retained in the rawHeaders
property, which is an array of [key, value, key2, value2, ...]. For
example, the previous message header object might have a rawHeaders
list like the following:

[ 'ConTent-Length', '123456',
  'content-LENGTH', '123',
  'content-type', 'text/plain',
  'CONNECTION', 'keep-alive',
  'Host', 'example.com',
  'accepT', '*/*' ] copy
Class: http.Agent#

Added in: v0.3.4

An Agent is responsible for managing connection persistence
and reuse for HTTP clients. It maintains a queue of pending requests
for a given host and port, reusing a single socket connection for each
until the queue is empty, at which time the socket is either destroyed
or put into a pool where it is kept to be used again for requests to the
same host and port. Whether it is destroyed or pooled depends on the
keepAlive option.
Pooled connections have TCP Keep-Alive enabled for them, but servers may
still close idle connections, in which case they will be removed from the
pool and a new connection will be made when a new HTTP request is made for
that host and port. Servers may also refuse to allow multiple requests
over the same connection, in which case the connection will have to be
remade for every request and cannot be pooled. The Agent will still make
the requests to that server, but each one will occur over a new connection.
When a connection is closed by the client or the server, it is removed
from the pool. Any unused sockets in the pool will be unrefed so as not
to keep the Node.js process running when there are no outstanding requests.
(see socket.unref()).
It is good practice, to destroy() an Agent instance when it is no
longer in use, because unused sockets consume OS resources.
Sockets are removed from an agent when the socket emits either
a 'close' event or an 'agentRemove' event. When intending to keep one
HTTP request open for a long time without keeping it in the agent, something
like the following may be done:
http.get(options, (res) => {
  // Do stuff
}).on('socket', (socket) => {
  socket.emit('agentRemove');
}); copy
An agent may also be used for an individual request. By providing
{agent: false} as an option to the http.get() or http.request()
functions, a one-time use Agent with default options will be used
for the client connection.
agent:false:
http.get({
  hostname: 'localhost',
  port: 80,
  path: '/',
  agent: false,  // Create a new agent just for this one request
}, (res) => {
  // Do stuff with response
}); copy

new Agent([options])#

History

VersionChanges
v15.6.0, v14.17.0
Change the default scheduling from 'fifo' to 'lifo'.
v14.5.0, v12.20.0
Add scheduling option to specify the free socket scheduling strategy.
v14.5.0, v12.19.0
Add maxTotalSockets option to agent constructor.
v0.3.4
Added in: v0.3.4




options <Object> Set of configurable options to set on the agent.
Can have the following fields:

keepAlive <boolean> Keep sockets around even when there are no
outstanding requests, so they can be used for future requests without
having to reestablish a TCP connection. Not to be confused with the
keep-alive value of the Connection header. The Connection: keep-alive
header is always sent when using an agent except when the Connection
header is explicitly specified or when the keepAlive and maxSockets
options are respectively set to false and Infinity, in which case
Connection: close will be used. Default: false.
keepAliveMsecs <number> When using the keepAlive option, specifies
the initial delay
for TCP Keep-Alive packets. Ignored when the
keepAlive option is false or undefined. Default: 1000.
maxSockets <number> Maximum number of sockets to allow per host.
If the same host opens multiple concurrent connections, each request
will use new socket until the maxSockets value is reached.
If the host attempts to open more connections than maxSockets,
the additional requests will enter into a pending request queue, and
will enter active connection state when an existing connection terminates.
This makes sure there are at most maxSockets active connections at
any point in time, from a given host.
Default: Infinity.
maxTotalSockets <number> Maximum number of sockets allowed for
all hosts in total. Each request will use a new socket
until the maximum is reached.
Default: Infinity.
maxFreeSockets <number> Maximum number of sockets per host to leave open
in a free state. Only relevant if keepAlive is set to true.
Default: 256.
scheduling <string> Scheduling strategy to apply when picking
the next free socket to use. It can be 'fifo' or 'lifo'.
The main difference between the two scheduling strategies is that 'lifo'
selects the most recently used socket, while 'fifo' selects
the least recently used socket.
In case of a low rate of request per second, the 'lifo' scheduling
will lower the risk of picking a socket that might have been closed
by the server due to inactivity.
In case of a high rate of request per second,
the 'fifo' scheduling will maximize the number of open sockets,
while the 'lifo' scheduling will keep it as low as possible.
Default: 'lifo'.
timeout <number> Socket timeout in milliseconds.
This will set the timeout when the socket is created.



options in socket.connect() are also supported.
To configure any of them, a custom http.Agent instance must be created.

import { Agent, request } from 'node:http';
const keepAliveAgent = new Agent({ keepAlive: true });
options.agent = keepAliveAgent;
request(options, onResponseCallback);const http = require('node:http');
const keepAliveAgent = new http.Agent({ keepAlive: true });
options.agent = keepAliveAgent;
http.request(options, onResponseCallback);copy

agent.createConnection(options[, callback])#

Added in: v0.11.4


options <Object> Options containing connection details. Check
net.createConnection() for the format of the options
callback <Function> Callback function that receives the created socket
Returns: <stream.Duplex>

Produces a socket/stream to be used for HTTP requests.
By default, this function is the same as net.createConnection(). However,
custom agents may override this method in case greater flexibility is desired.
A socket/stream can be supplied in one of two ways: by returning the
socket/stream from this function, or by passing the socket/stream to callback.
This method is guaranteed to return an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
callback has a signature of (err, stream).

agent.keepSocketAlive(socket)#

Added in: v8.1.0


socket <stream.Duplex>

Called when socket is detached from a request and could be persisted by the
Agent. Default behavior is to:
socket.setKeepAlive(true, this.keepAliveMsecs);
socket.unref();
return true; copy
This method can be overridden by a particular Agent subclass. If this
method returns a falsy value, the socket will be destroyed instead of persisting
it for use with the next request.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.reuseSocket(socket, request)#

Added in: v8.1.0


socket <stream.Duplex>
request <http.ClientRequest>

Called when socket is attached to request after being persisted because of
the keep-alive options. Default behavior is to:
socket.ref(); copy
This method can be overridden by a particular Agent subclass.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.destroy()#

Added in: v0.11.4

Destroy any sockets that are currently in use by the agent.
It is usually not necessary to do this. However, if using an
agent with keepAlive enabled, then it is best to explicitly shut down
the agent when it is no longer needed. Otherwise,
sockets might stay open for quite a long time before the server
terminates them.

agent.freeSockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.11.4
Added in: v0.11.4




<Object>

An object which contains arrays of sockets currently awaiting use by
the agent when keepAlive is enabled. Do not modify.
Sockets in the freeSockets list will be automatically destroyed and
removed from the array on 'timeout'.

agent.getName([options])#

History

VersionChanges
v17.7.0, v16.15.0
The options parameter is now optional.
v0.11.4
Added in: v0.11.4




options <Object> A set of options providing information for name generation

host <string> A domain name or IP address of the server to issue the
request to
port <number> Port of remote server
localAddress <string> Local interface to bind for network connections
when issuing the request
family <integer> Must be 4 or 6 if this doesn't equal undefined.


Returns: <string>

Get a unique name for a set of request options, to determine whether a
connection can be reused. For an HTTP agent, this returns
host:port:localAddress or host:port:localAddress:family. For an HTTPS agent,
the name includes the CA, cert, ciphers, and other HTTPS/TLS-specific options
that determine socket reusability.

agent.maxFreeSockets#

Added in: v0.11.7


<number>

By default set to 256. For agents with keepAlive enabled, this
sets the maximum number of sockets that will be left open in the free
state.

agent.maxSockets#

Added in: v0.3.6


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open per origin. Origin is the returned value of agent.getName().

agent.maxTotalSockets#

Added in: v14.5.0, v12.19.0


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open. Unlike maxSockets, this parameter applies across all origins.

agent.requests#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.5.9
Added in: v0.5.9




<Object>

An object which contains queues of requests that have not yet been assigned to
sockets. Do not modify.

agent.sockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.3.6
Added in: v0.3.6




<Object>

An object which contains arrays of sockets currently in use by the
agent. Do not modify.

Class: http.ClientRequest#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally and returned from http.request(). It
represents an in-progress request whose header has already been queued. The
header is still mutable using the setHeader(name, value),
getHeader(name), removeHeader(name) API. The actual header will
be sent along with the first data chunk or when calling request.end().
To get the response, add a listener for 'response' to the request object.
'response' will be emitted from the request object when the response
headers have been received. The 'response' event is executed with one
argument which is an instance of http.IncomingMessage.
During the 'response' event, one can add listeners to the
response object; particularly to listen for the 'data' event.
If no 'response' handler is added, then the response will be
entirely discarded. However, if a 'response' event handler is added,
then the data from the response object must be consumed, either by
calling response.read() whenever there is a 'readable' event, or
by adding a 'data' handler, or by calling the .resume() method.
Until the data is consumed, the 'end' event will not fire. Also, until
the data is read it will consume memory that can eventually lead to a
'process out of memory' error.
For backward compatibility, res will only emit 'error' if there is an
'error' listener registered.
Set Content-Length header to limit the response body size.
If response.strictContentLength is set to true, mismatching the
Content-Length header value will result in an Error being thrown,
identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.
Content-Length value should be in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes.

Event: 'abort'#

Added in: v1.4.1Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for the 'close' event instead.
Emitted when the request has been aborted by the client. This event is only
emitted on the first call to abort().

Event: 'close'#

Added in: v0.5.4

Indicates that the request is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'connect'#

Added in: v0.7.0


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with a CONNECT method. If
this event is not being listened for, clients receiving a CONNECT method will
have their connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client and server pair demonstrating how to listen for the 'connect' event:

import { createServer, request } from 'node:http';
import { connect } from 'node:net';
import { URL } from 'node:url';

// Create an HTTP tunneling proxy
const proxy = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});const http = require('node:http');
const net = require('node:net');
const { URL } = require('node:url');

// Create an HTTP tunneling proxy
const proxy = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = net.connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = http.request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});copy

Event: 'continue'#

Added in: v0.3.2

Emitted when the server sends a '100 Continue' HTTP response, usually because
the request contained 'Expect: 100-continue'. This is an instruction that
the client should send the request body.

Event: 'finish'#

Added in: v0.3.6

Emitted when the request has been sent. More specifically, this event is emitted
when the last segment of the response headers and body have been handed off to
the operating system for transmission over the network. It does not imply that
the server has received anything yet.

Event: 'information'#

Added in: v10.0.0


info <Object>

httpVersion <string>
httpVersionMajor <integer>
httpVersionMinor <integer>
statusCode <integer>
statusMessage <string>
headers <Object>
rawHeaders <string[]>



Emitted when the server sends a 1xx intermediate response (excluding 101
Upgrade). The listeners of this event will receive an object containing the
HTTP version, status code, status message, key-value headers object,
and array with the raw header names followed by their respective values.

import { request } from 'node:http';

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});const http = require('node:http');

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = http.request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});copy
101 Upgrade statuses do not fire this event due to their break from the
traditional HTTP request/response chain, such as web sockets, in-place TLS
upgrades, or HTTP 2.0. To be notified of 101 Upgrade notices, listen for the
'upgrade' event instead.

Event: 'response'#

Added in: v0.1.0


response <http.IncomingMessage>

Emitted when a response is received to this request. This event is emitted only
once.

Event: 'socket'#

Added in: v0.5.3


socket <stream.Duplex>

This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'timeout'#

Added in: v0.7.8

Emitted when the underlying socket times out from inactivity. This only notifies
that the socket has been idle. The request must be destroyed manually.
See also: request.setTimeout().

Event: 'upgrade'#

Added in: v0.1.94


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with an upgrade. If this
event is not being listened for and the response status code is 101 Switching
Protocols, clients receiving an upgrade header will have their connections
closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client server pair demonstrating how to listen for the 'upgrade' event.

import http from 'node:http';
import process from 'node:process';

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});const http = require('node:http');

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});copy

request.abort()#

Added in: v0.3.8Deprecated since: v14.1.0, v13.14.0

Stability: 0 - Deprecated: Use request.destroy() instead.
Marks the request as aborting. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.

request.aborted#

History

VersionChanges
v17.0.0, v16.12.0
Deprecated since: v17.0.0, v16.12.0
v11.0.0
The aborted property is no longer a timestamp number.
v0.11.14
Added in: v0.11.14



Stability: 0 - Deprecated. Check request.destroyed instead.

<boolean>

The request.aborted property will be true if the request has
been aborted.

request.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use request.socket.

<stream.Duplex>

See request.socket.

request.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

request.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ClientRequest.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

Finishes sending the request. If any parts of the body are
unsent, it will flush them to the stream. If the request is
chunked, this will send the terminating '0\r\n\r\n'.
If data is specified, it is equivalent to calling
request.write(data, encoding) followed by request.end(callback).
If callback is specified, it will be called when the request stream
is finished.

request.destroy([error])#

History

VersionChanges
v14.5.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error> Optional, an error to emit with 'error' event.
Returns: <this>

Destroy the request. Optionally emit an 'error' event,
and emit a 'close' event. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.
See writable.destroy() for further details.

request.destroyed#

Added in: v14.1.0, v13.14.0


<boolean>

Is true after request.destroy() has been called.
See writable.destroyed for further details.

request.finished#

Added in: v0.0.1Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use request.writableEnded.

<boolean>

The request.finished property will be true if request.end()
has been called. request.end() will automatically be called if the
request was initiated via http.get().

request.flushHeaders()#

Added in: v1.6.0

Flushes the request headers.
For efficiency reasons, Node.js normally buffers the request headers until
request.end() is called or the first chunk of request data is written. It
then tries to pack the request headers and data into a single TCP packet.
That's usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. request.flushHeaders() bypasses
the optimization and kickstarts the request.

request.getHeader(name)#

Added in: v1.6.0


name <string>
Returns: <any>

Reads out a header on the request. The name is case-insensitive.
The type of the return value depends on the arguments provided to
request.setHeader().
request.setHeader('content-type', 'text/html');
request.setHeader('Content-Length', Buffer.byteLength(body));
request.setHeader('Cookie', ['type=ninja', 'language=javascript']);
const contentType = request.getHeader('Content-Type');
// 'contentType' is 'text/html'
const contentLength = request.getHeader('Content-Length');
// 'contentLength' is of type number
const cookie = request.getHeader('Cookie');
// 'cookie' is of type string[] copy

request.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getHeaderNames();
// headerNames === ['foo', 'cookie'] copy

request.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the request.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headers = request.getHeaders();
// headers === { foo: 'bar', 'cookie': ['foo=bar', 'bar=baz'] } copy

request.getRawHeaderNames()#

Added in: v15.13.0, v14.17.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing raw
headers. Header names are returned with their exact casing being set.
request.setHeader('Foo', 'bar');
request.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getRawHeaderNames();
// headerNames === ['Foo', 'Set-Cookie'] copy

request.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = request.hasHeader('content-type'); copy

request.maxHeadersCount#

<number> Default: 2000

Limits maximum response headers count. If set to 0, no limit will be applied.

request.path#

Added in: v0.4.0


<string> The request path.


request.method#

Added in: v0.1.97


<string> The request method.


request.host#

Added in: v14.5.0, v12.19.0


<string> The request host.


request.protocol#

Added in: v14.5.0, v12.19.0


<string> The request protocol.


request.removeHeader(name)#

Added in: v1.6.0


name <string>

Removes a header that's already defined into headers object.
request.removeHeader('Content-Type'); copy

request.reusedSocket#

Added in: v13.0.0, v12.16.0


<boolean> Whether the request is send through a reused socket.

When sending request through a keep-alive enabled agent, the underlying socket
might be reused. But if server closes connection at unfortunate time, client
may run into a 'ECONNRESET' error.

import http from 'node:http';

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutconst http = require('node:http');

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutcopy
By marking a request whether it reused socket or not, we can do
automatic error retry base on it.

import http from 'node:http';
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();const http = require('node:http');
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();copy

request.setHeader(name, value)#

Added in: v1.6.0


name <string>
value <any>

Sets a single header value for headers object. If this header already exists in
the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, request.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission.
request.setHeader('Content-Type', 'application/json'); copy
or
request.setHeader('Cookie', ['type=ninja', 'language=javascript']); copy
When the value is a string an exception will be thrown if it contains
characters outside the latin1 encoding.
If you need to pass UTF-8 characters in the value please encode the value
using the RFC 8187 standard.
const filename = 'Rock 🎵.txt';
request.setHeader('Content-Disposition', `attachment; filename*=utf-8''${encodeURIComponent(filename)}`); copy

request.setNoDelay([noDelay])#

Added in: v0.5.9


noDelay <boolean>

Once a socket is assigned to this request and is connected
socket.setNoDelay() will be called.

request.setSocketKeepAlive([enable][, initialDelay])#

Added in: v0.5.9


enable <boolean>
initialDelay <number>

Once a socket is assigned to this request and is connected
socket.setKeepAlive() will be called.

request.setTimeout(timeout[, callback])#

History

VersionChanges
v9.0.0
Consistently set socket timeout only when the socket connects.
v0.5.9
Added in: v0.5.9




timeout <number> Milliseconds before a request times out.
callback <Function> Optional function to be called when a timeout occurs.
Same as binding to the 'timeout' event.
Returns: <http.ClientRequest>

Once a socket is assigned to this request and is connected
socket.setTimeout() will be called.

request.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket.

import http from 'node:http';
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});const http = require('node:http');
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

request.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

request.writableEnded#

Added in: v12.9.0


<boolean>

Is true after request.end() has been called. This property
does not indicate whether the data has been flushed, for this use
request.writableFinished instead.

request.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

request.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times. If no
Content-Length is set, data will automatically be encoded in HTTP Chunked
transfer encoding, so that server knows when the data ends. The
Transfer-Encoding: chunked header is added. Calling request.end()
is necessary to finish sending the request.
The encoding argument is optional and only applies when chunk is a string.
Defaults to 'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed, but only if the chunk is non-empty.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.
When write function is called with empty string or buffer, it does
nothing and waits for more input.

Class: http.Server#

Added in: v0.1.17


Extends: <net.Server>


Event: 'checkContinue'#

Added in: v0.3.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect: 100-continue is received.
If this event is not listened for, the server will automatically respond
with a 100 Continue as appropriate.
Handling this event involves calling response.writeContinue() if the
client should continue to send the request body, or generating an appropriate
HTTP response (e.g. 400 Bad Request) if the client should not continue to send
the request body.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'checkExpectation'#

Added in: v5.5.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect header is received, where the
value is not 100-continue. If this event is not listened for, the server will
automatically respond with a 417 Expectation Failed as appropriate.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'clientError'#

History

VersionChanges
v12.0.0
The default behavior will return a 431 Request Header Fields Too Large if a HPE_HEADER_OVERFLOW error occurs.
v9.4.0
The rawPacket is the current buffer that just parsed. Adding this buffer to the error object of 'clientError' event is to make it possible that developers can log the broken packet.
v6.0.0
The default action of calling .destroy() on the socket will no longer take place if there are listeners attached for 'clientError'.
v0.1.94
Added in: v0.1.94




exception <Error>
socket <stream.Duplex>

If a client connection emits an 'error' event, it will be forwarded here.
Listener of this event is responsible for closing/destroying the underlying
socket. For example, one may wish to more gracefully close the socket with a
custom HTTP response instead of abruptly severing the connection. The socket
must be closed or destroyed before the listener ends.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
Default behavior is to try close the socket with a HTTP '400 Bad Request',
or a HTTP '431 Request Header Fields Too Large' in the case of a
HPE_HEADER_OVERFLOW error. If the socket is not writable or headers
of the current attached http.ServerResponse has been sent, it is
immediately destroyed.
socket is the net.Socket object that the error originated from.

import http from 'node:http';

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);const http = require('node:http');

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);copy
When the 'clientError' event occurs, there is no request or response
object, so any HTTP response sent, including response headers and payload,
must be written directly to the socket object. Care must be taken to
ensure the response is a properly formatted HTTP response message.
err is an instance of Error with two extra columns:

bytesParsed: the bytes count of request packet that Node.js may have parsed
correctly;
rawPacket: the raw packet of current request.

In some cases, the client has already received the response and/or the socket
has already been destroyed, like in case of ECONNRESET errors. Before
trying to send data to the socket, it is better to check that it is still
writable.
server.on('clientError', (err, socket) => {
  if (err.code === 'ECONNRESET' || !socket.writable) {
    return;
  }

  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
}); copy

Event: 'close'#

Added in: v0.1.4

Emitted when the server closes.

Event: 'connect'#

Added in: v0.7.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the tunneling stream (may be empty)

Emitted each time a client requests an HTTP CONNECT method. If this event is
not listened for, then clients requesting a CONNECT method will have their
connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.

Event: 'connection'#

Added in: v0.1.0


socket <stream.Duplex>

This event is emitted when a new TCP stream is established. socket is
typically an object of type net.Socket. Usually users will not want to
access this event. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. The socket can
also be accessed at request.socket.
This event can also be explicitly emitted by users to inject connections
into the HTTP server. In that case, any Duplex stream can be passed.
If socket.setTimeout() is called here, the timeout will be replaced with
server.keepAliveTimeout when the socket has served a request (if
server.keepAliveTimeout is non-zero).
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'dropRequest'#

Added in: v18.7.0, v16.17.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client

When the number of requests on a socket reaches the threshold of
server.maxRequestsPerSocket, the server will drop new requests
and emit 'dropRequest' event instead, then send 503 to client.

Event: 'request'#

Added in: v0.1.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time there is a request. There may be multiple requests
per connection (in the case of HTTP Keep-Alive connections).

Event: 'upgrade'#

History

VersionChanges
v10.0.0
Not listening to this event no longer causes the socket to be destroyed if a client sends an Upgrade header.
v0.1.94
Added in: v0.1.94




request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the upgraded stream (may be empty)

Emitted each time a client requests an HTTP upgrade. Listening to this event
is optional and clients cannot insist on a protocol change.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

server.close([callback])#

History

VersionChanges
v19.0.0
The method closes idle connections before returning.
v0.1.90
Added in: v0.1.90




callback <Function>

Stops the server from accepting new connections and closes all connections
connected to this server which are not sending a request or waiting for
a response.
See net.Server.close().
const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
}, 10000); copy

server.closeAllConnections()#

Added in: v18.2.0

Closes all established HTTP(S) connections connected to this server, including
active connections connected to this server which are sending a request or
waiting for a response. This does not destroy sockets upgraded to a different
protocol, such as WebSocket or HTTP/2.

This is a forceful way of closing all connections and should be used with
caution. Whenever using this in conjunction with server.close, calling this
after server.close is recommended as to avoid race conditions where new
connections are created between a call to this and a call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes all connections, ensuring the server closes successfully
  server.closeAllConnections();
}, 10000); copy

server.closeIdleConnections()#

Added in: v18.2.0

Closes all connections connected to this server which are not sending a request
or waiting for a response.

Starting with Node.js 19.0.0, there's no need for calling this method in
conjunction with server.close to reap keep-alive connections. Using it
won't cause any harm though, and it can be useful to ensure backwards
compatibility for libraries and applications that need to support versions
older than 19.0.0. Whenever using this in conjunction with server.close,
calling this after server.close is recommended as to avoid race
conditions where new connections are created between a call to this and a
call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes idle connections, such as keep-alive connections. Server will close
  // once remaining active connections are terminated
  server.closeIdleConnections();
}, 10000); copy

server.headersTimeout#

History

VersionChanges
v19.4.0, v18.14.0
The default is now set to the minimum between 60000 (60 seconds) or requestTimeout.
v11.3.0, v10.14.0
Added in: v11.3.0, v10.14.0




<number> Default: The minimum between server.requestTimeout or 60000.

Limit the amount of time the parser will wait to receive the complete HTTP
headers.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.listen()#
Starts the HTTP server listening for connections.
This method is identical to server.listen() from net.Server.

server.listening#

Added in: v5.7.0


<boolean> Indicates whether or not the server is listening for connections.


server.maxHeadersCount#

Added in: v0.7.0


<number> Default: 2000

Limits maximum incoming headers count. If set to 0, no limit will be applied.

server.requestTimeout#

History

VersionChanges
v18.0.0
The default request timeout changed from no timeout to 300s (5 minutes).
v14.11.0
Added in: v14.11.0




<number> Default: 300000

Sets the timeout value in milliseconds for receiving the entire request from
the client.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.setTimeout([msecs][, callback])#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




msecs <number> Default: 0 (no timeout)
callback <Function>
Returns: <http.Server>

Sets the timeout value for sockets, and emits a 'timeout' event on
the Server object, passing the socket as an argument, if a timeout
occurs.
If there is a 'timeout' event listener on the Server object, then it
will be called with the timed-out socket as an argument.
By default, the Server does not timeout sockets. However, if a callback
is assigned to the Server's 'timeout' event, timeouts must be handled
explicitly.

server.maxRequestsPerSocket#

Added in: v16.10.0


<number> Requests per socket. Default: 0 (no limit)

The maximum number of requests socket can handle
before closing keep alive connection.
A value of 0 will disable the limit.
When the limit is reached it will set the Connection header value to close,
but will not actually close the connection, subsequent requests sent
after the limit is reached will get 503 Service Unavailable as a response.

server.timeout#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




<number> Timeout in milliseconds. Default: 0 (no timeout)

The number of milliseconds of inactivity before a socket is presumed
to have timed out.
A value of 0 will disable the timeout behavior on incoming connections.
The socket timeout logic is set up on connection, so changing this
value only affects new connections to the server, not any existing connections.

server.keepAliveTimeout#

Added in: v8.0.0


<number> Timeout in milliseconds. Default: 5000 (5 seconds).

The number of milliseconds of inactivity a server needs to wait for additional
incoming data, after it has finished writing the last response, before a socket
will be destroyed. If the server receives new data before the keep-alive
timeout has fired, it will reset the regular inactivity timeout, i.e.,
server.timeout.
A value of 0 will disable the keep-alive timeout behavior on incoming
connections.
A value of 0 makes the http server behave similarly to Node.js versions prior
to 8.0.0, which did not have a keep-alive timeout.
The socket timeout logic is set up on connection, so changing this value only
affects new connections to the server, not any existing connections.

server[Symbol.asyncDispose]()#

Added in: v20.4.0

Stability: 1 - Experimental
Calls server.close() and returns a promise that fulfills when the
server has closed.

Class: http.ServerResponse#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally by an HTTP server, not by the user. It is
passed as the second parameter to the 'request' event.

Event: 'close'#

Added in: v0.6.7

Indicates that the response is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'finish'#

Added in: v0.3.6

Emitted when the response has been sent. More specifically, this event is
emitted when the last segment of the response headers and body have been
handed off to the operating system for transmission over the network. It
does not imply that the client has received anything yet.

response.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

This method adds HTTP trailing headers (a header but at the end of the
message) to the response.
Trailers will only be emitted if chunked encoding is used for the
response; if it is not (e.g. if the request was HTTP/1.0), they will
be silently discarded.
HTTP requires the Trailer header to be sent in order to
emit trailers, with a list of the header fields in its value. E.g.,
response.writeHead(200, { 'Content-Type': 'text/plain',
                          'Trailer': 'Content-MD5' });
response.write(fileData);
response.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
response.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

response.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use response.socket.

<stream.Duplex>

See response.socket.

response.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

response.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ServerResponse.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

This method signals to the server that all of the response headers and body
have been sent; that server should consider this message complete.
The method, response.end(), MUST be called on each response.
If data is specified, it is similar in effect to calling
response.write(data, encoding) followed by response.end(callback).
If callback is specified, it will be called when the response stream
is finished.

response.finished#

Added in: v0.0.2Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use response.writableEnded.

<boolean>

The response.finished property will be true if response.end()
has been called.

response.flushHeaders()#

Added in: v1.6.0

Flushes the response headers. See also: request.flushHeaders().

response.getHeader(name)#

Added in: v0.4.0


name <string>
Returns: <any>

Reads out a header that's already been queued but not sent to the client.
The name is case-insensitive. The type of the return value depends
on the arguments provided to response.setHeader().
response.setHeader('Content-Type', 'text/html');
response.setHeader('Content-Length', Buffer.byteLength(body));
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']);
const contentType = response.getHeader('content-type');
// contentType is 'text/html'
const contentLength = response.getHeader('Content-Length');
// contentLength is of type number
const setCookie = response.getHeader('set-cookie');
// setCookie is of type string[] copy

response.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = response.getHeaderNames();
// headerNames === ['foo', 'set-cookie'] copy

response.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the response.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = response.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

response.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = response.hasHeader('content-type'); copy

response.headersSent#

Added in: v0.9.3


<boolean>

Boolean (read-only). True if headers were sent, false otherwise.

response.removeHeader(name)#

Added in: v0.4.0


name <string>

Removes a header that's queued for implicit sending.
response.removeHeader('Content-Encoding'); copy

response.req#

Added in: v15.7.0


<http.IncomingMessage>

A reference to the original HTTP request object.

response.sendDate#

Added in: v0.7.5


<boolean>

When true, the Date header will be automatically generated and sent in
the response if it is not already present in the headers. Defaults to true.
This should only be disabled for testing; HTTP requires the Date header
in responses.

response.setHeader(name, value)#

Added in: v0.4.0


name <string>
value <any>
Returns: <http.ServerResponse>

Returns the response object.
Sets a single header value for implicit headers. If this header already exists
in the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, response.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission. The same response object is returned to the caller,
to enable call chaining.
response.setHeader('Content-Type', 'text/html'); copy
or
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
If response.writeHead() method is called and this method has not been
called, it will directly write the supplied header values onto the network
channel without caching internally, and the response.getHeader() on the
header will not yield the expected result. If progressive population of headers
is desired with potential future retrieval and modification, use
response.setHeader() instead of response.writeHead().

response.setTimeout(msecs[, callback])#

Added in: v0.9.12


msecs <number>
callback <Function>
Returns: <http.ServerResponse>

Sets the Socket's timeout value to msecs. If a callback is
provided, then it is added as a listener on the 'timeout' event on
the response object.
If no 'timeout' listener is added to the request, the response, or
the server, then sockets are destroyed when they time out. If a handler is
assigned to the request, the response, or the server's 'timeout' events,
timed out sockets must be handled explicitly.

response.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. After
response.end(), the property is nulled.

import http from 'node:http';
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);const http = require('node:http');
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

response.statusCode#

Added in: v0.4.0


<number> Default: 200

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status code that will be sent to the client when
the headers get flushed.
response.statusCode = 404; copy
After response header was sent to the client, this property indicates the
status code which was sent out.

response.statusMessage#

Added in: v0.11.8


<string>

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status message that will be sent to the client when
the headers get flushed. If this is left as undefined then the standard
message for the status code will be used.
response.statusMessage = 'Not found'; copy
After response header was sent to the client, this property indicates the
status message which was sent out.

response.strictContentLength#

Added in: v18.10.0, v16.18.0


<boolean> Default: false

If set to true, Node.js will check whether the Content-Length
header value and the size of the body, in bytes, are equal.
Mismatching the Content-Length header value will result
in an Error being thrown, identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.

response.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

response.writableEnded#

Added in: v12.9.0


<boolean>

Is true after response.end() has been called. This property
does not indicate whether the data has been flushed, for this use
response.writableFinished instead.

response.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

response.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: 'utf8'
callback <Function>
Returns: <boolean>

If this method is called and response.writeHead() has not been called,
it will switch to implicit header mode and flush the implicit headers.
This sends a chunk of the response body. This method may
be called multiple times to provide successive parts of the body.
If rejectNonStandardBodyWrites is set to true in createServer
then writing to the body is not allowed when the request method or response
status do not support content. If an attempt is made to write to the body for a
HEAD request or as part of a 204 or 304response, a synchronous Error
with the code ERR_HTTP_BODY_NOT_ALLOWED is thrown.
chunk can be a string or a buffer. If chunk is a string,
the second parameter specifies how to encode it into a byte stream.
callback will be called when this chunk of data is flushed.
This is the raw HTTP body and has nothing to do with higher-level multi-part
body encodings that may be used.
The first time response.write() is called, it will send the buffered
header information and the first chunk of the body to the client. The second
time response.write() is called, Node.js assumes data will be streamed,
and sends the new data separately. That is, the response is buffered up to the
first chunk of the body.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.

response.writeContinue()#

Added in: v0.3.0

Sends an HTTP/1.1 100 Continue message to the client, indicating that
the request body should be sent. See the 'checkContinue' event on
Server.

response.writeEarlyHints(hints[, callback])#

History

VersionChanges
v18.11.0
Allow passing hints as an object.
v18.11.0
Added in: v18.11.0




hints <Object>
callback <Function>

Sends an HTTP/1.1 103 Early Hints message to the client with a Link header,
indicating that the user agent can preload/preconnect the linked resources.
The hints is an object containing the values of headers to be sent with
early hints message. The optional callback argument will be called when
the response message has been written.
Example
const earlyHintsLink = '</styles.css>; rel=preload; as=style';
response.writeEarlyHints({
  'link': earlyHintsLink,
});

const earlyHintsLinks = [
  '</styles.css>; rel=preload; as=style',
  '</scripts.js>; rel=preload; as=script',
];
response.writeEarlyHints({
  'link': earlyHintsLinks,
  'x-trace-id': 'id for diagnostics',
});

const earlyHintsCallback = () => console.log('early hints message sent');
response.writeEarlyHints({
  'link': earlyHintsLinks,
}, earlyHintsCallback); copy

response.writeHead(statusCode[, statusMessage][, headers])#

History

VersionChanges
v14.14.0
Allow passing headers as an array.
v11.10.0, v10.17.0
Return this from writeHead() to allow chaining with end().
v5.11.0, v4.4.5
A RangeError is thrown if statusCode is not a number in the range [100, 999].
v0.1.30
Added in: v0.1.30




statusCode <number>
statusMessage <string>
headers <Object> | <Array>
Returns: <http.ServerResponse>

Sends a response header to the request. The status code is a 3-digit HTTP
status code, like 404. The last argument, headers, are the response headers.
Optionally one can give a human-readable statusMessage as the second
argument.
headers may be an Array where the keys and values are in the same list.
It is not a list of tuples. So, the even-numbered offsets are key values,
and the odd-numbered offsets are the associated values. The array is in the same
format as request.rawHeaders.
Returns a reference to the ServerResponse, so that calls can be chained.
const body = 'hello world';
response
  .writeHead(200, {
    'Content-Length': Buffer.byteLength(body),
    'Content-Type': 'text/plain',
  })
  .end(body); copy
This method must only be called once on a message and it must
be called before response.end() is called.
If response.write() or response.end() are called before calling
this, the implicit/mutable headers will be calculated and call this function.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
If this method is called and response.setHeader() has not been called,
it will directly write the supplied header values onto the network channel
without caching internally, and the response.getHeader() on the header
will not yield the expected result. If progressive population of headers is
desired with potential future retrieval and modification, use
response.setHeader() instead.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
Content-Length is read in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes. Node.js
will check whether Content-Length and the length of the body which has
been transmitted are equal or not.
Attempting to set a header field name or value that contains invalid characters
will result in a [Error][] being thrown.

response.writeProcessing()#

Added in: v10.0.0

Sends a HTTP/1.1 102 Processing message to the client, indicating that
the request body should be sent.

Class: http.IncomingMessage#

History

VersionChanges
v15.5.0
The destroyed value returns true after the incoming data is consumed.
v13.1.0, v12.16.0
The readableHighWaterMark value mirrors that of the socket.
v0.1.17
Added in: v0.1.17




Extends: <stream.Readable>

An IncomingMessage object is created by http.Server or
http.ClientRequest and passed as the first argument to the 'request'
and 'response' event respectively. It may be used to access response
status, headers, and data.
Different from its socket value which is a subclass of <stream.Duplex>, the
IncomingMessage itself extends <stream.Readable> and is created separately to
parse and emit the incoming HTTP headers and payload, as the underlying socket
may be reused multiple times in case of keep-alive.

Event: 'aborted'#

Added in: v0.3.8Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for 'close' event instead.
Emitted when the request has been aborted.

Event: 'close'#

History

VersionChanges
v16.0.0
The close event is now emitted when the request has been completed and not when the underlying socket is closed.
v0.4.2
Added in: v0.4.2



Emitted when the request has been completed.

message.aborted#

Added in: v10.1.0Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Check message.destroyed from <stream.Readable>.

<boolean>

The message.aborted property will be true if the request has
been aborted.

message.complete#

Added in: v0.3.0


<boolean>

The message.complete property will be true if a complete HTTP message has
been received and successfully parsed.
This property is particularly useful as a means of determining if a client or
server fully transmitted a message before a connection was terminated:
const req = http.request({
  host: '127.0.0.1',
  port: 8080,
  method: 'POST',
}, (res) => {
  res.resume();
  res.on('end', () => {
    if (!res.complete)
      console.error(
        'The connection was terminated while the message was still being sent');
  });
}); copy

message.connection#

Added in: v0.1.90Deprecated since: v16.0.0

Stability: 0 - Deprecated. Use message.socket.
Alias for message.socket.

message.destroy([error])#

History

VersionChanges
v14.5.0, v12.19.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error>
Returns: <this>

Calls destroy() on the socket that received the IncomingMessage. If error
is provided, an 'error' event is emitted on the socket and error is passed
as an argument to any listeners on the event.

message.headers#

History

VersionChanges
v19.5.0, v18.14.0
The joinDuplicateHeaders option in the http.request() and http.createServer() functions ensures that duplicate headers are not discarded, but rather combined using a comma separator, in accordance with RFC 9110 Section 5.3.
v15.1.0
message.headers is now lazily computed using an accessor property on the prototype and is no longer enumerable.
v0.1.5
Added in: v0.1.5




<Object>

The request/response headers object.
Key-value pairs of header names and values. Header names are lower-cased.
// Prints something like:
//
// { 'user-agent': 'curl/7.22.0',
//   host: '127.0.0.1:8000',
//   accept: '*/*' }
console.log(request.headers); copy
Duplicates in raw headers are handled in the following ways, depending on the
header name:

Duplicates of age, authorization, content-length, content-type,
etag, expires, from, host, if-modified-since, if-unmodified-since,
last-modified, location, max-forwards, proxy-authorization, referer,
retry-after, server, or user-agent are discarded.
To allow duplicate values of the headers listed above to be joined,
use the option joinDuplicateHeaders in http.request()
and http.createServer(). See RFC 9110 Section 5.3 for more
information.
set-cookie is always an array. Duplicates are added to the array.
For duplicate cookie headers, the values are joined together with ; .
For all other headers, the values are joined together with , .


message.headersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.headers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
// Prints something like:
//
// { 'user-agent': ['curl/7.22.0'],
//   host: ['127.0.0.1:8000'],
//   accept: ['*/*'] }
console.log(request.headersDistinct); copy

message.httpVersion#

Added in: v0.1.1


<string>

In case of server request, the HTTP version sent by the client. In the case of
client response, the HTTP version of the connected-to server.
Probably either '1.1' or '1.0'.
Also message.httpVersionMajor is the first integer and
message.httpVersionMinor is the second.

message.method#

Added in: v0.1.1


<string>

Only valid for request obtained from http.Server.
The request method as a string. Read only. Examples: 'GET', 'DELETE'.

message.rawHeaders#

Added in: v0.11.6


<string[]>

The raw request/response headers list exactly as they were received.
The keys and values are in the same list. It is not a
list of tuples. So, the even-numbered offsets are key values, and the
odd-numbered offsets are the associated values.
Header names are not lowercased, and duplicates are not merged.
// Prints something like:
//
// [ 'user-agent',
//   'this is invalid because there can be only one',
//   'User-Agent',
//   'curl/7.22.0',
//   'Host',
//   '127.0.0.1:8000',
//   'ACCEPT',
//   '*/*' ]
console.log(request.rawHeaders); copy

message.rawTrailers#

Added in: v0.11.6


<string[]>

The raw request/response trailer keys and values exactly as they were
received. Only populated at the 'end' event.

message.setTimeout(msecs[, callback])#

Added in: v0.5.9


msecs <number>
callback <Function>
Returns: <http.IncomingMessage>

Calls message.socket.setTimeout(msecs, callback).

message.socket#

Added in: v0.3.0


<stream.Duplex>

The net.Socket object associated with the connection.
With HTTPS support, use request.socket.getPeerCertificate() to obtain the
client's authentication details.
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket> or internally nulled.

message.statusCode#

Added in: v0.1.1


<number>

Only valid for response obtained from http.ClientRequest.
The 3-digit HTTP response status code. E.G. 404.

message.statusMessage#

Added in: v0.11.10


<string>

Only valid for response obtained from http.ClientRequest.
The HTTP response status message (reason phrase). E.G. OK or Internal Server Error.

message.trailers#

Added in: v0.3.0


<Object>

The request/response trailers object. Only populated at the 'end' event.

message.trailersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.trailers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
Only populated at the 'end' event.

message.url#

Added in: v0.1.90


<string>

Only valid for request obtained from http.Server.
Request URL string. This contains only the URL that is present in the actual
HTTP request. Take the following request:
GET /status?name=ryan HTTP/1.1
Accept: text/plain copy
To parse the URL into its parts:
new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`); copy
When request.url is '/status?name=ryan' and process.env.HOST is undefined:
$ node
> new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`);
URL {
  href: 'http://localhost/status?name=ryan',
  origin: 'http://localhost',
  protocol: 'http:',
  username: '',
  password: '',
  host: 'localhost',
  hostname: 'localhost',
  port: '',
  pathname: '/status',
  search: '?name=ryan',
  searchParams: URLSearchParams { 'name' => 'ryan' },
  hash: ''
} copy
Ensure that you set process.env.HOST to the server's host name, or consider
replacing this part entirely. If using req.headers.host, ensure proper
validation is used, as clients may specify a custom Host header.

Class: http.OutgoingMessage#

Added in: v0.1.17


Extends: <Stream>

This class serves as the parent class of http.ClientRequest
and http.ServerResponse. It is an abstract outgoing message from
the perspective of the participants of an HTTP transaction.

Event: 'drain'#

Added in: v0.3.6

Emitted when the buffer of the message is free again.

Event: 'finish'#

Added in: v0.1.17

Emitted when the transmission is finished successfully.

Event: 'prefinish'#

Added in: v0.11.6

Emitted after outgoingMessage.end() is called.
When the event is emitted, all data has been processed but not necessarily
completely flushed.

outgoingMessage.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

Adds HTTP trailers (headers but at the end of the message) to the message.
Trailers will only be emitted if the message is chunked encoded. If not,
the trailers will be silently discarded.
HTTP requires the Trailer header to be sent to emit trailers,
with a list of header field names in its value, e.g.
message.writeHead(200, { 'Content-Type': 'text/plain',
                         'Trailer': 'Content-MD5' });
message.write(fileData);
message.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
message.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

outgoingMessage.appendHeader(name, value)#

Added in: v18.3.0, v16.17.0


name <string> Header name
value <string> | <string[]> Header value
Returns: <this>

Append a single header value to the header object.
If the value is an array, this is equivalent to calling this method multiple
times.
If there were no previous values for the header, this is equivalent to calling
outgoingMessage.setHeader(name, value).
Depending of the value of options.uniqueHeaders when the client request or the
server were created, this will end up in the header being sent multiple times or
a single time with values joined using ; .

outgoingMessage.connection#

Added in: v0.3.0Deprecated since: v15.12.0, v14.17.1

Stability: 0 - Deprecated: Use outgoingMessage.socket instead.
Alias of outgoingMessage.socket.

outgoingMessage.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

outgoingMessage.destroy([error])#

Added in: v0.3.0


error <Error> Optional, an error to emit with error event
Returns: <this>

Destroys the message. Once a socket is associated with the message
and is connected, that socket will be destroyed as well.

outgoingMessage.end(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
add callback argument.
v0.1.90
Added in: v0.1.90




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Optional, Default: utf8
callback <Function> Optional
Returns: <this>

Finishes the outgoing message. If any parts of the body are unsent, it will
flush them to the underlying system. If the message is chunked, it will
send the terminating chunk 0\r\n\r\n, and send the trailers (if any).
If chunk is specified, it is equivalent to calling
outgoingMessage.write(chunk, encoding), followed by
outgoingMessage.end(callback).
If callback is provided, it will be called when the message is finished
(equivalent to a listener of the 'finish' event).

outgoingMessage.flushHeaders()#

Added in: v1.6.0

Flushes the message headers.
For efficiency reason, Node.js normally buffers the message headers
until outgoingMessage.end() is called or the first chunk of message data
is written. It then tries to pack the headers and data into a single TCP
packet.
It is usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. outgoingMessage.flushHeaders()
bypasses the optimization and kickstarts the message.

outgoingMessage.getHeader(name)#

Added in: v0.4.0


name <string> Name of header
Returns: <string> | <undefined>

Gets the value of the HTTP header with the given name. If that header is not
set, the returned value will be undefined.

outgoingMessage.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All names are lowercase.

outgoingMessage.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow
copy is used, array values may be mutated without additional calls to
various header-related HTTP module methods. The keys of the returned
object are the header names and the values are the respective header
values. All header names are lowercase.
The object returned by the outgoingMessage.getHeaders() method does
not prototypically inherit from the JavaScript Object. This means that
typical Object methods such as obj.toString(), obj.hasOwnProperty(),
and others are not defined and will not work.
outgoingMessage.setHeader('Foo', 'bar');
outgoingMessage.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = outgoingMessage.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

outgoingMessage.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name is case-insensitive.
const hasContentType = outgoingMessage.hasHeader('content-type'); copy

outgoingMessage.headersSent#

Added in: v0.9.3


<boolean>

Read-only. true if the headers were sent, otherwise false.

outgoingMessage.pipe()#

Added in: v9.0.0

Overrides the stream.pipe() method inherited from the legacy Stream class
which is the parent class of http.OutgoingMessage.
Calling this method will throw an Error because outgoingMessage is a
write-only stream.

outgoingMessage.removeHeader(name)#

Added in: v0.4.0


name <string> Header name

Removes a header that is queued for implicit sending.
outgoingMessage.removeHeader('Content-Encoding'); copy

outgoingMessage.setHeader(name, value)#

Added in: v0.4.0


name <string> Header name
value <any> Header value
Returns: <this>

Sets a single header value. If the header already exists in the to-be-sent
headers, its value will be replaced. Use an array of strings to send multiple
headers with the same name.

outgoingMessage.setHeaders(headers)#

Added in: v19.6.0, v18.15.0


headers <Headers> | <Map>
Returns: <this>

Sets multiple header values for implicit headers.
headers must be an instance of Headers or Map,
if a header already exists in the to-be-sent headers,
its value will be replaced.
const headers = new Headers({ foo: 'bar' });
outgoingMessage.setHeaders(headers); copy
or
const headers = new Map([['foo', 'bar']]);
outgoingMessage.setHeaders(headers); copy
When headers have been set with outgoingMessage.setHeaders(),
they will be merged with any headers passed to response.writeHead(),
with the headers passed to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  const headers = new Headers({ 'Content-Type': 'text/html' });
  res.setHeaders(headers);
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy

outgoingMessage.setTimeout(msesc[, callback])#

Added in: v0.9.12


msesc <number>
callback <Function> Optional function to be called when a timeout
occurs. Same as binding to the timeout event.
Returns: <this>

Once a socket is associated with the message and is connected,
socket.setTimeout() will be called with msecs as the first parameter.

outgoingMessage.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually, users will not want to access
this property.
After calling outgoingMessage.end(), this property will be nulled.

outgoingMessage.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork()

outgoingMessage.writableCorked#

Added in: v13.2.0, v12.16.0


<number>

The number of times outgoingMessage.cork() has been called.

outgoingMessage.writableEnded#

Added in: v12.9.0


<boolean>

Is true if outgoingMessage.end() has been called. This property does
not indicate whether the data has been flushed. For that purpose, use
message.writableFinished instead.

outgoingMessage.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system.

outgoingMessage.writableHighWaterMark#

Added in: v12.9.0


<number>

The highWaterMark of the underlying socket if assigned. Otherwise, the default
buffer level when writable.write() starts returning false (16384).

outgoingMessage.writableLength#

Added in: v12.9.0


<number>

The number of buffered bytes.

outgoingMessage.writableObjectMode#

Added in: v12.9.0


<boolean>

Always false.

outgoingMessage.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
The callback argument was added.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: utf8
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times.
The encoding argument is only relevant when chunk is a string. Defaults to
'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in the user
memory. The 'drain' event will be emitted when the buffer is free again.

http.METHODS#

Added in: v0.11.8


<string[]>

A list of the HTTP methods that are supported by the parser.
http.STATUS_CODES#

Added in: v0.1.22


<Object>

A collection of all the standard HTTP response status codes, and the
short description of each. For example, http.STATUS_CODES[404] === 'Not Found'.
http.createServer([options][, requestListener])#

History

VersionChanges
v20.1.0, v18.17.0
The highWaterMark option is supported now.
v18.0.0
The requestTimeout, headersTimeout, keepAliveTimeout, and connectionsCheckingInterval options are supported now.
v18.0.0
The noDelay option now defaults to true.
v17.7.0, v16.15.0
The noDelay, keepAlive and keepAliveInitialDelay options are supported now.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v9.6.0, v8.12.0
The options argument is supported now.
v0.1.13
Added in: v0.1.13





options <Object>

connectionsCheckingInterval: Sets the interval value in milliseconds to
check for request and headers timeout in incomplete requests.
Default: 30000.
headersTimeout: Sets the timeout value in milliseconds for receiving
the complete HTTP headers from the client.
See server.headersTimeout for more information.
Default: 60000.
highWaterMark <number> Optionally overrides all sockets'
readableHighWaterMark and writableHighWaterMark. This affects
highWaterMark property of both IncomingMessage and ServerResponse.
Default: See stream.getDefaultHighWaterMark().
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false.
IncomingMessage <http.IncomingMessage> Specifies the IncomingMessage
class to be used. Useful for extending the original IncomingMessage.
Default: IncomingMessage.
joinDuplicateHeaders <boolean> If set to true, this option allows
joining the field line values of multiple headers in a request with
a comma (, ) instead of discarding the duplicates.
For more information, refer to message.headers.
Default: false.
keepAlive <boolean> If set to true, it enables keep-alive functionality
on the socket immediately after a new incoming connection is received,
similarly on what is done in [socket.setKeepAlive([enable][, initialDelay])][socket.setKeepAlive(enable, initialDelay)].
Default: false.
keepAliveInitialDelay <number> If set to a positive number, it sets the
initial delay before the first keepalive probe is sent on an idle socket.
Default: 0.
keepAliveTimeout: The number of milliseconds of inactivity a server
needs to wait for additional incoming data, after it has finished writing
the last response, before a socket will be destroyed.
See server.keepAliveTimeout for more information.
Default: 5000.
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size for requests received by this server, i.e.
the maximum length of request headers in bytes.
Default: 16384 (16 KiB).
noDelay <boolean> If set to true, it disables the use of Nagle's
algorithm immediately after a new incoming connection is received.
Default: true.
requestTimeout: Sets the timeout value in milliseconds for receiving
the entire request from the client.
See server.requestTimeout for more information.
Default: 300000.
requireHostHeader <boolean> If set to true, it forces the server to
respond with a 400 (Bad Request) status code to any HTTP/1.1
request message that lacks a Host header
(as mandated by the specification).
Default: true.
ServerResponse <http.ServerResponse> Specifies the ServerResponse class
to be used. Useful for extending the original ServerResponse. Default:
ServerResponse.
uniqueHeaders <Array> A list of response headers that should be sent only
once. If the header's value is an array, the items will be joined
using ; .
rejectNonStandardBodyWrites <boolean> If set to true, an error is thrown
when writing to an HTTP response which does not have a body.
Default: false.



requestListener <Function>


Returns: <http.Server>


Returns a new instance of http.Server.
The requestListener is a function which is automatically
added to the 'request' event.

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy
http.get(options[, callback])#
http.get(url[, options][, callback])#

History

VersionChanges
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object> Accepts the same options as
http.request(), with the method set to GET by default.
callback <Function>
Returns: <http.ClientRequest>

Since most requests are GET requests without bodies, Node.js provides this
convenience method. The only difference between this method and
http.request() is that it sets the method to GET by default and calls req.end()
automatically. The callback must take care to consume the response
data for reasons stated in http.ClientRequest section.
The callback is invoked with a single argument that is an instance of
http.IncomingMessage.
JSON fetching example:
http.get('http://localhost:8000/', (res) => {
  const { statusCode } = res;
  const contentType = res.headers['content-type'];

  let error;
  // Any 2xx status code signals a successful response but
  // here we're only checking for 200.
  if (statusCode !== 200) {
    error = new Error('Request Failed.\n' +
                      `Status Code: ${statusCode}`);
  } else if (!/^application\/json/.test(contentType)) {
    error = new Error('Invalid content-type.\n' +
                      `Expected application/json but received ${contentType}`);
  }
  if (error) {
    console.error(error.message);
    // Consume response data to free up memory
    res.resume();
    return;
  }

  res.setEncoding('utf8');
  let rawData = '';
  res.on('data', (chunk) => { rawData += chunk; });
  res.on('end', () => {
    try {
      const parsedData = JSON.parse(rawData);
      console.log(parsedData);
    } catch (e) {
      console.error(e.message);
    }
  });
}).on('error', (e) => {
  console.error(`Got error: ${e.message}`);
});

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000); copy
http.globalAgent#

History

VersionChanges
v19.0.0
The agent now uses HTTP Keep-Alive and a 5 second timeout by default.
v0.5.9
Added in: v0.5.9




<http.Agent>

Global instance of Agent which is used as the default for all HTTP client
requests. Diverges from a default Agent configuration by having keepAlive
enabled and a timeout of 5 seconds.
http.maxHeaderSize#

Added in: v11.6.0, v10.15.0


<number>

Read-only property specifying the maximum allowed size of HTTP headers in bytes.
Defaults to 16 KiB. Configurable using the --max-http-header-size CLI
option.
This can be overridden for servers and client requests by passing the
maxHeaderSize option.
http.request(options[, callback])#
http.request(url[, options][, callback])#

History

VersionChanges
v16.7.0, v14.18.0
When using a URL object parsed username and password will now be properly URI decoded.
v15.3.0, v14.17.0
It is possible to abort a request with an AbortSignal.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object>

agent <http.Agent> | <boolean> Controls Agent behavior. Possible
values:

undefined (default): use http.globalAgent for this host and port.
Agent object: explicitly use the passed in Agent.
false: causes a new Agent with default values to be used.


auth <string> Basic authentication ('user:password') to compute an
Authorization header.
createConnection <Function> A function that produces a socket/stream to
use for the request when the agent option is not used. This can be used to
avoid creating a custom Agent class just to override the default
createConnection function. See agent.createConnection() for more
details. Any Duplex stream is a valid return value.
defaultPort <number> Default port for the protocol. Default:
agent.defaultPort if an Agent is used, else undefined.
family <number> IP address family to use when resolving host or
hostname. Valid values are 4 or 6. When unspecified, both IP v4 and
v6 will be used.
headers <Object> An object containing request headers.
hints <number> Optional dns.lookup() hints.
host <string> A domain name or IP address of the server to issue the
request to. Default: 'localhost'.
hostname <string> Alias for host. To support url.parse(),
hostname will be used if both host and hostname are specified.
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false
joinDuplicateHeaders <boolean> It joins the field line values of
multiple headers in a request with ,  instead of discarding
the duplicates. See message.headers for more information.
Default: false.
localAddress <string> Local interface to bind for network connections.
localPort <number> Local port to connect from.
lookup <Function> Custom lookup function. Default: dns.lookup().
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size (the maximum length of response headers in
bytes) for responses received from the server.
Default: 16384 (16 KiB).
method <string> A string specifying the HTTP request method. Default:
'GET'.
path <string> Request path. Should include query string if any.
E.G. '/index.html?page=12'. An exception is thrown when the request path
contains illegal characters. Currently, only spaces are rejected but that
may change in the future. Default: '/'.
port <number> Port of remote server. Default: defaultPort if set,
else 80.
protocol <string> Protocol to use. Default: 'http:'.
setDefaultHeaders <boolean>: Specifies whether or not to automatically add
default headers such as Connection, Content-Length, Transfer-Encoding,
and Host. If set to false then all necessary headers must be added
manually. Defaults to true.
setHost <boolean>: Specifies whether or not to automatically add the
Host header. If provided, this overrides setDefaultHeaders. Defaults to
true.
signal <AbortSignal>: An AbortSignal that may be used to abort an ongoing
request.
socketPath <string> Unix domain socket. Cannot be used if one of host
or port is specified, as those specify a TCP Socket.
timeout <number>: A number specifying the socket timeout in milliseconds.
This will set the timeout before the socket is connected.
uniqueHeaders <Array> A list of request headers that should be sent
only once. If the header's value is an array, the items will be joined
using ; .


callback <Function>
Returns: <http.ClientRequest>

options in socket.connect() are also supported.
Node.js maintains several connections per server to make HTTP requests.
This function allows one to transparently issue requests.
url can be a string or a URL object. If url is a
string, it is automatically parsed with new URL(). If it is a URL
object, it will be automatically converted to an ordinary options object.
If both url and options are specified, the objects are merged, with the
options properties taking precedence.
The optional callback parameter will be added as a one-time listener for
the 'response' event.
http.request() returns an instance of the http.ClientRequest
class. The ClientRequest instance is a writable stream. If one needs to
upload a file with a POST request, then write to the ClientRequest object.

import http from 'node:http';
import { Buffer } from 'node:buffer';

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();const http = require('node:http');

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();copy
In the example req.end() was called. With http.request() one
must always call req.end() to signify the end of the request -
even if there is no data being written to the request body.
If any error is encountered during the request (be that with DNS resolution,
TCP level errors, or actual HTTP parse errors) an 'error' event is emitted
on the returned request object. As with all 'error' events, if no listeners
are registered the error will be thrown.
There are a few special headers that should be noted.


Sending a 'Connection: keep-alive' will notify Node.js that the connection to
the server should be persisted until the next request.


Sending a 'Content-Length' header will disable the default chunked encoding.


Sending an 'Expect' header will immediately send the request headers.
Usually, when sending 'Expect: 100-continue', both a timeout and a listener
for the 'continue' event should be set. See RFC 2616 Section 8.2.3 for more
information.


Sending an Authorization header will override using the auth option
to compute basic authentication.


Example using a URL as options:
const options = new URL('http://abc:xyz@example.com');

const req = http.request(options, (res) => {
  // ...
}); copy
In a successful request, the following events will be emitted in the following
order:

'socket'
'response'

'data' any number of times, on the res object
('data' will not be emitted at all if the response body is empty, for
instance, in most redirects)
'end' on the res object


'close'

In the case of a connection error, the following events will be emitted:

'socket'
'error'
'close'

In the case of a premature connection close before the response is received,
the following events will be emitted in the following order:

'socket'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

In the case of a premature connection close after the response is received,
the following events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(connection closed here)
'aborted' on the res object
'close'
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'
'close' on the res object

If req.destroy() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.destroy() called here)
'aborted' on the res object
'close'
'error' on the res object with an error with message 'Error: aborted'
and code 'ECONNRESET', or the error with which req.destroy() was called
'close' on the res object

If req.abort() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.abort() called here)
'abort'
'close'

If req.abort() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.abort() called here)
'abort'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

If req.abort() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.abort() called here)
'abort'
'aborted' on the res object
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'.
'close'
'close' on the res object

Setting the timeout option or using the setTimeout() function will
not abort the request or do anything besides add a 'timeout' event.
Passing an AbortSignal and then calling abort() on the corresponding
AbortController will behave the same way as calling .destroy() on the
request. Specifically, the 'error' event will be emitted with an error with
the message 'AbortError: The operation was aborted', the code 'ABORT_ERR'
and the cause, if one was provided.
http.validateHeaderName(name[, label])#

History

VersionChanges
v19.5.0, v18.14.0
The label parameter is added.
v14.3.0
Added in: v14.3.0




name <string>
label <string> Label for error message. Default: 'Header name'.

Performs the low-level validations on the provided name that are done when
res.setHeader(name, value) is called.
Passing illegal value as name will result in a TypeError being thrown,
identified by code: 'ERR_INVALID_HTTP_TOKEN'.
It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Example:

import { validateHeaderName } from 'node:http';

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}const { validateHeaderName } = require('node:http');

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}copy
http.validateHeaderValue(name, value)#

Added in: v14.3.0


name <string>
value <any>

Performs the low-level validations on the provided value that are done when
res.setHeader(name, value) is called.
Passing illegal value as value will result in a TypeError being thrown.

Undefined value error is identified by code: 'ERR_HTTP_INVALID_HEADER_VALUE'.
Invalid value character error is identified by code: 'ERR_INVALID_CHAR'.

It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Examples:

import { validateHeaderValue } from 'node:http';

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}const { validateHeaderValue } = require('node:http');

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}copy
http.setMaxIdleHTTPParsers(max)#

Added in: v18.8.0, v16.18.0


max <number> Default: 1000.

Set the maximum number of idle HTTP parsers.
WebSocket#

Added in: v22.5.0

A browser-compatible implementation of WebSocket.\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
HTTP

Class: http.Agent

new Agent([options])
agent.createConnection(options[, callback])
agent.keepSocketAlive(socket)
agent.reuseSocket(socket, request)
agent.destroy()
agent.freeSockets
agent.getName([options])
agent.maxFreeSockets
agent.maxSockets
agent.maxTotalSockets
agent.requests
agent.sockets


Class: http.ClientRequest

Event: 'abort'
Event: 'close'
Event: 'connect'
Event: 'continue'
Event: 'finish'
Event: 'information'
Event: 'response'
Event: 'socket'
Event: 'timeout'
Event: 'upgrade'
request.abort()
request.aborted
request.connection
request.cork()
request.end([data[, encoding]][, callback])
request.destroy([error])

request.destroyed


request.finished
request.flushHeaders()
request.getHeader(name)
request.getHeaderNames()
request.getHeaders()
request.getRawHeaderNames()
request.hasHeader(name)
request.maxHeadersCount
request.path
request.method
request.host
request.protocol
request.removeHeader(name)
request.reusedSocket
request.setHeader(name, value)
request.setNoDelay([noDelay])
request.setSocketKeepAlive([enable][, initialDelay])
request.setTimeout(timeout[, callback])
request.socket
request.uncork()
request.writableEnded
request.writableFinished
request.write(chunk[, encoding][, callback])


Class: http.Server

Event: 'checkContinue'
Event: 'checkExpectation'
Event: 'clientError'
Event: 'close'
Event: 'connect'
Event: 'connection'
Event: 'dropRequest'
Event: 'request'
Event: 'upgrade'
server.close([callback])
server.closeAllConnections()
server.closeIdleConnections()
server.headersTimeout
server.listen()
server.listening
server.maxHeadersCount
server.requestTimeout
server.setTimeout([msecs][, callback])
server.maxRequestsPerSocket
server.timeout
server.keepAliveTimeout
server[Symbol.asyncDispose]()


Class: http.ServerResponse

Event: 'close'
Event: 'finish'
response.addTrailers(headers)
response.connection
response.cork()
response.end([data[, encoding]][, callback])
response.finished
response.flushHeaders()
response.getHeader(name)
response.getHeaderNames()
response.getHeaders()
response.hasHeader(name)
response.headersSent
response.removeHeader(name)
response.req
response.sendDate
response.setHeader(name, value)
response.setTimeout(msecs[, callback])
response.socket
response.statusCode
response.statusMessage
response.strictContentLength
response.uncork()
response.writableEnded
response.writableFinished
response.write(chunk[, encoding][, callback])
response.writeContinue()
response.writeEarlyHints(hints[, callback])
response.writeHead(statusCode[, statusMessage][, headers])
response.writeProcessing()


Class: http.IncomingMessage

Event: 'aborted'
Event: 'close'
message.aborted
message.complete
message.connection
message.destroy([error])
message.headers
message.headersDistinct
message.httpVersion
message.method
message.rawHeaders
message.rawTrailers
message.setTimeout(msecs[, callback])
message.socket
message.statusCode
message.statusMessage
message.trailers
message.trailersDistinct
message.url


Class: http.OutgoingMessage

Event: 'drain'
Event: 'finish'
Event: 'prefinish'
outgoingMessage.addTrailers(headers)
outgoingMessage.appendHeader(name, value)
outgoingMessage.connection
outgoingMessage.cork()
outgoingMessage.destroy([error])
outgoingMessage.end(chunk[, encoding][, callback])
outgoingMessage.flushHeaders()
outgoingMessage.getHeader(name)
outgoingMessage.getHeaderNames()
outgoingMessage.getHeaders()
outgoingMessage.hasHeader(name)
outgoingMessage.headersSent
outgoingMessage.pipe()
outgoingMessage.removeHeader(name)
outgoingMessage.setHeader(name, value)
outgoingMessage.setHeaders(headers)
outgoingMessage.setTimeout(msesc[, callback])
outgoingMessage.socket
outgoingMessage.uncork()
outgoingMessage.writableCorked
outgoingMessage.writableEnded
outgoingMessage.writableFinished
outgoingMessage.writableHighWaterMark
outgoingMessage.writableLength
outgoingMessage.writableObjectMode
outgoingMessage.write(chunk[, encoding][, callback])


http.METHODS
http.STATUS_CODES
http.createServer([options][, requestListener])
http.get(options[, callback])
http.get(url[, options][, callback])
http.globalAgent
http.maxHeaderSize
http.request(options[, callback])
http.request(url[, options][, callback])
http.validateHeaderName(name[, label])
http.validateHeaderValue(name, value)
http.setMaxIdleHTTPParsers(max)
WebSocket




      
        HTTP#

Stability: 2 - Stable
Source Code: lib/http.js
This module, containing both a client and server, can be imported via
require('node:http') (CommonJS) or import * as http from 'node:http' (ES module).
The HTTP interfaces in Node.js are designed to support many features
of the protocol which have been traditionally difficult to use.
In particular, large, possibly chunk-encoded, messages. The interface is
careful to never buffer entire requests or responses, so the
user is able to stream data.
HTTP message headers are represented by an object like this:
{ "content-length": "123",
  "content-type": "text/plain",
  "connection": "keep-alive",
  "host": "example.com",
  "accept": "*/*" } copy
Keys are lowercased. Values are not modified.
In order to support the full spectrum of possible HTTP applications, the Node.js
HTTP API is very low-level. It deals with stream handling and message
parsing only. It parses a message into headers and body but it does not
parse the actual headers or the body.
See message.headers for details on how duplicate headers are handled.
The raw headers as they were received are retained in the rawHeaders
property, which is an array of [key, value, key2, value2, ...]. For
example, the previous message header object might have a rawHeaders
list like the following:

[ 'ConTent-Length', '123456',
  'content-LENGTH', '123',
  'content-type', 'text/plain',
  'CONNECTION', 'keep-alive',
  'Host', 'example.com',
  'accepT', '*/*' ] copy
Class: http.Agent#

Added in: v0.3.4

An Agent is responsible for managing connection persistence
and reuse for HTTP clients. It maintains a queue of pending requests
for a given host and port, reusing a single socket connection for each
until the queue is empty, at which time the socket is either destroyed
or put into a pool where it is kept to be used again for requests to the
same host and port. Whether it is destroyed or pooled depends on the
keepAlive option.
Pooled connections have TCP Keep-Alive enabled for them, but servers may
still close idle connections, in which case they will be removed from the
pool and a new connection will be made when a new HTTP request is made for
that host and port. Servers may also refuse to allow multiple requests
over the same connection, in which case the connection will have to be
remade for every request and cannot be pooled. The Agent will still make
the requests to that server, but each one will occur over a new connection.
When a connection is closed by the client or the server, it is removed
from the pool. Any unused sockets in the pool will be unrefed so as not
to keep the Node.js process running when there are no outstanding requests.
(see socket.unref()).
It is good practice, to destroy() an Agent instance when it is no
longer in use, because unused sockets consume OS resources.
Sockets are removed from an agent when the socket emits either
a 'close' event or an 'agentRemove' event. When intending to keep one
HTTP request open for a long time without keeping it in the agent, something
like the following may be done:
http.get(options, (res) => {
  // Do stuff
}).on('socket', (socket) => {
  socket.emit('agentRemove');
}); copy
An agent may also be used for an individual request. By providing
{agent: false} as an option to the http.get() or http.request()
functions, a one-time use Agent with default options will be used
for the client connection.
agent:false:
http.get({
  hostname: 'localhost',
  port: 80,
  path: '/',
  agent: false,  // Create a new agent just for this one request
}, (res) => {
  // Do stuff with response
}); copy

new Agent([options])#

History

VersionChanges
v15.6.0, v14.17.0
Change the default scheduling from 'fifo' to 'lifo'.
v14.5.0, v12.20.0
Add scheduling option to specify the free socket scheduling strategy.
v14.5.0, v12.19.0
Add maxTotalSockets option to agent constructor.
v0.3.4
Added in: v0.3.4




options <Object> Set of configurable options to set on the agent.
Can have the following fields:

keepAlive <boolean> Keep sockets around even when there are no
outstanding requests, so they can be used for future requests without
having to reestablish a TCP connection. Not to be confused with the
keep-alive value of the Connection header. The Connection: keep-alive
header is always sent when using an agent except when the Connection
header is explicitly specified or when the keepAlive and maxSockets
options are respectively set to false and Infinity, in which case
Connection: close will be used. Default: false.
keepAliveMsecs <number> When using the keepAlive option, specifies
the initial delay
for TCP Keep-Alive packets. Ignored when the
keepAlive option is false or undefined. Default: 1000.
maxSockets <number> Maximum number of sockets to allow per host.
If the same host opens multiple concurrent connections, each request
will use new socket until the maxSockets value is reached.
If the host attempts to open more connections than maxSockets,
the additional requests will enter into a pending request queue, and
will enter active connection state when an existing connection terminates.
This makes sure there are at most maxSockets active connections at
any point in time, from a given host.
Default: Infinity.
maxTotalSockets <number> Maximum number of sockets allowed for
all hosts in total. Each request will use a new socket
until the maximum is reached.
Default: Infinity.
maxFreeSockets <number> Maximum number of sockets per host to leave open
in a free state. Only relevant if keepAlive is set to true.
Default: 256.
scheduling <string> Scheduling strategy to apply when picking
the next free socket to use. It can be 'fifo' or 'lifo'.
The main difference between the two scheduling strategies is that 'lifo'
selects the most recently used socket, while 'fifo' selects
the least recently used socket.
In case of a low rate of request per second, the 'lifo' scheduling
will lower the risk of picking a socket that might have been closed
by the server due to inactivity.
In case of a high rate of request per second,
the 'fifo' scheduling will maximize the number of open sockets,
while the 'lifo' scheduling will keep it as low as possible.
Default: 'lifo'.
timeout <number> Socket timeout in milliseconds.
This will set the timeout when the socket is created.



options in socket.connect() are also supported.
To configure any of them, a custom http.Agent instance must be created.

import { Agent, request } from 'node:http';
const keepAliveAgent = new Agent({ keepAlive: true });
options.agent = keepAliveAgent;
request(options, onResponseCallback);const http = require('node:http');
const keepAliveAgent = new http.Agent({ keepAlive: true });
options.agent = keepAliveAgent;
http.request(options, onResponseCallback);copy

agent.createConnection(options[, callback])#

Added in: v0.11.4


options <Object> Options containing connection details. Check
net.createConnection() for the format of the options
callback <Function> Callback function that receives the created socket
Returns: <stream.Duplex>

Produces a socket/stream to be used for HTTP requests.
By default, this function is the same as net.createConnection(). However,
custom agents may override this method in case greater flexibility is desired.
A socket/stream can be supplied in one of two ways: by returning the
socket/stream from this function, or by passing the socket/stream to callback.
This method is guaranteed to return an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
callback has a signature of (err, stream).

agent.keepSocketAlive(socket)#

Added in: v8.1.0


socket <stream.Duplex>

Called when socket is detached from a request and could be persisted by the
Agent. Default behavior is to:
socket.setKeepAlive(true, this.keepAliveMsecs);
socket.unref();
return true; copy
This method can be overridden by a particular Agent subclass. If this
method returns a falsy value, the socket will be destroyed instead of persisting
it for use with the next request.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.reuseSocket(socket, request)#

Added in: v8.1.0


socket <stream.Duplex>
request <http.ClientRequest>

Called when socket is attached to request after being persisted because of
the keep-alive options. Default behavior is to:
socket.ref(); copy
This method can be overridden by a particular Agent subclass.
The socket argument can be an instance of <net.Socket>, a subclass of
<stream.Duplex>.

agent.destroy()#

Added in: v0.11.4

Destroy any sockets that are currently in use by the agent.
It is usually not necessary to do this. However, if using an
agent with keepAlive enabled, then it is best to explicitly shut down
the agent when it is no longer needed. Otherwise,
sockets might stay open for quite a long time before the server
terminates them.

agent.freeSockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.11.4
Added in: v0.11.4




<Object>

An object which contains arrays of sockets currently awaiting use by
the agent when keepAlive is enabled. Do not modify.
Sockets in the freeSockets list will be automatically destroyed and
removed from the array on 'timeout'.

agent.getName([options])#

History

VersionChanges
v17.7.0, v16.15.0
The options parameter is now optional.
v0.11.4
Added in: v0.11.4




options <Object> A set of options providing information for name generation

host <string> A domain name or IP address of the server to issue the
request to
port <number> Port of remote server
localAddress <string> Local interface to bind for network connections
when issuing the request
family <integer> Must be 4 or 6 if this doesn't equal undefined.


Returns: <string>

Get a unique name for a set of request options, to determine whether a
connection can be reused. For an HTTP agent, this returns
host:port:localAddress or host:port:localAddress:family. For an HTTPS agent,
the name includes the CA, cert, ciphers, and other HTTPS/TLS-specific options
that determine socket reusability.

agent.maxFreeSockets#

Added in: v0.11.7


<number>

By default set to 256. For agents with keepAlive enabled, this
sets the maximum number of sockets that will be left open in the free
state.

agent.maxSockets#

Added in: v0.3.6


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open per origin. Origin is the returned value of agent.getName().

agent.maxTotalSockets#

Added in: v14.5.0, v12.19.0


<number>

By default set to Infinity. Determines how many concurrent sockets the agent
can have open. Unlike maxSockets, this parameter applies across all origins.

agent.requests#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.5.9
Added in: v0.5.9




<Object>

An object which contains queues of requests that have not yet been assigned to
sockets. Do not modify.

agent.sockets#

History

VersionChanges
v16.0.0
The property now has a null prototype.
v0.3.6
Added in: v0.3.6




<Object>

An object which contains arrays of sockets currently in use by the
agent. Do not modify.

Class: http.ClientRequest#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally and returned from http.request(). It
represents an in-progress request whose header has already been queued. The
header is still mutable using the setHeader(name, value),
getHeader(name), removeHeader(name) API. The actual header will
be sent along with the first data chunk or when calling request.end().
To get the response, add a listener for 'response' to the request object.
'response' will be emitted from the request object when the response
headers have been received. The 'response' event is executed with one
argument which is an instance of http.IncomingMessage.
During the 'response' event, one can add listeners to the
response object; particularly to listen for the 'data' event.
If no 'response' handler is added, then the response will be
entirely discarded. However, if a 'response' event handler is added,
then the data from the response object must be consumed, either by
calling response.read() whenever there is a 'readable' event, or
by adding a 'data' handler, or by calling the .resume() method.
Until the data is consumed, the 'end' event will not fire. Also, until
the data is read it will consume memory that can eventually lead to a
'process out of memory' error.
For backward compatibility, res will only emit 'error' if there is an
'error' listener registered.
Set Content-Length header to limit the response body size.
If response.strictContentLength is set to true, mismatching the
Content-Length header value will result in an Error being thrown,
identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.
Content-Length value should be in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes.

Event: 'abort'#

Added in: v1.4.1Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for the 'close' event instead.
Emitted when the request has been aborted by the client. This event is only
emitted on the first call to abort().

Event: 'close'#

Added in: v0.5.4

Indicates that the request is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'connect'#

Added in: v0.7.0


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with a CONNECT method. If
this event is not being listened for, clients receiving a CONNECT method will
have their connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client and server pair demonstrating how to listen for the 'connect' event:

import { createServer, request } from 'node:http';
import { connect } from 'node:net';
import { URL } from 'node:url';

// Create an HTTP tunneling proxy
const proxy = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});const http = require('node:http');
const net = require('node:net');
const { URL } = require('node:url');

// Create an HTTP tunneling proxy
const proxy = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
proxy.on('connect', (req, clientSocket, head) => {
  // Connect to an origin server
  const { port, hostname } = new URL(`http://${req.url}`);
  const serverSocket = net.connect(port || 80, hostname, () => {
    clientSocket.write('HTTP/1.1 200 Connection Established\r\n' +
                    'Proxy-agent: Node.js-Proxy\r\n' +
                    '\r\n');
    serverSocket.write(head);
    serverSocket.pipe(clientSocket);
    clientSocket.pipe(serverSocket);
  });
});

// Now that proxy is running
proxy.listen(1337, '127.0.0.1', () => {

  // Make a request to a tunneling proxy
  const options = {
    port: 1337,
    host: '127.0.0.1',
    method: 'CONNECT',
    path: 'www.google.com:80',
  };

  const req = http.request(options);
  req.end();

  req.on('connect', (res, socket, head) => {
    console.log('got connected!');

    // Make a request over an HTTP tunnel
    socket.write('GET / HTTP/1.1\r\n' +
                 'Host: www.google.com:80\r\n' +
                 'Connection: close\r\n' +
                 '\r\n');
    socket.on('data', (chunk) => {
      console.log(chunk.toString());
    });
    socket.on('end', () => {
      proxy.close();
    });
  });
});copy

Event: 'continue'#

Added in: v0.3.2

Emitted when the server sends a '100 Continue' HTTP response, usually because
the request contained 'Expect: 100-continue'. This is an instruction that
the client should send the request body.

Event: 'finish'#

Added in: v0.3.6

Emitted when the request has been sent. More specifically, this event is emitted
when the last segment of the response headers and body have been handed off to
the operating system for transmission over the network. It does not imply that
the server has received anything yet.

Event: 'information'#

Added in: v10.0.0


info <Object>

httpVersion <string>
httpVersionMajor <integer>
httpVersionMinor <integer>
statusCode <integer>
statusMessage <string>
headers <Object>
rawHeaders <string[]>



Emitted when the server sends a 1xx intermediate response (excluding 101
Upgrade). The listeners of this event will receive an object containing the
HTTP version, status code, status message, key-value headers object,
and array with the raw header names followed by their respective values.

import { request } from 'node:http';

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});const http = require('node:http');

const options = {
  host: '127.0.0.1',
  port: 8080,
  path: '/length_request',
};

// Make a request
const req = http.request(options);
req.end();

req.on('information', (info) => {
  console.log(`Got information prior to main response: ${info.statusCode}`);
});copy
101 Upgrade statuses do not fire this event due to their break from the
traditional HTTP request/response chain, such as web sockets, in-place TLS
upgrades, or HTTP 2.0. To be notified of 101 Upgrade notices, listen for the
'upgrade' event instead.

Event: 'response'#

Added in: v0.1.0


response <http.IncomingMessage>

Emitted when a response is received to this request. This event is emitted only
once.

Event: 'socket'#

Added in: v0.5.3


socket <stream.Duplex>

This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'timeout'#

Added in: v0.7.8

Emitted when the underlying socket times out from inactivity. This only notifies
that the socket has been idle. The request must be destroyed manually.
See also: request.setTimeout().

Event: 'upgrade'#

Added in: v0.1.94


response <http.IncomingMessage>
socket <stream.Duplex>
head <Buffer>

Emitted each time a server responds to a request with an upgrade. If this
event is not being listened for and the response status code is 101 Switching
Protocols, clients receiving an upgrade header will have their connections
closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
A client server pair demonstrating how to listen for the 'upgrade' event.

import http from 'node:http';
import process from 'node:process';

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});const http = require('node:http');

// Create an HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('okay');
});
server.on('upgrade', (req, socket, head) => {
  socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\r\n' +
               'Upgrade: WebSocket\r\n' +
               'Connection: Upgrade\r\n' +
               '\r\n');

  socket.pipe(socket); // echo back
});

// Now that server is running
server.listen(1337, '127.0.0.1', () => {

  // make a request
  const options = {
    port: 1337,
    host: '127.0.0.1',
    headers: {
      'Connection': 'Upgrade',
      'Upgrade': 'websocket',
    },
  };

  const req = http.request(options);
  req.end();

  req.on('upgrade', (res, socket, upgradeHead) => {
    console.log('got upgraded!');
    socket.end();
    process.exit(0);
  });
});copy

request.abort()#

Added in: v0.3.8Deprecated since: v14.1.0, v13.14.0

Stability: 0 - Deprecated: Use request.destroy() instead.
Marks the request as aborting. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.

request.aborted#

History

VersionChanges
v17.0.0, v16.12.0
Deprecated since: v17.0.0, v16.12.0
v11.0.0
The aborted property is no longer a timestamp number.
v0.11.14
Added in: v0.11.14



Stability: 0 - Deprecated. Check request.destroyed instead.

<boolean>

The request.aborted property will be true if the request has
been aborted.

request.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use request.socket.

<stream.Duplex>

See request.socket.

request.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

request.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ClientRequest.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

Finishes sending the request. If any parts of the body are
unsent, it will flush them to the stream. If the request is
chunked, this will send the terminating '0\r\n\r\n'.
If data is specified, it is equivalent to calling
request.write(data, encoding) followed by request.end(callback).
If callback is specified, it will be called when the request stream
is finished.

request.destroy([error])#

History

VersionChanges
v14.5.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error> Optional, an error to emit with 'error' event.
Returns: <this>

Destroy the request. Optionally emit an 'error' event,
and emit a 'close' event. Calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.
See writable.destroy() for further details.

request.destroyed#

Added in: v14.1.0, v13.14.0


<boolean>

Is true after request.destroy() has been called.
See writable.destroyed for further details.

request.finished#

Added in: v0.0.1Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use request.writableEnded.

<boolean>

The request.finished property will be true if request.end()
has been called. request.end() will automatically be called if the
request was initiated via http.get().

request.flushHeaders()#

Added in: v1.6.0

Flushes the request headers.
For efficiency reasons, Node.js normally buffers the request headers until
request.end() is called or the first chunk of request data is written. It
then tries to pack the request headers and data into a single TCP packet.
That's usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. request.flushHeaders() bypasses
the optimization and kickstarts the request.

request.getHeader(name)#

Added in: v1.6.0


name <string>
Returns: <any>

Reads out a header on the request. The name is case-insensitive.
The type of the return value depends on the arguments provided to
request.setHeader().
request.setHeader('content-type', 'text/html');
request.setHeader('Content-Length', Buffer.byteLength(body));
request.setHeader('Cookie', ['type=ninja', 'language=javascript']);
const contentType = request.getHeader('Content-Type');
// 'contentType' is 'text/html'
const contentLength = request.getHeader('Content-Length');
// 'contentLength' is of type number
const cookie = request.getHeader('Cookie');
// 'cookie' is of type string[] copy

request.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getHeaderNames();
// headerNames === ['foo', 'cookie'] copy

request.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the request.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
request.setHeader('Foo', 'bar');
request.setHeader('Cookie', ['foo=bar', 'bar=baz']);

const headers = request.getHeaders();
// headers === { foo: 'bar', 'cookie': ['foo=bar', 'bar=baz'] } copy

request.getRawHeaderNames()#

Added in: v15.13.0, v14.17.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing raw
headers. Header names are returned with their exact casing being set.
request.setHeader('Foo', 'bar');
request.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = request.getRawHeaderNames();
// headerNames === ['Foo', 'Set-Cookie'] copy

request.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = request.hasHeader('content-type'); copy

request.maxHeadersCount#

<number> Default: 2000

Limits maximum response headers count. If set to 0, no limit will be applied.

request.path#

Added in: v0.4.0


<string> The request path.


request.method#

Added in: v0.1.97


<string> The request method.


request.host#

Added in: v14.5.0, v12.19.0


<string> The request host.


request.protocol#

Added in: v14.5.0, v12.19.0


<string> The request protocol.


request.removeHeader(name)#

Added in: v1.6.0


name <string>

Removes a header that's already defined into headers object.
request.removeHeader('Content-Type'); copy

request.reusedSocket#

Added in: v13.0.0, v12.16.0


<boolean> Whether the request is send through a reused socket.

When sending request through a keep-alive enabled agent, the underlying socket
might be reused. But if server closes connection at unfortunate time, client
may run into a 'ECONNRESET' error.

import http from 'node:http';

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutconst http = require('node:http');

// Server has a 5 seconds keep-alive timeout by default
http
  .createServer((req, res) => {
    res.write('hello\n');
    res.end();
  })
  .listen(3000);

setInterval(() => {
  // Adapting a keep-alive agent
  http.get('http://localhost:3000', { agent }, (res) => {
    res.on('data', (data) => {
      // Do nothing
    });
  });
}, 5000); // Sending request on 5s interval so it's easy to hit idle timeoutcopy
By marking a request whether it reused socket or not, we can do
automatic error retry base on it.

import http from 'node:http';
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();const http = require('node:http');
const agent = new http.Agent({ keepAlive: true });

function retriableRequest() {
  const req = http
    .get('http://localhost:3000', { agent }, (res) => {
      // ...
    })
    .on('error', (err) => {
      // Check if retry is needed
      if (req.reusedSocket && err.code === 'ECONNRESET') {
        retriableRequest();
      }
    });
}

retriableRequest();copy

request.setHeader(name, value)#

Added in: v1.6.0


name <string>
value <any>

Sets a single header value for headers object. If this header already exists in
the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, request.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission.
request.setHeader('Content-Type', 'application/json'); copy
or
request.setHeader('Cookie', ['type=ninja', 'language=javascript']); copy
When the value is a string an exception will be thrown if it contains
characters outside the latin1 encoding.
If you need to pass UTF-8 characters in the value please encode the value
using the RFC 8187 standard.
const filename = 'Rock 🎵.txt';
request.setHeader('Content-Disposition', `attachment; filename*=utf-8''${encodeURIComponent(filename)}`); copy

request.setNoDelay([noDelay])#

Added in: v0.5.9


noDelay <boolean>

Once a socket is assigned to this request and is connected
socket.setNoDelay() will be called.

request.setSocketKeepAlive([enable][, initialDelay])#

Added in: v0.5.9


enable <boolean>
initialDelay <number>

Once a socket is assigned to this request and is connected
socket.setKeepAlive() will be called.

request.setTimeout(timeout[, callback])#

History

VersionChanges
v9.0.0
Consistently set socket timeout only when the socket connects.
v0.5.9
Added in: v0.5.9




timeout <number> Milliseconds before a request times out.
callback <Function> Optional function to be called when a timeout occurs.
Same as binding to the 'timeout' event.
Returns: <http.ClientRequest>

Once a socket is assigned to this request and is connected
socket.setTimeout() will be called.

request.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket.

import http from 'node:http';
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});const http = require('node:http');
const options = {
  host: 'www.google.com',
};
const req = http.get(options);
req.end();
req.once('response', (res) => {
  const ip = req.socket.localAddress;
  const port = req.socket.localPort;
  console.log(`Your IP address is ${ip} and your source port is ${port}.`);
  // Consume response object
});copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

request.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

request.writableEnded#

Added in: v12.9.0


<boolean>

Is true after request.end() has been called. This property
does not indicate whether the data has been flushed, for this use
request.writableFinished instead.

request.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

request.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times. If no
Content-Length is set, data will automatically be encoded in HTTP Chunked
transfer encoding, so that server knows when the data ends. The
Transfer-Encoding: chunked header is added. Calling request.end()
is necessary to finish sending the request.
The encoding argument is optional and only applies when chunk is a string.
Defaults to 'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed, but only if the chunk is non-empty.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.
When write function is called with empty string or buffer, it does
nothing and waits for more input.

Class: http.Server#

Added in: v0.1.17


Extends: <net.Server>


Event: 'checkContinue'#

Added in: v0.3.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect: 100-continue is received.
If this event is not listened for, the server will automatically respond
with a 100 Continue as appropriate.
Handling this event involves calling response.writeContinue() if the
client should continue to send the request body, or generating an appropriate
HTTP response (e.g. 400 Bad Request) if the client should not continue to send
the request body.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'checkExpectation'#

Added in: v5.5.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time a request with an HTTP Expect header is received, where the
value is not 100-continue. If this event is not listened for, the server will
automatically respond with a 417 Expectation Failed as appropriate.
When this event is emitted and handled, the 'request' event will
not be emitted.

Event: 'clientError'#

History

VersionChanges
v12.0.0
The default behavior will return a 431 Request Header Fields Too Large if a HPE_HEADER_OVERFLOW error occurs.
v9.4.0
The rawPacket is the current buffer that just parsed. Adding this buffer to the error object of 'clientError' event is to make it possible that developers can log the broken packet.
v6.0.0
The default action of calling .destroy() on the socket will no longer take place if there are listeners attached for 'clientError'.
v0.1.94
Added in: v0.1.94




exception <Error>
socket <stream.Duplex>

If a client connection emits an 'error' event, it will be forwarded here.
Listener of this event is responsible for closing/destroying the underlying
socket. For example, one may wish to more gracefully close the socket with a
custom HTTP response instead of abruptly severing the connection. The socket
must be closed or destroyed before the listener ends.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
Default behavior is to try close the socket with a HTTP '400 Bad Request',
or a HTTP '431 Request Header Fields Too Large' in the case of a
HPE_HEADER_OVERFLOW error. If the socket is not writable or headers
of the current attached http.ServerResponse has been sent, it is
immediately destroyed.
socket is the net.Socket object that the error originated from.

import http from 'node:http';

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);const http = require('node:http');

const server = http.createServer((req, res) => {
  res.end();
});
server.on('clientError', (err, socket) => {
  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
});
server.listen(8000);copy
When the 'clientError' event occurs, there is no request or response
object, so any HTTP response sent, including response headers and payload,
must be written directly to the socket object. Care must be taken to
ensure the response is a properly formatted HTTP response message.
err is an instance of Error with two extra columns:

bytesParsed: the bytes count of request packet that Node.js may have parsed
correctly;
rawPacket: the raw packet of current request.

In some cases, the client has already received the response and/or the socket
has already been destroyed, like in case of ECONNRESET errors. Before
trying to send data to the socket, it is better to check that it is still
writable.
server.on('clientError', (err, socket) => {
  if (err.code === 'ECONNRESET' || !socket.writable) {
    return;
  }

  socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');
}); copy

Event: 'close'#

Added in: v0.1.4

Emitted when the server closes.

Event: 'connect'#

Added in: v0.7.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the tunneling stream (may be empty)

Emitted each time a client requests an HTTP CONNECT method. If this event is
not listened for, then clients requesting a CONNECT method will have their
connections closed.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.

Event: 'connection'#

Added in: v0.1.0


socket <stream.Duplex>

This event is emitted when a new TCP stream is established. socket is
typically an object of type net.Socket. Usually users will not want to
access this event. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. The socket can
also be accessed at request.socket.
This event can also be explicitly emitted by users to inject connections
into the HTTP server. In that case, any Duplex stream can be passed.
If socket.setTimeout() is called here, the timeout will be replaced with
server.keepAliveTimeout when the socket has served a request (if
server.keepAliveTimeout is non-zero).
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

Event: 'dropRequest'#

Added in: v18.7.0, v16.17.0


request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client

When the number of requests on a socket reaches the threshold of
server.maxRequestsPerSocket, the server will drop new requests
and emit 'dropRequest' event instead, then send 503 to client.

Event: 'request'#

Added in: v0.1.0


request <http.IncomingMessage>
response <http.ServerResponse>

Emitted each time there is a request. There may be multiple requests
per connection (in the case of HTTP Keep-Alive connections).

Event: 'upgrade'#

History

VersionChanges
v10.0.0
Not listening to this event no longer causes the socket to be destroyed if a client sends an Upgrade header.
v0.1.94
Added in: v0.1.94




request <http.IncomingMessage> Arguments for the HTTP request, as it is in
the 'request' event
socket <stream.Duplex> Network socket between the server and client
head <Buffer> The first packet of the upgraded stream (may be empty)

Emitted each time a client requests an HTTP upgrade. Listening to this event
is optional and clients cannot insist on a protocol change.
After this event is emitted, the request's socket will not have a 'data'
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.
This event is guaranteed to be passed an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specifies a socket
type other than <net.Socket>.

server.close([callback])#

History

VersionChanges
v19.0.0
The method closes idle connections before returning.
v0.1.90
Added in: v0.1.90




callback <Function>

Stops the server from accepting new connections and closes all connections
connected to this server which are not sending a request or waiting for
a response.
See net.Server.close().
const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
}, 10000); copy

server.closeAllConnections()#

Added in: v18.2.0

Closes all established HTTP(S) connections connected to this server, including
active connections connected to this server which are sending a request or
waiting for a response. This does not destroy sockets upgraded to a different
protocol, such as WebSocket or HTTP/2.

This is a forceful way of closing all connections and should be used with
caution. Whenever using this in conjunction with server.close, calling this
after server.close is recommended as to avoid race conditions where new
connections are created between a call to this and a call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes all connections, ensuring the server closes successfully
  server.closeAllConnections();
}, 10000); copy

server.closeIdleConnections()#

Added in: v18.2.0

Closes all connections connected to this server which are not sending a request
or waiting for a response.

Starting with Node.js 19.0.0, there's no need for calling this method in
conjunction with server.close to reap keep-alive connections. Using it
won't cause any harm though, and it can be useful to ensure backwards
compatibility for libraries and applications that need to support versions
older than 19.0.0. Whenever using this in conjunction with server.close,
calling this after server.close is recommended as to avoid race
conditions where new connections are created between a call to this and a
call to server.close.

const http = require('node:http');

const server = http.createServer({ keepAliveTimeout: 60000 }, (req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);
// Close the server after 10 seconds
setTimeout(() => {
  server.close(() => {
    console.log('server on port 8000 closed successfully');
  });
  // Closes idle connections, such as keep-alive connections. Server will close
  // once remaining active connections are terminated
  server.closeIdleConnections();
}, 10000); copy

server.headersTimeout#

History

VersionChanges
v19.4.0, v18.14.0
The default is now set to the minimum between 60000 (60 seconds) or requestTimeout.
v11.3.0, v10.14.0
Added in: v11.3.0, v10.14.0




<number> Default: The minimum between server.requestTimeout or 60000.

Limit the amount of time the parser will wait to receive the complete HTTP
headers.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.listen()#
Starts the HTTP server listening for connections.
This method is identical to server.listen() from net.Server.

server.listening#

Added in: v5.7.0


<boolean> Indicates whether or not the server is listening for connections.


server.maxHeadersCount#

Added in: v0.7.0


<number> Default: 2000

Limits maximum incoming headers count. If set to 0, no limit will be applied.

server.requestTimeout#

History

VersionChanges
v18.0.0
The default request timeout changed from no timeout to 300s (5 minutes).
v14.11.0
Added in: v14.11.0




<number> Default: 300000

Sets the timeout value in milliseconds for receiving the entire request from
the client.
If the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
It must be set to a non-zero value (e.g. 120 seconds) to protect against
potential Denial-of-Service attacks in case the server is deployed without a
reverse proxy in front.

server.setTimeout([msecs][, callback])#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




msecs <number> Default: 0 (no timeout)
callback <Function>
Returns: <http.Server>

Sets the timeout value for sockets, and emits a 'timeout' event on
the Server object, passing the socket as an argument, if a timeout
occurs.
If there is a 'timeout' event listener on the Server object, then it
will be called with the timed-out socket as an argument.
By default, the Server does not timeout sockets. However, if a callback
is assigned to the Server's 'timeout' event, timeouts must be handled
explicitly.

server.maxRequestsPerSocket#

Added in: v16.10.0


<number> Requests per socket. Default: 0 (no limit)

The maximum number of requests socket can handle
before closing keep alive connection.
A value of 0 will disable the limit.
When the limit is reached it will set the Connection header value to close,
but will not actually close the connection, subsequent requests sent
after the limit is reached will get 503 Service Unavailable as a response.

server.timeout#

History

VersionChanges
v13.0.0
The default timeout changed from 120s to 0 (no timeout).
v0.9.12
Added in: v0.9.12




<number> Timeout in milliseconds. Default: 0 (no timeout)

The number of milliseconds of inactivity before a socket is presumed
to have timed out.
A value of 0 will disable the timeout behavior on incoming connections.
The socket timeout logic is set up on connection, so changing this
value only affects new connections to the server, not any existing connections.

server.keepAliveTimeout#

Added in: v8.0.0


<number> Timeout in milliseconds. Default: 5000 (5 seconds).

The number of milliseconds of inactivity a server needs to wait for additional
incoming data, after it has finished writing the last response, before a socket
will be destroyed. If the server receives new data before the keep-alive
timeout has fired, it will reset the regular inactivity timeout, i.e.,
server.timeout.
A value of 0 will disable the keep-alive timeout behavior on incoming
connections.
A value of 0 makes the http server behave similarly to Node.js versions prior
to 8.0.0, which did not have a keep-alive timeout.
The socket timeout logic is set up on connection, so changing this value only
affects new connections to the server, not any existing connections.

server[Symbol.asyncDispose]()#

Added in: v20.4.0

Stability: 1 - Experimental
Calls server.close() and returns a promise that fulfills when the
server has closed.

Class: http.ServerResponse#

Added in: v0.1.17


Extends: <http.OutgoingMessage>

This object is created internally by an HTTP server, not by the user. It is
passed as the second parameter to the 'request' event.

Event: 'close'#

Added in: v0.6.7

Indicates that the response is completed, or its underlying connection was
terminated prematurely (before the response completion).

Event: 'finish'#

Added in: v0.3.6

Emitted when the response has been sent. More specifically, this event is
emitted when the last segment of the response headers and body have been
handed off to the operating system for transmission over the network. It
does not imply that the client has received anything yet.

response.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

This method adds HTTP trailing headers (a header but at the end of the
message) to the response.
Trailers will only be emitted if chunked encoding is used for the
response; if it is not (e.g. if the request was HTTP/1.0), they will
be silently discarded.
HTTP requires the Trailer header to be sent in order to
emit trailers, with a list of the header fields in its value. E.g.,
response.writeHead(200, { 'Content-Type': 'text/plain',
                          'Trailer': 'Content-MD5' });
response.write(fileData);
response.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
response.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

response.connection#

Added in: v0.3.0Deprecated since: v13.0.0

Stability: 0 - Deprecated. Use response.socket.

<stream.Duplex>

See response.socket.

response.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

response.end([data[, encoding]][, callback])#

History

VersionChanges
v15.0.0
The data parameter can now be a Uint8Array.
v10.0.0
This method now returns a reference to ServerResponse.
v0.1.90
Added in: v0.1.90




data <string> | <Buffer> | <Uint8Array>
encoding <string>
callback <Function>
Returns: <this>

This method signals to the server that all of the response headers and body
have been sent; that server should consider this message complete.
The method, response.end(), MUST be called on each response.
If data is specified, it is similar in effect to calling
response.write(data, encoding) followed by response.end(callback).
If callback is specified, it will be called when the response stream
is finished.

response.finished#

Added in: v0.0.2Deprecated since: v13.4.0, v12.16.0

Stability: 0 - Deprecated. Use response.writableEnded.

<boolean>

The response.finished property will be true if response.end()
has been called.

response.flushHeaders()#

Added in: v1.6.0

Flushes the response headers. See also: request.flushHeaders().

response.getHeader(name)#

Added in: v0.4.0


name <string>
Returns: <any>

Reads out a header that's already been queued but not sent to the client.
The name is case-insensitive. The type of the return value depends
on the arguments provided to response.setHeader().
response.setHeader('Content-Type', 'text/html');
response.setHeader('Content-Length', Buffer.byteLength(body));
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']);
const contentType = response.getHeader('content-type');
// contentType is 'text/html'
const contentLength = response.getHeader('Content-Length');
// contentLength is of type number
const setCookie = response.getHeader('set-cookie');
// setCookie is of type string[] copy

response.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All header names are lowercase.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headerNames = response.getHeaderNames();
// headerNames === ['foo', 'set-cookie'] copy

response.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. The keys of the returned object are the
header names and the values are the respective header values. All header names
are lowercase.
The object returned by the response.getHeaders() method does not
prototypically inherit from the JavaScript Object. This means that typical
Object methods such as obj.toString(), obj.hasOwnProperty(), and others
are not defined and will not work.
response.setHeader('Foo', 'bar');
response.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = response.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

response.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name matching is case-insensitive.
const hasContentType = response.hasHeader('content-type'); copy

response.headersSent#

Added in: v0.9.3


<boolean>

Boolean (read-only). True if headers were sent, false otherwise.

response.removeHeader(name)#

Added in: v0.4.0


name <string>

Removes a header that's queued for implicit sending.
response.removeHeader('Content-Encoding'); copy

response.req#

Added in: v15.7.0


<http.IncomingMessage>

A reference to the original HTTP request object.

response.sendDate#

Added in: v0.7.5


<boolean>

When true, the Date header will be automatically generated and sent in
the response if it is not already present in the headers. Defaults to true.
This should only be disabled for testing; HTTP requires the Date header
in responses.

response.setHeader(name, value)#

Added in: v0.4.0


name <string>
value <any>
Returns: <http.ServerResponse>

Returns the response object.
Sets a single header value for implicit headers. If this header already exists
in the to-be-sent headers, its value will be replaced. Use an array of strings
here to send multiple headers with the same name. Non-string values will be
stored without modification. Therefore, response.getHeader() may return
non-string values. However, the non-string values will be converted to strings
for network transmission. The same response object is returned to the caller,
to enable call chaining.
response.setHeader('Content-Type', 'text/html'); copy
or
response.setHeader('Set-Cookie', ['type=ninja', 'language=javascript']); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
If response.writeHead() method is called and this method has not been
called, it will directly write the supplied header values onto the network
channel without caching internally, and the response.getHeader() on the
header will not yield the expected result. If progressive population of headers
is desired with potential future retrieval and modification, use
response.setHeader() instead of response.writeHead().

response.setTimeout(msecs[, callback])#

Added in: v0.9.12


msecs <number>
callback <Function>
Returns: <http.ServerResponse>

Sets the Socket's timeout value to msecs. If a callback is
provided, then it is added as a listener on the 'timeout' event on
the response object.
If no 'timeout' listener is added to the request, the response, or
the server, then sockets are destroyed when they time out. If a handler is
assigned to the request, the response, or the server's 'timeout' events,
timed out sockets must be handled explicitly.

response.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually users will not want to access
this property. In particular, the socket will not emit 'readable' events
because of how the protocol parser attaches to the socket. After
response.end(), the property is nulled.

import http from 'node:http';
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);const http = require('node:http');
const server = http.createServer((req, res) => {
  const ip = res.socket.remoteAddress;
  const port = res.socket.remotePort;
  res.end(`Your IP address is ${ip} and your source port is ${port}.`);
}).listen(3000);copy
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket>.

response.statusCode#

Added in: v0.4.0


<number> Default: 200

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status code that will be sent to the client when
the headers get flushed.
response.statusCode = 404; copy
After response header was sent to the client, this property indicates the
status code which was sent out.

response.statusMessage#

Added in: v0.11.8


<string>

When using implicit headers (not calling response.writeHead() explicitly),
this property controls the status message that will be sent to the client when
the headers get flushed. If this is left as undefined then the standard
message for the status code will be used.
response.statusMessage = 'Not found'; copy
After response header was sent to the client, this property indicates the
status message which was sent out.

response.strictContentLength#

Added in: v18.10.0, v16.18.0


<boolean> Default: false

If set to true, Node.js will check whether the Content-Length
header value and the size of the body, in bytes, are equal.
Mismatching the Content-Length header value will result
in an Error being thrown, identified by code: 'ERR_HTTP_CONTENT_LENGTH_MISMATCH'.

response.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork().

response.writableEnded#

Added in: v12.9.0


<boolean>

Is true after response.end() has been called. This property
does not indicate whether the data has been flushed, for this use
response.writableFinished instead.

response.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system, immediately
before the 'finish' event is emitted.

response.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: 'utf8'
callback <Function>
Returns: <boolean>

If this method is called and response.writeHead() has not been called,
it will switch to implicit header mode and flush the implicit headers.
This sends a chunk of the response body. This method may
be called multiple times to provide successive parts of the body.
If rejectNonStandardBodyWrites is set to true in createServer
then writing to the body is not allowed when the request method or response
status do not support content. If an attempt is made to write to the body for a
HEAD request or as part of a 204 or 304response, a synchronous Error
with the code ERR_HTTP_BODY_NOT_ALLOWED is thrown.
chunk can be a string or a buffer. If chunk is a string,
the second parameter specifies how to encode it into a byte stream.
callback will be called when this chunk of data is flushed.
This is the raw HTTP body and has nothing to do with higher-level multi-part
body encodings that may be used.
The first time response.write() is called, it will send the buffered
header information and the first chunk of the body to the client. The second
time response.write() is called, Node.js assumes data will be streamed,
and sends the new data separately. That is, the response is buffered up to the
first chunk of the body.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in user memory.
'drain' will be emitted when the buffer is free again.

response.writeContinue()#

Added in: v0.3.0

Sends an HTTP/1.1 100 Continue message to the client, indicating that
the request body should be sent. See the 'checkContinue' event on
Server.

response.writeEarlyHints(hints[, callback])#

History

VersionChanges
v18.11.0
Allow passing hints as an object.
v18.11.0
Added in: v18.11.0




hints <Object>
callback <Function>

Sends an HTTP/1.1 103 Early Hints message to the client with a Link header,
indicating that the user agent can preload/preconnect the linked resources.
The hints is an object containing the values of headers to be sent with
early hints message. The optional callback argument will be called when
the response message has been written.
Example
const earlyHintsLink = '</styles.css>; rel=preload; as=style';
response.writeEarlyHints({
  'link': earlyHintsLink,
});

const earlyHintsLinks = [
  '</styles.css>; rel=preload; as=style',
  '</scripts.js>; rel=preload; as=script',
];
response.writeEarlyHints({
  'link': earlyHintsLinks,
  'x-trace-id': 'id for diagnostics',
});

const earlyHintsCallback = () => console.log('early hints message sent');
response.writeEarlyHints({
  'link': earlyHintsLinks,
}, earlyHintsCallback); copy

response.writeHead(statusCode[, statusMessage][, headers])#

History

VersionChanges
v14.14.0
Allow passing headers as an array.
v11.10.0, v10.17.0
Return this from writeHead() to allow chaining with end().
v5.11.0, v4.4.5
A RangeError is thrown if statusCode is not a number in the range [100, 999].
v0.1.30
Added in: v0.1.30




statusCode <number>
statusMessage <string>
headers <Object> | <Array>
Returns: <http.ServerResponse>

Sends a response header to the request. The status code is a 3-digit HTTP
status code, like 404. The last argument, headers, are the response headers.
Optionally one can give a human-readable statusMessage as the second
argument.
headers may be an Array where the keys and values are in the same list.
It is not a list of tuples. So, the even-numbered offsets are key values,
and the odd-numbered offsets are the associated values. The array is in the same
format as request.rawHeaders.
Returns a reference to the ServerResponse, so that calls can be chained.
const body = 'hello world';
response
  .writeHead(200, {
    'Content-Length': Buffer.byteLength(body),
    'Content-Type': 'text/plain',
  })
  .end(body); copy
This method must only be called once on a message and it must
be called before response.end() is called.
If response.write() or response.end() are called before calling
this, the implicit/mutable headers will be calculated and call this function.
When headers have been set with response.setHeader(), they will be merged
with any headers passed to response.writeHead(), with the headers passed
to response.writeHead() given precedence.
If this method is called and response.setHeader() has not been called,
it will directly write the supplied header values onto the network channel
without caching internally, and the response.getHeader() on the header
will not yield the expected result. If progressive population of headers is
desired with potential future retrieval and modification, use
response.setHeader() instead.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  res.setHeader('Content-Type', 'text/html');
  res.setHeader('X-Foo', 'bar');
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy
Content-Length is read in bytes, not characters. Use
Buffer.byteLength() to determine the length of the body in bytes. Node.js
will check whether Content-Length and the length of the body which has
been transmitted are equal or not.
Attempting to set a header field name or value that contains invalid characters
will result in a [Error][] being thrown.

response.writeProcessing()#

Added in: v10.0.0

Sends a HTTP/1.1 102 Processing message to the client, indicating that
the request body should be sent.

Class: http.IncomingMessage#

History

VersionChanges
v15.5.0
The destroyed value returns true after the incoming data is consumed.
v13.1.0, v12.16.0
The readableHighWaterMark value mirrors that of the socket.
v0.1.17
Added in: v0.1.17




Extends: <stream.Readable>

An IncomingMessage object is created by http.Server or
http.ClientRequest and passed as the first argument to the 'request'
and 'response' event respectively. It may be used to access response
status, headers, and data.
Different from its socket value which is a subclass of <stream.Duplex>, the
IncomingMessage itself extends <stream.Readable> and is created separately to
parse and emit the incoming HTTP headers and payload, as the underlying socket
may be reused multiple times in case of keep-alive.

Event: 'aborted'#

Added in: v0.3.8Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Listen for 'close' event instead.
Emitted when the request has been aborted.

Event: 'close'#

History

VersionChanges
v16.0.0
The close event is now emitted when the request has been completed and not when the underlying socket is closed.
v0.4.2
Added in: v0.4.2



Emitted when the request has been completed.

message.aborted#

Added in: v10.1.0Deprecated since: v17.0.0, v16.12.0

Stability: 0 - Deprecated. Check message.destroyed from <stream.Readable>.

<boolean>

The message.aborted property will be true if the request has
been aborted.

message.complete#

Added in: v0.3.0


<boolean>

The message.complete property will be true if a complete HTTP message has
been received and successfully parsed.
This property is particularly useful as a means of determining if a client or
server fully transmitted a message before a connection was terminated:
const req = http.request({
  host: '127.0.0.1',
  port: 8080,
  method: 'POST',
}, (res) => {
  res.resume();
  res.on('end', () => {
    if (!res.complete)
      console.error(
        'The connection was terminated while the message was still being sent');
  });
}); copy

message.connection#

Added in: v0.1.90Deprecated since: v16.0.0

Stability: 0 - Deprecated. Use message.socket.
Alias for message.socket.

message.destroy([error])#

History

VersionChanges
v14.5.0, v12.19.0
The function returns this for consistency with other Readable streams.
v0.3.0
Added in: v0.3.0




error <Error>
Returns: <this>

Calls destroy() on the socket that received the IncomingMessage. If error
is provided, an 'error' event is emitted on the socket and error is passed
as an argument to any listeners on the event.

message.headers#

History

VersionChanges
v19.5.0, v18.14.0
The joinDuplicateHeaders option in the http.request() and http.createServer() functions ensures that duplicate headers are not discarded, but rather combined using a comma separator, in accordance with RFC 9110 Section 5.3.
v15.1.0
message.headers is now lazily computed using an accessor property on the prototype and is no longer enumerable.
v0.1.5
Added in: v0.1.5




<Object>

The request/response headers object.
Key-value pairs of header names and values. Header names are lower-cased.
// Prints something like:
//
// { 'user-agent': 'curl/7.22.0',
//   host: '127.0.0.1:8000',
//   accept: '*/*' }
console.log(request.headers); copy
Duplicates in raw headers are handled in the following ways, depending on the
header name:

Duplicates of age, authorization, content-length, content-type,
etag, expires, from, host, if-modified-since, if-unmodified-since,
last-modified, location, max-forwards, proxy-authorization, referer,
retry-after, server, or user-agent are discarded.
To allow duplicate values of the headers listed above to be joined,
use the option joinDuplicateHeaders in http.request()
and http.createServer(). See RFC 9110 Section 5.3 for more
information.
set-cookie is always an array. Duplicates are added to the array.
For duplicate cookie headers, the values are joined together with ; .
For all other headers, the values are joined together with , .


message.headersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.headers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
// Prints something like:
//
// { 'user-agent': ['curl/7.22.0'],
//   host: ['127.0.0.1:8000'],
//   accept: ['*/*'] }
console.log(request.headersDistinct); copy

message.httpVersion#

Added in: v0.1.1


<string>

In case of server request, the HTTP version sent by the client. In the case of
client response, the HTTP version of the connected-to server.
Probably either '1.1' or '1.0'.
Also message.httpVersionMajor is the first integer and
message.httpVersionMinor is the second.

message.method#

Added in: v0.1.1


<string>

Only valid for request obtained from http.Server.
The request method as a string. Read only. Examples: 'GET', 'DELETE'.

message.rawHeaders#

Added in: v0.11.6


<string[]>

The raw request/response headers list exactly as they were received.
The keys and values are in the same list. It is not a
list of tuples. So, the even-numbered offsets are key values, and the
odd-numbered offsets are the associated values.
Header names are not lowercased, and duplicates are not merged.
// Prints something like:
//
// [ 'user-agent',
//   'this is invalid because there can be only one',
//   'User-Agent',
//   'curl/7.22.0',
//   'Host',
//   '127.0.0.1:8000',
//   'ACCEPT',
//   '*/*' ]
console.log(request.rawHeaders); copy

message.rawTrailers#

Added in: v0.11.6


<string[]>

The raw request/response trailer keys and values exactly as they were
received. Only populated at the 'end' event.

message.setTimeout(msecs[, callback])#

Added in: v0.5.9


msecs <number>
callback <Function>
Returns: <http.IncomingMessage>

Calls message.socket.setTimeout(msecs, callback).

message.socket#

Added in: v0.3.0


<stream.Duplex>

The net.Socket object associated with the connection.
With HTTPS support, use request.socket.getPeerCertificate() to obtain the
client's authentication details.
This property is guaranteed to be an instance of the <net.Socket> class,
a subclass of <stream.Duplex>, unless the user specified a socket
type other than <net.Socket> or internally nulled.

message.statusCode#

Added in: v0.1.1


<number>

Only valid for response obtained from http.ClientRequest.
The 3-digit HTTP response status code. E.G. 404.

message.statusMessage#

Added in: v0.11.10


<string>

Only valid for response obtained from http.ClientRequest.
The HTTP response status message (reason phrase). E.G. OK or Internal Server Error.

message.trailers#

Added in: v0.3.0


<Object>

The request/response trailers object. Only populated at the 'end' event.

message.trailersDistinct#

Added in: v18.3.0, v16.17.0


<Object>

Similar to message.trailers, but there is no join logic and the values are
always arrays of strings, even for headers received just once.
Only populated at the 'end' event.

message.url#

Added in: v0.1.90


<string>

Only valid for request obtained from http.Server.
Request URL string. This contains only the URL that is present in the actual
HTTP request. Take the following request:
GET /status?name=ryan HTTP/1.1
Accept: text/plain copy
To parse the URL into its parts:
new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`); copy
When request.url is '/status?name=ryan' and process.env.HOST is undefined:
$ node
> new URL(`http://${process.env.HOST ?? 'localhost'}${request.url}`);
URL {
  href: 'http://localhost/status?name=ryan',
  origin: 'http://localhost',
  protocol: 'http:',
  username: '',
  password: '',
  host: 'localhost',
  hostname: 'localhost',
  port: '',
  pathname: '/status',
  search: '?name=ryan',
  searchParams: URLSearchParams { 'name' => 'ryan' },
  hash: ''
} copy
Ensure that you set process.env.HOST to the server's host name, or consider
replacing this part entirely. If using req.headers.host, ensure proper
validation is used, as clients may specify a custom Host header.

Class: http.OutgoingMessage#

Added in: v0.1.17


Extends: <Stream>

This class serves as the parent class of http.ClientRequest
and http.ServerResponse. It is an abstract outgoing message from
the perspective of the participants of an HTTP transaction.

Event: 'drain'#

Added in: v0.3.6

Emitted when the buffer of the message is free again.

Event: 'finish'#

Added in: v0.1.17

Emitted when the transmission is finished successfully.

Event: 'prefinish'#

Added in: v0.11.6

Emitted after outgoingMessage.end() is called.
When the event is emitted, all data has been processed but not necessarily
completely flushed.

outgoingMessage.addTrailers(headers)#

Added in: v0.3.0


headers <Object>

Adds HTTP trailers (headers but at the end of the message) to the message.
Trailers will only be emitted if the message is chunked encoded. If not,
the trailers will be silently discarded.
HTTP requires the Trailer header to be sent to emit trailers,
with a list of header field names in its value, e.g.
message.writeHead(200, { 'Content-Type': 'text/plain',
                         'Trailer': 'Content-MD5' });
message.write(fileData);
message.addTrailers({ 'Content-MD5': '7895bf4b8828b55ceaf47747b4bca667' });
message.end(); copy
Attempting to set a header field name or value that contains invalid characters
will result in a TypeError being thrown.

outgoingMessage.appendHeader(name, value)#

Added in: v18.3.0, v16.17.0


name <string> Header name
value <string> | <string[]> Header value
Returns: <this>

Append a single header value to the header object.
If the value is an array, this is equivalent to calling this method multiple
times.
If there were no previous values for the header, this is equivalent to calling
outgoingMessage.setHeader(name, value).
Depending of the value of options.uniqueHeaders when the client request or the
server were created, this will end up in the header being sent multiple times or
a single time with values joined using ; .

outgoingMessage.connection#

Added in: v0.3.0Deprecated since: v15.12.0, v14.17.1

Stability: 0 - Deprecated: Use outgoingMessage.socket instead.
Alias of outgoingMessage.socket.

outgoingMessage.cork()#

Added in: v13.2.0, v12.16.0

See writable.cork().

outgoingMessage.destroy([error])#

Added in: v0.3.0


error <Error> Optional, an error to emit with error event
Returns: <this>

Destroys the message. Once a socket is associated with the message
and is connected, that socket will be destroyed as well.

outgoingMessage.end(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
add callback argument.
v0.1.90
Added in: v0.1.90




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Optional, Default: utf8
callback <Function> Optional
Returns: <this>

Finishes the outgoing message. If any parts of the body are unsent, it will
flush them to the underlying system. If the message is chunked, it will
send the terminating chunk 0\r\n\r\n, and send the trailers (if any).
If chunk is specified, it is equivalent to calling
outgoingMessage.write(chunk, encoding), followed by
outgoingMessage.end(callback).
If callback is provided, it will be called when the message is finished
(equivalent to a listener of the 'finish' event).

outgoingMessage.flushHeaders()#

Added in: v1.6.0

Flushes the message headers.
For efficiency reason, Node.js normally buffers the message headers
until outgoingMessage.end() is called or the first chunk of message data
is written. It then tries to pack the headers and data into a single TCP
packet.
It is usually desired (it saves a TCP round-trip), but not when the first
data is not sent until possibly much later. outgoingMessage.flushHeaders()
bypasses the optimization and kickstarts the message.

outgoingMessage.getHeader(name)#

Added in: v0.4.0


name <string> Name of header
Returns: <string> | <undefined>

Gets the value of the HTTP header with the given name. If that header is not
set, the returned value will be undefined.

outgoingMessage.getHeaderNames()#

Added in: v7.7.0


Returns: <string[]>

Returns an array containing the unique names of the current outgoing headers.
All names are lowercase.

outgoingMessage.getHeaders()#

Added in: v7.7.0


Returns: <Object>

Returns a shallow copy of the current outgoing headers. Since a shallow
copy is used, array values may be mutated without additional calls to
various header-related HTTP module methods. The keys of the returned
object are the header names and the values are the respective header
values. All header names are lowercase.
The object returned by the outgoingMessage.getHeaders() method does
not prototypically inherit from the JavaScript Object. This means that
typical Object methods such as obj.toString(), obj.hasOwnProperty(),
and others are not defined and will not work.
outgoingMessage.setHeader('Foo', 'bar');
outgoingMessage.setHeader('Set-Cookie', ['foo=bar', 'bar=baz']);

const headers = outgoingMessage.getHeaders();
// headers === { foo: 'bar', 'set-cookie': ['foo=bar', 'bar=baz'] } copy

outgoingMessage.hasHeader(name)#

Added in: v7.7.0


name <string>
Returns: <boolean>

Returns true if the header identified by name is currently set in the
outgoing headers. The header name is case-insensitive.
const hasContentType = outgoingMessage.hasHeader('content-type'); copy

outgoingMessage.headersSent#

Added in: v0.9.3


<boolean>

Read-only. true if the headers were sent, otherwise false.

outgoingMessage.pipe()#

Added in: v9.0.0

Overrides the stream.pipe() method inherited from the legacy Stream class
which is the parent class of http.OutgoingMessage.
Calling this method will throw an Error because outgoingMessage is a
write-only stream.

outgoingMessage.removeHeader(name)#

Added in: v0.4.0


name <string> Header name

Removes a header that is queued for implicit sending.
outgoingMessage.removeHeader('Content-Encoding'); copy

outgoingMessage.setHeader(name, value)#

Added in: v0.4.0


name <string> Header name
value <any> Header value
Returns: <this>

Sets a single header value. If the header already exists in the to-be-sent
headers, its value will be replaced. Use an array of strings to send multiple
headers with the same name.

outgoingMessage.setHeaders(headers)#

Added in: v19.6.0, v18.15.0


headers <Headers> | <Map>
Returns: <this>

Sets multiple header values for implicit headers.
headers must be an instance of Headers or Map,
if a header already exists in the to-be-sent headers,
its value will be replaced.
const headers = new Headers({ foo: 'bar' });
outgoingMessage.setHeaders(headers); copy
or
const headers = new Map([['foo', 'bar']]);
outgoingMessage.setHeaders(headers); copy
When headers have been set with outgoingMessage.setHeaders(),
they will be merged with any headers passed to response.writeHead(),
with the headers passed to response.writeHead() given precedence.
// Returns content-type = text/plain
const server = http.createServer((req, res) => {
  const headers = new Headers({ 'Content-Type': 'text/html' });
  res.setHeaders(headers);
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('ok');
}); copy

outgoingMessage.setTimeout(msesc[, callback])#

Added in: v0.9.12


msesc <number>
callback <Function> Optional function to be called when a timeout
occurs. Same as binding to the timeout event.
Returns: <this>

Once a socket is associated with the message and is connected,
socket.setTimeout() will be called with msecs as the first parameter.

outgoingMessage.socket#

Added in: v0.3.0


<stream.Duplex>

Reference to the underlying socket. Usually, users will not want to access
this property.
After calling outgoingMessage.end(), this property will be nulled.

outgoingMessage.uncork()#

Added in: v13.2.0, v12.16.0

See writable.uncork()

outgoingMessage.writableCorked#

Added in: v13.2.0, v12.16.0


<number>

The number of times outgoingMessage.cork() has been called.

outgoingMessage.writableEnded#

Added in: v12.9.0


<boolean>

Is true if outgoingMessage.end() has been called. This property does
not indicate whether the data has been flushed. For that purpose, use
message.writableFinished instead.

outgoingMessage.writableFinished#

Added in: v12.7.0


<boolean>

Is true if all data has been flushed to the underlying system.

outgoingMessage.writableHighWaterMark#

Added in: v12.9.0


<number>

The highWaterMark of the underlying socket if assigned. Otherwise, the default
buffer level when writable.write() starts returning false (16384).

outgoingMessage.writableLength#

Added in: v12.9.0


<number>

The number of buffered bytes.

outgoingMessage.writableObjectMode#

Added in: v12.9.0


<boolean>

Always false.

outgoingMessage.write(chunk[, encoding][, callback])#

History

VersionChanges
v15.0.0
The chunk parameter can now be a Uint8Array.
v0.11.6
The callback argument was added.
v0.1.29
Added in: v0.1.29




chunk <string> | <Buffer> | <Uint8Array>
encoding <string> Default: utf8
callback <Function>
Returns: <boolean>

Sends a chunk of the body. This method can be called multiple times.
The encoding argument is only relevant when chunk is a string. Defaults to
'utf8'.
The callback argument is optional and will be called when this chunk of data
is flushed.
Returns true if the entire data was flushed successfully to the kernel
buffer. Returns false if all or part of the data was queued in the user
memory. The 'drain' event will be emitted when the buffer is free again.

http.METHODS#

Added in: v0.11.8


<string[]>

A list of the HTTP methods that are supported by the parser.
http.STATUS_CODES#

Added in: v0.1.22


<Object>

A collection of all the standard HTTP response status codes, and the
short description of each. For example, http.STATUS_CODES[404] === 'Not Found'.
http.createServer([options][, requestListener])#

History

VersionChanges
v20.1.0, v18.17.0
The highWaterMark option is supported now.
v18.0.0
The requestTimeout, headersTimeout, keepAliveTimeout, and connectionsCheckingInterval options are supported now.
v18.0.0
The noDelay option now defaults to true.
v17.7.0, v16.15.0
The noDelay, keepAlive and keepAliveInitialDelay options are supported now.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v9.6.0, v8.12.0
The options argument is supported now.
v0.1.13
Added in: v0.1.13





options <Object>

connectionsCheckingInterval: Sets the interval value in milliseconds to
check for request and headers timeout in incomplete requests.
Default: 30000.
headersTimeout: Sets the timeout value in milliseconds for receiving
the complete HTTP headers from the client.
See server.headersTimeout for more information.
Default: 60000.
highWaterMark <number> Optionally overrides all sockets'
readableHighWaterMark and writableHighWaterMark. This affects
highWaterMark property of both IncomingMessage and ServerResponse.
Default: See stream.getDefaultHighWaterMark().
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false.
IncomingMessage <http.IncomingMessage> Specifies the IncomingMessage
class to be used. Useful for extending the original IncomingMessage.
Default: IncomingMessage.
joinDuplicateHeaders <boolean> If set to true, this option allows
joining the field line values of multiple headers in a request with
a comma (, ) instead of discarding the duplicates.
For more information, refer to message.headers.
Default: false.
keepAlive <boolean> If set to true, it enables keep-alive functionality
on the socket immediately after a new incoming connection is received,
similarly on what is done in [socket.setKeepAlive([enable][, initialDelay])][socket.setKeepAlive(enable, initialDelay)].
Default: false.
keepAliveInitialDelay <number> If set to a positive number, it sets the
initial delay before the first keepalive probe is sent on an idle socket.
Default: 0.
keepAliveTimeout: The number of milliseconds of inactivity a server
needs to wait for additional incoming data, after it has finished writing
the last response, before a socket will be destroyed.
See server.keepAliveTimeout for more information.
Default: 5000.
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size for requests received by this server, i.e.
the maximum length of request headers in bytes.
Default: 16384 (16 KiB).
noDelay <boolean> If set to true, it disables the use of Nagle's
algorithm immediately after a new incoming connection is received.
Default: true.
requestTimeout: Sets the timeout value in milliseconds for receiving
the entire request from the client.
See server.requestTimeout for more information.
Default: 300000.
requireHostHeader <boolean> If set to true, it forces the server to
respond with a 400 (Bad Request) status code to any HTTP/1.1
request message that lacks a Host header
(as mandated by the specification).
Default: true.
ServerResponse <http.ServerResponse> Specifies the ServerResponse class
to be used. Useful for extending the original ServerResponse. Default:
ServerResponse.
uniqueHeaders <Array> A list of response headers that should be sent only
once. If the header's value is an array, the items will be joined
using ; .
rejectNonStandardBodyWrites <boolean> If set to true, an error is thrown
when writing to an HTTP response which does not have a body.
Default: false.



requestListener <Function>


Returns: <http.Server>


Returns a new instance of http.Server.
The requestListener is a function which is automatically
added to the 'request' event.

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy

import http from 'node:http';

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);const http = require('node:http');

// Create a local server to receive data from
const server = http.createServer();

// Listen to the request event
server.on('request', (request, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000);copy
http.get(options[, callback])#
http.get(url[, options][, callback])#

History

VersionChanges
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object> Accepts the same options as
http.request(), with the method set to GET by default.
callback <Function>
Returns: <http.ClientRequest>

Since most requests are GET requests without bodies, Node.js provides this
convenience method. The only difference between this method and
http.request() is that it sets the method to GET by default and calls req.end()
automatically. The callback must take care to consume the response
data for reasons stated in http.ClientRequest section.
The callback is invoked with a single argument that is an instance of
http.IncomingMessage.
JSON fetching example:
http.get('http://localhost:8000/', (res) => {
  const { statusCode } = res;
  const contentType = res.headers['content-type'];

  let error;
  // Any 2xx status code signals a successful response but
  // here we're only checking for 200.
  if (statusCode !== 200) {
    error = new Error('Request Failed.\n' +
                      `Status Code: ${statusCode}`);
  } else if (!/^application\/json/.test(contentType)) {
    error = new Error('Invalid content-type.\n' +
                      `Expected application/json but received ${contentType}`);
  }
  if (error) {
    console.error(error.message);
    // Consume response data to free up memory
    res.resume();
    return;
  }

  res.setEncoding('utf8');
  let rawData = '';
  res.on('data', (chunk) => { rawData += chunk; });
  res.on('end', () => {
    try {
      const parsedData = JSON.parse(rawData);
      console.log(parsedData);
    } catch (e) {
      console.error(e.message);
    }
  });
}).on('error', (e) => {
  console.error(`Got error: ${e.message}`);
});

// Create a local server to receive data from
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'application/json' });
  res.end(JSON.stringify({
    data: 'Hello World!',
  }));
});

server.listen(8000); copy
http.globalAgent#

History

VersionChanges
v19.0.0
The agent now uses HTTP Keep-Alive and a 5 second timeout by default.
v0.5.9
Added in: v0.5.9




<http.Agent>

Global instance of Agent which is used as the default for all HTTP client
requests. Diverges from a default Agent configuration by having keepAlive
enabled and a timeout of 5 seconds.
http.maxHeaderSize#

Added in: v11.6.0, v10.15.0


<number>

Read-only property specifying the maximum allowed size of HTTP headers in bytes.
Defaults to 16 KiB. Configurable using the --max-http-header-size CLI
option.
This can be overridden for servers and client requests by passing the
maxHeaderSize option.
http.request(options[, callback])#
http.request(url[, options][, callback])#

History

VersionChanges
v16.7.0, v14.18.0
When using a URL object parsed username and password will now be properly URI decoded.
v15.3.0, v14.17.0
It is possible to abort a request with an AbortSignal.
v13.3.0
The maxHeaderSize option is supported now.
v13.8.0, v12.15.0, v10.19.0
The insecureHTTPParser option is supported now.
v10.9.0
The url parameter can now be passed along with a separate options object.
v7.5.0
The options parameter can be a WHATWG URL object.
v0.3.6
Added in: v0.3.6




url <string> | <URL>
options <Object>

agent <http.Agent> | <boolean> Controls Agent behavior. Possible
values:

undefined (default): use http.globalAgent for this host and port.
Agent object: explicitly use the passed in Agent.
false: causes a new Agent with default values to be used.


auth <string> Basic authentication ('user:password') to compute an
Authorization header.
createConnection <Function> A function that produces a socket/stream to
use for the request when the agent option is not used. This can be used to
avoid creating a custom Agent class just to override the default
createConnection function. See agent.createConnection() for more
details. Any Duplex stream is a valid return value.
defaultPort <number> Default port for the protocol. Default:
agent.defaultPort if an Agent is used, else undefined.
family <number> IP address family to use when resolving host or
hostname. Valid values are 4 or 6. When unspecified, both IP v4 and
v6 will be used.
headers <Object> An object containing request headers.
hints <number> Optional dns.lookup() hints.
host <string> A domain name or IP address of the server to issue the
request to. Default: 'localhost'.
hostname <string> Alias for host. To support url.parse(),
hostname will be used if both host and hostname are specified.
insecureHTTPParser <boolean> If set to true, it will use a HTTP parser
with leniency flags enabled. Using the insecure parser should be avoided.
See --insecure-http-parser for more information.
Default: false
joinDuplicateHeaders <boolean> It joins the field line values of
multiple headers in a request with ,  instead of discarding
the duplicates. See message.headers for more information.
Default: false.
localAddress <string> Local interface to bind for network connections.
localPort <number> Local port to connect from.
lookup <Function> Custom lookup function. Default: dns.lookup().
maxHeaderSize <number> Optionally overrides the value of
--max-http-header-size (the maximum length of response headers in
bytes) for responses received from the server.
Default: 16384 (16 KiB).
method <string> A string specifying the HTTP request method. Default:
'GET'.
path <string> Request path. Should include query string if any.
E.G. '/index.html?page=12'. An exception is thrown when the request path
contains illegal characters. Currently, only spaces are rejected but that
may change in the future. Default: '/'.
port <number> Port of remote server. Default: defaultPort if set,
else 80.
protocol <string> Protocol to use. Default: 'http:'.
setDefaultHeaders <boolean>: Specifies whether or not to automatically add
default headers such as Connection, Content-Length, Transfer-Encoding,
and Host. If set to false then all necessary headers must be added
manually. Defaults to true.
setHost <boolean>: Specifies whether or not to automatically add the
Host header. If provided, this overrides setDefaultHeaders. Defaults to
true.
signal <AbortSignal>: An AbortSignal that may be used to abort an ongoing
request.
socketPath <string> Unix domain socket. Cannot be used if one of host
or port is specified, as those specify a TCP Socket.
timeout <number>: A number specifying the socket timeout in milliseconds.
This will set the timeout before the socket is connected.
uniqueHeaders <Array> A list of request headers that should be sent
only once. If the header's value is an array, the items will be joined
using ; .


callback <Function>
Returns: <http.ClientRequest>

options in socket.connect() are also supported.
Node.js maintains several connections per server to make HTTP requests.
This function allows one to transparently issue requests.
url can be a string or a URL object. If url is a
string, it is automatically parsed with new URL(). If it is a URL
object, it will be automatically converted to an ordinary options object.
If both url and options are specified, the objects are merged, with the
options properties taking precedence.
The optional callback parameter will be added as a one-time listener for
the 'response' event.
http.request() returns an instance of the http.ClientRequest
class. The ClientRequest instance is a writable stream. If one needs to
upload a file with a POST request, then write to the ClientRequest object.

import http from 'node:http';
import { Buffer } from 'node:buffer';

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();const http = require('node:http');

const postData = JSON.stringify({
  'msg': 'Hello World!',
});

const options = {
  hostname: 'www.google.com',
  port: 80,
  path: '/upload',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(postData),
  },
};

const req = http.request(options, (res) => {
  console.log(`STATUS: ${res.statusCode}`);
  console.log(`HEADERS: ${JSON.stringify(res.headers)}`);
  res.setEncoding('utf8');
  res.on('data', (chunk) => {
    console.log(`BODY: ${chunk}`);
  });
  res.on('end', () => {
    console.log('No more data in response.');
  });
});

req.on('error', (e) => {
  console.error(`problem with request: ${e.message}`);
});

// Write data to request body
req.write(postData);
req.end();copy
In the example req.end() was called. With http.request() one
must always call req.end() to signify the end of the request -
even if there is no data being written to the request body.
If any error is encountered during the request (be that with DNS resolution,
TCP level errors, or actual HTTP parse errors) an 'error' event is emitted
on the returned request object. As with all 'error' events, if no listeners
are registered the error will be thrown.
There are a few special headers that should be noted.


Sending a 'Connection: keep-alive' will notify Node.js that the connection to
the server should be persisted until the next request.


Sending a 'Content-Length' header will disable the default chunked encoding.


Sending an 'Expect' header will immediately send the request headers.
Usually, when sending 'Expect: 100-continue', both a timeout and a listener
for the 'continue' event should be set. See RFC 2616 Section 8.2.3 for more
information.


Sending an Authorization header will override using the auth option
to compute basic authentication.


Example using a URL as options:
const options = new URL('http://abc:xyz@example.com');

const req = http.request(options, (res) => {
  // ...
}); copy
In a successful request, the following events will be emitted in the following
order:

'socket'
'response'

'data' any number of times, on the res object
('data' will not be emitted at all if the response body is empty, for
instance, in most redirects)
'end' on the res object


'close'

In the case of a connection error, the following events will be emitted:

'socket'
'error'
'close'

In the case of a premature connection close before the response is received,
the following events will be emitted in the following order:

'socket'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

In the case of a premature connection close after the response is received,
the following events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(connection closed here)
'aborted' on the res object
'close'
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'
'close' on the res object

If req.destroy() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.destroy() called here)
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET', or the error with which req.destroy() was called
'close'

If req.destroy() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.destroy() called here)
'aborted' on the res object
'close'
'error' on the res object with an error with message 'Error: aborted'
and code 'ECONNRESET', or the error with which req.destroy() was called
'close' on the res object

If req.abort() is called before a socket is assigned, the following
events will be emitted in the following order:

(req.abort() called here)
'abort'
'close'

If req.abort() is called before the connection succeeds, the following
events will be emitted in the following order:

'socket'
(req.abort() called here)
'abort'
'error' with an error with message 'Error: socket hang up' and code
'ECONNRESET'
'close'

If req.abort() is called after the response is received, the following
events will be emitted in the following order:

'socket'
'response'

'data' any number of times, on the res object


(req.abort() called here)
'abort'
'aborted' on the res object
'error' on the res object with an error with message
'Error: aborted' and code 'ECONNRESET'.
'close'
'close' on the res object

Setting the timeout option or using the setTimeout() function will
not abort the request or do anything besides add a 'timeout' event.
Passing an AbortSignal and then calling abort() on the corresponding
AbortController will behave the same way as calling .destroy() on the
request. Specifically, the 'error' event will be emitted with an error with
the message 'AbortError: The operation was aborted', the code 'ABORT_ERR'
and the cause, if one was provided.
http.validateHeaderName(name[, label])#

History

VersionChanges
v19.5.0, v18.14.0
The label parameter is added.
v14.3.0
Added in: v14.3.0




name <string>
label <string> Label for error message. Default: 'Header name'.

Performs the low-level validations on the provided name that are done when
res.setHeader(name, value) is called.
Passing illegal value as name will result in a TypeError being thrown,
identified by code: 'ERR_INVALID_HTTP_TOKEN'.
It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Example:

import { validateHeaderName } from 'node:http';

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}const { validateHeaderName } = require('node:http');

try {
  validateHeaderName('');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code); // --> 'ERR_INVALID_HTTP_TOKEN'
  console.error(err.message); // --> 'Header name must be a valid HTTP token [""]'
}copy
http.validateHeaderValue(name, value)#

Added in: v14.3.0


name <string>
value <any>

Performs the low-level validations on the provided value that are done when
res.setHeader(name, value) is called.
Passing illegal value as value will result in a TypeError being thrown.

Undefined value error is identified by code: 'ERR_HTTP_INVALID_HEADER_VALUE'.
Invalid value character error is identified by code: 'ERR_INVALID_CHAR'.

It is not necessary to use this method before passing headers to an HTTP request
or response. The HTTP module will automatically validate such headers.
Examples:

import { validateHeaderValue } from 'node:http';

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}const { validateHeaderValue } = require('node:http');

try {
  validateHeaderValue('x-my-header', undefined);
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_HTTP_INVALID_HEADER_VALUE'); // --> true
  console.error(err.message); // --> 'Invalid value "undefined" for header "x-my-header"'
}

try {
  validateHeaderValue('x-my-header', 'oʊmɪɡə');
} catch (err) {
  console.error(err instanceof TypeError); // --> true
  console.error(err.code === 'ERR_INVALID_CHAR'); // --> true
  console.error(err.message); // --> 'Invalid character in header content ["x-my-header"]'
}copy
http.setMaxIdleHTTPParsers(max)#

Added in: v18.8.0, v16.18.0


max <number> Default: 1000.

Set the maximum number of idle HTTP parsers.
WebSocket#

Added in: v22.5.0

A browser-compatible implementation of WebSocket.\n\n\n\nBranding of Node.js
Please review the trademark policy for information about permissible use of Node.js® logos and marks.
Node.js® Mascot
Credit to Angela Angelini for designing and contributing the Rocket Turtle.

Node.js® Logo
Node.js® Horizontal Logo

Node.js® Stacked Logo

JS Icons\n\n\n\nProject Governance
Consensus Seeking Process
The Node.js project follows a Consensus Seeking decision making model.
Collaborators
The nodejs/node core GitHub repository is maintained by the Collaborators
who are nominated by other existing Collaborators on an ongoing basis.
Individuals making significant and valuable contributions are made Collaborators
and given commit-access to the project. These individuals are identified by other
Collaborators and their nomination is discussed with the existing Collaborators.
For the current list of Collaborators, see the project's README.md.
A guide for Collaborators is maintained at collaborator-guide.md.
Technical Steering Committee
The project is governed by the Technical Steering Committee (TSC)
which is responsible for high-level guidance of the project. TSC is a
subset of active Collaborators who are nominated by other existing TSC
members.\n\n\n\nSecurity Reporting
For more details on active Security Policies, checkout this page.
Reporting a bug in Node.js
Report security bugs in Node.js via HackerOne.
Your report will be acknowledged within 5 days, and you'll receive a more
detailed response to your report within 10 days indicating the next steps in
handling your submission.
After the initial reply to your report, the security team will endeavor to keep
you informed of the progress being made towards a fix and full announcement,
and may ask for additional information or guidance surrounding the reported
issue.
Node.js bug bounty program
The Node.js project engages in an official bug bounty program for security
researchers and responsible public disclosures. The program is managed through
the HackerOne platform. See https://hackerone.com/nodejs for further details.
Reporting a bug in a third party module
Security bugs in third party modules should be reported to their respective
maintainers.
Disclosure policy
Here is the security disclosure policy for Node.js


The security report is received and is assigned a primary handler. This
person will coordinate the fix and release process. The problem is confirmed
and a list of all affected versions is determined. Code is audited to find
any potential similar problems. Fixes are prepared for all releases which are
still under maintenance. These fixes are not committed to the public
repository but rather held locally pending the announcement.


A suggested embargo date for this vulnerability is chosen and a CVE (Common
Vulnerabilities and Exposures (CVE®)) is requested for the vulnerability.


On the embargo date, the Node.js security mailing list is sent a copy of the
announcement. The changes are pushed to the public repository and new builds
are deployed to nodejs.org. Within 6 hours of the mailing list being
notified, a copy of the advisory will be published on the Node.js blog.


Typically the embargo date will be set 72 hours from the time the CVE is
issued. However, this may vary depending on the severity of the bug or
difficulty in applying a fix.


This process can take some time, especially when coordination is required
with maintainers of other projects. Every effort will be made to handle the
bug in as timely a manner as possible; however, it's important that we follow
the release process above to ensure that the disclosure is handled in a
consistent manner.


Receiving security updates
Security notifications will be distributed via the following methods.

Google Group
Node.js Blog

Comments on this policy
If you have suggestions on how this process could be improved please submit a
pull request or
file an issue to discuss.
OpenSSF Best Practices

The Open Source Security Foundation (OpenSSF) Best Practices badge is a way for Free/Libre and Open Source Software (FLOSS) projects to show that they follow best practices. Projects can voluntarily self-certify how they follow each best practice. Consumers of the badge can quickly assess which FLOSS projects are following best practices and as a result are more likely to produce higher-quality secure software.\n\n\n\nGet Involved
If you are interested in getting involved with the Node.js community, there are many ways to do so. The Node.js project is a large and diverse community with many ways to contribute beyond just writing code.
Community Discussion

The nodejs/node GitHub repository is the place to discuss Node.js core features and reporting issues.
The nodejs/help GitHub repository is the official place to ask questions about Node.js.
Node.js's official Discord server is a place to chat with other Node.js developers and get official news from the Node.js project.
Node.js's project calendar with all public Node.js team meetings.

Learning Materials
If you are looking to learn more about Node.js, there are many resources available to you.

Node.js's official learning materials.
Node.js's official API reference documentation.
NodeSchool.io teaches Node.js concepts via interactive command-line games.
StackOverflow's Node.js tag contains a large number of threads with helpful resources.
The DEV Community Node.js's tag contains articles and content related to Node.js.

Unofficial Discussion Areas
There are several unofficial discussion areas if you are looking for a more informal place to discuss Node.js.
Please note that the Node.js project does not officially endorse these. Please follow their respective codes of conduct/rules.

Node Slackers is a Node.js-focused Slack community.
OpenJSF Slack is a Slack workspace for the OpenJS Foundation. There are several channels related to Node.js. (channels prefixed by #nodejs- are related to the project)
irc.libera.chat in the #node.js channel with an IRC client or connect in your web browser to the channel using a web client.\n\n\n\nCollaboration Summit
Node.js's Collaboration Summit is an un-conference for bringing current and
potential contributors together to discuss Node.js with lively collaboration,
education, and knowledge sharing. Teams, working groups and contributors
from the community come together twice per year to have discussions that
help decision-making while also working on some exciting efforts they
want to push forward in-person.
Who attends?
The Collaboration Summit is primarily attended by existing contributors and
community members, but it also welcomes those who are not yet a contributor
and want to get onboard. If you are new to contributing to Node.js, the
Collaboration Summit can be a good opportunity to help you learn what is
happening within the community and contribute with the skills you have
and would like to hone.
Prior to the summit, contributors and community members send session proposals to
create a schedule. Attendees can familiarize themselves with the session before
getting onsite, having the general collaborator discussions, and then diving
into sessions. There will also be plenty of opportunities for hallway tracks
and brainstorms.
For information about upcoming and past Collaboration Summits, check out the
Summit repo. Have a look at the
issues filed that share what
contributors and community members are proposing to discuss in-person.\n\n\n\nUpcoming Events
Node.js events are open and available to the public. Anyone is welcome to join and participate.
Upcoming Node.js® Meetings
The Node.js project holds numerous meetings throughout the year to discuss and plan aspects of the project.
The following meetings are upcoming in the next 7 days.
April 174:00 PM-5:00 PM(UTC)Diagnostics WG Meeting3:00 PM-4:00 PM(UTC)Build WG MeetingApril 183:00 PM-4:00 PM(UTC)Node API team meetingApril 233:30 PM-4:30 PM(UTC)TypeScript team meeting2:00 PM-3:00 PM(UTC)Node.js TSC Meeting\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
Child process

Asynchronous process creation

Spawning .bat and .cmd files on Windows
child_process.exec(command[, options][, callback])
child_process.execFile(file[, args][, options][, callback])
child_process.fork(modulePath[, args][, options])
child_process.spawn(command[, args][, options])

options.detached
options.stdio




Synchronous process creation

child_process.execFileSync(file[, args][, options])
child_process.execSync(command[, options])
child_process.spawnSync(command[, args][, options])


Class: ChildProcess

Event: 'close'
Event: 'disconnect'
Event: 'error'
Event: 'exit'
Event: 'message'
Event: 'spawn'
subprocess.channel

subprocess.channel.ref()
subprocess.channel.unref()


subprocess.connected
subprocess.disconnect()
subprocess.exitCode
subprocess.kill([signal])
subprocess[Symbol.dispose]()
subprocess.killed
subprocess.pid
subprocess.ref()
subprocess.send(message[, sendHandle[, options]][, callback])

Example: sending a server object
Example: sending a socket object


subprocess.signalCode
subprocess.spawnargs
subprocess.spawnfile
subprocess.stderr
subprocess.stdin
subprocess.stdio
subprocess.stdout
subprocess.unref()


maxBuffer and Unicode
Shell requirements
Default Windows shell
Advanced serialization



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
Child process

Asynchronous process creation

Spawning .bat and .cmd files on Windows
child_process.exec(command[, options][, callback])
child_process.execFile(file[, args][, options][, callback])
child_process.fork(modulePath[, args][, options])
child_process.spawn(command[, args][, options])

options.detached
options.stdio




Synchronous process creation

child_process.execFileSync(file[, args][, options])
child_process.execSync(command[, options])
child_process.spawnSync(command[, args][, options])


Class: ChildProcess

Event: 'close'
Event: 'disconnect'
Event: 'error'
Event: 'exit'
Event: 'message'
Event: 'spawn'
subprocess.channel

subprocess.channel.ref()
subprocess.channel.unref()


subprocess.connected
subprocess.disconnect()
subprocess.exitCode
subprocess.kill([signal])
subprocess[Symbol.dispose]()
subprocess.killed
subprocess.pid
subprocess.ref()
subprocess.send(message[, sendHandle[, options]][, callback])

Example: sending a server object
Example: sending a socket object


subprocess.signalCode
subprocess.spawnargs
subprocess.spawnfile
subprocess.stderr
subprocess.stdin
subprocess.stdio
subprocess.stdout
subprocess.unref()


maxBuffer and Unicode
Shell requirements
Default Windows shell
Advanced serialization




      
        Child process#

Stability: 2 - Stable
Source Code: lib/child_process.js
The node:child_process module provides the ability to spawn subprocesses in
a manner that is similar, but not identical, to popen(3). This capability
is primarily provided by the child_process.spawn() function:

const { spawn } = require('node:child_process');
const ls = spawn('ls', ['-lh', '/usr']);

ls.stdout.on('data', (data) => {
  console.log(`stdout: ${data}`);
});

ls.stderr.on('data', (data) => {
  console.error(`stderr: ${data}`);
});

ls.on('close', (code) => {
  console.log(`child process exited with code ${code}`);
});import { spawn } from 'node:child_process';
const ls = spawn('ls', ['-lh', '/usr']);

ls.stdout.on('data', (data) => {
  console.log(`stdout: ${data}`);
});

ls.stderr.on('data', (data) => {
  console.error(`stderr: ${data}`);
});

ls.on('close', (code) => {
  console.log(`child process exited with code ${code}`);
});copy
By default, pipes for stdin, stdout, and stderr are established between
the parent Node.js process and the spawned subprocess. These pipes have
limited (and platform-specific) capacity. If the subprocess writes to
stdout in excess of that limit without the output being captured, the
subprocess blocks waiting for the pipe buffer to accept more data. This is
identical to the behavior of pipes in the shell. Use the { stdio: 'ignore' }
option if the output will not be consumed.
The command lookup is performed using the options.env.PATH environment
variable if env is in the options object. Otherwise, process.env.PATH is
used. If options.env is set without PATH, lookup on Unix is performed
on a default search path search of /usr/bin:/bin (see your operating system's
manual for execvpe/execvp), on Windows the current processes environment
variable PATH is used.
On Windows, environment variables are case-insensitive. Node.js
lexicographically sorts the env keys and uses the first one that
case-insensitively matches. Only first (in lexicographic order) entry will be
passed to the subprocess. This might lead to issues on Windows when passing
objects to the env option that have multiple variants of the same key, such as
PATH and Path.
The child_process.spawn() method spawns the child process asynchronously,
without blocking the Node.js event loop. The child_process.spawnSync()
function provides equivalent functionality in a synchronous manner that blocks
the event loop until the spawned process either exits or is terminated.
For convenience, the node:child_process module provides a handful of
synchronous and asynchronous alternatives to child_process.spawn() and
child_process.spawnSync(). Each of these alternatives are implemented on
top of child_process.spawn() or child_process.spawnSync().

child_process.exec(): spawns a shell and runs a command within that
shell, passing the stdout and stderr to a callback function when
complete.
child_process.execFile(): similar to child_process.exec() except
that it spawns the command directly without first spawning a shell by
default.
child_process.fork(): spawns a new Node.js process and invokes a
specified module with an IPC communication channel established that allows
sending messages between parent and child.
child_process.execSync(): a synchronous version of
child_process.exec() that will block the Node.js event loop.
child_process.execFileSync(): a synchronous version of
child_process.execFile() that will block the Node.js event loop.

For certain use cases, such as automating shell scripts, the
synchronous counterparts may be more convenient. In many cases, however,
the synchronous methods can have significant impact on performance due to
stalling the event loop while spawned processes complete.
Asynchronous process creation#
The child_process.spawn(), child_process.fork(), child_process.exec(),
and child_process.execFile() methods all follow the idiomatic asynchronous
programming pattern typical of other Node.js APIs.
Each of the methods returns a ChildProcess instance. These objects
implement the Node.js EventEmitter API, allowing the parent process to
register listener functions that are called when certain events occur during
the life cycle of the child process.
The child_process.exec() and child_process.execFile() methods
additionally allow for an optional callback function to be specified that is
invoked when the child process terminates.

Spawning .bat and .cmd files on Windows#
The importance of the distinction between child_process.exec() and
child_process.execFile() can vary based on platform. On Unix-type
operating systems (Unix, Linux, macOS) child_process.execFile() can be
more efficient because it does not spawn a shell by default. On Windows,
however, .bat and .cmd files are not executable on their own without a
terminal, and therefore cannot be launched using child_process.execFile().
When running on Windows, .bat and .cmd files can be invoked using
child_process.spawn() with the shell option set, with
child_process.exec(), or by spawning cmd.exe and passing the .bat or
.cmd file as an argument (which is what the shell option and
child_process.exec() do). In any case, if the script filename contains
spaces it needs to be quoted.

// OR...
const { exec, spawn } = require('node:child_process');

exec('my.bat', (err, stdout, stderr) => {
  if (err) {
    console.error(err);
    return;
  }
  console.log(stdout);
});

// Script with spaces in the filename:
const bat = spawn('"my script.cmd" a b', { shell: true });
// or:
exec('"my script.cmd" a b', (err, stdout, stderr) => {
  // ...
});// OR...
import { exec, spawn } from 'node:child_process';

exec('my.bat', (err, stdout, stderr) => {
  if (err) {
    console.error(err);
    return;
  }
  console.log(stdout);
});

// Script with spaces in the filename:
const bat = spawn('"my script.cmd" a b', { shell: true });
// or:
exec('"my script.cmd" a b', (err, stdout, stderr) => {
  // ...
});copy

child_process.exec(command[, options][, callback])#

History

VersionChanges
v15.4.0
AbortSignal support was added.
v16.4.0, v14.18.0
The cwd option can be a WHATWG URL object using file: protocol.
v8.8.0
The windowsHide option is supported now.
v0.1.90
Added in: v0.1.90




command <string> The command to run, with space-separated arguments.
options <Object>

cwd <string> | <URL> Current working directory of the child process.
Default: process.cwd().
env <Object> Environment key-value pairs. Default: process.env.
encoding <string> Default: 'utf8'
shell <string> Shell to execute the command with. See
Shell requirements and Default Windows shell. Default:
'/bin/sh' on Unix, process.env.ComSpec on Windows.
signal <AbortSignal> allows aborting the child process using an
AbortSignal.
timeout <number> Default: 0
maxBuffer <number> Largest amount of data in bytes allowed on stdout or
stderr. If exceeded, the child process is terminated and any output is
truncated. See caveat at maxBuffer and Unicode.
Default: 1024 * 1024.
killSignal <string> | <integer> Default: 'SIGTERM'
uid <number> Sets the user identity of the process (see setuid(2)).
gid <number> Sets the group identity of the process (see setgid(2)).
windowsHide <boolean> Hide the subprocess console window that would
normally be created on Windows systems. Default: false.


callback <Function> called with the output when process terminates.

error <Error>
stdout <string> | <Buffer>
stderr <string> | <Buffer>


Returns: <ChildProcess>

Spawns a shell then executes the command within that shell, buffering any
generated output. The command string passed to the exec function is processed
directly by the shell and special characters (vary based on
shell)
need to be dealt with accordingly:

const { exec } = require('node:child_process');

exec('"/path/to/test file/test.sh" arg1 arg2');
// Double quotes are used so that the space in the path is not interpreted as
// a delimiter of multiple arguments.

exec('echo "The \\$HOME variable is $HOME"');
// The $HOME variable is escaped in the first instance, but not in the second.import { exec } from 'node:child_process';

exec('"/path/to/test file/test.sh" arg1 arg2');
// Double quotes are used so that the space in the path is not interpreted as
// a delimiter of multiple arguments.

exec('echo "The \\$HOME variable is $HOME"');
// The $HOME variable is escaped in the first instance, but not in the second.copy
Never pass unsanitized user input to this function. Any input containing shell
metacharacters may be used to trigger arbitrary command execution.
If a callback function is provided, it is called with the arguments
(error, stdout, stderr). On success, error will be null. On error,
error will be an instance of Error. The error.code property will be
the exit code of the process. By convention, any exit code other than 0
indicates an error. error.signal will be the signal that terminated the
process.
The stdout and stderr arguments passed to the callback will contain the
stdout and stderr output of the child process. By default, Node.js will decode
the output as UTF-8 and pass strings to the callback. The encoding option
can be used to specify the character encoding used to decode the stdout and
stderr output. If encoding is 'buffer', or an unrecognized character
encoding, Buffer objects will be passed to the callback instead.

const { exec } = require('node:child_process');
exec('cat *.js missing_file | wc -l', (error, stdout, stderr) => {
  if (error) {
    console.error(`exec error: ${error}`);
    return;
  }
  console.log(`stdout: ${stdout}`);
  console.error(`stderr: ${stderr}`);
});import { exec } from 'node:child_process';
exec('cat *.js missing_file | wc -l', (error, stdout, stderr) => {
  if (error) {
    console.error(`exec error: ${error}`);
    return;
  }
  console.log(`stdout: ${stdout}`);
  console.error(`stderr: ${stderr}`);
});copy
If timeout is greater than 0, the parent process will send the signal
identified by the killSignal property (the default is 'SIGTERM') if the
child process runs longer than timeout milliseconds.
Unlike the exec(3) POSIX system call, child_process.exec() does not replace
the existing process and uses a shell to execute the command.
If this method is invoked as its util.promisify()ed version, it returns
a Promise for an Object with stdout and stderr properties. The returned
ChildProcess instance is attached to the Promise as a child property. In
case of an error (including any error resulting in an exit code other than 0), a
rejected promise is returned, with the same error object given in the
callback, but with two additional properties stdout and stderr.

const util = require('node:util');
const exec = util.promisify(require('node:child_process').exec);

async function lsExample() {
  const { stdout, stderr } = await exec('ls');
  console.log('stdout:', stdout);
  console.error('stderr:', stderr);
}
lsExample();import { promisify } from 'node:util';
import child_process from 'node:child_process';
const exec = promisify(child_process.exec);

async function lsExample() {
  const { stdout, stderr } = await exec('ls');
  console.log('stdout:', stdout);
  console.error('stderr:', stderr);
}
lsExample();copy
If the signal option is enabled, calling .abort() on the corresponding
AbortController is similar to calling .kill() on the child process except
the error passed to the callback will be an AbortError:

const { exec } = require('node:child_process');
const controller = new AbortController();
const { signal } = controller;
const child = exec('grep ssh', { signal }, (error) => {
  console.error(error); // an AbortError
});
controller.abort();import { exec } from 'node:child_process';
const controller = new AbortController();
const { signal } = controller;
const child = exec('grep ssh', { signal }, (error) => {
  console.error(error); // an AbortError
});
controller.abort();copy

child_process.execFile(file[, args][, options][, callback])#

History

VersionChanges
v23.11.0
Passing args when shell is set to true is deprecated.
v16.4.0, v14.18.0
The cwd option can be a WHATWG URL object using file: protocol.
v15.4.0, v14.17.0
AbortSignal support was added.
v8.8.0
The windowsHide option is supported now.
v0.1.91
Added in: v0.1.91




file <string> The name or path of the executable file to run.
args <string[]> List of string arguments.
options <Object>

cwd <string> | <URL> Current working directory of the child process.
env <Object> Environment key-value pairs. Default: process.env.
encoding <string> Default: 'utf8'
timeout <number> Default: 0
maxBuffer <number> Largest amount of data in bytes allowed on stdout or
stderr. If exceeded, the child process is terminated and any output is
truncated. See caveat at maxBuffer and Unicode.
Default: 1024 * 1024.
killSignal <string> | <integer> Default: 'SIGTERM'
uid <number> Sets the user identity of the process (see setuid(2)).
gid <number> Sets the group identity of the process (see setgid(2)).
windowsHide <boolean> Hide the subprocess console window that would
normally be created on Windows systems. Default: false.
windowsVerbatimArguments <boolean> No quoting or escaping of arguments is
done on Windows. Ignored on Unix. Default: false.
shell <boolean> | <string> If true, runs command inside of a shell. Uses
'/bin/sh' on Unix, and process.env.ComSpec on Windows. A different
shell can be specified as a string. See Shell requirements and
Default Windows shell. Default: false (no shell).
signal <AbortSignal> allows aborting the child process using an
AbortSignal.


callback <Function> Called with the output when process terminates.

error <Error>
stdout <string> | <Buffer>
stderr <string> | <Buffer>


Returns: <ChildProcess>

The child_process.execFile() function is similar to child_process.exec()
except that it does not spawn a shell by default. Rather, the specified
executable file is spawned directly as a new process making it slightly more
efficient than child_process.exec().
The same options as child_process.exec() are supported. Since a shell is
not spawned, behaviors such as I/O redirection and file globbing are not
supported.

const { execFile } = require('node:child_process');
const child = execFile('node', ['--version'], (error, stdout, stderr) => {
  if (error) {
    throw error;
  }
  console.log(stdout);
});import { execFile } from 'node:child_process';
const child = execFile('node', ['--version'], (error, stdout, stderr) => {
  if (error) {
    throw error;
  }
  console.log(stdout);
});copy
The stdout and stderr arguments passed to the callback will contain the
stdout and stderr output of the child process. By default, Node.js will decode
the output as UTF-8 and pass strings to the callback. The encoding option
can be used to specify the character encoding used to decode the stdout and
stderr output. If encoding is 'buffer', or an unrecognized character
encoding, Buffer objects will be passed to the callback instead.
If this method is invoked as its util.promisify()ed version, it returns
a Promise for an Object with stdout and stderr properties. The returned
ChildProcess instance is attached to the Promise as a child property. In
case of an error (including any error resulting in an exit code other than 0), a
rejected promise is returned, with the same error object given in the
callback, but with two additional properties stdout and stderr.

const util = require('node:util');
const execFile = util.promisify(require('node:child_process').execFile);
async function getVersion() {
  const { stdout } = await execFile('node', ['--version']);
  console.log(stdout);
}
getVersion();import { promisify } from 'node:util';
import child_process from 'node:child_process';
const execFile = promisify(child_process.execFile);
async function getVersion() {
  const { stdout } = await execFile('node', ['--version']);
  console.log(stdout);
}
getVersion();copy
If the shell option is enabled, do not pass unsanitized user input to this
function. Any input containing shell metacharacters may be used to trigger
arbitrary command execution.
If the signal option is enabled, calling .abort() on the corresponding
AbortController is similar to calling .kill() on the child process except
the error passed to the callback will be an AbortError:

const { execFile } = require('node:child_process');
const controller = new AbortController();
const { signal } = controller;
const child = execFile('node', ['--version'], { signal }, (error) => {
  console.error(error); // an AbortError
});
controller.abort();import { execFile } from 'node:child_process';
const controller = new AbortController();
const { signal } = controller;
const child = execFile('node', ['--version'], { signal }, (error) => {
  console.error(error); // an AbortError
});
controller.abort();copy

child_process.fork(modulePath[, args][, options])#

History

VersionChanges
v17.4.0, v16.14.0
The modulePath parameter can be a WHATWG URL object using file: protocol.
v16.4.0, v14.18.0
The cwd option can be a WHATWG URL object using file: protocol.
v15.13.0, v14.18.0
timeout was added.
v15.11.0, v14.18.0
killSignal for AbortSignal was added.
v15.6.0, v14.17.0
AbortSignal support was added.
v13.2.0, v12.16.0
The serialization option is supported now.
v8.0.0
The stdio option can now be a string.
v6.4.0
The stdio option is supported now.
v0.5.0
Added in: v0.5.0




modulePath <string> | <URL> The module to run in the child.
args <string[]> List of string arguments.
options <Object>

cwd <string> | <URL> Current working directory of the child process.
detached <boolean> Prepare child process to run independently of its
parent process. Specific behavior depends on the platform (see
options.detached).
env <Object> Environment key-value pairs. Default: process.env.
execPath <string> Executable used to create the child process.
execArgv <string[]> List of string arguments passed to the executable.
Default: process.execArgv.
gid <number> Sets the group identity of the process (see setgid(2)).
serialization <string> Specify the kind of serialization used for sending
messages between processes. Possible values are 'json' and 'advanced'.
See Advanced serialization for more details. Default: 'json'.
signal <AbortSignal> Allows closing the child process using an
AbortSignal.
killSignal <string> | <integer> The signal value to be used when the spawned
process will be killed by timeout or abort signal. Default: 'SIGTERM'.
silent <boolean> If true, stdin, stdout, and stderr of the child
process will be piped to the parent process, otherwise they will be inherited
from the parent process, see the 'pipe' and 'inherit' options for
child_process.spawn()'s stdio for more details.
Default: false.
stdio <Array> | <string> See child_process.spawn()'s stdio.
When this option is provided, it overrides silent. If the array variant
is used, it must contain exactly one item with value 'ipc' or an error
will be thrown. For instance [0, 1, 2, 'ipc'].
uid <number> Sets the user identity of the process (see setuid(2)).
windowsVerbatimArguments <boolean> No quoting or escaping of arguments is
done on Windows. Ignored on Unix. Default: false.
timeout <number> In milliseconds the maximum amount of time the process
is allowed to run. Default: undefined.


Returns: <ChildProcess>

The child_process.fork() method is a special case of
child_process.spawn() used specifically to spawn new Node.js processes.
Like child_process.spawn(), a ChildProcess object is returned. The
returned ChildProcess will have an additional communication channel
built-in that allows messages to be passed back and forth between the parent and
child. See subprocess.send() for details.
Keep in mind that spawned Node.js child processes are
independent of the parent with exception of the IPC communication channel
that is established between the two. Each process has its own memory, with
their own V8 instances. Because of the additional resource allocations
required, spawning a large number of child Node.js processes is not
recommended.
By default, child_process.fork() will spawn new Node.js instances using the
process.execPath of the parent process. The execPath property in the
options object allows for an alternative execution path to be used.
Node.js processes launched with a custom execPath will communicate with the
parent process using the file descriptor (fd) identified using the
environment variable NODE_CHANNEL_FD on the child process.
Unlike the fork(2) POSIX system call, child_process.fork() does not clone the
current process.
The shell option available in child_process.spawn() is not supported by
child_process.fork() and will be ignored if set.
If the signal option is enabled, calling .abort() on the corresponding
AbortController is similar to calling .kill() on the child process except
the error passed to the callback will be an AbortError:

const { fork } = require('node:child_process');
const process = require('node:process');

if (process.argv[2] === 'child') {
  setTimeout(() => {
    console.log(`Hello from ${process.argv[2]}!`);
  }, 1_000);
} else {
  const controller = new AbortController();
  const { signal } = controller;
  const child = fork(__filename, ['child'], { signal });
  child.on('error', (err) => {
    // This will be called with err being an AbortError if the controller aborts
  });
  controller.abort(); // Stops the child process
}import { fork } from 'node:child_process';
import process from 'node:process';

if (process.argv[2] === 'child') {
  setTimeout(() => {
    console.log(`Hello from ${process.argv[2]}!`);
  }, 1_000);
} else {
  const controller = new AbortController();
  const { signal } = controller;
  const child = fork(import.meta.url, ['child'], { signal });
  child.on('error', (err) => {
    // This will be called with err being an AbortError if the controller aborts
  });
  controller.abort(); // Stops the child process
}copy

child_process.spawn(command[, args][, options])#

History

VersionChanges
v23.11.0
Passing args when shell is set to true is deprecated.
v16.4.0, v14.18.0
The cwd option can be a WHATWG URL object using file: protocol.
v15.13.0, v14.18.0
timeout was added.
v15.11.0, v14.18.0
killSignal for AbortSignal was added.
v15.5.0, v14.17.0
AbortSignal support was added.
v13.2.0, v12.16.0
The serialization option is supported now.
v8.8.0
The windowsHide option is supported now.
v6.4.0
The argv0 option is supported now.
v5.7.0
The shell option is supported now.
v0.1.90
Added in: v0.1.90




command <string> The command to run.
args <string[]> List of string arguments.
options <Object>

cwd <string> | <URL> Current working directory of the child process.
env <Object> Environment key-value pairs. Default: process.env.
argv0 <string> Explicitly set the value of argv[0] sent to the child
process. This will be set to command if not specified.
stdio <Array> | <string> Child's stdio configuration (see
options.stdio).
detached <boolean> Prepare child process to run independently of
its parent process. Specific behavior depends on the platform (see
options.detached).
uid <number> Sets the user identity of the process (see setuid(2)).
gid <number> Sets the group identity of the process (see setgid(2)).
serialization <string> Specify the kind of serialization used for sending
messages between processes. Possible values are 'json' and 'advanced'.
See Advanced serialization for more details. Default: 'json'.
shell <boolean> | <string> If true, runs command inside of a shell. Uses
'/bin/sh' on Unix, and process.env.ComSpec on Windows. A different
shell can be specified as a string. See Shell requirements and
Default Windows shell. Default: false (no shell).
windowsVerbatimArguments <boolean> No quoting or escaping of arguments is
done on Windows. Ignored on Unix. This is set to true automatically
when shell is specified and is CMD. Default: false.
windowsHide <boolean> Hide the subprocess console window that would
normally be created on Windows systems. Default: false.
signal <AbortSignal> allows aborting the child process using an
AbortSignal.
timeout <number> In milliseconds the maximum amount of time the process
is allowed to run. Default: undefined.
killSignal <string> | <integer> The signal value to be used when the spawned
process will be killed by timeout or abort signal. Default: 'SIGTERM'.


Returns: <ChildProcess>

The child_process.spawn() method spawns a new process using the given
command, with command-line arguments in args. If omitted, args defaults
to an empty array.
If the shell option is enabled, do not pass unsanitized user input to this
function. Any input containing shell metacharacters may be used to trigger
arbitrary command execution.
A third argument may be used to specify additional options, with these defaults:
const defaults = {
  cwd: undefined,
  env: process.env,
}; copy
Use cwd to specify the working directory from which the process is spawned.
If not given, the default is to inherit the current working directory. If given,
but the path does not exist, the child process emits an ENOENT error
and exits immediately. ENOENT is also emitted when the command
does not exist.
Use env to specify environment variables that will be visible to the new
process, the default is process.env.
undefined values in env will be ignored.
Example of running ls -lh /usr, capturing stdout, stderr, and the
exit code:

const { spawn } = require('node:child_process');
const ls = spawn('ls', ['-lh', '/usr']);

ls.stdout.on('data', (data) => {
  console.log(`stdout: ${data}`);
});

ls.stderr.on('data', (data) => {
  console.error(`stderr: ${data}`);
});

ls.on('close', (code) => {
  console.log(`child process exited with code ${code}`);
});import { spawn } from 'node:child_process';
const ls = spawn('ls', ['-lh', '/usr']);

ls.stdout.on('data', (data) => {
  console.log(`stdout: ${data}`);
});

ls.stderr.on('data', (data) => {
  console.error(`stderr: ${data}`);
});

ls.on('close', (code) => {
  console.log(`child process exited with code ${code}`);
});copy
Example: A very elaborate way to run ps ax | grep ssh

const { spawn } = require('node:child_process');
const ps = spawn('ps', ['ax']);
const grep = spawn('grep', ['ssh']);

ps.stdout.on('data', (data) => {
  grep.stdin.write(data);
});

ps.stderr.on('data', (data) => {
  console.error(`ps stderr: ${data}`);
});

ps.on('close', (code) => {
  if (code !== 0) {
    console.log(`ps process exited with code ${code}`);
  }
  grep.stdin.end();
});

grep.stdout.on('data', (data) => {
  console.log(data.toString());
});

grep.stderr.on('data', (data) => {
  console.error(`grep stderr: ${data}`);
});

grep.on('close', (code) => {
  if (code !== 0) {
    console.log(`grep process exited with code ${code}`);
  }
});import { spawn } from 'node:child_process';
const ps = spawn('ps', ['ax']);
const grep = spawn('grep', ['ssh']);

ps.stdout.on('data', (data) => {
  grep.stdin.write(data);
});

ps.stderr.on('data', (data) => {
  console.error(`ps stderr: ${data}`);
});

ps.on('close', (code) => {
  if (code !== 0) {
    console.log(`ps process exited with code ${code}`);
  }
  grep.stdin.end();
});

grep.stdout.on('data', (data) => {
  console.log(data.toString());
});

grep.stderr.on('data', (data) => {
  console.error(`grep stderr: ${data}`);
});

grep.on('close', (code) => {
  if (code !== 0) {
    console.log(`grep process exited with code ${code}`);
  }
});copy
Example of checking for failed spawn:

const { spawn } = require('node:child_process');
const subprocess = spawn('bad_command');

subprocess.on('error', (err) => {
  console.error('Failed to start subprocess.');
});import { spawn } from 'node:child_process';
const subprocess = spawn('bad_command');

subprocess.on('error', (err) => {
  console.error('Failed to start subprocess.');
});copy
Certain platforms (macOS, Linux) will use the value of argv[0] for the process
title while others (Windows, SunOS) will use command.
Node.js overwrites argv[0] with process.execPath on startup, so
process.argv[0] in a Node.js child process will not match the argv0
parameter passed to spawn from the parent. Retrieve it with the
process.argv0 property instead.
If the signal option is enabled, calling .abort() on the corresponding
AbortController is similar to calling .kill() on the child process except
the error passed to the callback will be an AbortError:

const { spawn } = require('node:child_process');
const controller = new AbortController();
const { signal } = controller;
const grep = spawn('grep', ['ssh'], { signal });
grep.on('error', (err) => {
  // This will be called with err being an AbortError if the controller aborts
});
controller.abort(); // Stops the child processimport { spawn } from 'node:child_process';
const controller = new AbortController();
const { signal } = controller;
const grep = spawn('grep', ['ssh'], { signal });
grep.on('error', (err) => {
  // This will be called with err being an AbortError if the controller aborts
});
controller.abort(); // Stops the child processcopy

options.detached#

Added in: v0.7.10

On Windows, setting options.detached to true makes it possible for the
child process to continue running after the parent exits. The child process
will have its own console window. Once enabled for a child process,
it cannot be disabled.
On non-Windows platforms, if options.detached is set to true, the child
process will be made the leader of a new process group and session. Child
processes may continue running after the parent exits regardless of whether
they are detached or not. See setsid(2) for more information.
By default, the parent will wait for the detached child process to exit.
To prevent the parent process from waiting for a given subprocess to exit, use
the subprocess.unref() method. Doing so will cause the parent process' event
loop to not include the child process in its reference count, allowing the
parent process to exit independently of the child process, unless there is an established
IPC channel between the child and the parent processes.
When using the detached option to start a long-running process, the process
will not stay running in the background after the parent exits unless it is
provided with a stdio configuration that is not connected to the parent.
If the parent process' stdio is inherited, the child process will remain attached
to the controlling terminal.
Example of a long-running process, by detaching and also ignoring its parent
stdio file descriptors, in order to ignore the parent's termination:

const { spawn } = require('node:child_process');
const process = require('node:process');

const subprocess = spawn(process.argv[0], ['child_program.js'], {
  detached: true,
  stdio: 'ignore',
});

subprocess.unref();import { spawn } from 'node:child_process';
import process from 'node:process';

const subprocess = spawn(process.argv[0], ['child_program.js'], {
  detached: true,
  stdio: 'ignore',
});

subprocess.unref();copy
Alternatively one can redirect the child process' output into files:

const { openSync } = require('node:fs');
const { spawn } = require('node:child_process');
const out = openSync('./out.log', 'a');
const err = openSync('./out.log', 'a');

const subprocess = spawn('prg', [], {
  detached: true,
  stdio: [ 'ignore', out, err ],
});

subprocess.unref();import { openSync } from 'node:fs';
import { spawn } from 'node:child_process';
const out = openSync('./out.log', 'a');
const err = openSync('./out.log', 'a');

const subprocess = spawn('prg', [], {
  detached: true,
  stdio: [ 'ignore', out, err ],
});

subprocess.unref();copy

options.stdio#

History

VersionChanges
v15.6.0, v14.18.0
Added the overlapped stdio flag.
v3.3.1
The value 0 is now accepted as a file descriptor.
v0.7.10
Added in: v0.7.10



The options.stdio option is used to configure the pipes that are established
between the parent and child process. By default, the child's stdin, stdout,
and stderr are redirected to corresponding subprocess.stdin,
subprocess.stdout, and subprocess.stderr streams on the
ChildProcess object. This is equivalent to setting the options.stdio
equal to ['pipe', 'pipe', 'pipe'].
For convenience, options.stdio may be one of the following strings:

'pipe': equivalent to ['pipe', 'pipe', 'pipe'] (the default)
'overlapped': equivalent to ['overlapped', 'overlapped', 'overlapped']
'ignore': equivalent to ['ignore', 'ignore', 'ignore']
'inherit': equivalent to ['inherit', 'inherit', 'inherit'] or [0, 1, 2]

Otherwise, the value of options.stdio is an array where each index corresponds
to an fd in the child. The fds 0, 1, and 2 correspond to stdin, stdout,
and stderr, respectively. Additional fds can be specified to create additional
pipes between the parent and child. The value is one of the following:


'pipe': Create a pipe between the child process and the parent process.
The parent end of the pipe is exposed to the parent as a property on the
child_process object as subprocess.stdio[fd]. Pipes
created for fds 0, 1, and 2 are also available as subprocess.stdin,
subprocess.stdout and subprocess.stderr, respectively.
These are not actual Unix pipes and therefore the child process
can not use them by their descriptor files,
e.g. /dev/fd/2 or /dev/stdout.


'overlapped': Same as 'pipe' except that the FILE_FLAG_OVERLAPPED flag
is set on the handle. This is necessary for overlapped I/O on the child
process's stdio handles. See the
docs
for more details. This is exactly the same as 'pipe' on non-Windows
systems.


'ipc': Create an IPC channel for passing messages/file descriptors
between parent and child. A ChildProcess may have at most one IPC
stdio file descriptor. Setting this option enables the
subprocess.send() method. If the child process is a Node.js instance,
the presence of an IPC channel will enable process.send() and
process.disconnect() methods, as well as 'disconnect' and
'message' events within the child process.
Accessing the IPC channel fd in any way other than process.send()
or using the IPC channel with a child process that is not a Node.js instance
is not supported.


'ignore': Instructs Node.js to ignore the fd in the child. While Node.js
will always open fds 0, 1, and 2 for the processes it spawns, setting the fd
to 'ignore' will cause Node.js to open /dev/null and attach it to the
child's fd.


'inherit': Pass through the corresponding stdio stream to/from the
parent process. In the first three positions, this is equivalent to
process.stdin, process.stdout, and process.stderr, respectively. In
any other position, equivalent to 'ignore'.


<Stream> object: Share a readable or writable stream that refers to a tty,
file, socket, or a pipe with the child process. The stream's underlying
file descriptor is duplicated in the child process to the fd that
corresponds to the index in the stdio array. The stream must have an
underlying descriptor (file streams do not start until the 'open' event has
occurred).
NOTE: While it is technically possible to pass stdin as a writable or
stdout/stderr as readable, it is not recommended.
Readable and writable streams are designed with distinct behaviors, and using
them incorrectly (e.g., passing a readable stream where a writable stream is
expected) can lead to unexpected results or errors. This practice is discouraged
as it may result in undefined behavior or dropped callbacks if the stream
encounters errors. Always ensure that stdin is used as writable and
stdout/stderr as readable to maintain the intended flow of data between
the parent and child processes.


Positive integer: The integer value is interpreted as a file descriptor
that is open in the parent process. It is shared with the child
process, similar to how <Stream> objects can be shared. Passing sockets
is not supported on Windows.


null, undefined: Use default value. For stdio fds 0, 1, and 2 (in other
words, stdin, stdout, and stderr) a pipe is created. For fd 3 and up, the
default is 'ignore'.



const { spawn } = require('node:child_process');
const process = require('node:process');

// Child will use parent's stdios.
spawn('prg', [], { stdio: 'inherit' });

// Spawn child sharing only stderr.
spawn('prg', [], { stdio: ['pipe', 'pipe', process.stderr] });

// Open an extra fd=4, to interact with programs presenting a
// startd-style interface.
spawn('prg', [], { stdio: ['pipe', null, null, null, 'pipe'] });import { spawn } from 'node:child_process';
import process from 'node:process';

// Child will use parent's stdios.
spawn('prg', [], { stdio: 'inherit' });

// Spawn child sharing only stderr.
spawn('prg', [], { stdio: ['pipe', 'pipe', process.stderr] });

// Open an extra fd=4, to interact with programs presenting a
// startd-style interface.
spawn('prg', [], { stdio: ['pipe', null, null, null, 'pipe'] });copy
It is worth noting that when an IPC channel is established between the
parent and child processes, and the child process is a Node.js instance,
the child process is launched with the IPC channel unreferenced (using
unref()) until the child process registers an event handler for the
'disconnect' event or the 'message' event. This allows the
child process to exit normally without the process being held open by the
open IPC channel.
See also: child_process.exec() and child_process.fork().

Synchronous process creation#
The child_process.spawnSync(), child_process.execSync(), and
child_process.execFileSync() methods are synchronous and will block the
Node.js event loop, pausing execution of any additional code until the spawned
process exits.
Blocking calls like these are mostly useful for simplifying general-purpose
scripting tasks and for simplifying the loading/processing of application
configuration at startup.

child_process.execFileSync(file[, args][, options])#

History

VersionChanges
v16.4.0, v14.18.0
The cwd option can be a WHATWG URL object using file: protocol.
v10.10.0
The input option can now be any TypedArray or a DataView.
v8.8.0
The windowsHide option is supported now.
v8.0.0
The input option can now be a Uint8Array.
v6.2.1, v4.5.0
The encoding option can now explicitly be set to buffer.
v0.11.12
Added in: v0.11.12




file <string> The name or path of the executable file to run.
args <string[]> List of string arguments.
options <Object>

cwd <string> | <URL> Current working directory of the child process.
input <string> | <Buffer> | <TypedArray> | <DataView> The value which will be passed
as stdin to the spawned process. If stdio[0] is set to 'pipe', Supplying
this value will override stdio[0].
stdio <string> | <Array> Child's stdio configuration.
See child_process.spawn()'s stdio. stderr by default will
be output to the parent process' stderr unless stdio is specified.
Default: 'pipe'.
env <Object> Environment key-value pairs. Default: process.env.
uid <number> Sets the user identity of the process (see setuid(2)).
gid <number> Sets the group identity of the process (see setgid(2)).
timeout <number> In milliseconds the maximum amount of time the process
is allowed to run. Default: undefined.
killSignal <string> | <integer> The signal value to be used when the spawned
process will be killed. Default: 'SIGTERM'.
maxBuffer <number> Largest amount of data in bytes allowed on stdout or
stderr. If exceeded, the child process is terminated. See caveat at
maxBuffer and Unicode. Default: 1024 * 1024.
encoding <string> The encoding used for all stdio inputs and outputs.
Default: 'buffer'.
windowsHide <boolean> Hide the subprocess console window that would
normally be created on Windows systems. Default: false.
shell <boolean> | <string> If true, runs command inside of a shell. Uses
'/bin/sh' on Unix, and process.env.ComSpec on Windows. A different
shell can be specified as a string. See Shell requirements and
Default Windows shell. Default: false (no shell).


Returns: <Buffer> | <string> The stdout from the command.

The child_process.execFileSync() method is generally identical to
child_process.execFile() with the exception that the method will not
return until the child process has fully closed. When a timeout has been
encountered and killSignal is sent, the method won't return until the process
has completely exited.
If the child process intercepts and handles the SIGTERM signal and
does not exit, the parent process will still wait until the child process has
exited.
If the process times out or has a non-zero exit code, this method will throw an
Error that will include the full result of the underlying
child_process.spawnSync().
If the shell option is enabled, do not pass unsanitized user input to this
function. Any input containing shell metacharacters may be used to trigger
arbitrary command execution.

const { execFileSync } = require('node:child_process');

try {
  const stdout = execFileSync('my-script.sh', ['my-arg'], {
    // Capture stdout and stderr from child process. Overrides the
    // default behavior of streaming child stderr to the parent stderr
    stdio: 'pipe',

    // Use utf8 encoding for stdio pipes
    encoding: 'utf8',
  });

  console.log(stdout);
} catch (err) {
  if (err.code) {
    // Spawning child process failed
    console.error(err.code);
  } else {
    // Child was spawned but exited with non-zero exit code
    // Error contains any stdout and stderr from the child
    const { stdout, stderr } = err;

    console.error({ stdout, stderr });
  }
}import { execFileSync } from 'node:child_process';

try {
  const stdout = execFileSync('my-script.sh', ['my-arg'], {
    // Capture stdout and stderr from child process. Overrides the
    // default behavior of streaming child stderr to the parent stderr
    stdio: 'pipe',

    // Use utf8 encoding for stdio pipes
    encoding: 'utf8',
  });

  console.log(stdout);
} catch (err) {
  if (err.code) {
    // Spawning child process failed
    console.error(err.code);
  } else {
    // Child was spawned but exited with non-zero exit code
    // Error contains any stdout and stderr from the child
    const { stdout, stderr } = err;

    console.error({ stdout, stderr });
  }
}copy

child_process.execSync(command[, options])#

History

VersionChanges
v16.4.0, v14.18.0
The cwd option can be a WHATWG URL object using file: protocol.
v10.10.0
The input option can now be any TypedArray or a DataView.
v8.8.0
The windowsHide option is supported now.
v8.0.0
The input option can now be a Uint8Array.
v0.11.12
Added in: v0.11.12




command <string> The command to run.
options <Object>

cwd <string> | <URL> Current working directory of the child process.
input <string> | <Buffer> | <TypedArray> | <DataView> The value which will be passed
as stdin to the spawned process. If stdio[0] is set to 'pipe', Supplying
this value will override stdio[0].
stdio <string> | <Array> Child's stdio configuration.
See child_process.spawn()'s stdio. stderr by default will
be output to the parent process' stderr unless stdio is specified.
Default: 'pipe'.
env <Object> Environment key-value pairs. Default: process.env.
shell <string> Shell to execute the command with. See
Shell requirements and Default Windows shell. Default:
'/bin/sh' on Unix, process.env.ComSpec on Windows.
uid <number> Sets the user identity of the process. (See setuid(2)).
gid <number> Sets the group identity of the process. (See setgid(2)).
timeout <number> In milliseconds the maximum amount of time the process
is allowed to run. Default: undefined.
killSignal <string> | <integer> The signal value to be used when the spawned
process will be killed. Default: 'SIGTERM'.
maxBuffer <number> Largest amount of data in bytes allowed on stdout or
stderr. If exceeded, the child process is terminated and any output is
truncated. See caveat at maxBuffer and Unicode.
Default: 1024 * 1024.
encoding <string> The encoding used for all stdio inputs and outputs.
Default: 'buffer'.
windowsHide <boolean> Hide the subprocess console window that would
normally be created on Windows systems. Default: false.


Returns: <Buffer> | <string> The stdout from the command.

The child_process.execSync() method is generally identical to
child_process.exec() with the exception that the method will not return
until the child process has fully closed. When a timeout has been encountered
and killSignal is sent, the method won't return until the process has
completely exited. If the child process intercepts and handles the SIGTERM
signal and doesn't exit, the parent process will wait until the child process
has exited.
If the process times out or has a non-zero exit code, this method will throw.
The Error object will contain the entire result from
child_process.spawnSync().
Never pass unsanitized user input to this function. Any input containing shell
metacharacters may be used to trigger arbitrary command execution.

child_process.spawnSync(command[, args][, options])#

History

VersionChanges
v16.4.0, v14.18.0
The cwd option can be a WHATWG URL object using file: protocol.
v10.10.0
The input option can now be any TypedArray or a DataView.
v8.8.0
The windowsHide option is supported now.
v8.0.0
The input option can now be a Uint8Array.
v5.7.0
The shell option is supported now.
v6.2.1, v4.5.0
The encoding option can now explicitly be set to buffer.
v0.11.12
Added in: v0.11.12




command <string> The command to run.
args <string[]> List of string arguments.
options <Object>

cwd <string> | <URL> Current working directory of the child process.
input <string> | <Buffer> | <TypedArray> | <DataView> The value which will be passed
as stdin to the spawned process. If stdio[0] is set to 'pipe', Supplying
this value will override stdio[0].
argv0 <string> Explicitly set the value of argv[0] sent to the child
process. This will be set to command if not specified.
stdio <string> | <Array> Child's stdio configuration.
See child_process.spawn()'s stdio. Default: 'pipe'.
env <Object> Environment key-value pairs. Default: process.env.
uid <number> Sets the user identity of the process (see setuid(2)).
gid <number> Sets the group identity of the process (see setgid(2)).
timeout <number> In milliseconds the maximum amount of time the process
is allowed to run. Default: undefined.
killSignal <string> | <integer> The signal value to be used when the spawned
process will be killed. Default: 'SIGTERM'.
maxBuffer <number> Largest amount of data in bytes allowed on stdout or
stderr. If exceeded, the child process is terminated and any output is
truncated. See caveat at maxBuffer and Unicode.
Default: 1024 * 1024.
encoding <string> The encoding used for all stdio inputs and outputs.
Default: 'buffer'.
shell <boolean> | <string> If true, runs command inside of a shell. Uses
'/bin/sh' on Unix, and process.env.ComSpec on Windows. A different
shell can be specified as a string. See Shell requirements and
Default Windows shell. Default: false (no shell).
windowsVerbatimArguments <boolean> No quoting or escaping of arguments is
done on Windows. Ignored on Unix. This is set to true automatically
when shell is specified and is CMD. Default: false.
windowsHide <boolean> Hide the subprocess console window that would
normally be created on Windows systems. Default: false.


Returns: <Object>

pid <number> Pid of the child process.
output <Array> Array of results from stdio output.
stdout <Buffer> | <string> The contents of output[1].
stderr <Buffer> | <string> The contents of output[2].
status <number> | <null> The exit code of the subprocess, or null if the
subprocess terminated due to a signal.
signal <string> | <null> The signal used to kill the subprocess, or null if
the subprocess did not terminate due to a signal.
error <Error> The error object if the child process failed or timed out.



The child_process.spawnSync() method is generally identical to
child_process.spawn() with the exception that the function will not return
until the child process has fully closed. When a timeout has been encountered
and killSignal is sent, the method won't return until the process has
completely exited. If the process intercepts and handles the SIGTERM signal
and doesn't exit, the parent process will wait until the child process has
exited.
If the shell option is enabled, do not pass unsanitized user input to this
function. Any input containing shell metacharacters may be used to trigger
arbitrary command execution.

Class: ChildProcess#

Added in: v2.2.0


Extends: <EventEmitter>

Instances of the ChildProcess represent spawned child processes.
Instances of ChildProcess are not intended to be created directly. Rather,
use the child_process.spawn(), child_process.exec(),
child_process.execFile(), or child_process.fork() methods to create
instances of ChildProcess.

Event: 'close'#

Added in: v0.7.7


code <number> The exit code if the child process exited on its own.
signal <string> The signal by which the child process was terminated.

The 'close' event is emitted after a process has ended and the stdio
streams of a child process have been closed. This is distinct from the
'exit' event, since multiple processes might share the same stdio
streams. The 'close' event will always emit after 'exit' was
already emitted, or 'error' if the child process failed to spawn.

const { spawn } = require('node:child_process');
const ls = spawn('ls', ['-lh', '/usr']);

ls.stdout.on('data', (data) => {
  console.log(`stdout: ${data}`);
});

ls.on('close', (code) => {
  console.log(`child process close all stdio with code ${code}`);
});

ls.on('exit', (code) => {
  console.log(`child process exited with code ${code}`);
});import { spawn } from 'node:child_process';
const ls = spawn('ls', ['-lh', '/usr']);

ls.stdout.on('data', (data) => {
  console.log(`stdout: ${data}`);
});

ls.on('close', (code) => {
  console.log(`child process close all stdio with code ${code}`);
});

ls.on('exit', (code) => {
  console.log(`child process exited with code ${code}`);
});copy

Event: 'disconnect'#

Added in: v0.7.2

The 'disconnect' event is emitted after calling the
subprocess.disconnect() method in parent process or
process.disconnect() in child process. After disconnecting it is no longer
possible to send or receive messages, and the subprocess.connected
property is false.

Event: 'error'#

err <Error> The error.

The 'error' event is emitted whenever:

The process could not be spawned.
The process could not be killed.
Sending a message to the child process failed.
The child process was aborted via the signal option.

The 'exit' event may or may not fire after an error has occurred. When
listening to both the 'exit' and 'error' events, guard
against accidentally invoking handler functions multiple times.
See also subprocess.kill() and subprocess.send().

Event: 'exit'#

Added in: v0.1.90


code <number> The exit code if the child process exited on its own.
signal <string> The signal by which the child process was terminated.

The 'exit' event is emitted after the child process ends. If the process
exited, code is the final exit code of the process, otherwise null. If the
process terminated due to receipt of a signal, signal is the string name of
the signal, otherwise null. One of the two will always be non-null.
When the 'exit' event is triggered, child process stdio streams might still be
open.
Node.js establishes signal handlers for SIGINT and SIGTERM and Node.js
processes will not terminate immediately due to receipt of those signals.
Rather, Node.js will perform a sequence of cleanup actions and then will
re-raise the handled signal.
See waitpid(2).

Event: 'message'#

Added in: v0.5.9


message <Object> A parsed JSON object or primitive value.
sendHandle <Handle> | <undefined> undefined or a net.Socket,
net.Server, or dgram.Socket object.

The 'message' event is triggered when a child process uses
process.send() to send messages.
The message goes through serialization and parsing. The resulting
message might not be the same as what is originally sent.
If the serialization option was set to 'advanced' used when spawning the
child process, the message argument can contain data that JSON is not able
to represent.
See Advanced serialization for more details.

Event: 'spawn'#

Added in: v15.1.0, v14.17.0

The 'spawn' event is emitted once the child process has spawned successfully.
If the child process does not spawn successfully, the 'spawn' event is not
emitted and the 'error' event is emitted instead.
If emitted, the 'spawn' event comes before all other events and before any
data is received via stdout or stderr.
The 'spawn' event will fire regardless of whether an error occurs within
the spawned process. For example, if bash some-command spawns successfully,
the 'spawn' event will fire, though bash may fail to spawn some-command.
This caveat also applies when using { shell: true }.

subprocess.channel#

History

VersionChanges
v14.0.0
The object no longer accidentally exposes native C++ bindings.
v7.1.0
Added in: v7.1.0




<Object> A pipe representing the IPC channel to the child process.

The subprocess.channel property is a reference to the child's IPC channel. If
no IPC channel exists, this property is undefined.

subprocess.channel.ref()#

Added in: v7.1.0

This method makes the IPC channel keep the event loop of the parent process
running if .unref() has been called before.

subprocess.channel.unref()#

Added in: v7.1.0

This method makes the IPC channel not keep the event loop of the parent process
running, and lets it finish even while the channel is open.

subprocess.connected#

Added in: v0.7.2


<boolean> Set to false after subprocess.disconnect() is called.

The subprocess.connected property indicates whether it is still possible to
send and receive messages from a child process. When subprocess.connected is
false, it is no longer possible to send or receive messages.

subprocess.disconnect()#

Added in: v0.7.2

Closes the IPC channel between parent and child processes, allowing the child
process to exit gracefully once there are no other connections keeping it alive.
After calling this method the subprocess.connected and
process.connected properties in both the parent and child processes
(respectively) will be set to false, and it will be no longer possible
to pass messages between the processes.
The 'disconnect' event will be emitted when there are no messages in the
process of being received. This will most often be triggered immediately after
calling subprocess.disconnect().
When the child process is a Node.js instance (e.g. spawned using
child_process.fork()), the process.disconnect() method can be invoked
within the child process to close the IPC channel as well.

subprocess.exitCode#

<integer>

The subprocess.exitCode property indicates the exit code of the child process.
If the child process is still running, the field will be null.

subprocess.kill([signal])#

Added in: v0.1.90


signal <number> | <string>
Returns: <boolean>

The subprocess.kill() method sends a signal to the child process. If no
argument is given, the process will be sent the 'SIGTERM' signal. See
signal(7) for a list of available signals. This function returns true if
kill(2) succeeds, and false otherwise.

const { spawn } = require('node:child_process');
const grep = spawn('grep', ['ssh']);

grep.on('close', (code, signal) => {
  console.log(
    `child process terminated due to receipt of signal ${signal}`);
});

// Send SIGHUP to process.
grep.kill('SIGHUP');import { spawn } from 'node:child_process';
const grep = spawn('grep', ['ssh']);

grep.on('close', (code, signal) => {
  console.log(
    `child process terminated due to receipt of signal ${signal}`);
});

// Send SIGHUP to process.
grep.kill('SIGHUP');copy
The ChildProcess object may emit an 'error' event if the signal
cannot be delivered. Sending a signal to a child process that has already exited
is not an error but may have unforeseen consequences. Specifically, if the
process identifier (PID) has been reassigned to another process, the signal will
be delivered to that process instead which can have unexpected results.
While the function is called kill, the signal delivered to the child process
may not actually terminate the process.
See kill(2) for reference.
On Windows, where POSIX signals do not exist, the signal argument will be
ignored except for 'SIGKILL', 'SIGTERM', 'SIGINT' and 'SIGQUIT', and the
process will always be killed forcefully and abruptly (similar to 'SIGKILL').
See Signal Events for more details.
On Linux, child processes of child processes will not be terminated
when attempting to kill their parent. This is likely to happen when running a
new process in a shell or with the use of the shell option of ChildProcess:

const { spawn } = require('node:child_process');

const subprocess = spawn(
  'sh',
  [
    '-c',
    `node -e "setInterval(() => {
      console.log(process.pid, 'is alive')
    }, 500);"`,
  ], {
    stdio: ['inherit', 'inherit', 'inherit'],
  },
);

setTimeout(() => {
  subprocess.kill(); // Does not terminate the Node.js process in the shell.
}, 2000);import { spawn } from 'node:child_process';

const subprocess = spawn(
  'sh',
  [
    '-c',
    `node -e "setInterval(() => {
      console.log(process.pid, 'is alive')
    }, 500);"`,
  ], {
    stdio: ['inherit', 'inherit', 'inherit'],
  },
);

setTimeout(() => {
  subprocess.kill(); // Does not terminate the Node.js process in the shell.
}, 2000);copy

subprocess[Symbol.dispose]()#

Added in: v20.5.0, v18.18.0

Stability: 1 - Experimental
Calls subprocess.kill() with 'SIGTERM'.

subprocess.killed#

Added in: v0.5.10


<boolean> Set to true after subprocess.kill() is used to successfully
send a signal to the child process.

The subprocess.killed property indicates whether the child process
successfully received a signal from subprocess.kill(). The killed property
does not indicate that the child process has been terminated.

subprocess.pid#

Added in: v0.1.90


<integer> | <undefined>

Returns the process identifier (PID) of the child process. If the child process
fails to spawn due to errors, then the value is undefined and error is
emitted.

const { spawn } = require('node:child_process');
const grep = spawn('grep', ['ssh']);

console.log(`Spawned child pid: ${grep.pid}`);
grep.stdin.end();import { spawn } from 'node:child_process';
const grep = spawn('grep', ['ssh']);

console.log(`Spawned child pid: ${grep.pid}`);
grep.stdin.end();copy

subprocess.ref()#

Added in: v0.7.10

Calling subprocess.ref() after making a call to subprocess.unref() will
restore the removed reference count for the child process, forcing the parent
process to wait for the child process to exit before exiting itself.

const { spawn } = require('node:child_process');
const process = require('node:process');

const subprocess = spawn(process.argv[0], ['child_program.js'], {
  detached: true,
  stdio: 'ignore',
});

subprocess.unref();
subprocess.ref();import { spawn } from 'node:child_process';
import process from 'node:process';

const subprocess = spawn(process.argv[0], ['child_program.js'], {
  detached: true,
  stdio: 'ignore',
});

subprocess.unref();
subprocess.ref();copy

subprocess.send(message[, sendHandle[, options]][, callback])#

History

VersionChanges
v5.8.0
The options parameter, and the keepOpen option in particular, is supported now.
v5.0.0
This method returns a boolean for flow control now.
v4.0.0
The callback parameter is supported now.
v0.5.9
Added in: v0.5.9




message <Object>
sendHandle <Handle> | <undefined> undefined, or a net.Socket,
net.Server, or dgram.Socket object.
options <Object> The options argument, if present, is an object used to
parameterize the sending of certain types of handles. options supports
the following properties:

keepOpen <boolean> A value that can be used when passing instances of
net.Socket. When true, the socket is kept open in the sending process.
Default: false.


callback <Function>
Returns: <boolean>

When an IPC channel has been established between the parent and child processes
( i.e. when using child_process.fork()), the subprocess.send() method
can be used to send messages to the child process. When the child process is a
Node.js instance, these messages can be received via the 'message' event.
The message goes through serialization and parsing. The resulting
message might not be the same as what is originally sent.
For example, in the parent script:

const { fork } = require('node:child_process');
const forkedProcess = fork(`${__dirname}/sub.js`);

forkedProcess.on('message', (message) => {
  console.log('PARENT got message:', message);
});

// Causes the child to print: CHILD got message: { hello: 'world' }
forkedProcess.send({ hello: 'world' });import { fork } from 'node:child_process';
const forkedProcess = fork(`${import.meta.dirname}/sub.js`);

forkedProcess.on('message', (message) => {
  console.log('PARENT got message:', message);
});

// Causes the child to print: CHILD got message: { hello: 'world' }
forkedProcess.send({ hello: 'world' });copy
And then the child script, 'sub.js' might look like this:
process.on('message', (message) => {
  console.log('CHILD got message:', message);
});

// Causes the parent to print: PARENT got message: { foo: 'bar', baz: null }
process.send({ foo: 'bar', baz: NaN }); copy
Child Node.js processes will have a process.send() method of their own
that allows the child process to send messages back to the parent process.
There is a special case when sending a {cmd: 'NODE_foo'} message. Messages
containing a NODE_ prefix in the cmd property are reserved for use within
Node.js core and will not be emitted in the child's 'message'
event. Rather, such messages are emitted using the
'internalMessage' event and are consumed internally by Node.js.
Applications should avoid using such messages or listening for
'internalMessage' events as it is subject to change without notice.
The optional sendHandle argument that may be passed to subprocess.send() is
for passing a TCP server or socket object to the child process. The child process will
receive the object as the second argument passed to the callback function
registered on the 'message' event. Any data that is received
and buffered in the socket will not be sent to the child. Sending IPC sockets is
not supported on Windows.
The optional callback is a function that is invoked after the message is
sent but before the child process may have received it. The function is called with a
single argument: null on success, or an Error object on failure.
If no callback function is provided and the message cannot be sent, an
'error' event will be emitted by the ChildProcess object. This can
happen, for instance, when the child process has already exited.
subprocess.send() will return false if the channel has closed or when the
backlog of unsent messages exceeds a threshold that makes it unwise to send
more. Otherwise, the method returns true. The callback function can be
used to implement flow control.

Example: sending a server object#
The sendHandle argument can be used, for instance, to pass the handle of
a TCP server object to the child process as illustrated in the example below:

const { fork } = require('node:child_process');
const { createServer } = require('node:net');

const subprocess = fork('subprocess.js');

// Open up the server object and send the handle.
const server = createServer();
server.on('connection', (socket) => {
  socket.end('handled by parent');
});
server.listen(1337, () => {
  subprocess.send('server', server);
});import { fork } from 'node:child_process';
import { createServer } from 'node:net';

const subprocess = fork('subprocess.js');

// Open up the server object and send the handle.
const server = createServer();
server.on('connection', (socket) => {
  socket.end('handled by parent');
});
server.listen(1337, () => {
  subprocess.send('server', server);
});copy
The child process would then receive the server object as:
process.on('message', (m, server) => {
  if (m === 'server') {
    server.on('connection', (socket) => {
      socket.end('handled by child');
    });
  }
}); copy
Once the server is now shared between the parent and child, some connections
can be handled by the parent and some by the child.
While the example above uses a server created using the node:net module,
node:dgram module servers use exactly the same workflow with the exceptions of
listening on a 'message' event instead of 'connection' and using
server.bind() instead of server.listen(). This is, however, only
supported on Unix platforms.

Example: sending a socket object#
Similarly, the sendHandler argument can be used to pass the handle of a
socket to the child process. The example below spawns two children that each
handle connections with "normal" or "special" priority:

const { fork } = require('node:child_process');
const { createServer } = require('node:net');

const normal = fork('subprocess.js', ['normal']);
const special = fork('subprocess.js', ['special']);

// Open up the server and send sockets to child. Use pauseOnConnect to prevent
// the sockets from being read before they are sent to the child process.
const server = createServer({ pauseOnConnect: true });
server.on('connection', (socket) => {

  // If this is special priority...
  if (socket.remoteAddress === '74.125.127.100') {
    special.send('socket', socket);
    return;
  }
  // This is normal priority.
  normal.send('socket', socket);
});
server.listen(1337);import { fork } from 'node:child_process';
import { createServer } from 'node:net';

const normal = fork('subprocess.js', ['normal']);
const special = fork('subprocess.js', ['special']);

// Open up the server and send sockets to child. Use pauseOnConnect to prevent
// the sockets from being read before they are sent to the child process.
const server = createServer({ pauseOnConnect: true });
server.on('connection', (socket) => {

  // If this is special priority...
  if (socket.remoteAddress === '74.125.127.100') {
    special.send('socket', socket);
    return;
  }
  // This is normal priority.
  normal.send('socket', socket);
});
server.listen(1337);copy
The subprocess.js would receive the socket handle as the second argument
passed to the event callback function:
process.on('message', (m, socket) => {
  if (m === 'socket') {
    if (socket) {
      // Check that the client socket exists.
      // It is possible for the socket to be closed between the time it is
      // sent and the time it is received in the child process.
      socket.end(`Request handled with ${process.argv[2]} priority`);
    }
  }
}); copy
Do not use .maxConnections on a socket that has been passed to a subprocess.
The parent cannot track when the socket is destroyed.
Any 'message' handlers in the subprocess should verify that socket exists,
as the connection may have been closed during the time it takes to send the
connection to the child.

subprocess.signalCode#

<string> | <null>

The subprocess.signalCode property indicates the signal received by
the child process if any, else null.

subprocess.spawnargs#

<Array>

The subprocess.spawnargs property represents the full list of command-line
arguments the child process was launched with.

subprocess.spawnfile#

<string>

The subprocess.spawnfile property indicates the executable file name of
the child process that is launched.
For child_process.fork(), its value will be equal to
process.execPath.
For child_process.spawn(), its value will be the name of
the executable file.
For child_process.exec(),  its value will be the name of the shell
in which the child process is launched.

subprocess.stderr#

Added in: v0.1.90


<stream.Readable> | <null> | <undefined>

A Readable Stream that represents the child process's stderr.
If the child process was spawned with stdio[2] set to anything other than 'pipe',
then this will be null.
subprocess.stderr is an alias for subprocess.stdio[2]. Both properties will
refer to the same value.
The subprocess.stderr property can be null or undefined
if the child process could not be successfully spawned.

subprocess.stdin#

Added in: v0.1.90


<stream.Writable> | <null> | <undefined>

A Writable Stream that represents the child process's stdin.
If a child process waits to read all of its input, the child process will not continue
until this stream has been closed via end().
If the child process was spawned with stdio[0] set to anything other than 'pipe',
then this will be null.
subprocess.stdin is an alias for subprocess.stdio[0]. Both properties will
refer to the same value.
The subprocess.stdin property can be null or undefined
if the child process could not be successfully spawned.

subprocess.stdio#

Added in: v0.7.10


<Array>

A sparse array of pipes to the child process, corresponding with positions in
the stdio option passed to child_process.spawn() that have been set
to the value 'pipe'. subprocess.stdio[0], subprocess.stdio[1], and
subprocess.stdio[2] are also available as subprocess.stdin,
subprocess.stdout, and subprocess.stderr, respectively.
In the following example, only the child's fd 1 (stdout) is configured as a
pipe, so only the parent's subprocess.stdio[1] is a stream, all other values
in the array are null.

const assert = require('node:assert');
const fs = require('node:fs');
const child_process = require('node:child_process');

const subprocess = child_process.spawn('ls', {
  stdio: [
    0, // Use parent's stdin for child.
    'pipe', // Pipe child's stdout to parent.
    fs.openSync('err.out', 'w'), // Direct child's stderr to a file.
  ],
});

assert.strictEqual(subprocess.stdio[0], null);
assert.strictEqual(subprocess.stdio[0], subprocess.stdin);

assert(subprocess.stdout);
assert.strictEqual(subprocess.stdio[1], subprocess.stdout);

assert.strictEqual(subprocess.stdio[2], null);
assert.strictEqual(subprocess.stdio[2], subprocess.stderr);import assert from 'node:assert';
import fs from 'node:fs';
import child_process from 'node:child_process';

const subprocess = child_process.spawn('ls', {
  stdio: [
    0, // Use parent's stdin for child.
    'pipe', // Pipe child's stdout to parent.
    fs.openSync('err.out', 'w'), // Direct child's stderr to a file.
  ],
});

assert.strictEqual(subprocess.stdio[0], null);
assert.strictEqual(subprocess.stdio[0], subprocess.stdin);

assert(subprocess.stdout);
assert.strictEqual(subprocess.stdio[1], subprocess.stdout);

assert.strictEqual(subprocess.stdio[2], null);
assert.strictEqual(subprocess.stdio[2], subprocess.stderr);copy
The subprocess.stdio property can be undefined if the child process could
not be successfully spawned.

subprocess.stdout#

Added in: v0.1.90


<stream.Readable> | <null> | <undefined>

A Readable Stream that represents the child process's stdout.
If the child process was spawned with stdio[1] set to anything other than 'pipe',
then this will be null.
subprocess.stdout is an alias for subprocess.stdio[1]. Both properties will
refer to the same value.

const { spawn } = require('node:child_process');

const subprocess = spawn('ls');

subprocess.stdout.on('data', (data) => {
  console.log(`Received chunk ${data}`);
});import { spawn } from 'node:child_process';

const subprocess = spawn('ls');

subprocess.stdout.on('data', (data) => {
  console.log(`Received chunk ${data}`);
});copy
The subprocess.stdout property can be null or undefined
if the child process could not be successfully spawned.

subprocess.unref()#

Added in: v0.7.10

By default, the parent process will wait for the detached child process to exit.
To prevent the parent process from waiting for a given subprocess to exit, use the
subprocess.unref() method. Doing so will cause the parent's event loop to not
include the child process in its reference count, allowing the parent to exit
independently of the child, unless there is an established IPC channel between
the child and the parent processes.

const { spawn } = require('node:child_process');
const process = require('node:process');

const subprocess = spawn(process.argv[0], ['child_program.js'], {
  detached: true,
  stdio: 'ignore',
});

subprocess.unref();import { spawn } from 'node:child_process';
import process from 'node:process';

const subprocess = spawn(process.argv[0], ['child_program.js'], {
  detached: true,
  stdio: 'ignore',
});

subprocess.unref();copy

maxBuffer and Unicode#
The maxBuffer option specifies the largest number of bytes allowed on stdout
or stderr. If this value is exceeded, then the child process is terminated.
This impacts output that includes multibyte character encodings such as UTF-8 or
UTF-16. For instance, console.log('中文测试') will send 13 UTF-8 encoded bytes
to stdout although there are only 4 characters.
Shell requirements#
The shell should understand the -c switch. If the shell is 'cmd.exe', it
should understand the /d /s /c switches and command-line parsing should be
compatible.
Default Windows shell#
Although Microsoft specifies %COMSPEC% must contain the path to
'cmd.exe' in the root environment, child processes are not always subject to
the same requirement. Thus, in child_process functions where a shell can be
spawned, 'cmd.exe' is used as a fallback if process.env.ComSpec is
unavailable.
Advanced serialization#

Added in: v13.2.0, v12.16.0

Child processes support a serialization mechanism for IPC that is based on the
serialization API of the node:v8 module, based on the
HTML structured clone algorithm. This is generally more powerful and
supports more built-in JavaScript object types, such as BigInt, Map
and Set, ArrayBuffer and TypedArray, Buffer, Error, RegExp etc.
However, this format is not a full superset of JSON, and e.g. properties set on
objects of such built-in types will not be passed on through the serialization
step. Additionally, performance may not be equivalent to that of JSON, depending
on the structure of the passed data.
Therefore, this feature requires opting in by setting the
serialization option to 'advanced' when calling child_process.spawn()
or child_process.fork().\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
Cluster

How it works
Class: Worker

Event: 'disconnect'
Event: 'error'
Event: 'exit'
Event: 'listening'
Event: 'message'
Event: 'online'
worker.disconnect()
worker.exitedAfterDisconnect
worker.id
worker.isConnected()
worker.isDead()
worker.kill([signal])
worker.process
worker.send(message[, sendHandle[, options]][, callback])


Event: 'disconnect'
Event: 'exit'
Event: 'fork'
Event: 'listening'
Event: 'message'
Event: 'online'
Event: 'setup'
cluster.disconnect([callback])
cluster.fork([env])
cluster.isMaster
cluster.isPrimary
cluster.isWorker
cluster.schedulingPolicy
cluster.settings
cluster.setupMaster([settings])
cluster.setupPrimary([settings])
cluster.worker
cluster.workers



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
Cluster

How it works
Class: Worker

Event: 'disconnect'
Event: 'error'
Event: 'exit'
Event: 'listening'
Event: 'message'
Event: 'online'
worker.disconnect()
worker.exitedAfterDisconnect
worker.id
worker.isConnected()
worker.isDead()
worker.kill([signal])
worker.process
worker.send(message[, sendHandle[, options]][, callback])


Event: 'disconnect'
Event: 'exit'
Event: 'fork'
Event: 'listening'
Event: 'message'
Event: 'online'
Event: 'setup'
cluster.disconnect([callback])
cluster.fork([env])
cluster.isMaster
cluster.isPrimary
cluster.isWorker
cluster.schedulingPolicy
cluster.settings
cluster.setupMaster([settings])
cluster.setupPrimary([settings])
cluster.worker
cluster.workers




      
        Cluster#

Stability: 2 - Stable
Source Code: lib/cluster.js
Clusters of Node.js processes can be used to run multiple instances of Node.js
that can distribute workloads among their application threads. When process
isolation is not needed, use the worker_threads module instead, which
allows running multiple application threads within a single Node.js instance.
The cluster module allows easy creation of child processes that all share
server ports.

import cluster from 'node:cluster';
import http from 'node:http';
import { availableParallelism } from 'node:os';
import process from 'node:process';

const numCPUs = availableParallelism();

if (cluster.isPrimary) {
  console.log(`Primary ${process.pid} is running`);

  // Fork workers.
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  cluster.on('exit', (worker, code, signal) => {
    console.log(`worker ${worker.process.pid} died`);
  });
} else {
  // Workers can share any TCP connection
  // In this case it is an HTTP server
  http.createServer((req, res) => {
    res.writeHead(200);
    res.end('hello world\n');
  }).listen(8000);

  console.log(`Worker ${process.pid} started`);
}const cluster = require('node:cluster');
const http = require('node:http');
const numCPUs = require('node:os').availableParallelism();
const process = require('node:process');

if (cluster.isPrimary) {
  console.log(`Primary ${process.pid} is running`);

  // Fork workers.
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  cluster.on('exit', (worker, code, signal) => {
    console.log(`worker ${worker.process.pid} died`);
  });
} else {
  // Workers can share any TCP connection
  // In this case it is an HTTP server
  http.createServer((req, res) => {
    res.writeHead(200);
    res.end('hello world\n');
  }).listen(8000);

  console.log(`Worker ${process.pid} started`);
}copy
Running Node.js will now share port 8000 between the workers:
$ node server.js
Primary 3596 is running
Worker 4324 started
Worker 4520 started
Worker 6056 started
Worker 5644 started copy
On Windows, it is not yet possible to set up a named pipe server in a worker.
How it works#

The worker processes are spawned using the child_process.fork() method,
so that they can communicate with the parent via IPC and pass server
handles back and forth.
The cluster module supports two methods of distributing incoming
connections.
The first one (and the default one on all platforms except Windows)
is the round-robin approach, where the primary process listens on a
port, accepts new connections and distributes them across the workers
in a round-robin fashion, with some built-in smarts to avoid
overloading a worker process.
The second approach is where the primary process creates the listen
socket and sends it to interested workers. The workers then accept
incoming connections directly.
The second approach should, in theory, give the best performance.
In practice however, distribution tends to be very unbalanced due
to operating system scheduler vagaries. Loads have been observed
where over 70% of all connections ended up in just two processes,
out of a total of eight.
Because server.listen() hands off most of the work to the primary
process, there are three cases where the behavior between a normal
Node.js process and a cluster worker differs:

server.listen({fd: 7}) Because the message is passed to the primary,
file descriptor 7 in the parent will be listened on, and the
handle passed to the worker, rather than listening to the worker's
idea of what the number 7 file descriptor references.
server.listen(handle) Listening on handles explicitly will cause
the worker to use the supplied handle, rather than talk to the primary
process.
server.listen(0) Normally, this will cause servers to listen on a
random port. However, in a cluster, each worker will receive the
same "random" port each time they do listen(0). In essence, the
port is random the first time, but predictable thereafter. To listen
on a unique port, generate a port number based on the cluster worker ID.

Node.js does not provide routing logic. It is therefore important to design an
application such that it does not rely too heavily on in-memory data objects for
things like sessions and login.
Because workers are all separate processes, they can be killed or
re-spawned depending on a program's needs, without affecting other
workers. As long as there are some workers still alive, the server will
continue to accept connections. If no workers are alive, existing connections
will be dropped and new connections will be refused. Node.js does not
automatically manage the number of workers, however. It is the application's
responsibility to manage the worker pool based on its own needs.
Although a primary use case for the node:cluster module is networking, it can
also be used for other use cases requiring worker processes.
Class: Worker#

Added in: v0.7.0


Extends: <EventEmitter>

A Worker object contains all public information and method about a worker.
In the primary it can be obtained using cluster.workers. In a worker
it can be obtained using cluster.worker.

Event: 'disconnect'#

Added in: v0.7.7

Similar to the cluster.on('disconnect') event, but specific to this worker.
cluster.fork().on('disconnect', () => {
  // Worker has disconnected
}); copy

Event: 'error'#

Added in: v0.7.3

This event is the same as the one provided by child_process.fork().
Within a worker, process.on('error') may also be used.

Event: 'exit'#

Added in: v0.11.2


code <number> The exit code, if it exited normally.
signal <string> The name of the signal (e.g. 'SIGHUP') that caused
the process to be killed.

Similar to the cluster.on('exit') event, but specific to this worker.

import cluster from 'node:cluster';

if (cluster.isPrimary) {
  const worker = cluster.fork();
  worker.on('exit', (code, signal) => {
    if (signal) {
      console.log(`worker was killed by signal: ${signal}`);
    } else if (code !== 0) {
      console.log(`worker exited with error code: ${code}`);
    } else {
      console.log('worker success!');
    }
  });
}const cluster = require('node:cluster');

if (cluster.isPrimary) {
  const worker = cluster.fork();
  worker.on('exit', (code, signal) => {
    if (signal) {
      console.log(`worker was killed by signal: ${signal}`);
    } else if (code !== 0) {
      console.log(`worker exited with error code: ${code}`);
    } else {
      console.log('worker success!');
    }
  });
}copy

Event: 'listening'#

Added in: v0.7.0


address <Object>

Similar to the cluster.on('listening') event, but specific to this worker.

cluster.fork().on('listening', (address) => {
  // Worker is listening
});cluster.fork().on('listening', (address) => {
  // Worker is listening
});copy
It is not emitted in the worker.

Event: 'message'#

Added in: v0.7.0


message <Object>
handle <undefined> | <Object>

Similar to the 'message' event of cluster, but specific to this worker.
Within a worker, process.on('message') may also be used.
See process event: 'message'.
Here is an example using the message system. It keeps a count in the primary
process of the number of HTTP requests received by the workers:

import cluster from 'node:cluster';
import http from 'node:http';
import { availableParallelism } from 'node:os';
import process from 'node:process';

if (cluster.isPrimary) {

  // Keep track of http requests
  let numReqs = 0;
  setInterval(() => {
    console.log(`numReqs = ${numReqs}`);
  }, 1000);

  // Count requests
  function messageHandler(msg) {
    if (msg.cmd && msg.cmd === 'notifyRequest') {
      numReqs += 1;
    }
  }

  // Start workers and listen for messages containing notifyRequest
  const numCPUs = availableParallelism();
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  for (const id in cluster.workers) {
    cluster.workers[id].on('message', messageHandler);
  }

} else {

  // Worker processes have a http server.
  http.Server((req, res) => {
    res.writeHead(200);
    res.end('hello world\n');

    // Notify primary about the request
    process.send({ cmd: 'notifyRequest' });
  }).listen(8000);
}const cluster = require('node:cluster');
const http = require('node:http');
const numCPUs = require('node:os').availableParallelism();
const process = require('node:process');

if (cluster.isPrimary) {

  // Keep track of http requests
  let numReqs = 0;
  setInterval(() => {
    console.log(`numReqs = ${numReqs}`);
  }, 1000);

  // Count requests
  function messageHandler(msg) {
    if (msg.cmd && msg.cmd === 'notifyRequest') {
      numReqs += 1;
    }
  }

  // Start workers and listen for messages containing notifyRequest
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  for (const id in cluster.workers) {
    cluster.workers[id].on('message', messageHandler);
  }

} else {

  // Worker processes have a http server.
  http.Server((req, res) => {
    res.writeHead(200);
    res.end('hello world\n');

    // Notify primary about the request
    process.send({ cmd: 'notifyRequest' });
  }).listen(8000);
}copy

Event: 'online'#

Added in: v0.7.0

Similar to the cluster.on('online') event, but specific to this worker.
cluster.fork().on('online', () => {
  // Worker is online
}); copy
It is not emitted in the worker.

worker.disconnect()#

History

VersionChanges
v7.3.0
This method now returns a reference to worker.
v0.7.7
Added in: v0.7.7




Returns: <cluster.Worker> A reference to worker.

In a worker, this function will close all servers, wait for the 'close' event
on those servers, and then disconnect the IPC channel.
In the primary, an internal message is sent to the worker causing it to call
.disconnect() on itself.
Causes .exitedAfterDisconnect to be set.
After a server is closed, it will no longer accept new connections,
but connections may be accepted by any other listening worker. Existing
connections will be allowed to close as usual. When no more connections exist,
see server.close(), the IPC channel to the worker will close allowing it
to die gracefully.
The above applies only to server connections, client connections are not
automatically closed by workers, and disconnect does not wait for them to close
before exiting.
In a worker, process.disconnect exists, but it is not this function;
it is disconnect().
Because long living server connections may block workers from disconnecting, it
may be useful to send a message, so application specific actions may be taken to
close them. It also may be useful to implement a timeout, killing a worker if
the 'disconnect' event has not been emitted after some time.
if (cluster.isPrimary) {
  const worker = cluster.fork();
  let timeout;

  worker.on('listening', (address) => {
    worker.send('shutdown');
    worker.disconnect();
    timeout = setTimeout(() => {
      worker.kill();
    }, 2000);
  });

  worker.on('disconnect', () => {
    clearTimeout(timeout);
  });

} else if (cluster.isWorker) {
  const net = require('node:net');
  const server = net.createServer((socket) => {
    // Connections never end
  });

  server.listen(8000);

  process.on('message', (msg) => {
    if (msg === 'shutdown') {
      // Initiate graceful close of any connections to server
    }
  });
} copy

worker.exitedAfterDisconnect#

Added in: v6.0.0


<boolean>

This property is true if the worker exited due to .disconnect().
If the worker exited any other way, it is false. If the
worker has not exited, it is undefined.
The boolean worker.exitedAfterDisconnect allows distinguishing between
voluntary and accidental exit, the primary may choose not to respawn a worker
based on this value.
cluster.on('exit', (worker, code, signal) => {
  if (worker.exitedAfterDisconnect === true) {
    console.log('Oh, it was just voluntary – no need to worry');
  }
});

// kill worker
worker.kill(); copy

worker.id#

Added in: v0.8.0


<integer>

Each new worker is given its own unique id, this id is stored in the
id.
While a worker is alive, this is the key that indexes it in
cluster.workers.

worker.isConnected()#

Added in: v0.11.14

This function returns true if the worker is connected to its primary via its
IPC channel, false otherwise. A worker is connected to its primary after it
has been created. It is disconnected after the 'disconnect' event is emitted.

worker.isDead()#

Added in: v0.11.14

This function returns true if the worker's process has terminated (either
because of exiting or being signaled). Otherwise, it returns false.

import cluster from 'node:cluster';
import http from 'node:http';
import { availableParallelism } from 'node:os';
import process from 'node:process';

const numCPUs = availableParallelism();

if (cluster.isPrimary) {
  console.log(`Primary ${process.pid} is running`);

  // Fork workers.
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  cluster.on('fork', (worker) => {
    console.log('worker is dead:', worker.isDead());
  });

  cluster.on('exit', (worker, code, signal) => {
    console.log('worker is dead:', worker.isDead());
  });
} else {
  // Workers can share any TCP connection. In this case, it is an HTTP server.
  http.createServer((req, res) => {
    res.writeHead(200);
    res.end(`Current process\n ${process.pid}`);
    process.kill(process.pid);
  }).listen(8000);
}const cluster = require('node:cluster');
const http = require('node:http');
const numCPUs = require('node:os').availableParallelism();
const process = require('node:process');

if (cluster.isPrimary) {
  console.log(`Primary ${process.pid} is running`);

  // Fork workers.
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  cluster.on('fork', (worker) => {
    console.log('worker is dead:', worker.isDead());
  });

  cluster.on('exit', (worker, code, signal) => {
    console.log('worker is dead:', worker.isDead());
  });
} else {
  // Workers can share any TCP connection. In this case, it is an HTTP server.
  http.createServer((req, res) => {
    res.writeHead(200);
    res.end(`Current process\n ${process.pid}`);
    process.kill(process.pid);
  }).listen(8000);
}copy

worker.kill([signal])#

Added in: v0.9.12


signal <string> Name of the kill signal to send to the worker
process. Default: 'SIGTERM'

This function will kill the worker. In the primary worker, it does this by
disconnecting the worker.process, and once disconnected, killing with
signal. In the worker, it does it by killing the process with signal.
The kill() function kills the worker process without waiting for a graceful
disconnect, it has the same behavior as worker.process.kill().
This method is aliased as worker.destroy() for backwards compatibility.
In a worker, process.kill() exists, but it is not this function;
it is kill().

worker.process#

Added in: v0.7.0


<ChildProcess>

All workers are created using child_process.fork(), the returned object
from this function is stored as .process. In a worker, the global process
is stored.
See: Child Process module.
Workers will call process.exit(0) if the 'disconnect' event occurs
on process and .exitedAfterDisconnect is not true. This protects against
accidental disconnection.

worker.send(message[, sendHandle[, options]][, callback])#

History

VersionChanges
v4.0.0
The callback parameter is supported now.
v0.7.0
Added in: v0.7.0




message <Object>
sendHandle <Handle>
options <Object> The options argument, if present, is an object used to
parameterize the sending of certain types of handles. options supports
the following properties:

keepOpen <boolean> A value that can be used when passing instances of
net.Socket. When true, the socket is kept open in the sending process.
Default: false.


callback <Function>
Returns: <boolean>

Send a message to a worker or primary, optionally with a handle.
In the primary, this sends a message to a specific worker. It is identical to
ChildProcess.send().
In a worker, this sends a message to the primary. It is identical to
process.send().
This example will echo back all messages from the primary:
if (cluster.isPrimary) {
  const worker = cluster.fork();
  worker.send('hi there');

} else if (cluster.isWorker) {
  process.on('message', (msg) => {
    process.send(msg);
  });
} copy

Event: 'disconnect'#

Added in: v0.7.9


worker <cluster.Worker>

Emitted after the worker IPC channel has disconnected. This can occur when a
worker exits gracefully, is killed, or is disconnected manually (such as with
worker.disconnect()).
There may be a delay between the 'disconnect' and 'exit' events. These
events can be used to detect if the process is stuck in a cleanup or if there
are long-living connections.
cluster.on('disconnect', (worker) => {
  console.log(`The worker #${worker.id} has disconnected`);
}); copy
Event: 'exit'#

Added in: v0.7.9


worker <cluster.Worker>
code <number> The exit code, if it exited normally.
signal <string> The name of the signal (e.g. 'SIGHUP') that caused
the process to be killed.

When any of the workers die the cluster module will emit the 'exit' event.
This can be used to restart the worker by calling .fork() again.
cluster.on('exit', (worker, code, signal) => {
  console.log('worker %d died (%s). restarting...',
              worker.process.pid, signal || code);
  cluster.fork();
}); copy
See child_process event: 'exit'.
Event: 'fork'#

Added in: v0.7.0


worker <cluster.Worker>

When a new worker is forked the cluster module will emit a 'fork' event.
This can be used to log worker activity, and create a custom timeout.
const timeouts = [];
function errorMsg() {
  console.error('Something must be wrong with the connection ...');
}

cluster.on('fork', (worker) => {
  timeouts[worker.id] = setTimeout(errorMsg, 2000);
});
cluster.on('listening', (worker, address) => {
  clearTimeout(timeouts[worker.id]);
});
cluster.on('exit', (worker, code, signal) => {
  clearTimeout(timeouts[worker.id]);
  errorMsg();
}); copy
Event: 'listening'#

Added in: v0.7.0


worker <cluster.Worker>
address <Object>

After calling listen() from a worker, when the 'listening' event is emitted
on the server, a 'listening' event will also be emitted on cluster in the
primary.
The event handler is executed with two arguments, the worker contains the
worker object and the address object contains the following connection
properties: address, port, and addressType. This is very useful if the
worker is listening on more than one address.
cluster.on('listening', (worker, address) => {
  console.log(
    `A worker is now connected to ${address.address}:${address.port}`);
}); copy
The addressType is one of:

4 (TCPv4)
6 (TCPv6)
-1 (Unix domain socket)
'udp4' or 'udp6' (UDPv4 or UDPv6)

Event: 'message'#

History

VersionChanges
v6.0.0
The worker parameter is passed now; see below for details.
v2.5.0
Added in: v2.5.0




worker <cluster.Worker>
message <Object>
handle <undefined> | <Object>

Emitted when the cluster primary receives a message from any worker.
See child_process event: 'message'.
Event: 'online'#

Added in: v0.7.0


worker <cluster.Worker>

After forking a new worker, the worker should respond with an online message.
When the primary receives an online message it will emit this event.
The difference between 'fork' and 'online' is that fork is emitted when the
primary forks a worker, and 'online' is emitted when the worker is running.
cluster.on('online', (worker) => {
  console.log('Yay, the worker responded after it was forked');
}); copy
Event: 'setup'#

Added in: v0.7.1


settings <Object>

Emitted every time .setupPrimary() is called.
The settings object is the cluster.settings object at the time
.setupPrimary() was called and is advisory only, since multiple calls to
.setupPrimary() can be made in a single tick.
If accuracy is important, use cluster.settings.
cluster.disconnect([callback])#

Added in: v0.7.7


callback <Function> Called when all workers are disconnected and handles are
closed.

Calls .disconnect() on each worker in cluster.workers.
When they are disconnected all internal handles will be closed, allowing the
primary process to die gracefully if no other event is waiting.
The method takes an optional callback argument which will be called when
finished.
This can only be called from the primary process.
cluster.fork([env])#

Added in: v0.6.0


env <Object> Key/value pairs to add to worker process environment.
Returns: <cluster.Worker>

Spawn a new worker process.
This can only be called from the primary process.
cluster.isMaster#

Added in: v0.8.1Deprecated since: v16.0.0

Stability: 0 - Deprecated
Deprecated alias for cluster.isPrimary.
cluster.isPrimary#

Added in: v16.0.0


<boolean>

True if the process is a primary. This is determined
by the process.env.NODE_UNIQUE_ID. If process.env.NODE_UNIQUE_ID is
undefined, then isPrimary is true.
cluster.isWorker#

Added in: v0.6.0


<boolean>

True if the process is not a primary (it is the negation of cluster.isPrimary).
cluster.schedulingPolicy#

Added in: v0.11.2

The scheduling policy, either cluster.SCHED_RR for round-robin or
cluster.SCHED_NONE to leave it to the operating system. This is a
global setting and effectively frozen once either the first worker is spawned,
or .setupPrimary() is called, whichever comes first.
SCHED_RR is the default on all operating systems except Windows.
Windows will change to SCHED_RR once libuv is able to effectively
distribute IOCP handles without incurring a large performance hit.
cluster.schedulingPolicy can also be set through the
NODE_CLUSTER_SCHED_POLICY environment variable. Valid
values are 'rr' and 'none'.
cluster.settings#

History

VersionChanges
v13.2.0, v12.16.0
The serialization option is supported now.
v9.5.0
The cwd option is supported now.
v9.4.0
The windowsHide option is supported now.
v8.2.0
The inspectPort option is supported now.
v6.4.0
The stdio option is supported now.
v0.7.1
Added in: v0.7.1




<Object>

execArgv <string[]> List of string arguments passed to the Node.js
executable. Default: process.execArgv.
exec <string> File path to worker file. Default: process.argv[1].
args <string[]> String arguments passed to worker.
Default: process.argv.slice(2).
cwd <string> Current working directory of the worker process. Default:
undefined (inherits from parent process).
serialization <string> Specify the kind of serialization used for sending
messages between processes. Possible values are 'json' and 'advanced'.
See Advanced serialization for child_process for more details.
Default: false.
silent <boolean> Whether or not to send output to parent's stdio.
Default: false.
stdio <Array> Configures the stdio of forked processes. Because the
cluster module relies on IPC to function, this configuration must contain an
'ipc' entry. When this option is provided, it overrides silent. See
child_process.spawn()'s stdio.
uid <number> Sets the user identity of the process. (See setuid(2).)
gid <number> Sets the group identity of the process. (See setgid(2).)
inspectPort <number> | <Function> Sets inspector port of worker.
This can be a number, or a function that takes no arguments and returns a
number. By default each worker gets its own port, incremented from the
primary's process.debugPort.
windowsHide <boolean> Hide the forked processes console window that would
normally be created on Windows systems. Default: false.



After calling .setupPrimary() (or .fork()) this settings object will
contain the settings, including the default values.
This object is not intended to be changed or set manually.
cluster.setupMaster([settings])#

History

VersionChanges
v16.0.0
Deprecated since: v16.0.0
v6.4.0
The stdio option is supported now.
v0.7.1
Added in: v0.7.1



Stability: 0 - Deprecated
Deprecated alias for .setupPrimary().
cluster.setupPrimary([settings])#

Added in: v16.0.0


settings <Object> See cluster.settings.

setupPrimary is used to change the default 'fork' behavior. Once called,
the settings will be present in cluster.settings.
Any settings changes only affect future calls to .fork() and have no
effect on workers that are already running.
The only attribute of a worker that cannot be set via .setupPrimary() is
the env passed to .fork().
The defaults above apply to the first call only; the defaults for later
calls are the current values at the time of cluster.setupPrimary() is called.

import cluster from 'node:cluster';

cluster.setupPrimary({
  exec: 'worker.js',
  args: ['--use', 'https'],
  silent: true,
});
cluster.fork(); // https worker
cluster.setupPrimary({
  exec: 'worker.js',
  args: ['--use', 'http'],
});
cluster.fork(); // http workerconst cluster = require('node:cluster');

cluster.setupPrimary({
  exec: 'worker.js',
  args: ['--use', 'https'],
  silent: true,
});
cluster.fork(); // https worker
cluster.setupPrimary({
  exec: 'worker.js',
  args: ['--use', 'http'],
});
cluster.fork(); // http workercopy
This can only be called from the primary process.
cluster.worker#

Added in: v0.7.0


<Object>

A reference to the current worker object. Not available in the primary process.

import cluster from 'node:cluster';

if (cluster.isPrimary) {
  console.log('I am primary');
  cluster.fork();
  cluster.fork();
} else if (cluster.isWorker) {
  console.log(`I am worker #${cluster.worker.id}`);
}const cluster = require('node:cluster');

if (cluster.isPrimary) {
  console.log('I am primary');
  cluster.fork();
  cluster.fork();
} else if (cluster.isWorker) {
  console.log(`I am worker #${cluster.worker.id}`);
}copy
cluster.workers#

Added in: v0.7.0


<Object>

A hash that stores the active worker objects, keyed by id field. This makes it
easy to loop through all the workers. It is only available in the primary
process.
A worker is removed from cluster.workers after the worker has disconnected
and exited. The order between these two events cannot be determined in
advance. However, it is guaranteed that the removal from the cluster.workers
list happens before the last 'disconnect' or 'exit' event is emitted.

import cluster from 'node:cluster';

for (const worker of Object.values(cluster.workers)) {
  worker.send('big announcement to all workers');
}const cluster = require('node:cluster');

for (const worker of Object.values(cluster.workers)) {
  worker.send('big announcement to all workers');
}copy\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nRun JavaScript EverywhereNode.js® is a free, open-source, cross-platform JavaScript runtime environment
that lets developers create servers, web apps, command line tools and scripts.Download Node.js (LTS)Download Node.js (LTS)Downloads Node.js v22.14.01 with long-term support. Node.js can also be installed via version managers.Want new features sooner? Get Node.js v23.11.01 instead.
Create an HTTP ServerWrite TestsRead and Hash a FileStreams PipelineWork with Threads// server.mjs
import { createServer } from 'node:http';

const server = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello World!\n');
});

// starts a simple http server locally on port 3000
server.listen(3000, '127.0.0.1', () => {
  console.log('Listening on 127.0.0.1:3000');
});

// run with `node server.mjs`
JavaScriptCopy to clipboardLearn more what Node.js is able to offer with our Learning materials.\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nAbout Node.js®
As an asynchronous event-driven JavaScript runtime, Node.js is designed to build
scalable network applications. In the following "hello world" example, many
connections can be handled concurrently. Upon each connection, the callback is
fired, but if there is no work to be done, Node.js will sleep.
CJSMJSconst { createServer } = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});
JavaScriptCopy to clipboardThis is in contrast to today's more common concurrency model, in which OS threads
are employed. Thread-based networking is relatively inefficient and very
difficult to use. Furthermore, users of Node.js are free from worries of
dead-locking the process, since there are no locks. Almost no function in
Node.js directly performs I/O, so the process never blocks except when the I/O is performed using
synchronous methods of Node.js standard library. Because nothing blocks, scalable systems are very
reasonable to develop in Node.js.
If some of this language is unfamiliar, there is a full article on
Blocking vs. Non-Blocking.

Node.js is similar in design to, and influenced by, systems like Ruby's
Event Machine and Python's Twisted. Node.js takes the event model a bit
further. It presents an event loop as a runtime construct instead of as a library. In other systems,
there is always a blocking call to start the event-loop.
Typically, behavior is defined through callbacks at the beginning of a script, and
at the end a server is started through a blocking call like EventMachine::run().
In Node.js, there is no such start-the-event-loop call. Node.js simply enters the event loop after executing the input script. Node.js
exits the event loop when there are no more callbacks to perform. This behavior
is like browser JavaScript — the event loop is hidden from the user.
HTTP is a first-class citizen in Node.js, designed with streaming and low
latency in mind. This makes Node.js well suited for the foundation of a web
library or framework.
Node.js being designed without threads doesn't mean you can't take
advantage of multiple cores in your environment. Child processes can be spawned
by using our child_process.fork() API, and are designed to be easy to
communicate with. Built upon that same interface is the cluster module,
which allows you to share sockets between processes to enable load balancing
over your cores.
Official Node.js Resources
To ensure authenticity and security when working with Node.js, always use official sources. Avoid trusting emails,
binaries, or downloads from unofficial sources.
Official Node.js Domains
For downloading Node.js binaries and accessing official documentation, use only these domains:

nodejs.org
nodejs.dev (Redirects to https://nodejs.org)
iojs.org (Redirects to https://nodejs.org)

Official npm Packages
The Node.js team maintains the following official npm package scopes:

@node-core
@pkgjs

Additionally, the Node.js team maintains packages published by the nodejs-foundation npm account,
though other Node.js-related packages (like undici) may also be maintained by contributors closely
tied to the project.
Using packages from the Node.js team guarantees that you are working with officially supported Node.js components.
Official GitHub Organizations
Node.js and related projects are maintained under these official GitHub organizations:

nodejs
pkgjs

Official Communication Channels
Node.js and the OpenJS Foundation communicate through various official and community-supported channels. You can find details on
how to get involved on the Get Involved page.
Reporting Website Issues & Downtime
If you encounter issues with the Node.js website, report them at the Node.js website repository.
For real-time updates on outages, visit the Node.js Status Page.\n\n\n\nNode v22.14.0 (LTS)Antoine du HamelNode v22.14.0 (LTS)2025-02-11, Version 22.14.0 'Jod' (LTS), @aduh95
Notable Changes

[82a9000e9e] - crypto: update root certificates to NSS 3.107 (Node.js GitHub Bot) #56566
[b7fe54fc88] - (SEMVER-MINOR) fs: allow exclude option in globs to accept glob patterns (Daeyeon Jeong) #56489
[3ac92ef607] - (SEMVER-MINOR) lib: add typescript support to STDIN eval (Marco Ippolito) #56359
[1614e8e7bc] - (SEMVER-MINOR) module: add ERR_UNSUPPORTED_TYPESCRIPT_SYNTAX (Marco Ippolito) #56610
[6d6cffa9cc] - (SEMVER-MINOR) module: add findPackageJSON util (Jacob Smith) #55412
[d35333ae18] - (SEMVER-MINOR) process: add process.ref() and process.unref() methods (James M Snell) #56400
[07ff3ddcb5] - (SEMVER-MINOR) sqlite: support TypedArray and DataView in StatementSync (Alex Yang) #56385
[94d3fe1b62] - (SEMVER-MINOR) src: add --disable-sigusr1 to prevent signal i/o thread (Rafael Gonzaga) #56441
[5afffb4415] - (SEMVER-MINOR) src,worker: add isInternalWorker (Carlos Espa) #56469
[697a851fb3] - (SEMVER-MINOR) test_runner: add TestContext.prototype.waitFor() (Colin Ihrig) #56595
[047537b48c] - (SEMVER-MINOR) test_runner: add t.assert.fileSnapshot() (Colin Ihrig) #56459
[926cf84e95] - (SEMVER-MINOR) test_runner: add assert.register() API (Colin Ihrig) #56434
[c658a8afdf] - (SEMVER-MINOR) worker: add eval ts input (Marco Ippolito) #56394

Commits

[bad1ad8650] - assert: make myers_diff function more performant (Giovanni Bucci) #56303
[e222e36f3b] - assert: make partialDeepStrictEqual work with urls and File prototypes (Giovanni Bucci) #56231
[e232789fe2] - assert: show diff when doing partial comparisons (Giovanni Bucci) #56211
[c99de1fdcf] - assert: make partialDeepStrictEqual throw when comparing [0] with [-0] (Giovanni) #56237
[2386fd5840] - benchmark: add validateStream to styleText bench (Rafael Gonzaga) #56556
[b197dfa7ec] - build: fix GN build for ngtcp2 (Cheng) #56300
[2a3cdd34ff] - build: test macos-13 on GitHub actions (Michaël Zasso) #56307
[12f716be0a] - build: build v8 with -fvisibility=hidden on macOS (Joyee Cheung) #56275
[c5ca15bd34] - child_process: fix parsing messages with splitted length field (Maksim Gorkov) #56106
[8346b8fc2c] - crypto: add missing return value check (Michael Dawson) #56615
[82a9000e9e] - crypto: update root certificates to NSS 3.107 (Node.js GitHub Bot) #56566
[890eef20a1] - crypto: fix checkPrime crash with large buffers (Santiago Gimeno) #56559
[5edb7b5e87] - crypto: fix warning of ignoring return value (Cheng) #56527
[b89f123a0b] - crypto: make generatePrime/checkPrime interruptible (James M Snell) #56460
[63c1859e01] - deps: update corepack to 0.31.0 (Node.js GitHub Bot) #56795
[a48430d4d3] - deps: move inspector_protocol to deps (Chengzhong Wu) #56649
[74cccc824f] - deps: macro ENODATA is deprecated in libc++ (Cheng) #56698
[fa869ea0f2] - deps: fixup some minor coverity warnings (James M Snell) #56612
[1a4fa2b015] - deps: update amaro to 0.3.0 (Node.js GitHub Bot) #56568
[b47076fd82] - deps: update amaro to 0.2.2 (Node.js GitHub Bot) #56568
[46bd4b8731] - deps: update simdutf to 6.0.3 (Node.js GitHub Bot) #56567
[8ead9c693b] - deps: update simdutf to 5.7.2 (Node.js GitHub Bot) #56388
[18d4b502af] - deps: update amaro to 0.2.1 (Node.js GitHub Bot) #56390
[d938d7cc86] - deps: update googletest to 7d76a23 (Node.js GitHub Bot) #56387
[9761e7dccb] - deps: update googletest to e54519b (Node.js GitHub Bot) #56370
[8319dc6bc5] - deps: update ngtcp2 to 1.10.0 (Node.js GitHub Bot) #56334
[6eacd19d6a] - deps: update simdutf to 5.7.0 (Node.js GitHub Bot) #56332
[28bec2dda3] - diagnostics_channel: capture console messages (Stephen Belanger) #56292
[d519d33502] - doc: update macOS and Xcode versions for releases (Michaël Zasso) #56337
[fcfe650507] - doc: add note for features using InternalWorker with permission model (Antoine du Hamel) #56706
[efbba182b5] - doc: add entry to changelog about SQLite Session Extension (Bart Louwers) #56318
[31bf9c7dd9] - doc: move anatoli to emeritus (Michael Dawson) #56592
[6096e38c7c] - doc: fix styles of the expandable TOC (Antoine du Hamel) #56755
[d423638281] - doc: add "Skip to content" button (Antoine du Hamel) #56750
[edeb157d75] - doc: improve accessibility of expandable lists (Antoine du Hamel) #56749
[1a79e87687] - doc: add note regarding commit message trailers (Dario Piotrowicz) #56736
[927c7e47e4] - doc: fix typo in example code for util.styleText (Robin Mehner) #56720
[fade522538] - doc: fix inconsistencies in WeakSet and WeakMap comparison details (Shreyans Pathak) #56683
[55533bf147] - doc: add RafaelGSS as latest sec release stewards (Rafael Gonzaga) #56682
[8e978bdee1] - doc: clarify cjs/esm diff in queueMicrotask() vs process.nextTick() (Dario Piotrowicz) #56659
[ae360c30dc] - doc: WeakSet and WeakMap comparison details (Shreyans Pathak) #56648
[acd2a2fda5] - doc: mention prepare --security (Rafael Gonzaga) #56617
[d3c0a2831d] - doc: tweak info on reposts in ambassador program (Michael Dawson) #56589
[3299505b49] - doc: add type stripping to ambassadors program (Marco Ippolito) #56598
[b1a6ffa4e4] - doc: improve internal documentation on built-in snapshot (Joyee Cheung) #56505
[1641a28930] - doc: document CLI way to open the nodejs/bluesky PR (Antoine du Hamel) #56506
[2042628fda] - doc: add section about using npx with permission model (Rafael Gonzaga) #56539
[ace19a0263] - doc: update gcc-version for ubuntu-lts (Kunal Kumar) #56553
[4aa57b50f8] - doc: fix parentheses in options (Tobias Nießen) #56563
[b40b01b4d3] - doc: include CVE to EOL lines as sec release process (Rafael Gonzaga) #56520
[6701360113] - doc: add esm examples to node:trace_events (Alfredo González) #56514
[d3207cca3e] - doc: add message for Ambassadors to promote (Michael Dawson) #56235
[97ece4ae06] - doc: allow request for TSC reviews via the GitHub UI (Antoine du Hamel) #56493
[03f25055ab] - doc: add example for piping ReadableStream (Gabriel Schulhof) #56415
[516d07482c] - doc: expand description of parseArg's default (Kevin Gibbons) #54431
[a6491effcb] - doc: use <ul> instead of <ol> in SECURITY.md (Antoine du Hamel) #56346
[e4ec134b21] - doc: clarify that WASM is trusted (Matteo Collina) #56345
[0f7aed8a59] - doc: fix the crc32 documentation (Kevin Toshihiro Uehara) #55898
[721104a296] - doc: fix links in module.md (Antoine du Hamel) #56283
[928540d792] - doc: fix typos (Nathan Baulch) #55066
[e69d35f03b] - doc: add history info for Permission Model (Antoine du Hamel) #56707
[c6fd867ab5] - esm: fix jsdoc type refs to ModuleJobBase in esm/loader (Jacob Smith) #56499
[9cf9046bd7] - Revert "events: add hasEventListener util for validate" (origranot) #56282
[b7fe54fc88] - (SEMVER-MINOR) fs: allow exclude option in globs to accept glob patterns (Daeyeon Jeong) #56489
[6ca27c2a59] - http2: omit server name when HTTP2 host is IP address (islandryu) #56530
[9f1fa199bf] - inspector: roll inspector_protocol (Chengzhong Wu) #56649
[0dae4bb3ab] - inspector: add undici http tracking support (Chengzhong Wu) #56488
[2c6124cec4] - inspector: report loadingFinished until the response data is consumed (Chengzhong Wu) #56372
[96ec862ce2] - lib: refactor execution.js (Marco Ippolito) #56358
[3ac92ef607] - (SEMVER-MINOR) lib: add typescript support to STDIN eval (Marco Ippolito) #56359
[d5bf3db0cf] - lib: allow skipping source maps in node_modules (Chengzhong Wu) #56639
[d33eaf2bcb] - lib: ensure FORCE_COLOR forces color output in non-TTY environments (Pietro Marchini) #55404
[dc003218a8] - lib: optimize prepareStackTrace on builtin frames (Chengzhong Wu) #56299
[df06524863] - lib: suppress source map lookup exceptions (Chengzhong Wu) #56299
[35335a5a66] - meta: move one or more collaborators to emeritus (Node.js GitHub Bot) #56580
[1faabdb150] - meta: add codeowners of security release document (Rafael Gonzaga) #56521
[b4ece22ef5] - meta: move one or more collaborators to emeritus (Node.js GitHub Bot) #56342
[9ec67e7ce0] - meta: move MoLow to TSC regular member (Moshe Atlow) #56276
[bae4b2e20a] - module: use more defensive code when handling SWC errors (Antoine du Hamel) #56646
[1614e8e7bc] - (SEMVER-MINOR) module: add ERR_UNSUPPORTED_TYPESCRIPT_SYNTAX (Marco Ippolito) #56610
[174d88eab1] - module: support eval with ts syntax detection (Marco Ippolito) #56285
[299d6fa829] - module: fix jsdoc for format parameter in cjs/loader (pacexy) #56501
[0307e4dd59] - module: unify TypeScript and .mjs handling in CommonJS (Joyee Cheung) #55590
[1f4f9be93d] - module: fix async resolution error within the sync findPackageJSON (Jacob Smith) #56382
[bbedffa0f0] - module: simplify findPackageJSON implementation (Antoine du Hamel) #55543
[6d6cffa9cc] - (SEMVER-MINOR) module: add findPackageJSON util (Jacob Smith) #55412
[cd7ce18233] - module: fix bad require.resolve with option paths for . and .. (Dario Piotrowicz) #56735
[152df4da21] - module: rethrow amaro error message (Marco Ippolito) #56568
[acba5dc87e] - module: use buffer.toString base64 (Chengzhong Wu) #56315
[01e69be8ff] - node-api: define version 10 (Gabriel Schulhof) #55676
[724524528e] - node-api: remove deprecated attribute from napi_module_register (Vladimir Morozov) #56162
[c78e11064f] - process: remove support for undocumented symbol (Antoine du Hamel) #56552
[3f69b18a23] - process: fix symbol key and mark experimental new node:process methods (Antoine du Hamel) #56517
[d35333ae18] - (SEMVER-MINOR) process: add process.ref() and process.unref() methods (James M Snell) #56400
[fa49f0f7d5] - punycode: limit deprecation warning (Colin Ihrig) #56632
[d77c7073b7] - sqlite: disable memstatus APIs at build time (Colin Ihrig) #56541
[07ff3ddcb5] - (SEMVER-MINOR) sqlite: support TypedArray and DataView in StatementSync (Alex Yang) #56385
[b6c2e91365] - sqlite: enable SQL math functions (Colin Ihrig) #56447
[3462263e8b] - sqlite: pass conflict type to conflict resolution handler (Bart Louwers) #56352
[89ba3af743] - src: add nullptr handling from X509_STORE_new() (Burkov Egor) #56700
[89a7c82e0c] - src: add default value for RSACipherConfig mode field (Burkov Egor) #56701
[7bae51e62e] - src: fix build with GCC 15 (tjuhaszrh) #56740
[432a4b8bd6] - src: fix to generate path from wchar_t via wstring (yamachu) #56696
[8c9eaf82f0] - src: initialize FSReqWrapSync in path that uses it (Michaël Zasso) #56613
[bcdb42d40b] - src: handle duplicate paths granted (Rafael Gonzaga) #56591
[d6a7acc207] - src: update ECKeyPointer in ncrypto (James M Snell) #56526
[01922f8b1f] - src: update ECPointPointer in ncrypto (James M Snell) #56526
[2a3a36eceb] - src: update ECGroupPointer in ncrypto (James M Snell) #56526
[67c10cdacb] - src: update ECDASSigPointer implementation in ncrypto (James M Snell) #56526
[17f931c68b] - src: cleaning up more crypto internals for ncrypto (James M Snell) #56526
[94d3fe1b62] - (SEMVER-MINOR) src: add --disable-sigusr1 to prevent signal i/o thread (Rafael Gonzaga) #56441
[6594ee8dff] - src: fix undefined script name in error source (Chengzhong Wu) #56502
[b46bad3e91] - src: refactor --trace-env to reuse option selection and handling (Joyee Cheung) #56293
[76921b822b] - src: minor cleanups on OneByteString usage (James M Snell) #56482
[3f0d1dd4fe] - src: move more crypto impl detail to ncrypto dep (James M Snell) #56421
[04f623b283] - src: fixup more ToLocalChecked uses in node_file (James M Snell) #56484
[5aa436f5a1] - src: make some minor ToLocalChecked cleanups (James M Snell) #56483
[6eec5e7ec2] - src: lock the thread properly in snapshot builder (Joyee Cheung) #56327
[5614993968] - src: drain platform tasks before creating startup snapshot (Chengzhong Wu) #56403
[48493e9fd5] - src: use LocalVector in more places (James M Snell) #56457
[7e5ea0681e] - src: use v8::LocalVector consistently with other minor cleanups (James M Snell) #56417
[ad3d857f2b] - src: use starts_with in fs_permission.cc (ishabi) #55811
[5afffb4415] - (SEMVER-MINOR) src,worker: add isInternalWorker (Carlos Espa) #56469
[7d1676e72e] - stream: fix typo in ReadableStreamBYOBReader.readIntoRequests (Mattias Buelens) #56560
[e658ea6b26] - stream: validate undefined sizeAlgorithm in WritableStream (Jason Zhang) #56067
[e4f133c20c] - test: add ts eval snapshots (Marco Ippolito) #56358
[f041742400] - test: remove empty lines from snapshots (Marco Ippolito) #56358
[801cde91f6] - test: reduce number of written chunks (Luigi Pinca) #56757
[6fdf1879ab] - test: fix invalid common.mustSucceed() usage (Luigi Pinca) #56756
[d2bfbfa364] - test: use strict mode in global setters test (Rich Trott) #56742
[5c030da42f] - test: cleanup and simplify test-crypto-aes-wrap (James M Snell) #56748
[f1442d6eaf] - test: do not use common.isMainThread (Luigi Pinca) #56768
[49405bd9e7] - test: make some requires lazy in common/index (James M Snell) #56715
[52ef376788] - test: add test that uses multibyte for path and resolves modules (yamachu) #56696
[b811dea85a] - test: replace more uses of global with globalThis (James M Snell) #56712
[eb97076199] - test: make common/index slightly less node.js specific (James M Snell) #56712
[1795202d19] - test: rely less on duplicative common test harness utilities (James M Snell) #56712
[5be29a274e] - test: simplify common/index.js (James M Snell) #56712
[92e99780f0] - test: move hasMultiLocalhost to common/net (James M Snell) #56716
[1c3204a4cc] - test: move crypto related common utilities in common/crypto (James M Snell) #56714
[fe79d63be0] - test: add missing test for env file (Jonas) #56642
[e08af61537] - test: enforce strict mode in test-zlib-const (Rich Trott) #56689
[c96792d7f8] - test: fix localization data for ICU 74.2 (Antoine du Hamel) #56661
[48b72f1195] - test: use --permission instead of --experimental-permission (Rafael Gonzaga) #56685
[de81d90fce] - test: test-stream-compose.js doesn't need internals (Meghan Denny) #56619
[f5b8499ad0] - test: add maxCount and gcOptions to gcUntil() (Joyee Cheung) #56522
[d9e5a81041] - test: add line break at end of file (Rafael Gonzaga) #56588
[59be346fbf] - test: mark test-worker-prof as flaky on smartos (Joyee Cheung) #56583
[12a2cae9e5] - test: update test-child-process-bad-stdio to use node:test (Colin Ihrig) #56562
[2dc4a30e19] - test: disable openssl 3.4.0 incompatible tests (Jelle van der Waa) #56160
[1950fbf51d] - test: make test-crypto-hash compatible with OpenSSL > 3.4.0 (Jelle van der Waa) #56160
[a533420a91] - test: clarify fork inherit permission flags (Rafael Gonzaga) #56523
[697e799dc1] - test: add error only reporter for node:test (Carlos Espa) #56438
[4844fa212d] - test: mark test-http-server-request-timeouts-mixed as flaky (Joyee Cheung) #56503
[843c2389b9] - test: update error code in tls-psk-circuit for for OpenSSL 3.4 (sebastianas) #56420
[ccb2ddbd83] - test: update compiled sqlite tests to match other tests (Colin Ihrig) #56446
[b40f50324d] - test: add initial test426 coverage (Chengzhong Wu) #56436
[059f81e4fd] - test: update test-set-http-max-http-headers to use node:test (Colin Ihrig) #56439
[ec2940b418] - test: update test-child-process-windows-hide to use node:test (Colin Ihrig) #56437
[0362924880] - test: use unusual chars in the path to ensure our tests are robust (Antoine du Hamel) #48409
[b6c3869910] - test: improve abort signal dropping test (Edy Silva) #56339
[cc648ef923] - test: enable ts test on win arm64 (Marco Ippolito) #56349
[68819b4997] - test: deflake test-watch-file-shared-dependency (Luigi Pinca) #56344
[ca6ed2190c] - test: skip test-sqlite-extensions when SQLite is not built by us (Antoine du Hamel) #56341
[8ffeb8b58c] - test: increase spin for eventloop test on s390 (Michael Dawson) #56228
[6ae9950f08] - test: migrate message eval tests from Python to JS (Yiyun Lei) #50482
[4352bf69e9] - test: check typescript loader (Marco Ippolito) #54657
[406e7db9c3] - test: remove async-hooks/test-writewrap flaky designation (Luigi Pinca) #56048
[fa56ab2bba] - test: deflake test-esm-loader-hooks-inspect-brk (Luigi Pinca) #56050
[8e149aac99] - test: add test case for listeners (origranot) #56282
[a3f5ef22cd] - test: make test-permission-sqlite-load-extension more robust (Antoine du Hamel) #56295
[8cbb7cc838] - test_runner: print failing assertion only once with spec reporter (Pietro Marchini) #56662
[1f426bad9a] - test_runner: remove unused errors (Pietro Marchini) #56607
[697a851fb3] - (SEMVER-MINOR) test_runner: add TestContext.prototype.waitFor() (Colin Ihrig) #56595
[047537b48c] - (SEMVER-MINOR) test_runner: add t.assert.fileSnapshot() (Colin Ihrig) #56459
[19b4aa4b14] - test_runner: run single test file benchmark (Pietro Marchini) #56479
[926cf84e95] - (SEMVER-MINOR) test_runner: add assert.register() API (Colin Ihrig) #56434
[fb4661a4cf] - test_runner: finish marking snapshot testing as stable (Colin Ihrig) #56425
[900c6c3940] - tls: fix error stack conversion in cryptoErrorListToException() (Joyee Cheung) #56554
[e9f185b658] - tools: update doc to new version (Node.js GitHub Bot) #56259
[7644c7e619] - tools: update inspector_protocol roller (Chengzhong Wu) #56649
[362272b0a4] - tools: do not throw on missing create-release-proposal.sh (Antoine du Hamel) #56704
[df8b835953] - tools: fix tools-deps-update (Daniel Lemire) #56684
[feba5d3274] - tools: do not throw on missing create-release-proposal.sh (Antoine du Hamel) #56695
[9827f7d395] - tools: fix permissions in lint-release-proposal workflow (Antoine du Hamel) #56614
[14c562c0dc] - tools: remove github reporter (Carlos Espa) #56468
[ed1785d0ae] - tools: edit create-release-proposal workflow (Antoine du Hamel) #56540
[294e4c42f5] - tools: validate commit list as part of lint-release-commit (Antoine du Hamel) #56291
[98d3474267] - tools: fix loong64 build failed (Xiao-Tao) #56466
[3e729ceec8] - tools: disable unneeded rule ignoring in Python linting (Rich Trott) #56429
[d5c05328e2] - tools: use a configurable value for number of open dependabot PRs (Antoine du Hamel) #56427
[1705cbe002] - tools: bump the eslint group in /tools/eslint with 4 updates (dependabot[bot]) #56426
[53b29b0469] - tools: fix require-common-first lint rule from subfolder (Antoine du Hamel) #56325
[105c4ed4fb] - tools: add release line label when opening release proposal (Antoine du Hamel) #56317
[30f61f4aa5] - url: use resolved path to convert UNC paths to URL (Antoine du Hamel) #56302
[a0aef4dfb6] - util: inspect: do not crash on an Error stack that contains a Symbol (Jordan Harband) #56573
[a8a060341f] - util: inspect: do not crash on an Error with a regex name (Jordan Harband) #56574
[ea66bf3553] - util: rename CallSite.column to columnNumber (Chengzhong Wu) #56584
[9cdc3b373c] - util: do not crash on inspecting function with Symbol name (Jordan Harband) #56572
[0bfbb68569] - util: expose CallSite.scriptId (Chengzhong Wu) #56551
[5dd7116e09] - watch: reload env file for --env-file-if-exists (Jonas) #56643
[c658a8afdf] - (SEMVER-MINOR) worker: add eval ts input (Marco Ippolito) #56394
[2e5d038f48] - worker: refactor stdio to improve performance (Matteo Collina) #56630
[f959805d01] - worker: flush stdout and stderr on exit (Matteo Collina) #56428

Windows 32-bit Installer: https://nodejs.org/dist/v22.14.0/node-v22.14.0-x86.msi 
Windows 64-bit Installer: https://nodejs.org/dist/v22.14.0/node-v22.14.0-x64.msi 
Windows ARM 64-bit Installer: https://nodejs.org/dist/v22.14.0/node-v22.14.0-arm64.msi 
Windows 32-bit Binary: https://nodejs.org/dist/v22.14.0/win-x86/node.exe 
Windows 64-bit Binary: https://nodejs.org/dist/v22.14.0/win-x64/node.exe 
Windows ARM 64-bit Binary: https://nodejs.org/dist/v22.14.0/win-arm64/node.exe 
macOS 64-bit Installer: https://nodejs.org/dist/v22.14.0/node-v22.14.0.pkg 
macOS Apple Silicon 64-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-darwin-arm64.tar.gz 
macOS Intel 64-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-darwin-x64.tar.gz 
Linux 64-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-linux-x64.tar.xz 
Linux PPC LE 64-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-linux-ppc64le.tar.xz 
Linux s390x 64-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-linux-s390x.tar.xz 
AIX 64-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-aix-ppc64.tar.gz 
ARMv7 32-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-linux-armv7l.tar.xz 
ARMv8 64-bit Binary: https://nodejs.org/dist/v22.14.0/node-v22.14.0-linux-arm64.tar.xz 
Source Code: https://nodejs.org/dist/v22.14.0/node-v22.14.0.tar.gz 
Other release files: https://nodejs.org/dist/v22.14.0/ 
Documentation: https://nodejs.org/docs/v22.14.0/api/
SHASUMS
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

ddec69aded8f826efbef8b4af09baa451f5ed8b057e84d836accf33de04d2d7a  node-v22.14.0-aix-ppc64.tar.gz
d733bc14c2da6b69af4d89117b39d33cc1a7c7a75a21cf9fa4bfe81884db70e5  node-v22.14.0-arm64.msi
e9404633bc02a5162c5c573b1e2490f5fb44648345d64a958b17e325729a5e42  node-v22.14.0-darwin-arm64.tar.gz
4e845cb71b4e897289312743b2e31c405a8a48720655404d82a4dce23fc43527  node-v22.14.0-darwin-arm64.tar.xz
6698587713ab565a94a360e091df9f6d91c8fadda6d00f0cf6526e9b40bed250  node-v22.14.0-darwin-x64.tar.gz
deb5b211c25f3f803cd49c1c3fc3964e6c3725546d7d9608d994270388dcbf02  node-v22.14.0-darwin-x64.tar.xz
715aedf641a024efdeeccd545ce4acdc4759155e08c32efdfe9069921fcfa86b  node-v22.14.0-headers.tar.gz
63406b4329c080a8ff3e5c8794cae82ad9ef1ebb8b8d8c73f82d2077810e0eaf  node-v22.14.0-headers.tar.xz
8cf30ff7250f9463b53c18f89c6c606dfda70378215b2c905d0a9a8b08bd45e0  node-v22.14.0-linux-arm64.tar.gz
08bfbf538bad0e8cbb0269f0173cca28d705874a67a22f60b57d99dc99e30050  node-v22.14.0-linux-arm64.tar.xz
1cadf5aee7d71b6f0921235faec05e42d908ba5e8a76959f0697968fe0741204  node-v22.14.0-linux-armv7l.tar.gz
32804b6d7fca03e668765e91ea892dd329ff928d02e6c61d4b3b3c4afac178c6  node-v22.14.0-linux-armv7l.tar.xz
a0818ece898175db71a1df81dc4fdc3794a14b03a3901894a88e465e745ea429  node-v22.14.0-linux-ppc64le.tar.gz
70aeb7b16dabce5395b1ed383f60416c8d8ed693927003f948c0b5390a35ce5f  node-v22.14.0-linux-ppc64le.tar.xz
0e4232e4b3c0312a391bb9c0c36524623b3b616cac5d0338d743ae4228f984d1  node-v22.14.0-linux-s390x.tar.gz
a666b8ce5e442dbdbb5a2280f29caad603a723017aac5acf5baff5b16e648981  node-v22.14.0-linux-s390x.tar.xz
9d942932535988091034dc94cc5f42b6dc8784d6366df3a36c4c9ccb3996f0c2  node-v22.14.0-linux-x64.tar.gz
69b09dba5c8dcb05c4e4273a4340db1005abeafe3927efda2bc5b249e80437ec  node-v22.14.0-linux-x64.tar.xz
06113f553199227be915d80c191031283622d6363c57c83aad137ff33f9ac9e4  node-v22.14.0-win-arm64.7z
2d71f5f9b2fffa33baa108c07d74b0d24e0c3dd8f441d567772ae0e3dd4b1a22  node-v22.14.0-win-arm64.zip
4c6018bd170ca46bfc7112bc4ca0f43cf55759a0c2bb9fccd50d8f8c3f7bdb14  node-v22.14.0-win-x64.7z
55b639295920b219bb2acbcfa00f90393a2789095b7323f79475c9f34795f217  node-v22.14.0-win-x64.zip
5f7514392d0330c1b0c76e0dd299ac44786139b59231bf3d115f85c9c763bd4f  node-v22.14.0-win-x86.7z
4bf00caba7b0f3c7a4e8ee6a5b73049db19b5ee5510473219ae5fb649c2017b6  node-v22.14.0-win-x86.zip
2c0cc97ec64c1e4111362e1e32e0547fd870e4d9c79ec844c117da583f21b386  node-v22.14.0-x64.msi
5e91ec1da6c7de32406f3f7fdd2b8de163e871ecd2ebb466bf1b526e6379755c  node-v22.14.0-x86.msi
3931585e6af0785f01af897d31d67b7318e724af07845ffb04d432ab1a4532b4  node-v22.14.0.pkg
6c4e31ed5702dc45cfd8c435af56a36a474427e1bd7afe74c346136060beba8a  node-v22.14.0.tar.gz
c609946bf793b55c7954c26582760808d54c16185d79cb2fb88065e52de21914  node-v22.14.0.tar.xz
b37c6950508f266d066deb91abe2050fcd3f19e34c86ca89eed72efb40090b57  win-arm64/node.exe
988eb8c60a5ade17e652dbdb60d56d3c6ad5e599a99ce04932b8c4c86583cdaf  win-arm64/node.lib
37bf09d40005ba618a49aec998c275c56e390df501c824ed73c87520834de4b3  win-arm64/node_pdb.7z
6d51bdae6ec86490338d22eb133ae34c00be9052455f94cb44d08062097ed02c  win-arm64/node_pdb.zip
33b1bc1a8aca11fd5a4f2699e51019c63c0af30cf437701d07af69be7706771b  win-x64/node.exe
65e45757c026c93a170743a811ef1b921ae12d6d9dd62d258bbbca0626687626  win-x64/node.lib
652acb96f8c81f6fb27517f729465a5644c02efd27d60925389323b11cb44ee8  win-x64/node_pdb.7z
b4722c573f3a387de9e259b116cefd36870516947c1048fda425f1641d72a7b9  win-x64/node_pdb.zip
d52f9f1b03eca305dbaa23e251fe612efbb48a99aaeab6ffa073ee1c111b28e0  win-x86/node.exe
79bae10059e833ce7fa4de05e5601034461327e2e7cb75c2144b87d4ab5ac547  win-x86/node.lib
2f2313b969b715af1c59aa7d7a84c36f7d09cc5e0c6c6dc9365bad6341e06ae2  win-x86/node_pdb.7z
45edff3492d39ffee520881ab1050afc2b515cb6817285a153291af04af2c668  win-x86/node_pdb.zip
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEEwNYkhDnx1WBKr/tAIdkA/9sjN1YFAmerKiUACgkQIdkA/9sj
N1baDw/9GGx+nxucDIL5sfN6dnU4i0bodGVlCGh8RKotUuexbZkZ2SrlYokuJfM8
c+NFUeUTimNgeGXxuReQZ0U5oCC9YVrEsoSFyHsUJjx9D+mNhKw/beWdZrA1m5L2
I6BzGOZwkJ1PygJM0mQ/l0Wp0ZMbMVW2N9cvj+u/2HtJ7a/H9n938R51uMofELy2
I7l0hm/Q53X8nhpYfNXEhrqVRWs8mWZ9ha/pA1ydI/YWPlIs1jdPkD+ULteMsc37
7vv0JqYHAvnIov2H5SDW9WMfX+RUAqSWL570PhmmK1InnI+VfFAPhOozueugtVmd
CsKy+nUMHV3R5P5Nalq0X6lfGiaFGMjOVMZ1TFAG/cwOl8ZK/pWK1VVArKfN6Vgq
pOUFh8iSohfBseglcBbK+OA9HVM8tGGGydfprKkuzLIekRyl8yoInGLRtEVpcHQx
wsEG3Ycgwxd4ico8IlWburUswSzplmmeJhiyOzk3y449rrohgDD7Z0SQgaVe5vNu
br3o/bRwqufjUqaG63Wu4iYRgKuUdTAOEc1x4ne71L37rL6aA51SfqAKjYYNsbRi
BJ9/2R9J0a899nd0Sin5w+3Bo18ABnFn7Zgnyd2ow3qN3p74hjzBw4Msc5kJ1mQy
DyOevQux9/w9cYj7UOp+Ahyp8Egs/0QtxT651zL4LzXD7TQE+5M=
=DK2a
-----END PGP SIGNATURE-----
PrevNode v23.8.0 (Current)NextNode v20.18.3 (LTS)\n\n\n\nIndex of /download/nightly/\nv0.10.41-nightly20151203036580393d/\nv0.10.42-nightly2016011328ab7b0e75/\nv0.10.42-nightly20160128b125512a5c/\nv0.12.10-nightly20160113801f6ad8a3/\nv0.12.10-nightly20160128a305339f66/\nv10.0.0-nightly2017110118df171307/\nv10.0.0-nightly20171201e9e9863ca7/\nv10.0.0-nightly20180201ad94be84f9/\nv10.0.0-nightly20180301740c426b21/\nv10.0.0-nightly20180401141be923f3/\nv10.15.3-nightly20190301156e4c8e89/\nv10.15.4-nightly201905019101d9368f/\nv10.16.2-nightly20190801f025f8524b/\nv10.5.1-nightly20180701b28fd37a69/\nv10.7.1-nightly2018080162fd84528e/\nv10.9.0-nightly201809018da21a762f/\nv10.9.0-nightly20180906707a37f74f/\nv11.0.0-nightly20180501d3abb60b05/\nv11.0.0-nightly201807017bdc694267/\nv11.0.0-nightly2018080119aa41c6fc/\nv11.0.0-nightly20180901c8880ea276/\nv11.0.0-nightly201810011be804d625/\nv11.1.0-nightly20181101af6d26281f/\nv12.0.0-nightly2018110151294c897f/\nv12.0.0-nightly20181201f34ca9f783/\nv12.0.0-nightly20190101da9a4d0fb4/\nv12.0.0-nightly201902017c9fba30ef/\nv12.0.0-nightly20190301584305841d/\nv12.0.0-nightly2019040166b95362df/\nv12.22.10-nightly2022012660c9d3bb95/\nv13.0.0-nightly201905014206e7c2c4/\nv13.0.0-nightly20190601aa8b820aaa/\nv13.0.0-nightly2019070120d099af7a/\nv13.0.0-nightly201908018492acfd57/\nv13.0.0-nightly201909019ab1e07774/\nv13.0.0-nightly2019100116e47b976b/\nv13.12.1-nightly20200401ffdd82ba3f/\nv13.2.1-nightly20191201cbd50262c0/\nv13.9.1-nightly202003014d05508aa8/\nv13.9.1-nightly202003041bca7b6c70/\nv14.0.0-nightly201911012d8307e199/\nv14.0.0-nightly2019120149fb529139/\nv14.0.0-nightly20200101c052113238/\nv14.0.0-nightly2020020122724894c9/\nv14.0.0-nightly2020030111b7684522/\nv14.0.0-nightly202004019c00af0716/\nv14.0.0-nightly20200421c3554307c6/\nv15.0.0-nightly20200501e9518254d7/\nv15.0.0-nightly20200701e2b468eb5c/\nv15.0.0-nightly20200801cc7ec889e8/\nv15.0.0-nightly2020090159cad32b51/\nv15.0.0-nightly20201001726143e683/\nv15.0.0-nightly2020102011f1ad939f/\nv16.0.0-nightly202011015735525404/\nv16.0.0-nightly202012012dc6bf0d12/\nv16.0.0-nightly2021010151b4367506/\nv16.0.0-nightly20210201683754c6f3/\nv16.0.0-nightly202103011e34df139c/\nv16.0.0-nightly2021040131fe3b215f/\nv16.0.0-nightly20210420a0261d231c/\nv17.0.0-nightly20210501c975dff3c0/\nv17.0.0-nightly20210601910009d5c6/\nv17.0.0-nightly202107017ebf36ca98/\nv17.0.0-nightly202108014f9fd8d244/\nv17.0.0-nightly20210901926152a38c/\nv17.0.0-nightly2021100135dc3861cd/\nv17.5.0-nightly20220209e43808936a/\nv18.0.0-nightly2021110186099a375a/\nv18.0.0-nightly20211201bbed2a77d3/\nv18.0.0-nightly20220101d0c1176533/\nv18.0.0-nightly202202017123a00b03/\nv18.0.0-nightly20220301ce1c53665e/\nv18.0.0-nightly202204018dbdca8ed3/\nv18.0.0-nightly20220419bde889bd4e/\nv19.0.0-nightly20220501d268cf5a22/\nv19.0.0-nightly202206017ad5b420ae/\nv19.0.0-nightly2022070156c15f1c95/\nv19.0.0-nightly2022080107d7e1b0c1/\nv19.0.0-nightly20220901f36813c598/\nv19.0.0-nightly202210182672219b78/\nv20.0.0-nightly20221101590cf569fe/\nv20.0.0-nightly202212013bed5f11e0/\nv20.0.0-nightly20230111384e1b5174/\nv20.0.0-nightly2023012127c51243b5/\nv20.0.0-nightly20230131938341ac61/\nv20.0.0-nightly202302019e46e0bbc2/\nv20.0.0-nightly20230211a37c083dc1/\nv20.0.0-nightly20230221b85b5ba10c/\nv20.0.0-nightly202303010597f1b673/\nv20.0.0-nightly20230311359172868f/\nv20.0.0-nightly20230321965ea9d586/\nv20.0.0-nightly20230331978b57d750/\nv20.0.0-nightly2023040139a08ee8b8/\nv20.0.0-nightly2023041197d3912eb8/\nv20.11.0-nightly202312211a0be537da/\nv20.7.0-nightly202309173557c436181c526f41bb6ce775>\nv21.0.0-nightly202304192e0152ccf1/\nv21.0.0-nightly20230420b68cedd4d8/\nv21.0.0-nightly20230421472ffdef73/\nv21.0.0-nightly20230422595b2b3fd2/\nv21.0.0-nightly20230423511d40cecd/\nv21.0.0-nightly202304242fbe124e68/\nv21.0.0-nightly20230425a437bb9e6d/\nv21.0.0-nightly202304262e44a14cfb/\nv21.0.0-nightly202304278b66dc6ea9/\nv21.0.0-nightly202304287ae1360358/\nv21.0.0-nightly20230429c968361829/\nv21.0.0-nightly202304300a3f6a9d07/\nv21.0.0-nightly20230501b5fe45fb9a/\nv21.0.0-nightly2023050232778b8d0e/\nv21.0.0-nightly20230503b0ca77032a/\nv21.0.0-nightly2023050476ae7be78d/\nv21.0.0-nightly20230505af9b48a2f1/\nv21.0.0-nightly20230506a90a1459ee/\nv21.0.0-nightly202305076fb10cad30/\nv21.0.0-nightly202305089398ff1dea/\nv21.0.0-nightly202305090b3fcfcf35/\nv21.0.0-nightly202305101b177932a1/\nv21.0.0-nightly20230511ea8fd2dfe0/\nv21.0.0-nightly202305122e67401710/\nv21.0.0-nightly202305132dd6d76c89/\nv21.0.0-nightly20230514cdd20cfd71/\nv21.0.0-nightly202305158b3777d0c8/\nv21.0.0-nightly20230516f36461dc6a/\nv21.0.0-nightly20230517ca096563e0/\nv21.0.0-nightly2023051862fd6bc4aa/\nv21.0.0-nightly202305191918241190/\nv21.0.0-nightly2023052085ac915045/\nv21.0.0-nightly2023052119fa9f1bc4/\nv21.0.0-nightly20230522c7fe303eaf/\nv21.0.0-nightly2023052392a938b4dd/\nv21.0.0-nightly20230524d39ba8aaf4/\nv21.0.0-nightly20230525d2a1f71c3e/\nv21.0.0-nightly20230526847b9e0884/\nv21.0.0-nightly20230527be469d85da/\nv21.0.0-nightly20230528186307a484/\nv21.0.0-nightly20230529607c8f4eac/\nv21.0.0-nightly2023053049412c9632/\nv21.0.0-nightly202305318aa02e81d0/\nv21.0.0-nightly202306015e98a74327/\nv21.0.0-nightly202306022fca7ea2be/\nv21.0.0-nightly20230603e9e1181781/\nv21.0.0-nightly202306044bb06dbd0a/\nv21.0.0-nightly202306055c27cc2afb/\nv21.0.0-nightly202306061b4ce6981d/\nv21.0.0-nightly2023060772ba09905a/\nv21.0.0-nightly20230608077fd7d83d/\nv21.0.0-nightly20230609da1c9e3ecb/\nv21.0.0-nightly20230610ad0bbaf34a/\nv21.0.0-nightly20230611718f62bfcf/\nv21.0.0-nightly20230612a40a6c890a/\nv21.0.0-nightly20230613d402e2ab78/\nv21.0.0-nightly20230614b0e08d178e/\nv21.0.0-nightly20230615f3b713d187/\nv21.0.0-nightly202306163c35cd4a74/\nv21.0.0-nightly202306170d725d6fa0/\nv21.0.0-nightly20230618ff14b24e12/\nv21.0.0-nightly202306199bdd17230d/\nv21.0.0-nightly2023062043c3d9f9b4/\nv21.0.0-nightly2023062149951ec450/\nv21.0.0-nightly20230622da80964a3d/\nv21.0.0-nightly20230623640a791831/\nv21.0.0-nightly20230624578ffe1edb/\nv21.0.0-nightly2023062542d8143ce5/\nv21.0.0-nightly20230626b38bc9fc89/\nv21.0.0-nightly202306279117d45bc1/\nv21.0.0-nightly20230628900ae1bda7/\nv21.0.0-nightly20230629fe4ac37ecf/\nv21.0.0-nightly20230630951da5282c/\nv21.0.0-nightly202307011936160c31/\nv21.0.0-nightly2023070250477fa353/\nv21.0.0-nightly20230703acf071e9d5/\nv21.0.0-nightly202307042f369ccacf/\nv21.0.0-nightly202307053ca45cf8c7/\nv21.0.0-nightly20230706b5e16adb1d/\nv21.0.0-nightly20230707d9438ccbd8/\nv21.0.0-nightly2023070838dee8a1c0/\nv21.0.0-nightly20230709eece8d755c/\nv21.0.0-nightly2023071141f70568a4/\nv21.0.0-nightly202307120e9138d173/\nv21.0.0-nightly20230713b76862df0a/\nv21.0.0-nightly202307148efdc7d61a/\nv21.0.0-nightly202307155da84a6341/\nv21.0.0-nightly20230716373848a457/\nv21.0.0-nightly202307177196946d7f/\nv21.0.0-nightly202307188f7c4e9cbf/\nv21.0.0-nightly2023071938cc538453/\nv21.0.0-nightly20230720c301404105/\nv21.0.0-nightly202307214ee4718857/\nv21.0.0-nightly202307229dd574c9e2/\nv21.0.0-nightly202307236c08b1fc02/\nv21.0.0-nightly202307241ceb8c113d/\nv21.0.0-nightly202307259edea976a1/\nv21.0.0-nightly20230726d246536924/\nv21.0.0-nightly202307270da3f61a19/\nv21.0.0-nightly2023072848345d0f62/\nv21.0.0-nightly202307298f0f17e1e3/\nv21.0.0-nightly20230730a955c534a8/\nv21.0.0-nightly20230801d396a041f7/\nv21.0.0-nightly202308025f2539cd9a/\nv21.0.0-nightly202308036ad8318373/\nv21.0.0-nightly202308043af7cfe7d4/\nv21.0.0-nightly20230805556b1ca900/\nv21.0.0-nightly20230806d150316a8e/\nv21.0.0-nightly202308096432060c7c/\nv21.0.0-nightly20230810b68e5e7981/\nv21.0.0-nightly202308117f2c810814/\nv21.0.0-nightly20230812bb52656fc6/\nv21.0.0-nightly20230813ee8b7f1f18/\nv21.0.0-nightly20230814de4553f3d1/\nv21.0.0-nightly20230815802c52fb8b/\nv21.0.0-nightly202308166391b3b95d/\nv21.0.0-nightly20230817b4a2be457c/\nv21.0.0-nightly20230818c021b27221/\nv21.0.0-nightly2023081964a5c01b99/\nv21.0.0-nightly202308202557932db2/\nv21.0.0-nightly20230821484ad83358/\nv21.0.0-nightly2023082262b2cf30f2/\nv21.0.0-nightly202308230d080a269e/\nv21.0.0-nightly20230824feb5b0fef8/\nv21.0.0-nightly2023082533710e7e7d/\nv21.0.0-nightly202308264178683a26/\nv21.0.0-nightly2023082756c3263049/\nv21.0.0-nightly202308280409cdd91c/\nv21.0.0-nightly202308297104a715e0/\nv21.0.0-nightly202308309848352974/\nv21.0.0-nightly202308313a6a80a4e1/\nv21.0.0-nightly2023090192fb7dd818/\nv21.0.0-nightly2023090212ee988e95/\nv21.0.0-nightly202309030add7a8f0c/\nv21.0.0-nightly202309048dfe4248ca/\nv21.0.0-nightly202309056a5394ea7d/\nv21.0.0-nightly2023090654021850f9/\nv21.0.0-nightly2023090763c3846d0a/\nv21.0.0-nightly202309080bce573bd4/\nv21.0.0-nightly202309090a2ab4c77c/\nv21.0.0-nightly20230910ae115d68e0/\nv21.0.0-nightly20230911b3fc9173ed/\nv21.0.0-nightly20230912e4d1259e5f/\nv21.0.0-nightly202309132a14a79c99/\nv21.0.0-nightly202309144efa3744a0/\nv21.0.0-nightly202309156a489df73b/\nv21.0.0-nightly20230916db8217b1bf/\nv21.0.0-nightly2023091756ecf29283/\nv21.0.0-nightly202309181995eca29b/\nv21.0.0-nightly20230919f91b4e2bf0/\nv21.0.0-nightly20230920346abdd060/\nv21.0.0-nightly20230921480ab8c3a4/\nv21.0.0-nightly202309229718a9465c/\nv21.0.0-nightly20230923645b788bea/\nv21.0.0-nightly2023092455fde47b1d/\nv21.0.0-nightly2023092577597d3aea/\nv21.0.0-nightly20230926f16f41c5b3/\nv21.0.0-nightly20230927a6ad048b89/\nv21.0.0-nightly202309283838b579e4/\nv21.0.0-nightly202309295570c29780/\nv21.0.0-nightly2023093051f4ff2450/\nv21.0.0-nightly20231001092fb9f541/\nv21.0.0-nightly2023100285c09f178c/\nv21.0.0-nightly202310039c683204db/\nv21.0.0-nightly202310041a839f388e/\nv21.0.0-nightly202310051d220b55ac/\nv21.0.0-nightly20231006f73650ea52/\nv21.0.0-nightly202310076b599a3f4e/\nv21.0.0-nightly20231008fce8fbadcd/\nv21.0.0-nightly20231009387e2929fe/\nv21.0.0-nightly202310100522ac086c/\nv21.0.0-nightly20231011ed49722a8a/\nv21.0.0-nightly20231012bf0f0789da/\nv21.0.0-nightly20231013766198b9e1/\nv21.0.0-nightly20231014d1ef6aa2db/\nv21.0.0-nightly20231015aad8002b88/\nv21.0.0-nightly20231016f09a50c39d/\nv21.0.0-nightly20231017ea595ebbf2/\nv21.0.0-nightly20231018f971106072/\nv21.0.0-nightly20231019c016397276/\nv21.0.0-nightly20231020eed33c9dea/\nv21.0.0-nightly2023102163e9d0282d/\nv21.0.0-nightly202310220cec82277c/\nv21.0.0-nightly2023102325576b5118/\nv21.0.0-nightly202310240fb512344f/\nv22.0.0-nightly20231025d1ccca9d2b/\nv22.0.0-nightly20231026c41cf6fd49/\nv22.0.0-nightly2023102767b1383149/\nv22.0.0-nightly2023102814af167209/\nv22.0.0-nightly2023102965087c0486/\nv22.0.0-nightly202310302aaa21f9f6/\nv22.0.0-nightly20231031aa4248dc84/\nv22.0.0-nightly20231101ffb326c583/\nv22.0.0-nightly20231102401ea75bdd/\nv22.0.0-nightly20231103a450eedffa/\nv22.0.0-nightly2023110494156e35df/\nv22.0.0-nightly2023110577b0595518/\nv22.0.0-nightly202311062a1bd660bd/\nv22.0.0-nightly2023110733704c46e3/\nv22.0.0-nightly202311084d6c8a09e0/\nv22.0.0-nightly20231109bb2dd0e90c/\nv22.0.0-nightly202311103e14cfbbcf/\nv22.0.0-nightly202311113cce03a03f/\nv22.0.0-nightly20231112468b152a77/\nv22.0.0-nightly202311135e250bd726/\nv22.0.0-nightly20231114996d198101/\nv22.0.0-nightly202311151d8483e713/\nv22.0.0-nightly2023111659b27d6990/\nv22.0.0-nightly20231117d1326e5b54/\nv22.0.0-nightly202311184e23d6904c/\nv22.0.0-nightly2023111959ebf6d397/\nv22.0.0-nightly202311205b73da02e5/\nv22.0.0-nightly20231121b6b05d92f0/\nv22.0.0-nightly202311221858341377/\nv22.0.0-nightly20231123ea88a3e1f2/\nv22.0.0-nightly20231124fbfa77b712/\nv22.0.0-nightly202311250bb5d88871/\nv22.0.0-nightly20231126ed5cb37ccb/\nv22.0.0-nightly202311274466deeb34/\nv22.0.0-nightly2023112869866bf1b6/\nv22.0.0-nightly20231129a3ee1870fd/\nv22.0.0-nightly20231130431f32e302/\nv22.0.0-nightly202312012f4065250e/\nv22.0.0-nightly202312021b16bf6561/\nv22.0.0-nightly202312032e458d9736/\nv22.0.0-nightly20231204951d00d36b/\nv22.0.0-nightly202312059def0a9f94/\nv22.0.0-nightly20231206ac9e594e32/\nv22.0.0-nightly202312073f4ea7ad7f/\nv22.0.0-nightly202312088cdb7cae98/\nv22.0.0-nightly202312091ba508d51b/\nv22.0.0-nightly2023121037ba7a36e9/\nv22.0.0-nightly20231211fc102f2180/\nv22.0.0-nightly202312121b60054fff/\nv22.0.0-nightly20231213228bc5c457/\nv22.0.0-nightly2023121499f6084ef0/\nv22.0.0-nightly202312155ac658102d/\nv22.0.0-nightly2023121662ca05017d/\nv22.0.0-nightly20231217154afbed5b/\nv22.0.0-nightly2023121871e19e8f08/\nv22.0.0-nightly202312198573146f72/\nv22.0.0-nightly202312208b690a1fc1/\nv22.0.0-nightly20231221abbdc3efaa/\nv22.0.0-nightly20231222e1ad7fb38d/\nv22.0.0-nightly20231223b8fe07df9b/\nv22.0.0-nightly20231224a717fa2111/\nv22.0.0-nightly202312256a1abd2c03/\nv22.0.0-nightly2023122689ddc98b95/\nv22.0.0-nightly20231227c3664227a8/\nv22.0.0-nightly202312285e6448310d/\nv22.0.0-nightly202312299fe0424baa/\nv22.0.0-nightly2023123055f3993e3e/\nv22.0.0-nightly202312315fb6305971/\nv22.0.0-nightly20240102477d6d7cd5/\nv22.0.0-nightly20240103508490f79d/\nv22.0.0-nightly20240104084d761dfc/\nv22.0.0-nightly2024010568c8472ed9/\nv22.0.0-nightly2024010657c22e4a22/\nv22.0.0-nightly20240107921406ce71/\nv22.0.0-nightly202401081a5acd0638/\nv22.0.0-nightly202401090090c10782/\nv22.0.0-nightly20240110be755e06c7/\nv22.0.0-nightly20240111d102d16e98/\nv22.0.0-nightly20240112af8ba37335/\nv22.0.0-nightly20240113c25878d370/\nv22.0.0-nightly2024011594f824a19d/\nv22.0.0-nightly20240116e133e5115a/\nv22.0.0-nightly202401186ab15a1aa8/\nv22.0.0-nightly20240119eb4432c12c/\nv22.0.0-nightly202401209a25438a62/\nv22.0.0-nightly2024012127d839f468/\nv22.0.0-nightly20240122e57a32a03a/\nv22.0.0-nightly20240123f820efe085/\nv22.0.0-nightly2024012426f63be878/\nv22.0.0-nightly202401256ae20aa63d/\nv22.0.0-nightly2024012609da597535/\nv22.0.0-nightly20240127af3e2b2493/\nv22.0.0-nightly2024012864c6d97463/\nv22.0.0-nightly202401300f461aad4c/\nv22.0.0-nightly202402013fbe1579ce/\nv22.0.0-nightly2024020268885d5126/\nv22.0.0-nightly202402036f85b13c86/\nv22.0.0-nightly20240204eca4b1a403/\nv22.0.0-nightly202402059448c61e08/\nv22.0.0-nightly202402068a41d9b636/\nv22.0.0-nightly20240212a8de25ed15/\nv22.0.0-nightly20240213544cfc5ef1/\nv22.0.0-nightly20240214bf39716735/\nv22.0.0-nightly2024021536dcd399c0/\nv22.0.0-nightly2024021668fd5cbd5a/\nv22.0.0-nightly2024021778273ed0d1/\nv22.0.0-nightly202402190550bc149c/\nv22.0.0-nightly202402209642532784/\nv22.0.0-nightly202402210951e7b79d/\nv22.0.0-nightly202402225dfff3ad90/\nv22.0.0-nightly20240223f29d2b7053/\nv22.0.0-nightly202402240161ad0baf/\nv22.0.0-nightly20240225a492646ff1/\nv22.0.0-nightly20240226fc0f2cf475/\nv22.0.0-nightly202402276bb7c4d916/\nv22.0.0-nightly20240228a51efa2bcf/\nv22.0.0-nightly20240229f4af4b111c/\nv22.0.0-nightly2024030130c9181994/\nv22.0.0-nightly2024030214293814a7/\nv22.0.0-nightly202403032a70831d27/\nv22.0.0-nightly20240304b34512e38e/\nv22.0.0-nightly202403057f0b80525a/\nv22.0.0-nightly20240306384fd17876/\nv22.0.0-nightly202403074f3cf4e89a/\nv22.0.0-nightly2024030873025c4dec/\nv22.0.0-nightly20240309f0e6acde2d/\nv22.0.0-nightly20240310575ced8139/\nv22.0.0-nightly202403120b4cdb4b42/\nv22.0.0-nightly2024031378be0d0f1c/\nv22.0.0-nightly20240314639c096004/\nv22.0.0-nightly202403156ad5353764/\nv22.0.0-nightly20240316ba06c5c509/\nv22.0.0-nightly2024031705db979c01/\nv22.0.0-nightly20240318454d0806a1/\nv22.0.0-nightly20240319a21b15a14e/\nv22.0.0-nightly20240320f1949ac1ae/\nv22.0.0-nightly20240321c714cda9a7/\nv22.0.0-nightly202403220b676736a0/\nv22.0.0-nightly20240323f4a0a3b04b/\nv22.0.0-nightly20240324bae14b7914/\nv22.0.0-nightly20240325a7f170e45a/\nv22.0.0-nightly202403261264414700/\nv22.0.0-nightly202403276d2d3f17ba/\nv22.0.0-nightly2024032827493a1dd7/\nv22.0.0-nightly2024032929de7f82cd/\nv22.0.0-nightly202403307c02486f1f/\nv22.0.0-nightly20240331021cf91208/\nv22.0.0-nightly202404019efc84a2cb/\nv22.0.0-nightly202404022c024cd24d/\nv22.0.0-nightly202404032241e8c5b3/\nv22.0.0-nightly20240404dd711d221a/\nv22.0.0-nightly20240405433bd1b04d/\nv22.0.0-nightly202404063f5ff8dc20/\nv22.0.0-nightly2024040747c934e464/\nv22.0.0-nightly20240408128c60d906/\nv22.0.0-nightly202404095bae73df90/\nv22.0.0-nightly20240410c82f3c9e80/\nv22.0.0-nightly2024041152f8dcfccc/\nv22.0.0-nightly20240412422163174c/\nv22.0.0-nightly202404132cd3073e0f/\nv22.0.0-nightly2024041411f8765475/\nv22.0.0-nightly202404150debfa460f/\nv22.0.0-nightly20240416d54e55a44a/\nv22.0.0-nightly20240417139624c788/\nv22.0.0-nightly2024041887abd023aa/\nv22.0.0-nightly2024041907f481cfcf/\nv22.0.0-nightly20240420d545984a02/\nv22.0.0-nightly20240421654508bd9a/\nv22.0.0-nightly2024042244f81a1b7d/\nv22.0.0-nightly20240423a2f3b1df93/\nv22.0.0-nightly20240424ddd0a9e494/\nv23.0.0-nightly20240425708bffa999/\nv23.0.0-nightly20240426172820342b/\nv23.0.0-nightly2024042740ef9d541e/\nv23.0.0-nightly202404297c3dce0e4f/\nv23.0.0-nightly202404304487e37e70/\nv23.0.0-nightly202405015976985a58/\nv23.0.0-nightly20240502b876e00b47/\nv23.0.0-nightly2024050315456e4e57/\nv23.0.0-nightly2024050471a1fa3043/\nv23.0.0-nightly2024050577db3911be/\nv23.0.0-nightly20240506f5001070dd/\nv23.0.0-nightly20240507be8d64ec14/\nv23.0.0-nightly2024050822cb99d073/\nv23.0.0-nightly202405092863c54257/\nv23.0.0-nightly20240510a923fed874/\nv23.0.0-nightly20240511c70fa2c6dc/\nv23.0.0-nightly20240512d78537b3df/\nv23.0.0-nightly202405131d7d094a98/\nv23.0.0-nightly202405148125a7e89f/\nv23.0.0-nightly202405159807ede6fb/\nv23.0.0-nightly202405166a2d6df83d/\nv23.0.0-nightly20240517075853ed19/\nv23.0.0-nightly20240518559212e64c/\nv23.0.0-nightly20240519a619789ef0/\nv23.0.0-nightly20240520b1c1fafd95/\nv23.0.0-nightly20240521786cb42956/\nv23.0.0-nightly202405224a54a80aa3/\nv23.0.0-nightly202405231b965270a9/\nv23.0.0-nightly20240524c7a63b0740/\nv23.0.0-nightly202405252079a7aec4/\nv23.0.0-nightly20240526aaca18b54e/\nv23.0.0-nightly20240527ff659faeb8/\nv23.0.0-nightly202405288e9686d9c9/\nv23.0.0-nightly20240529c0c598d753/\nv23.0.0-nightly20240530d953861daf/\nv23.0.0-nightly2024053154035ac0ca/\nv23.0.0-nightly2024060100a86fe76f/\nv23.0.0-nightly2024060241d90aaf62/\nv23.0.0-nightly202406038c5c2c18b6/\nv23.0.0-nightly20240604f2f45a0762/\nv23.0.0-nightly20240605b26a260ce5/\nv23.0.0-nightly202406065469d04f8f/\nv23.0.0-nightly20240607479b8e5232/\nv23.0.0-nightly2024060850695e5de1/\nv23.0.0-nightly202406113a7d8c8e9f/\nv23.0.0-nightly20240612d6c77ded0f/\nv23.0.0-nightly202406133597070ef8/\nv23.0.0-nightly202406142e1f7720df/\nv23.0.0-nightly20240615d3025372d8/\nv23.0.0-nightly20240616672e4ccf05/\nv23.0.0-nightly20240617474d2f4834/\nv23.0.0-nightly2024061826f2cbdd59/\nv23.0.0-nightly20240619f9207e9207/\nv23.0.0-nightly20240620399eb338f1/\nv23.0.0-nightly20240621fbdfe9399c/\nv23.0.0-nightly20240622d335487e3f/\nv23.0.0-nightly2024062378ea6cee1b/\nv23.0.0-nightly202406244c730aed7f/\nv23.0.0-nightly20240625a0cb507ea6/\nv23.0.0-nightly20240626eca806b54a/\nv23.0.0-nightly2024062753ac448022/\nv23.0.0-nightly202406289cd9aa8dfd/\nv23.0.0-nightly2024062977710251e1/\nv23.0.0-nightly202406302e5fc8aa1a/\nv23.0.0-nightly20240701c1dc307221/\nv23.0.0-nightly202407028f71a1b248/\nv23.0.0-nightly2024070366b76e24e2/\nv23.0.0-nightly20240704ce2faef3a7/\nv23.0.0-nightly202407055a775b3b9e/\nv23.0.0-nightly2024070610099bb3f7/\nv23.0.0-nightly2024070711f4efc73a/\nv23.0.0-nightly2024070841cb292778/\nv23.0.0-nightly20240709b9289a6e29/\nv23.0.0-nightly20240710e849dd6632/\nv23.0.0-nightly2024071197918364f6/\nv23.0.0-nightly20240712fc233627ed/\nv23.0.0-nightly2024071338b7ce3b1e/\nv23.0.0-nightly20240714e08a654fae/\nv23.0.0-nightly2024071586415e4688/\nv23.0.0-nightly20240716362afa52eb/\nv23.0.0-nightly202407170b1ff6965e/\nv23.0.0-nightly20240718bcec922e3e/\nv23.0.0-nightly20240719a523c345b1/\nv23.0.0-nightly20240720cf8e5356d9/\nv23.0.0-nightly2024072179759fa0ac/\nv23.0.0-nightly20240722259163802c/\nv23.0.0-nightly20240723cad73dadba/\nv23.0.0-nightly20240724aeaffbb385/\nv23.0.0-nightly202407253de7a4c374/\nv23.0.0-nightly20240726d955497874/\nv23.0.0-nightly202407272d1b4a8cf7/\nv23.0.0-nightly2024072820aff2b6ff/\nv23.0.0-nightly202407295210cd860a/\nv23.0.0-nightly20240730890760b8e5/\nv23.0.0-nightly20240731e7edcf38cd/\nv23.0.0-nightly20240801e2b7e41e23/\nv23.0.0-nightly202408028e1e3a8dea/\nv23.0.0-nightly20240803492032f34c/\nv23.0.0-nightly20240804d172da8d01/\nv23.0.0-nightly20240805ca2ed88f94/\nv23.0.0-nightly202408061bcdba233d/\nv23.0.0-nightly20240807a4f609faf5/\nv23.0.0-nightly2024080890dea9e3e0/\nv23.0.0-nightly2024080930f6c56e03/\nv23.0.0-nightly2024081090d91abbfe/\nv23.0.0-nightly2024081190f257176c/\nv23.0.0-nightly20240812559985cb7a/\nv23.0.0-nightly20240813a199c529bc/\nv23.0.0-nightly2024081402b30954a8/\nv23.0.0-nightly20240815ccf05ef751/\nv23.0.0-nightly202408168d37584874/\nv23.0.0-nightly20240817e4f61de14f/\nv23.0.0-nightly202408189e83853294/\nv23.0.0-nightly202408194f94397650/\nv23.0.0-nightly20240820561bc87c76/\nv23.0.0-nightly20240821821ffab0f7/\nv23.0.0-nightly202408228b0c699f2a/\nv23.0.0-nightly20240823a21af4bfb5/\nv23.0.0-nightly2024082422daeba24d/\nv23.0.0-nightly202408254e68b541fd/\nv23.0.0-nightly202408261399d4ea8a/\nv23.0.0-nightly20240827c00ea01f2b/\nv23.0.0-nightly20240828885692a34f/\nv23.0.0-nightly2024082929cf623567/\nv23.0.0-nightly2024083001f751b529/\nv23.0.0-nightly20240831d6f523480b/\nv23.0.0-nightly2024090171b36b3068/\nv23.0.0-nightly20240902981c701400/\nv23.0.0-nightly20240903298dea0c63/\nv23.0.0-nightly2024090403fe00e3da/\nv23.0.0-nightly20240905dc74f17f6c/\nv23.0.0-nightly20240906b2dc908644/\nv23.0.0-nightly20240907dcc2ed944f/\nv23.0.0-nightly2024090867357ba8ff/\nv23.0.0-nightly202409099404d3aaaf/\nv23.0.0-nightly20240910741004a3b8/\nv23.0.0-nightly20240911123bb4cb22/\nv23.0.0-nightly202409129db6327af3/\nv23.0.0-nightly20240913e21984b70c/\nv23.0.0-nightly20240914d64835f97e/\nv23.0.0-nightly20240915a65105ec28/\nv23.0.0-nightly2024091653ede878a5/\nv23.0.0-nightly20240917d5c29ba12d/\nv23.0.0-nightly2024091887e7aeb672/\nv23.0.0-nightly20240919c42d8461b0/\nv23.0.0-nightly202409204f70132972/\nv23.0.0-nightly2024092129b9c72b05/\nv23.0.0-nightly2024092265362f0181/\nv23.0.0-nightly202409236a6c957be7/\nv23.0.0-nightly202409243c5ceff85f/\nv23.0.0-nightly20240925773e7c67cf/\nv23.0.0-nightly20240926da5887d8e9/\nv23.0.0-nightly20240927668e523392/\nv23.0.0-nightly2024092818acff0d01/\nv23.0.0-nightly2024092927dab9d916/\nv23.0.0-nightly2024093099e0d0d218/\nv23.0.0-nightly20241001c08bb75618/\nv23.0.0-nightly20241002d24c7313f7/\nv23.0.0-nightly202410036b9413e41a/\nv23.0.0-nightly20241004b2161d3a13/\nv23.0.0-nightly2024100532efeea0c0/\nv23.0.0-nightly2024100620d8b85d34/\nv23.0.0-nightly202410079a9409ff1f/\nv23.0.0-nightly202410088dbca2d35b/\nv23.0.0-nightly2024100909d10b50dc/\nv23.0.0-nightly20241010e79ae1bf0c/\nv23.0.0-nightly2024101182dab76d63/\nv23.0.0-nightly202410129ba339119a/\nv23.0.0-nightly20241013d881fcba86/\nv23.0.0-nightly20241014129ca9e319/\nv23.0.0-nightly2024101587da1f3929/\nv23.0.0-nightly20241016019efe1453/\nv24.0.0-nightly20241017e2242b4e25/\nv24.0.0-nightly202410188ac867fd93/\nv24.0.0-nightly202410199f5000e0f2/\nv24.0.0-nightly2024102078b72ca7ba/\nv24.0.0-nightly202410217e60b5e15b/\nv24.0.0-nightly20241022e4ca097f56/\nv24.0.0-nightly20241023e90704cd9e/\nv24.0.0-nightly202410247b5d660bb1/\nv24.0.0-nightly202410257ddd2c2282/\nv24.0.0-nightly202410260668e64cea/\nv24.0.0-nightly202410275d4fee8975/\nv24.0.0-nightly202410282a965493a9/\nv24.0.0-nightly20241029ece37bc88c/\nv24.0.0-nightly202410306dea41d2f7/\nv24.0.0-nightly20241031996708042b/\nv24.0.0-nightly20241101824c149e79/\nv24.0.0-nightly202411025ce3d1078d/\nv24.0.0-nightly20241103c2ff449e3f/\nv24.0.0-nightly2024110432ff100bfa/\nv24.0.0-nightly2024110573cfaba8b6/\nv24.0.0-nightly20241106ccac4ee19d/\nv24.0.0-nightly20241107f38cefa04b/\nv24.0.0-nightly202411086af5c4e2b4/\nv24.0.0-nightly20241109069ec1b983/\nv24.0.0-nightly20241110c5a13605cb/\nv24.0.0-nightly20241113598bbf4833/\nv24.0.0-nightly20241115b02cd411c2/\nv24.0.0-nightly20241118746b17e1a5/\nv24.0.0-nightly20241120f4cd4d9749/\nv24.0.0-nightly202411215ba3b540d4/\nv24.0.0-nightly20241122ead8bd157a/\nv24.0.0-nightly2024112392c7dde664/\nv24.0.0-nightly20241124e92499c963/\nv24.0.0-nightly20241125c9bf257180/\nv24.0.0-nightly20241126fb8de039c9/\nv24.0.0-nightly20241127ae8280c95d/\nv24.0.0-nightly20241128853b304db8/\nv24.0.0-nightly20241129b5eb77fd5d/\nv24.0.0-nightly2024113003ec900e07/\nv24.0.0-nightly202412013fb2ea8689/\nv24.0.0-nightly2024120261e4ad5352/\nv24.0.0-nightly202412035ef4985175/\nv24.0.0-nightly202412047bedcfd4a2/\nv24.0.0-nightly20241205c4aa34aa4d/\nv24.0.0-nightly20241206f26a1da909/\nv24.0.0-nightly202412079c046ea804/\nv24.0.0-nightly20241208b5056be854/\nv24.0.0-nightly20241209dbfcbe371c/\nv24.0.0-nightly202412106c03beba46/\nv24.0.0-nightly20241211fd9a79df7d/\nv24.0.0-nightly202412126cd1805364/\nv24.0.0-nightly20241213e698bd0943/\nv24.0.0-nightly2024121426d0559f14/\nv24.0.0-nightly202412152d4e35c74d/\nv24.0.0-nightly20241216c39875a32d/\nv24.0.0-nightly202412172cd385ef67/\nv24.0.0-nightly202412188253290d60/\nv24.0.0-nightly20241219756077867b/\nv24.0.0-nightly20241220219b900384/\nv24.0.0-nightly20241221d780b90b58/\nv24.0.0-nightly20241222b814038447/\nv24.0.0-nightly2024122348c75bc02b/\nv24.0.0-nightly20241224270a2f14aa/\nv24.0.0-nightly20241225821b0a733e/\nv24.0.0-nightly202412261d1d8f2df3/\nv24.0.0-nightly20241227ba5992831b/\nv24.0.0-nightly2024122838cd81aea4/\nv24.0.0-nightly2024122967b647edc7/\nv24.0.0-nightly20241230d2af881997/\nv24.0.0-nightly202412314633e5daa3/\nv24.0.0-nightly2025010135742a2d0b/\nv24.0.0-nightly2025010298d4ebc6d4/\nv24.0.0-nightly20250103639db215f7/\nv24.0.0-nightly20250104804d41f9c7/\nv24.0.0-nightly20250105338d70b752/\nv24.0.0-nightly20250106b736028c7f/\nv24.0.0-nightly20250107062ae6f3cb/\nv24.0.0-nightly20250108ea493c18b2/\nv24.0.0-nightly20250109dc5d0f9bb4/\nv24.0.0-nightly20250110a627a999f0/\nv24.0.0-nightly20250111ad68d088a3/\nv24.0.0-nightly20250112c61504bda4/\nv24.0.0-nightly20250113f4fcf0e613/\nv24.0.0-nightly202501145770972dc6/\nv24.0.0-nightly20250115f16cd10946/\nv24.0.0-nightly202501160e7ec5e7a1/\nv24.0.0-nightly2025011700d49649da/\nv24.0.0-nightly202501186f946c95b9/\nv24.0.0-nightly20250119009d53ec3c/\nv24.0.0-nightly20250120da5f7aca6a/\nv24.0.0-nightly202501217bc2946293/\nv24.0.0-nightly20250122bf59539b98/\nv24.0.0-nightly20250123309924f65d/\nv24.0.0-nightly2025012408eeddfa83/\nv24.0.0-nightly202501254a5d2c7538/\nv24.0.0-nightly20250126ce6a628720/\nv24.0.0-nightly202501271263efdfca/\nv24.0.0-nightly20250128532fff6b27/\nv24.0.0-nightly20250129e346323109/\nv24.0.0-nightly202501307f18407d0f/\nv24.0.0-nightly20250131be9b614f58/\nv24.0.0-nightly20250201b78df6c402/\nv24.0.0-nightly20250203793c7936c3/\nv24.0.0-nightly20250204eb11adfc02/\nv24.0.0-nightly2025020524cae2d7b8/\nv24.0.0-nightly202502061671921c4d/\nv24.0.0-nightly2025020719bfc83354/\nv24.0.0-nightly202502081b2d2f7e68/\nv24.0.0-nightly20250209b18153598b/\nv24.0.0-nightly202502101781f63633/\nv24.0.0-nightly20250211de1b34557b/\nv24.0.0-nightly20250212b07ff551db/\nv24.0.0-nightly20250213a7f648c8ba/\nv24.0.0-nightly2025021469d32d1cfe/\nv24.0.0-nightly20250215cc7018ec51/\nv24.0.0-nightly20250216d3841517e9/\nv24.0.0-nightly20250217f1b951fd22/\nv24.0.0-nightly202502186fe0723e40/\nv24.0.0-nightly20250219a724a9e8dd/\nv24.0.0-nightly20250220f6ce48636b/\nv24.0.0-nightly20250221772c609eb4/\nv24.0.0-nightly2025022290ab559f4d/\nv24.0.0-nightly20250223e2bc395c42/\nv24.0.0-nightly202502248c2df73db6/\nv24.0.0-nightly202502259fd90d9df7/\nv24.0.0-nightly20250226e03af77418/\nv24.0.0-nightly20250227269c851240/\nv24.0.0-nightly202502284ec9d42d48/\nv24.0.0-nightly20250301ed22a68254/\nv24.0.0-nightly202503026b0af1748c/\nv24.0.0-nightly20250303208f07adda/\nv24.0.0-nightly20250304635aed90c0/\nv24.0.0-nightly20250305c566639026/\nv24.0.0-nightly202503062a6f90813f/\nv24.0.0-nightly20250307365faa7a4f/\nv24.0.0-nightly202503083a497dc688/\nv24.0.0-nightly202503099a9a45adb7/\nv24.0.0-nightly2025031096457b433f/\nv24.0.0-nightly20250311a446e3bdc9/\nv24.0.0-nightly20250312a2a53cb728/\nv24.0.0-nightly20250313a0139e06a0/\nv24.0.0-nightly202503145d9b63dbd4/\nv24.0.0-nightly20250315d765e70802/\nv24.0.0-nightly202503168a5a849a44/\nv24.0.0-nightly20250317fab6906c5d/\nv24.0.0-nightly2025031836e89dd13c/\nv24.0.0-nightly20250319fe5817e06c/\nv24.0.0-nightly202503206b42554342/\nv24.0.0-nightly20250321afe3909483/\nv24.0.0-nightly20250322ffc1cf6205/\nv24.0.0-nightly20250323a4f556fc36/\nv24.0.0-nightly20250324c3b6f94974/\nv24.0.0-nightly20250325b4280ef72c/\nv24.0.0-nightly20250327186bbf7dfd/\nv24.0.0-nightly202503280a91e988cf/\nv24.0.0-nightly20250329e57841f87e/\nv24.0.0-nightly20250331eab0fe264b/\nv24.0.0-nightly202504015812a61a68/\nv24.0.0-nightly202504023db54912fa/\nv24.0.0-nightly20250403657f818532/\nv24.0.0-nightly20250404870dec25f7/\nv24.0.0-nightly202504057e43337fdd/\nv24.0.0-nightly2025040674722a55a6/\nv24.0.0-nightly20250407a2de5b9150/\nv24.0.0-nightly2025040877f88b9485/\nv24.0.0-nightly20250409437c6aac16/\nv24.0.0-nightly202504109bbbe60f6b/\nv24.0.0-nightly20250411795dd8eb79/\nv24.0.0-nightly2025041295b0f9d448/\nv24.0.0-nightly2025041309ecd2e84a/\nv24.0.0-nightly20250414004ecc12eb/\nv24.0.0-nightly202504152204587d76/\nv24.0.0-nightly2025041691d8a524ad/\nv24.0.0-nightly20250417e61937b82c/\nv4.0.0-nightly201509079cae65c510/\nv5.11.1-nightly2016050199920480ae/\nv5.11.2-nightly2016060121552bd0c5/\nv5.7.1-nightly20160301c83725c604/\nv5.7.2-nightly2016030801c331ea37/\nv6.0.0-nightly20160301d9f7a597e4/\nv6.5.1-nightly20160901180867d6a6/\nv6.7.1-nightly20161001d7454e7547/\nv6.9.6-nightly201702013f61aae59d/\nv7.0.0-nightly201605019f8d0ea6db/\nv7.0.0-nightly20160601f81f0c351a/\nv7.0.0-nightly2016080175c6d9dd95/\nv7.0.0-nightly201609013504a98b72/\nv7.0.0-nightly20161001c8c2544cd9/\nv7.5.1-nightly2017020113a024d531/\nv7.7.1-nightly201703019c75f4c78a/\nv7.9.1-nightly20170411675ece47b3/\nv8.0.0-nightly2016110121427fded9/\nv8.0.0-nightly20161201cf719152b0/\nv8.0.0-nightly20170101b465cd07fe/\nv8.0.0-nightly201702011b30df1003/\nv8.0.0-nightly2017030187a039d721/\nv8.0.0-nightly201704010ea45707a4/\nv8.0.0-nightly201705010f58d3cbef/\nv8.2.1-nightly2017080132b30d519e/\nv8.9.1-nightly20171104a815e1b6a2/\nv9.0.0-nightly2017070171ca122def/\nv9.0.0-nightly20170801cee8d6d65e/\nv9.0.0-nightly20170901dd52cad044/\nv9.0.0-nightly20171001f9be5fe52a/\nv9.5.1-nightly2018020181da708f73/\nv9.7.1-nightly201803013f3995b7b7/\nv9.9.1-nightly201803256591d9f761/\n\n\nIndex of /download/release/\nv0.10.16-isaacs-manual/\n\n\nMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025Previous12345...158Next\n\n\n\nMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node.js 22 is now available!AnnouncementsNode.js 22 is now available!The Node.js ProjectApr 24, 2024Diving into the Node.js Website RedesignAnnouncementsDiving into the Node.js Website RedesignBrian MuenzenmeyerMar 24, 2024Node.js 21 is now available!AnnouncementsNode.js 21 is now available!The Node.js ProjectOct 17, 2023Node.js 20 is now available!AnnouncementsNode.js 20 is now available!The Node.js ProjectApr 18, 2023\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesAnnouncementsMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node.js 22 is now available!AnnouncementsNode.js 22 is now available!The Node.js ProjectApr 24, 2024Diving into the Node.js Website RedesignAnnouncementsDiving into the Node.js Website RedesignBrian MuenzenmeyerMar 24, 2024Node.js 21 is now available!AnnouncementsNode.js 21 is now available!The Node.js ProjectOct 17, 2023Node.js 20 is now available!AnnouncementsNode.js 20 is now available!The Node.js ProjectApr 18, 2023Previous123456Next\n\n\n\nNode v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025Node v20.19.0 (LTS)ReleasesNode v20.19.0 (LTS)Marco IppolitoMar 13, 2025Node v23.9.0 (Current)ReleasesNode v23.9.0 (Current)Michaël ZassoFeb 26, 2025Node v18.20.7 (LTS)ReleasesNode v18.20.7 (LTS)Antoine du HamelFeb 20, 2025\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesReleasesNode v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025Node v20.19.0 (LTS)ReleasesNode v20.19.0 (LTS)Marco IppolitoMar 13, 2025Node v23.9.0 (Current)ReleasesNode v23.9.0 (Current)Michaël ZassoFeb 26, 2025Node v18.20.7 (LTS)ReleasesNode v18.20.7 (LTS)Antoine du HamelFeb 20, 2025Previous12345...121Next\n\n\n\nNode.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Updates on CVE for End-of-Life VersionsVulnerabilitiesUpdates on CVE for End-of-Life VersionsRafael GonzagaMar 07, 2025Tuesday, January 21, 2025 Security ReleasesVulnerabilitiesTuesday, January 21, 2025 Security ReleasesThe Node.js ProjectJan 21, 2025Upcoming CVE for End-of-Life Node.js VersionsVulnerabilitiesUpcoming CVE for End-of-Life Node.js VersionsThe Node.js ProjectJan 06, 2025Monday, July 8, 2024 Security ReleasesVulnerabilitiesMonday, July 8, 2024 Security ReleasesRafael GonzagaJul 08, 2024Wednesday, April 10, 2024 Security ReleasesVulnerabilitiesWednesday, April 10, 2024 Security ReleasesRafael GonzagaApr 10, 2024\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesVulnerabilitiesNode.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Updates on CVE for End-of-Life VersionsVulnerabilitiesUpdates on CVE for End-of-Life VersionsRafael GonzagaMar 07, 2025Tuesday, January 21, 2025 Security ReleasesVulnerabilitiesTuesday, January 21, 2025 Security ReleasesThe Node.js ProjectJan 21, 2025Upcoming CVE for End-of-Life Node.js VersionsVulnerabilitiesUpcoming CVE for End-of-Life Node.js VersionsThe Node.js ProjectJan 06, 2025Monday, July 8, 2024 Security ReleasesVulnerabilitiesMonday, July 8, 2024 Security ReleasesRafael GonzagaJul 08, 2024Wednesday, April 10, 2024 Security ReleasesVulnerabilitiesWednesday, April 10, 2024 Security ReleasesRafael GonzagaApr 10, 2024Previous12345...12Next\n\n\n\nTrip report: Node.js collaboration summit (2024 Dublin)EventsTrip report: Node.js collaboration summit (2024 Dublin)AugustinMauroyNov 11, 2024Trip report: Node.js collaboration summit (2024 London)EventsTrip report: Node.js collaboration summit (2024 London)Joyee CheungApr 15, 2024\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEventsTrip report: Node.js collaboration summit (2024 Dublin)EventsTrip report: Node.js collaboration summit (2024 Dublin)AugustinMauroyNov 11, 2024Trip report: Node.js collaboration summit (2024 London)EventsTrip report: Node.js collaboration summit (2024 London)Joyee CheungApr 15, 2024Previous1Next\n\n\n\nMaking Node.js Downloads Reliableflakey5Making Node.js Downloads ReliableLast year, we shared the details behind Node.js's brand new website.
Today we're back, talking about the new infrastructure serving Node.js' release assets.
This blog post goes into what Node.js' web infrastructure looks like, its history, and where it stands today.
We're also going to be covering what exactly we had in mind and were prioritizing with this infrastructure overhaul.
Some Brief History
At the start of the project in 2009, Node.js release assets (binaries and documentation) were stored on a publicly accessible S3 bucket.
This was later changed around May of 2010 to a VPS, which also hosted the Node.js website.
When the io.js fork happened in 2014, they also had used a VPS to host the website and release assets.
After the Node.js and io.js merge in 2015, io.js' VPS (which will be referred to as the origin server from hereon) was repurposed to host both io.js and Node.js releases, along with the Node.js website.
A backup server was also created, which acted almost like an exact copy of the origin server.
It served two main purposes:

Serve any traffic that the origin server couldn't handle.
Serve as a backup for the binaries and documentation in case if something went wrong with the origin server.

The entire architecture looked like this:

How Node.js Releases are Released
When a mainline release is ready to be released, the releaser triggers a release CI job.
These jobs build the Node.js binaries for all supported platforms and generate the documentation files.
Upon a release CI job completing successfully, its assets are uploaded to the origin server in a staging directory.
When all of the release CI jobs finish, the releaser then manually "promotes" the assets to the production directory, making the release publicly accessible.
For nightly or test releases, the promotion happens automatically.
Refs: Release Overview, Node.js release process.
Growing Pains
Nowadays, the nodejs.org domain sees over 2.4 billion requests and 3+ petabytes of traffic per month, with a majority of that going towards release assets.
This averages to about 925 requests per second and an average bandwidth of 1.2 gb per second.

The origin server does not have enough resources for this, and it struggled to keep up with the demand.
The nodejs.org website was moved off of the origin server as part of the redesign which did help, but it still wasn't enough.
Caching was being used rather inefficiently as well.
Node.js makes heavy use of symlinks with its release assets.
Some of these symlinks include directories like https://nodejs.org/dist/latest/, which changes with each release.
When a release is promoted, it needs to clear the cache in order to update these symlinks so users actually get the latest version.
However, there wasn't a great way to clear only what we needed to clear from the cache.
This resulted in us needing to purge the entire cache with each release.
Node.js has nightly releases, meaning this was happening once every 24 hours at a minimum.
So, everyday at roughly midnight UTC, the origin server got effectively DDoS'ed from the amount of uncached traffic being sent to it.
There were also a handful of other issues with the origin server pertaining to its maintenance:

Documentation of things running on the server was spotty; some things were well documented and others not at all.
Changes performed in the nodejs/build repository needed to be deployed manually by a Build WG member with access. There was also no guarantee that what's in the build repository is what's actually on the server.
There's no staging environment other than a backup instance.
Rollbacks could only be done via disk images through the VPS providers' web portal.

Attempts Were Made
All of these issues combined created for scenarios where the origin server wasn't touched unless necessary, including a period in which it had over 3 years of uptime.
These factors also contributed to incidents such as the one that occurred from March 15th, 2023 to March 17th, 2023, where the Node.js release assets were unavailable for 2 days due to the origin server being overloaded and improper caching rules.
Between incidents like this and the daily outages that were occuring, users were being effected and were painfully aware of the unreliability of the infrastructure.
This needed to be fixed.
However, attempts made to remediate these issues in this infrastructure could only go so far.
The NGINX configuration was changed to modify its resource consumption and add in missing documentation.
Cloudflare WAF was integrated to block spammy requests from repositories such as Artifactory.
Load balancing changes were made to try to help lessen the load.
But, these ultimately weren't as effective as they needed to be.
The Rewrite
In August of 2023, Claudio Wunder and I (flakey5) started working on a proof-of-concept for a new way to serve Node.js release assets.
The idea was "simple": create a new service that solved all of the issues we had with the previous infrastructure and showed no noticeable difference from the old infrastructure to users.
In order to meet these requirements, we prioritized three main goals with this new service:

Reliabiliy: The service needs to be as close to 100% uptime as possible.
Maintainability: Maintainers should not have to worry about things toppling over because they changed something. The service needs to be well-documented and as clean and simple as possible.
Efficiency: Whatever platform was used, the service would need to make full use of it to provide the best performance and experience as possible.

In order to meet these requirements, we ended up using Cloudflare Workers and R2 for a handful of reasons:

Workers and R2 are, and historically have been, reliable and fast.
Workers takes care of the infrastructure for us, so we just need to maintain the service itself. This heavily lessens the cost of maintenance, especially for a team of volunteers.
Node.js had already had previous usage of Cloudflare services; it makes sense to look into expanding it.
Pricing. Cloudflare was gracious enough to provide Node.js with free access to Workers and R2 under Project Alexandria.

In September of 2023, the proof-of-concept was ready to be reviewed, and an issue was made in the nodejs/build repository (#3461) seeking approval from the Build WG.
After the Build WG discussed the change, it was approved and we started working on getting the service, which is now referred to as the Release Worker, deployed to production.
The Journey to Production
With developing the Release Worker came a lot of trial and error and learning over numerous iterations of the service.
It needed to have all of the same features and similar, if not the exact same, behaviors as its predecessor, NGINX.
What It Needed To Do
For starters, it needed to be able to provide the latest releases of Node.js as soon as soon as they're released.
Secondly, it needed to handle routing correctly.
Most assets don't have 1:1 mappings of their URL to where they are located in the file system.
Where a URL maps to can even change depending on the Node.js version that's being requested.
For instance,
URLFile Path/dist/index.jsonnodejs/release/index.json/dist/latest-iron/...nodejs/release/v20.x.x/.../docs/v0.1.20/...nodejs/docs/v0.1.20/.../docs/v22.0.0/...nodejs/release/v22.0.0/docs/api/.../dist/v0.4.9/node-v0.4.9.tar.gznodejs/release/node-v0.4.9.tar.gz/dist/v0.4.9/SHASUMS256.txtnodejs/release/v0.4.9/SHASUMS256.txt
This behavior was created from multiple different changes throughout the release cycle and the way release assets were distributed, and was achieved through symlinks.
However, R2 doesn't support symlinks, meaning we needed to come up with a solution on our own.
Finally, we needed to meet the reliability goal.
To do this, we implemented four things:

Any request to R2 that fails is retried 3 times (in additon to the retries that Workers already performs).
A "fallback" system. Any request to R2 that fails all retries is rewritten to the old infrastructure.
When an error does happen, it's recorded in Sentry and we're notified so we can take appropriate action.
Slack alerts are in place for Sentry and for any critical point of failure in the Release Worker (ex/ deployment failure).

The Iterations
We first started off with an incredibly simple worker.
Given a path in the URL, it checked if the file requested existed in the R2 bucket.
If it did, it returned it.
Otherwise, it rewrote the request back to the origin server.
For requests that resulted in a directory listing, the worker just forwarded those over to the origin server.
This iteration obviously didn't cover nearly any of the requirements, so it was back to the drawing board.
The second iteration was based off the popular R2 directory listing library render2, developed by Kotx.
The library worked well for the more generic use cases we needed to cover, however, it fell short in the more unique use cases that we had.
So we forked it, adding what we needed for it to work for us.
However, it became rather messy and thus fell short of our maintainability goal.
This led us to our third iteration, which was a complete rewrite while still keeping some aspects of render2.
It worked for the most part, but this too was also a mess and didn't meet our maintainability goal.
It was also designed in the exact way that we needed the service to behave as well.
If we needed to change that behavior in any way, we would need to refactor significant portions of the codebase.
We knew we could do better than this.
This led us to the fourth and current iteration of the Release Worker, which was yet again another rewrite.
This time however, it was designed to be a lot more modular and with a middleware-centric design.
This allowed for code that was a lot easier to keep track of and maintain, and, as of November 2024, is what is currently deployed to production.
Release Process Changes
The goal with integrating R2 into the release process was to change as little as possible.
To accomplish this, a similar workflow to what was already in place was made.
Release CI jobs now upload their assets to a dist-staging bucket in addition to the staging directory on the origin server.
Then, when a release is promoted, the assets in dist-staging are copied over to a dist-prod bucket, which is what the Release Worker reads from.
Maintainability
As said in our previous blog post on the website redesign, an open source project is only as good as its documentation.
In order for the Release Worker to be maintainable, it needed to well documented.
This was achieved not only through thorough comments in the codebase but also documents such as,

README
Collaborator Guide
Contributing Guide
General Documentation on things
Standard Operating Procedures for things such as incident flows.

What's next?
The work isn't done just yet.
We still want to,

Look into any performance improvements that could be made.

This includes looking into integrating Cloudflare KV for directory listings.


Have better tests and a better development environment (PR!).
Metrics to give us more visibility into how the Release Worker is behaving and if there's anything that we can improve.

Thanks
Many people and organizations have contributed to this effort in many different ways.
We'd like to thank:

All of the contributors and collaborators that make this project possible.
Cloudflare for providing the infrastructure that serves Node.js' Website and the Release Worker. Also specifically to the R2 team for the technical support they have given us.
Sentry for providing an open source license for their error reporting, monitoring, and diagnostic tools.
OpenJS Foundation for their support and guidance.


Here is your weekly reminder that the Node.js project is driven by volunteers. Therefore every feature that lands is because someone spent time (or money) to make it happen. This is called Open Governance.
Matteo Collina, via social media

Want to get involved? Check out the project on GitHub.

Fun fact! Did you know that Node.js has a status page? No? You're not alone! We've been rather bad at using it to communicate these issues to users, but we're working on improving that.
NextNode.js Launches Official Community Space on Discord\n\n\n\nNode v23.11.0 (Current)Antoine du HamelNode v23.11.0 (Current)2025-04-01, Version 23.11.0 (Current), @aduh95
Notable Changes

[64b086740a] - (SEMVER-MINOR) assert: implement partial error comparison (Ruben Bridgewater) #57370
[053cef70e0] - (SEMVER-MINOR) crypto: add optional callback to crypto.diffieHellman (Filip Skokan) #57274
[f8aff90235] - (SEMVER-MINOR) process: add execve (Paolo Insogna) #56496
[4b04c92d7d] - (SEMVER-MINOR) sqlite: add StatementSync.prototype.columns() (Colin Ihrig) #57490
[1b8d1d3a3a] - (SEMVER-MINOR) util: expose diff function used by the assertion errors (Giovanni Bucci) #57462

Commits

[7b72396c8b] - assert: improve partialDeepStrictEqual performance (Ruben Bridgewater) #57509
[64b086740a] - (SEMVER-MINOR) assert: implement partial error comparison (Ruben Bridgewater) #57370
[f694d7de0e] - (SEMVER-MINOR) assert: improve partialDeepStrictEqual (Ruben Bridgewater) #57370
[80d9d5653f] - (SEMVER-MINOR) assert,util: improve performance (Ruben Bridgewater) #57370
[d52a71f832] - (SEMVER-MINOR) benchmark: adjust assert runtimes (Ruben Bridgewater) #57370
[7592cf4cd7] - (SEMVER-MINOR) benchmark: skip running some assert benchmarks by default (Ruben Bridgewater) #57370
[e4cc54a746] - (SEMVER-MINOR) benchmark: add assert partialDeepStrictEqual benchmark (Ruben Bridgewater) #57370
[de48407011] - build: fix update-wpt workflow (Jonas) #57468
[52cd0954f9] - cli: clarify --cpu-prof-name allowed values (Eugenio Ceschia) #57433
[7611fc14de] - crypto: fix output of privateDecrypt with zero-length data (Filip Skokan) #57575
[cc42ee8fc7] - crypto: ensure expected JWK alg in SubtleCrypto.importKey RSA imports (Filip Skokan) #57450
[053cef70e0] - (SEMVER-MINOR) crypto: add optional callback to crypto.diffieHellman (Filip Skokan) #57274
[1f08864fd7] - debugger: fix behavior of plain object exec in debugger repl (Dario Piotrowicz) #57498
[162b2828eb] - deps: update undici to 6.21.2 (Matteo Collina) #57442
[43bea6bb80] - deps: V8: cherry-pick c172ffc5bf54 (Choongwoo Han) #57437
[99f93afb9d] - deps: update ada to v3.2.1 (Yagiz Nizipli) #57429
[30e5658f12] - deps: update googletest to 0bdccf4 (Node.js GitHub Bot) #57380
[573467c070] - deps: update acorn to 8.14.1 (Node.js GitHub Bot) #57382
[affeaac0c7] - doc: add gurgunday as triager (Gürgün Dayıoğlu) #57594
[4ed1a098f5] - doc: clarify behaviour of node-api adjust function (Michael Dawson) #57463
[921041b284] - doc: remove Corepack documentation (Antoine du Hamel) #57635
[99dbd8b391] - doc: remove mention of --require not supporting ES modules (Huáng Jùnliàng) #57620
[8c76b2949e] - doc: mention reports should align with Node.js CoC (Rafael Gonzaga) #57607
[ee1c78a7a3] - doc: add section stating that very stale PRs should be closed (Dario Piotrowicz) #57541
[595e9e5ad6] - doc: add bjohansebas as triager (Sebastian Beltran) #57564
[3742d2a198] - doc: update support channels (Claudio W.) #57538
[717c44dead] - doc: make stability labels more consistent (Antoine du Hamel) #57516
[b4576a6f57] - doc: remove cryptoStream API reference (Jonas) #57579
[2c4f894036] - doc: module resolution pseudocode corrections (Marcel Laverdet) #57080
[c45894f90c] - doc: add history entry for DEP0190 in child_process.md (Antoine du Hamel) #57544
[c21068b696] - doc: remove deprecated pattern in child_process.md (Antoine du Hamel) #57568
[87e0dda352] - doc: mark multiple experimental APIS as stable (James M Snell) #57510
[d637763e4e] - doc: remove mertcanaltin from Triagers (Mert Can Altin) #57531
[ee6025495d] - doc: recommend watching the collaborators repo in the onboarding doc (Darshan Sen) #57527
[706b64638b] - doc: remove mention of visa fees from onboarding doc (Darshan Sen) #57526
[176d951bd0] - doc: deprecate passing args to spawn and execFile (Antoine du Hamel) #57389
[5c05ba119b] - doc: remove some inconsistencies in deprecations.md (Antoine du Hamel) #57512
[9d5be4bb8c] - doc: run license-builder (github-actions[bot]) #57511
[273607edb4] - doc: add new writing-docs contributing md (Dario Piotrowicz) #57502
[e28c723f24] - doc: add node.js streams references to Web Streams doc (Dario Piotrowicz) #57393
[47296492ba] - doc: replace NOTEs that do not render properly (Colin Ihrig) #57484
[db9c37f792] - doc: prefer to sign commits under nodejs repository (Rafael Gonzaga) #57311
[e5e3987ae7] - doc: fixed the incorrect splitting of multiple words (letianpailove) #57454
[91a824e43b] - doc: add review guidelines for collaborator nominations (Antoine du Hamel) #57449
[2a5fcb2172] - doc: fix typo in url.md (Allon Murienik) #57467
[17ccf9282f] - doc: add history info for --use-system-ca (Darshan Sen) #57432
[9adaaeb965] - doc: remove typo YAML snippet from tls.getCACertificates doc (Darshan Sen) #57459
[ee4e855f8e] - doc: fix typo in sqlite.md (Tobias Nießen) #57473
[8cb3441443] - doc: explicit mention arbitrary code execution as a vuln (Rafael Gonzaga) #57426
[27f183ad03] - doc: update maintaining-openssl.md for openssl (Richard Lau) #57413
[ca67145d60] - doc: add missing deprecated badges in fs.md (Yukihiro Hasegawa) #57384
[3687390510] - doc: fix small typo in process.md (Felix Rieseberg) #57333
[097d9926e3] - doc: add note about sync nodejs-private branches (Rafael Gonzaga) #57404
[5006627969] - fs: apply exclude function to root path (Rich Trott) #57420
[0583c3db92] - http: coerce content-length to number (Marco Ippolito) #57458
[2a580b9332] - lib: add warning when binding inspector to public IP (Demian Parkhomenko) #55736
[fda56b9837] - lib: limit split function calls to prevent excessive array length (Gürgün Dayıoğlu) #57501
[d5a26f6525] - lib: make getCallSites sourceMap option truly optional (James M Snell) #57388
[00a5b18043] - meta: add some clarification to the nomination process (James M Snell) #57503
[d0c96c463c] - meta: remove collaborator self-nomination (Rich Trott) #57537
[a9a93f31ee] - meta: edit collaborator nomination process (Antoine du Hamel) #57483
[0ca362f5f2] - meta: move ovflowd to emeritus (Claudio W.) #57443
[f8aff90235] - (SEMVER-MINOR) process: add execve (Paolo Insogna) #56496
[e8d4a31d4b] - sqlite: add support for unknown named parameters (Colin Ihrig) #57552
[5652da642d] - sqlite: add DatabaseSync.prototype.isOpen (Colin Ihrig) #57522
[5c976f16cd] - sqlite: add DatabaseSync.prototype[Symbol.dispose]() (Colin Ihrig) #57506
[4b04c92d7d] - (SEMVER-MINOR) sqlite: add StatementSync.prototype.columns() (Colin Ihrig) #57490
[7f5e31645c] - src: ensure primordials are initialized exactly once (Chengzhong Wu) #57519
[9611980f58] - src: improve error handling in multiple files (James M Snell) #57507
[3ddc5cd875] - src: cache urlpattern properties (JonasBa) #57465
[b9d9ee4da2] - src: make minor cleanups in encoding_binding.cc (James M Snell) #57448
[f8acf2dd2a] - src: make minor cleanups in compile_cache.cc (James M Snell) #57448
[6ee15c6509] - src: define urlpattern components using a macro (JonasBa) #57452
[4ab3c1690a] - src: cleanup crypto more (James M Snell) #57323
[5be80b1748] - src: refine ncrypto more (James M Snell) #57300
[6a13319a6e] - src: cleanup aliased_buffer.h (Mohammed Keyvanzadeh) #57395
[3cff7f80bb] - src: suggest --use-system-ca when a certificate error occurs (Aditi) #57362
[3d372ad9f3] - test: update WPT for urlpattern to 6ceca69d26 (Node.js GitHub Bot) #57486
[481ea665af] - test: add more number cases for buffer.indexOf (Meghan Denny) #57200
[27b01ed4e7] - test: update parallel/test-tls-dhe for OpenSSL 3.5 (Richard Lau) #57477
[8f7debcf41] - timers: optimize timer functions with improved argument handling (Gürgün Dayıoğlu) #57072
[d4abd9d3fb] - timers: remove unnecessary allocation of _onTimeout (Gürgün Dayıoğlu) #57497
[f8f81c8ba2] - timers: remove unused parameter from insertGuarded (Gürgün Dayıoğlu) #57251
[c4fdb27b51] - tls: remove unnecessary type check on normalize (Yagiz Nizipli) #57336
[ad5dcc5798] - tools: fix WPT update cron string (Antoine du Hamel) #57665
[7faa482588] - tools: remove stalled label on unstalled issues and PRs (Rich Trott) #57630
[e3bb26da2b] - tools: update sccache to support GH cache changes (Michaël Zasso) #57573
[f0c9f505d9] - tools: bump @babel/helpers from 7.26.9 to 7.26.10 in /tools/eslint (dependabot[bot]) #57444
[a40ff1f646] - url: fix constructor error message for URLPattern (jakecastelli) #57482
[f36bee4b89] - util: avoid run debug when enabled is false (fengmk2) #57494
[1b8d1d3a3a] - (SEMVER-MINOR) util: expose diff function used by the assertion errors (Giovanni Bucci) #57462
[1f7b08a317] - win,test: disable test case failing with ClangCL (Stefan Stojanovic) #57397

Windows 64-bit Installer: https://nodejs.org/dist/v23.11.0/node-v23.11.0-x64.msi 
Windows ARM 64-bit Installer: https://nodejs.org/dist/v23.11.0/node-v23.11.0-arm64.msi 
Windows 64-bit Binary: https://nodejs.org/dist/v23.11.0/win-x64/node.exe 
Windows ARM 64-bit Binary: https://nodejs.org/dist/v23.11.0/win-arm64/node.exe 
macOS 64-bit Installer: https://nodejs.org/dist/v23.11.0/node-v23.11.0.pkg 
macOS Apple Silicon 64-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-darwin-arm64.tar.gz 
macOS Intel 64-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-darwin-x64.tar.gz 
Linux 64-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-linux-x64.tar.xz 
Linux PPC LE 64-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-linux-ppc64le.tar.xz 
Linux s390x 64-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-linux-s390x.tar.xz 
AIX 64-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-aix-ppc64.tar.gz 
ARMv7 32-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-linux-armv7l.tar.xz 
ARMv8 64-bit Binary: https://nodejs.org/dist/v23.11.0/node-v23.11.0-linux-arm64.tar.xz 
Source Code: https://nodejs.org/dist/v23.11.0/node-v23.11.0.tar.gz 
Other release files: https://nodejs.org/dist/v23.11.0/ 
Documentation: https://nodejs.org/docs/v23.11.0/api/
SHASUMS
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

281c642514a14bc326b2b4953acef169dd20aac78d138607b778eabf5521a2b6  node-v23.11.0-aix-ppc64.tar.gz
33ac1b7ad8c619187c6436fbd4ff9766598b0c50d48fbc9a1a96eea9a214a59a  node-v23.11.0-arm64.msi
635990b46610238e3c008cd01480c296e0c2bfe7ec59ea9a8cd789d5ac621bb0  node-v23.11.0-darwin-arm64.tar.gz
d020d7703959f65e289fc8fc51e216e87793dd297cd40f410f6e2fc18d321c11  node-v23.11.0-darwin-arm64.tar.xz
a5782655748d4602c1ee1ee62732e0a16d29d3e4faac844db395b0fbb1c9dab8  node-v23.11.0-darwin-x64.tar.gz
0a47bae567703255d3dddc5918a875a9cc4ce07f7c6feca480e9d564cadecafb  node-v23.11.0-darwin-x64.tar.xz
48586eebb7ddcd51109c092b2bfc0218f9fbac504b3622293e404fd9274c19ff  node-v23.11.0-headers.tar.gz
130707d6de3df0fd6de70f47b835511b0708ac3ffbe7e7c4ebf23b8bc2602ce7  node-v23.11.0-headers.tar.xz
12b29a87a7ccd7e1b97392d1e1533470d596578dad900430cff403e404fe72a7  node-v23.11.0-linux-arm64.tar.gz
85915f885fe7eab2be4a6e3de840cb83db4fc53749274d31383a0e1721a883c6  node-v23.11.0-linux-arm64.tar.xz
f20ced60e7f5b0136582ac7e3f5b2dafb4e320332dba81abb450e1f50ea64da4  node-v23.11.0-linux-armv7l.tar.gz
a0012b90af20ec5f219ead4ea9321249c0c4e74b077855e5d41634b157047089  node-v23.11.0-linux-armv7l.tar.xz
7fd70c5470d782dd03c31ca45505e563e6958e21790a8c7bf12a4c0fdd62b010  node-v23.11.0-linux-ppc64le.tar.gz
2e974a15d60583b26e82ef59c9bd507c897c7a6ee57c382f8f10d87aa27095db  node-v23.11.0-linux-ppc64le.tar.xz
3535e5ca971b9163c5f1b0232ea1a3c827bfcbe2cc0627458d43087e337934b8  node-v23.11.0-linux-s390x.tar.gz
bfda95d724b78a85f56697671e516ce8664dceb579fd72ba0571e9744c92d853  node-v23.11.0-linux-s390x.tar.xz
66f768a7f2d89ecdda8fe1e33ee71ac04ed9180111cbf1c5fb944655fe7c90c7  node-v23.11.0-linux-x64.tar.gz
fa9ae28d8796a6cfb7057397e1eea30ca1c61002b42b8897f354563a254e7cf5  node-v23.11.0-linux-x64.tar.xz
3635315a6d8d2d51ef466ea7b9c6b32b2f9f00a04c3e70cf01e16a44e26396ca  node-v23.11.0-win-arm64.7z
174936f8a36e955c4e16cf5252187a148f9148aa9c5b6d26366d7d9e1d0e49cc  node-v23.11.0-win-arm64.zip
35f424f7eadbbbcd6e65a841f994b231e42f790e2ad2727b196f2d194f9ab993  node-v23.11.0-win-x64.7z
42749f1e4583907ab92bac4b4bdb031201a0b3f7b028ea6b6e0d5bb40e433ae9  node-v23.11.0-win-x64.zip
4d345be9222ffcc052921865a6ac498ac4d93c833ac5c37b84784bd6738c7dca  node-v23.11.0-x64.msi
951ca83a4b4c70686019f88761b5ac6096519ae5e113add8cdffa45b839307d4  node-v23.11.0.pkg
0080beb9a481e2536543f70b59880093cb6c42470092efad8193563219efdc80  node-v23.11.0.tar.gz
f2c5db21fc5d3c3d78c7e8823bff770cef0da8078c3b5ac4fa6d17d5a41be99d  node-v23.11.0.tar.xz
7ee966060ec29ad00a8853f7af86698e29e838780312bbaec47076d2f9a1e75b  win-arm64/node.exe
b04fc2c4f04273feaae3a2bc372d507fa631d7d6fd5f11265219d5b10d083a3e  win-arm64/node.lib
5eb8a4b6eb129c5938b39cf06d874699aa92ea9f7156f8b8748eccb5e3f75445  win-arm64/node_pdb.7z
a5404d2a165ae4044f1e379a2c5cb817e48b2a45b063605ad1b627d5977d20ba  win-arm64/node_pdb.zip
806777a1aa8d511a3eaffb6f75200e85c5a67e78bb921bc4c340353fb343b33d  win-x64/node.exe
7005ad268c2a3b55e741957609696fd707982d8d7727325c44e2cbf23f00f565  win-x64/node.lib
922615546546d6c66ab6cbf1fcd2366a7e942061fad50c88134c9fb54824749e  win-x64/node_pdb.7z
c304a55e0228e6a767dedaa66d64421cb99f6c75acbed06adcf01958e9aa1a63  win-x64/node_pdb.zip
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEEwNYkhDnx1WBKr/tAIdkA/9sjN1YFAmfr948ACgkQIdkA/9sj
N1bt8xAAmPC/y4YavKY4mfdxVNfNRbv6jJ1ChtFPysfyROCB/5vhsCACefCVOdLZ
ACjU0wuOkZ08/pEBQdzk+BFE2e1Vytj3Sfz2g27iDCK3VG/uZVzcgeAOc3MVrTQN
g/gLdL5t2HF3ane2VCGHOLqC3xAyNAlVIz5Kf4qYEuCXtsv4eIEQgEo9Gpph6NWo
uO3AE3CO235lMJambhAIV9eGtPsivcDnrAojOgHTwUAEA8eYNxZHYl2eqoaqgR6h
GLg0wC8mf+7uD7GrDDhxhJ/H9p5oGTA5mp8j/hDSlUcgISht24FCBMGLDDx1G7hA
znAgOqa7Aj0CAwak8gyor+Fm9yvcdQ+/5vdKms16ESYf9dcrqvuRcYjyky7/I0Ue
gNfoOec3UrbDquBPyEB5eSUlLxBFC+JTJOyTgywC0CQC8p5zNY+2qFzGG9ClwJq1
u7QPA2PlDfb44HR1n/yv+R7tuf9ws7KOtbXWClPz4vz/jberzCBFvdWnAntFouMq
oyIQp7JcAkVfrlrlT2WbTzsHYhfafvPnX/STx9u/lL5DSRwlFfPP3JP5i2AkLh6B
+DqjDqQUQXVea0rj7C/c3E9tUydl6Uwlgj3BG6aDb4r7svyW8LfcUtkTe5AOdgBp
euLoL2KzfyEHo8Umg4lAw9ThfEhdYO9XcG1ExjSNTj5/iAPeItY=
=9mLp
-----END PGP SIGNATURE-----
NextNode v18.20.8 (LTS)\n\n\n\nNode.js Test CI Security IncidentNode.js Technical Steering CommitteeNode.js Test CI Security IncidentOn March 21st, the Node.js project received a security report regarding our development infrastructure via our bug bounty program. We immediately restricted access while implementing corrective actions.
The reported issue did not impact the Node.js runtime and there is no risk to users of Node.js. No action by Node.js users is required.
The development infrastructure is expected to be available to the community by April 15 or sooner.
A full report of this incident will be available forthcoming. We appreciate the time investment from our amazing volunteers who assisted in this response.
Contact and future updates
The current Node.js security policy can be found at https://nodejs.org/security/. Please follow the process outlined in https://github.com/nodejs/node/security/policy if you wish to report a vulnerability in Node.js.
Subscribe to the low-volume announcement-only nodejs-sec mailing list at https://groups.google.com/forum/#!forum/nodejs-sec to stay up to date on security vulnerabilities and security-related releases of Node.js and the projects maintained in the nodejs GitHub organization.NextUpdates on CVE for End-of-Life Versions\n\n\n\nNode v18.20.8 (LTS)Richard LauNode v18.20.8 (LTS)2025-03-27, Version 18.20.8 'Hydrogen' (LTS), @richardlau
Please note that Node.js 18 is scheduled to reach End-of-Life on 30 April 2025. It is recommended to update to Node.js 20 or 22 as Node.js 18 will no longer receive security updates once it reaches End-of-Life.
Extended support for Node.js 18 may be available through third party commercial partners.
Notable Changes
This release updates OpenSSL to 3.0.16 and root certificates to NSS 3.108.
Commits

[f737a79073] - async_hooks,inspector: implement inspector api without async_wrap (Gabriel Bota) #51501
[fce923ba69] - build: update gcovr to 7.2 and codecov config (Benjamin E. Coe) #54019
[8b7ffd807c] - build: fix compatibility with V8's depot_tools (Richard Lau) #57330
[ee9a343413] - crypto: update root certificates to NSS 3.108 (Node.js GitHub Bot) #57381
[738bf8aea4] - crypto: update root certificates to NSS 3.104 (Richard Lau) #55681
[69d661d591] - deps: update undici to v5.29.0 (Matteo Collina) #57557
[59fcf43b0e] - deps: update corepack to 0.32.0 (Node.js GitHub Bot) #57265
[1b72869503] - deps: update archs files for openssl-3.0.16 (Node.js GitHub Bot) #57335
[a566560235] - deps: upgrade openssl sources to quictls/openssl-3.0.16 (Node.js GitHub Bot) #57335
[50c4e1da2f] - doc: add missing deprecated badges in fs.md (Yukihiro Hasegawa) #57384
[c3babb4671] - doc: update Xcode version used for arm64 and pkg (Michaël Zasso) #57104
[784da606a6] - doc: fix link and history of SourceMap sections (Antoine du Hamel) #57098
[f5dbceccbe] - test: update error code in tls-psk-circuit for for OpenSSL 3.4 (sebastianas) #56420

Windows 32-bit Installer: https://nodejs.org/dist/v18.20.8/node-v18.20.8-x86.msi 
Windows 64-bit Installer: https://nodejs.org/dist/v18.20.8/node-v18.20.8-x64.msi 
Windows 32-bit Binary: https://nodejs.org/dist/v18.20.8/win-x86/node.exe 
Windows 64-bit Binary: https://nodejs.org/dist/v18.20.8/win-x64/node.exe 
macOS 64-bit Installer: https://nodejs.org/dist/v18.20.8/node-v18.20.8.pkg 
macOS Apple Silicon 64-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-darwin-arm64.tar.gz 
macOS Intel 64-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-darwin-x64.tar.gz 
Linux 64-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-linux-x64.tar.xz 
Linux PPC LE 64-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-linux-ppc64le.tar.xz 
Linux s390x 64-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-linux-s390x.tar.xz 
AIX 64-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-aix-ppc64.tar.gz 
ARMv7 32-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-linux-armv7l.tar.xz 
ARMv8 64-bit Binary: https://nodejs.org/dist/v18.20.8/node-v18.20.8-linux-arm64.tar.xz 
Source Code: https://nodejs.org/dist/v18.20.8/node-v18.20.8.tar.gz 
Other release files: https://nodejs.org/dist/v18.20.8/ 
Documentation: https://nodejs.org/docs/v18.20.8/api/
SHASUMS
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

5a962ef966f9548117dea4d57615012efc6386a7b55c9a12c16fd7e674af375b  node-v18.20.8-aix-ppc64.tar.gz
bae4965d29d29bd32f96364eefbe3bca576a03e917ddbb70b9330d75f2cacd76  node-v18.20.8-darwin-arm64.tar.gz
6626fb7526fb2d84c8200ab915496934f44f0cfa7bd6c73318322e43ce21457a  node-v18.20.8-darwin-arm64.tar.xz
ed2554677188f4afc0d050ecd8bd56effb2572d6518f8da6d40321ede6698509  node-v18.20.8-darwin-x64.tar.gz
8e15d82da8e0265eaff6207add4b5c5bc23d05f7afb78e5694d1e3d93ba05139  node-v18.20.8-darwin-x64.tar.xz
10ef9e563840b5be0f291a114afb726523178ee5f51d23bf532d28f22c23bce9  node-v18.20.8-headers.tar.gz
b188159acff29d512fc553b990e52d1f13f39045115472176f9e231ed974170d  node-v18.20.8-headers.tar.xz
2e3dfc51154e6fea9fc86a90c4ea8f3ecb8b60acaf7367c4b76691da192571c1  node-v18.20.8-linux-arm64.tar.gz
224e569dbe7b0ea4628ce383d9d482494b57ee040566583f1c54072c86d1116b  node-v18.20.8-linux-arm64.tar.xz
d09ea19ff5eb7b0ff47d80316c708092ac401c138254e018e21b89bb6ed9abd0  node-v18.20.8-linux-armv7l.tar.gz
33a35700b8d5ff73a1223fc11282f0790dabf7e7d087612d8afd59319743d828  node-v18.20.8-linux-armv7l.tar.xz
3c0c7e5f414c2123b185924e3afac3bc6fcc3edbe14ec2782e9d5210a76d8b8e  node-v18.20.8-linux-ppc64le.tar.gz
57fde85b5f8e6b3c5d45e48a82dbd28f1d2dde816b0da2b5fc250c1a63ea49e6  node-v18.20.8-linux-ppc64le.tar.xz
6db3d48cabcb22f1f4af29633431b62d1040099a6e27182ad9f018c90f09d65b  node-v18.20.8-linux-s390x.tar.gz
35c81e08d26e25598e47dc4d1755f6bc0be802457a2f5bf775ead2ffccb73e89  node-v18.20.8-linux-s390x.tar.xz
27a9f3f14d5e99ad05a07ed3524ba3ee92f8ff8b6db5ff80b00f9feb5ec8097a  node-v18.20.8-linux-x64.tar.gz
5467ee62d6af1411d46b6a10e3fb5cacc92734dbcef465fea14e7b90993001c9  node-v18.20.8-linux-x64.tar.xz
d4cc1b92f87eade68a8a025ff186609958966639e64d0b993bf88691f3273118  node-v18.20.8.pkg
ec60a6d2344ef9e1f093991ca1bb6bbe92c61c29d1762c4b99e08f87dbb91e2f  node-v18.20.8.tar.gz
36a7bf1a76d62ce4badd881ee5974a323c70e1d8d19165732684e145632460d9  node-v18.20.8.tar.xz
a3aec6ddc837df1d838978568378ce877d36f1b1ade497b65db5ce93d7254c4f  node-v18.20.8-win-x64.7z
1a1e40260a6facba83636e4cd0ba01eb5bd1386896824b36645afba44857384a  node-v18.20.8-win-x64.zip
fea3bf9abebf12d96d76fd879326050af7d4e207a6203c4efddef875e3118e0f  node-v18.20.8-win-x86.7z
96327c25f8dab9a2403b95ac60ad0b715962aeac67d3cefdbe457e37f065aff2  node-v18.20.8-win-x86.zip
bd8b92343e12dbe9e573d2f5d5fec6953fcf12e00add58a29a6774391d02f342  node-v18.20.8-x64.msi
c7a04a1378cb66f5716f2b67c65921497972e883541bc48c0b19c92668f1713d  node-v18.20.8-x86.msi
3bc28aab59b609a6f359f456641789d646081291fc6505d6df2335e6eac89ae3  win-x64/node.exe
64d93225aaece04e3cd45177d6dea2b22df49e127281fefa3ade43ac46a36cc6  win-x64/node.lib
66e1addcf18db62f22ca754dfbf54fd17c634d88341a65c25453b19bbfa3d0bd  win-x64/node_pdb.7z
bb89d9899aea449680cb39942fab775a4a3355a94c32cca6941dbff4c6157f6c  win-x64/node_pdb.zip
878db391e180fa2cd8e5ef22ea23a7ef75506dfc7c63126975afb4e99bb6c002  win-x86/node.exe
8d068c16a75ff561595f2af576191d09a4669a87374abaff8782f8ce4e7d715f  win-x86/node.lib
e7c632600ffdf957c67aef16b06e824a97e5faaa27738494d8352b98126e6841  win-x86/node_pdb.7z
95616d5176f29174225fe774fe08ccef4092a757766478965dce8a48f2076d33  win-x86/node_pdb.zip
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEEyC+jrhy+3Gvka5NgxDzsRcF6uTwFAmflTfAACgkQxDzsRcF6
uTw3JQ//R5Yj7rqy3N0ZLJT5T2TgM4/NNOMHXpJ3lyG+KYPBcahHTky6MRkx5PUV
4zupyFQCYBQAJZGyE9wtuZ3IFlWZKjd6FUcD5rwv8AJ2PyrMmGssJKY2gVg9Zgrc
er+qro0cJMSA3vOFd1lL8BKFSx4vIYWBixBMAxnak3GDDzUbReozuDTi2RkP82dJ
Y1O7+rW7lTLaMGDPGRHHayD66L1AB5VEXu01Kj13sFvY9gu/+4LgaafKzY9QzdsO
xcmNOqxHQ6Aw3F95MQbmrkh9729i6fkh0F5mai5m6qVfTcsktVVvGgNmZBcGIDb7
XkXZdpuJLL5iHBAsyIxZQmJLqx+wC7ZavnM9c+IWb2GgOuSpatxbjsz/BOOl1NbP
N5+4Oi+X3wgMaYLGK6GASFpgZFu+L4ybcvCNaY1XSOpjqvGV5c16SD54XzaK6Dx0
tul8IKsli4xN7vMeVgplEV+OFuR2p+TFWe8tCWnm9u67+8UInj6K9ffPYHZ58R9r
rX3jm7qboKzoLLifoyr87xinlHNPdE6AnyyDvnFL+7ET7eT3O4ZokQLLM0sRBCT1
l/PUF2UnrohvdORAEr5YUfB0lQ4xiN5M6KWJV0df/VVP3KCJSnximxVng9ciVLcU
P6iluYUI2Sh6fcgmaiQJMDeaco9YZkWba341FgHbD6FSXgTw5n0=
=gHz8
-----END PGP SIGNATURE-----
PrevNode v23.11.0 (Current)NextNode v23.10.0 (Current)\n\n\n\nNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderNode.js Launches Official Community Space on DiscordFirst, the news: The OpenJS Foundation and Reactiflux have collaborated to bring forth an official Discord community for Node.js 🎉 You can join the Node.js Discord now.
Over the past several years, Discord has become the de-facto platform for communities to connect and communicate. Many Node.js community members already use Discord to discuss Node.js, seek advice, and share their projects. By establishing an official Node.js Discord server, we aim to gather these conversations and provide a safe and well-moderated space for our online community to congregate. Lots of other open-source projects, such as TypeScript, Rust, and Python, have successfully built their communities on Discord.
Our Node.js Discord space will be perfect for:

Hosting livestreams by Node.js Ambassadors and community members
Facilitating discussions about Node.js, answering questions, and sharing projects
Connecting with Node.js maintainers, influencers, and contributors
Receiving the latest updates on Node.js releases and events hosted by the OpenJS Foundation

But this is not a professionally-operated corporate space with full-time staff and a marketing budget, this is an organic community operated by volunteers because they love using JavaScript on the backend. If there's something you feel is missing, shout it out in #meta-admin — resources are limited but enthusiasm can go a long way 🫶
The Journey So Far
Our journey began with this issue on GitHub. We partnered with the Reactiflux/Nodeiflux community to rebrand their existing Nodeiflux server as the official Node.js Discord server. Over the past few months, we have been preparing the server for everyone to use, and you can follow some of our progress in the issue.
Reactiflux was one of the first large commuinities on Discord, joining the platform less than 6 months after its publish launch and operating continuously since then. In 2018, one of its administrators started a "sister server" for those using JS on the backend; Nodeiflux!
Now, 7 years on, the Nodeiflux server has joined up with the OpenJS Foundation to serve as the official Node.js Discord server. We are excited to see how our community will thrive in this new environment — the server will be jointly managed by the Node.js project and the Nodeiflux community, with the Node.js Technical Steering Committee (TSC) providing advisory support. The Nodeiflux community will handle the day-to-day administration of the server.
For all intents and purposes, this is an official Node.js space, and we are eager to see how it will grow and evolve. Join the Node.js Discord and become part of our growing community. See you there!
Looking Ahead
We are eager to see how our community will grow and interact in this new space. We look forward to hosting events and livestreams in the future. If you have any ideas or feedback, please share them in the #discord-feedback channel on the server.

A warm hug from @vcarl and @ovflowd. We hope to see you on the Node.js Discord server soon!PrevMaking Node.js Downloads ReliableNextNode.js 22 is now available!\n\n\n\nNode v23.10.0 (Current)Antoine du HamelNode v23.10.0 (Current)2025-03-13, Version 23.10.0 (Current), @aduh95
Notable Changes
Introducing --experimental-config-file
With the introduction of test runner, SEA, and other feature that require a lot
of flags, a JSON config flag would improve by a lot the developer experience and
increase adoption.
You can have a node.config.json containing:
{
  "$schema": "https://nodejs.org/dist/v23.10.0/docs/node-config-schema.json",
  "nodeOptions": {
    "test-coverage-lines": 80,
    "test-coverage-branches": 60
  }
}
JSONCopy to clipboard
You can run your tests without passing the flags defined in the config file.
node --experimental-default-config-file --test --experimental-test-coverage
ShellCopy to clipboard
or
node --experimental-config-file=node.config.json --test --experimental-test-coverage
ShellCopy to clipboard
Node.js will not sanitize or perform validation on the user-provided configuration,
so only ever use trusted configuration files.
Contributed by Marco Ippolito in #57016
and #57171.
Other Notable Changes

[323e3ac93c] - crypto: update root certificates to NSS 3.108 (Node.js GitHub Bot) #57381
[6fd2ec6816] - doc: add @geeksilva97 to collaborators (Edy Silva) #57241
[d8937f1742] - (SEMVER-MINOR) src: create THROW_ERR_OPTIONS_BEFORE_BOOTSTRAPPING (Marco Ippolito) #57016
[5054fc7941] - (SEMVER-MINOR) test_runner: change ts default glob (Marco Ippolito) #57359
[75f11ae1cc] - (SEMVER-MINOR) tls: implement tls.getCACertificates() (Joyee Cheung) #57107
[a22c21ceb8] - (SEMVER-MINOR) v8: add v8.getCppHeapStatistics() method (Aditi) #57146

Commits

[2daee76b26] - assert: improve myers diff performance (Giovanni Bucci) #57279
[2fbd3bbea7] - build: fix compatibility with V8's depot_tools (Richard Lau) #57330
[6a2e4c5fc1] - build,win: disable node pch with ccache (Stefan Stojanovic) #57224
[323e3ac93c] - crypto: update root certificates to NSS 3.108 (Node.js GitHub Bot) #57381
[906f23d0e7] - crypto: add support for intermediate certs in --use-system-ca (Tim Jacomb) #57164
[03cd7920c8] - deps: update simdjson to 3.12.2 (Node.js GitHub Bot) #57084
[9e1fce9a5c] - deps: update archs files for openssl-3.0.16 (Node.js GitHub Bot) #57335
[4056c1f83e] - deps: upgrade openssl sources to quictls/openssl-3.0.16 (Node.js GitHub Bot) #57335
[b402799070] - deps: update corepack to 0.32.0 (Node.js GitHub Bot) #57265
[ce1cfff79a] - deps: update amaro to 0.4.1 (marco-ippolito) #57121
[0ac977d679] - deps: update gyp file for ngtcp2 1.11.0 (Richard Lau) #57225
[f34d78df1f] - deps: update ada to 3.1.3 (Node.js GitHub Bot) #57222
[4fe9916701] - dns: remove redundant code using common variable (Deokjin Kim) #57386
[1c271b162b] - doc: make first parameter optional in util.getCallSites (Deokjin Kim) #57387
[77668fffec] - doc: fix usage of module.registerSync in comment (Timo Kössler) #57328
[9b4f7aac69] - doc: add Darshan back as voting TSC member (Michael Dawson) #57402
[d44ccb319c] - doc: revise webcrypto.md types, interfaces, and added versions (Filip Skokan) #57376
[f4de7cef01] - doc: add info on how project manages social media (Michael Dawson) #57318
[792ef16921] - doc: revise tsconfig.json note (Steven) #57353
[4e438c3fa3] - doc: use more clear name in getSystemErrorMessage's example (ikuma-t) #57310
[5c9f1a40e4] - doc: recommend setting noEmit: true in tsconfig.json (Steven) #57320
[e178acf9d8] - doc: ping nodejs/tsc for each security pull request (Rafael Gonzaga) #57309
[fbe464e28c] - doc: fix Windows ccache section position (Stefan Stojanovic) #57326
[3fe8eac0ba] - doc: update node-api version matrix (Chengzhong Wu) #57287
[d2f49e7fcf] - doc: recommend erasableSyntaxOnly in ts docs (Rob Palmer) #57271
[03844d99f8] - doc: clarify path.isAbsolute is not path traversal mitigation (Eric Fortis) #57073
[0f8cd32986] - doc: fix rendering of DEP0174 description (David Sanders) #56835
[f95ecca71f] - doc: add 1ilsang to triage team (1ilsang) #57183
[6fd2ec6816] - doc: add @geeksilva97 to collaborators (Edy Silva) #57241
[b74e0ff7d7] - doc: add missing assert return types (Colin Ihrig) #57219
[83eed33562] - doc: add streamResetBurst and streamResetRate (Sujal Raj) #57195
[7f48811295] - doc: add esm examples to node:util (Alfredo González) #56793
[5c20dcc166] - esm: fix module.exports export on CJS modules (Guy Bedford) #57366
[041a217a4d] - fs: fix rmSync error code (Paul Schwabauer) #57103
[cea50b7f39] - lib: optimize priority queue (Gürgün Dayıoğlu) #57100
[5204d495ae] - meta: bump codecov/codecov-action from 5.3.1 to 5.4.0 (dependabot[bot]) #57257
[89599be988] - meta: bump github/codeql-action from 3.28.8 to 3.28.10 (dependabot[bot]) #57254
[66cd3850bc] - meta: bump ossf/scorecard-action from 2.4.0 to 2.4.1 (dependabot[bot]) #57253
[6c22e446bc] - meta: set nodejs/config as codeowner (Marco Ippolito) #57237
[ee5ce5ccde] - meta: move RaisinTen back to collaborators, triagers and SEA champion (Darshan Sen) #57292
[0b0c9cc0f5] - meta: bump actions/download-artifact from 4.1.8 to 4.1.9 (dependabot[bot]) #57260
[e6a98af8bd] - meta: bump peter-evans/create-pull-request from 7.0.6 to 7.0.7 (dependabot[bot]) #57259
[91394aaf3d] - meta: bump step-security/harden-runner from 2.10.4 to 2.11.0 (dependabot[bot]) #57258
[63dbbe7c91] - meta: bump actions/cache from 4.2.0 to 4.2.2 (dependabot[bot]) #57256
[d5ccf174ad] - meta: bump actions/upload-artifact from 4.6.0 to 4.6.1 (dependabot[bot]) #57255
[46b06be9a3] - module: handle cached linked async jobs in require(esm) (Joyee Cheung) #57187
[718305db6f] - module: add dynamic file-specific ESM warnings (Mert Can Altin) #56628
[4762f4ada5] - net: validate non-string host for socket.connect (Daeyeon Jeong) #57198
[d07bd79ac5] - net: replace brand checks with identity checks (Yagiz Nizipli) #57341
[a757f00747] - net: emit an error when custom lookup resolves to a non-string address (Edy Silva) #57192
[984f7ef5bd] - readline: add support for Symbol.dispose (Antoine du Hamel) #57276
[21b6423b9b] - sqlite: reset statement immediately in run() (Colin Ihrig) #57350
[e80bbb7355] - sqlite,test,doc: allow Buffer and URL as database location (Edy Silva) #56991
[3dc3207298] - src: do not pass nullptr to std::string ctor (Charles Kerr) #57354
[5e51c62569] - src: fix process exit listeners not receiving unsettled tla codes (Dario Piotrowicz) #56872
[bf788d9d86] - src: refactor SubtleCrypto algorithm and length validations (Filip Skokan) #57319
[37664e8485] - src: fix node_config_file.h compilation error in GN build (Cheng) #57210
[274c18a365] - (SEMVER-MINOR) src: set default config as node.config.json (Marco Ippolito) #57171
[433657de8c] - src: namespace config file flags (Marco Ippolito) #57170
[d8937f1742] - (SEMVER-MINOR) src: create THROW_ERR_OPTIONS_BEFORE_BOOTSTRAPPING (Marco Ippolito) #57016
[9fd217daa9] - (SEMVER-MINOR) src: add config file support (Marco Ippolito) #57016
[b17163b130] - src: allow embedder customization of OOMErrorHandler (Shelley Vohr) #57325
[6f1c622466] - src: use Maybe<void> in ProcessEmitWarningSync (Daeyeon Jeong) #57250
[4d86a42aa4] - src: remove redundant qualifiers in src/quic (Yagiz Nizipli) #56967
[41ea5a2864] - src: make even more improvements to error handling (James M Snell) #57264
[7a554d9bf3] - src: use cached emit v8::String (Daeyeon Jeong) #57249
[b10ac9a958] - src: refactor SubtleCrypto algorithm and length validations (Filip Skokan) #57273
[90cd780ca6] - src: make more error handling improvements (James M Snell) #57262
[17c9e76722] - src: fix typo in comment (Antoine du Hamel) #57291
[35c283a3f3] - src: reduce string allocations on sqlite (Yagiz Nizipli) #57227
[185d1ffe93] - src: improve error handling in node_messaging.cc (James M Snell) #57211
[96b2bfb88c] - src: improve error handling in tty_wrap.cc (James M Snell) #57211
[f845ad953e] - src: improve error handling in tcp_wrap.cc (James M Snell) #57211
[350f62de6c] - src: fix ThrowInvalidURL call in PathToFileURL (Daniel M Brasil) #57141
[936a9997b2] - src: improve error handling in buffer and dotenv (James M Snell) #57189
[975e2a5c1d] - src: improve error handling in module_wrap (James M Snell) #57188
[3d103ecfbe] - src: improve error handling in spawn_sync (James M Snell) #57185
[98d328a1d6] - src: detect whether the string is one byte representation or not (theweipeng) #56147
[15d7908656] - stream: fix sizeAlgorithm validation in WritableStream (Daeyeon Jeong) #57280
[b866755299] - test: test runner run plan (Pietro Marchini) #57304
[e05e0e5772] - test: update WPT for urlpattern to 3b6b19853a (Node.js GitHub Bot) #57377
[36542b5611] - test: update WPT for WebCryptoAPI to edd42c005c (Node.js GitHub Bot) #57365
[28792ee59a] - test: skip test-config-json-schema with quic (Richard Lau) #57225
[5a21fa4573] - test: add more coverage to node_config_file (Marco Ippolito) #57170
[99b2369142] - test: simplify test-tls-connect-abort-controller.js (Yagiz Nizipli) #57338
[4af2f7f9a8] - test: use assert.match in test-esm-import-meta (Antoine du Hamel) #57290
[99abfb6172] - test: update compression wpt (Yagiz Nizipli) #56960
[f8dde3a391] - test: skip uv-thread-name on IBM i (Abdirahim Musse) #57299
[3bf546c317] - Revert "test: temporary remove resource check from fs read-write" (Rafael Gonzaga) #56906
[8d0f1a7dbf] - test: module syntax should throw (Marco Ippolito) #57121
[0fd3d91e3a] - test: more common.mustNotCall in net, tls (Meghan Denny) #57246
[f803d6ca29] - test: swap assert.strictEqual() parameters (Luigi Pinca) #57217
[eb3576fde0] - test: assert write return values in buffer-bigint64 (Meghan Denny) #57212
[a08981025a] - test: allow embedder running async context frame test (Shelley Vohr) #57193
[20c032ed98] - test: resolve race condition in test-net-write-fully-async-* (Matteo Collina) #57022
[5054fc7941] - (SEMVER-MINOR) test_runner: change ts default glob (Marco Ippolito) #57359
[0ad450f295] - timers: simplify the compareTimersLists function (Gürgün Dayıoğlu) #57110
[75f11ae1cc] - (SEMVER-MINOR) tls: implement tls.getCACertificates() (Joyee Cheung) #57107
[2b2267f203] - tools: add config subspace (Marco Ippolito) #57239
[8e64d38e91] - tools: import rather than require ESLint plugins (Michaël Zasso) #57315
[2569e56b95] - tools: switch back to official OpenSSL (Richard Lau) #57301
[fd49144378] - tools: extract target abseil to abseil.gyp (Chengzhong Wu) #57289
[77e1a85d24] - tools: revert to use @stylistic/eslint-plugin-js v3 (Joyee Cheung) #57314
[2fa6e65262] - tools: add more details about rolling inspector_protocol (Chengzhong Wu) #57167
[5788574cdf] - tools: bump the eslint group in /tools/eslint with 5 updates (dependabot[bot]) #57261
[5955acadba] - tools: remove deps/zlib/GN-scraper.py (Chengzhong Wu) #57238
[a22c21ceb8] - (SEMVER-MINOR) v8: add v8.getCppHeapStatistics() method (Aditi) #57146
[17d4074114] - win,build: add option to enable Control Flow Guard (Hüseyin Açacak) #56605

Windows 64-bit Installer: https://nodejs.org/dist/v23.10.0/node-v23.10.0-x64.msi 
Windows ARM 64-bit Installer: https://nodejs.org/dist/v23.10.0/node-v23.10.0-arm64.msi 
Windows 64-bit Binary: https://nodejs.org/dist/v23.10.0/win-x64/node.exe 
Windows ARM 64-bit Binary: https://nodejs.org/dist/v23.10.0/win-arm64/node.exe 
macOS 64-bit Installer: https://nodejs.org/dist/v23.10.0/node-v23.10.0.pkg 
macOS Apple Silicon 64-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-darwin-arm64.tar.gz 
macOS Intel 64-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-darwin-x64.tar.gz 
Linux 64-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-linux-x64.tar.xz 
Linux PPC LE 64-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-linux-ppc64le.tar.xz 
Linux s390x 64-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-linux-s390x.tar.xz 
AIX 64-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-aix-ppc64.tar.gz 
ARMv7 32-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-linux-armv7l.tar.xz 
ARMv8 64-bit Binary: https://nodejs.org/dist/v23.10.0/node-v23.10.0-linux-arm64.tar.xz 
Source Code: https://nodejs.org/dist/v23.10.0/node-v23.10.0.tar.gz 
Other release files: https://nodejs.org/dist/v23.10.0/ 
Documentation: https://nodejs.org/docs/v23.10.0/api/
SHASUMS
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

269cc2c2ec25228285a9f4221567a9d501171958ad3a3a468ea4d5b422b6df23  node-v23.10.0-aix-ppc64.tar.gz
3aeb0efbc5bddeccec4881f2c513ed542dab80d3111a6d21f1637579996bdecb  node-v23.10.0-arm64.msi
f44ff5990735df42c79820654e2e394b6351ac9fd5eb140c3cae953cf7af302c  node-v23.10.0-darwin-arm64.tar.gz
906014f3ea4d304e94066cf6a1cccd917004f91c3c5750ea72d6360847a77794  node-v23.10.0-darwin-arm64.tar.xz
6c60ebd659dddca673ae7deff9e8e1cc4048f81a6311811a98ba4fe98372275f  node-v23.10.0-darwin-x64.tar.gz
af866d719bfb543937e06073dcbaa95b6a441a7b37f916ddbabf95192c5a1db8  node-v23.10.0-darwin-x64.tar.xz
bcacf2e9aeeff7937e500602ea4b5552272a71ff4aad254cd9c7e155823d4752  node-v23.10.0-headers.tar.gz
aaf870014a959855c2e945ca32fb35ff5f261aa511768142abdcc7f515e2e591  node-v23.10.0-headers.tar.xz
a7a2d642f43436ea22b0a050a3e7b8b9876ea33410e0d74cb18c0901cc9635d0  node-v23.10.0-linux-arm64.tar.gz
7ae7c3faffbbdaccace20cc7877ea4800bd551998ca0042c69d9f191358ee1d2  node-v23.10.0-linux-arm64.tar.xz
fac3f7d4f42dac741e3a538f9fa356b23c81272fe7bc135b944a30b7a4b3face  node-v23.10.0-linux-armv7l.tar.gz
48adacddfc26605c9f0bf228a288b5c81bda80b4dd736372511e1cec999925d9  node-v23.10.0-linux-armv7l.tar.xz
ef494e1627423e15aae09c283de854f5e745bcd95bc5dc58652c040307ffded5  node-v23.10.0-linux-ppc64le.tar.gz
6350023929c0abc926e18bdfa03fe6d170a1e274a8d7a8c4697910e1ab521108  node-v23.10.0-linux-ppc64le.tar.xz
78c176dcb05578c31cb5866ab294ba361b2659e3b20ac816fd644b234e3a0394  node-v23.10.0-linux-s390x.tar.gz
b8753a73b000dac3f607de2f4f3c4384cf707ae876027f20e5e2392a1d71df36  node-v23.10.0-linux-s390x.tar.xz
26f85defcb75e3f8f00c6ad429f9a5b2fb1766e955045fe97e31b11c44315f2d  node-v23.10.0-linux-x64.tar.gz
c0ab11362d0d671469b3a0bfcc6fb484fee5747ca96f93a3917d9ba34f12f535  node-v23.10.0-linux-x64.tar.xz
1f845932ee31868e60f1008ab8909ce2fbc4abce4c38d5d9ab2b4ec81096a16b  node-v23.10.0.pkg
27fea91e7df3f3a18875f76afb63ba4ac60926ffb10ad2be557160ef48787266  node-v23.10.0.tar.gz
b39e5fbd3debb8318ddea6af3e89b07dafb891421fb7ca99fbe19c99adabe5fd  node-v23.10.0.tar.xz
8bca1cc8618b45ba3d4673deac5f6fe023f6e80d0f221a7e034bade294c29299  node-v23.10.0-win-arm64.7z
8ccbf4f4128b7d474b5c0a681dc2bf3dc2d5010fd7a40febe0340d0ad42f0efa  node-v23.10.0-win-arm64.zip
8d56f1d2a39ec17f45b53f35c201ec206db8c9d052fdc32d9a36efd41608c7e4  node-v23.10.0-win-x64.7z
3f6a293a669a3b9e887fa2f9acb64df37923f165a76de635d72f992b0dbf51bd  node-v23.10.0-win-x64.zip
c66e9f5cf6ccb829dcdd9a2de5d1e9c57a9e622b6e71305538fa379d98b49014  node-v23.10.0-x64.msi
23b358a865d69a93bf59e3c696221b5d1754b7de1f66a105da94e7d110add8da  win-arm64/node.exe
b04fc2c4f04273feaae3a2bc372d507fa631d7d6fd5f11265219d5b10d083a3e  win-arm64/node.lib
18b6e9ffdd83f7ad3e3a59a5a5e210f4beb7d55c3d7913fd072813f6e3907603  win-arm64/node_pdb.7z
aacfae8af0893328cb1d2f25d65be4b466d38bf0808a477e50de8291a3cb6e8e  win-arm64/node_pdb.zip
bfa36e570411930744920e9333cd2eca94d8021f1181efc7dccbf14499427b3b  win-x64/node.exe
7005ad268c2a3b55e741957609696fd707982d8d7727325c44e2cbf23f00f565  win-x64/node.lib
667ccd6c5f4af88956085ec03648526b3379dbe4693ab37ee08eae0af5f19bce  win-x64/node_pdb.7z
dd1073af01c2c5ca68d22600ba781f641fb1151d5ed69e6c1de068552255ec14  win-x64/node_pdb.zip
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEEwNYkhDnx1WBKr/tAIdkA/9sjN1YFAmfTYyUACgkQIdkA/9sj
N1YjcRAAlAonkkTWhUX3wxlVYGmXsBsWfKn5tty06i0YHmAqtwqq9IyutcuP//V6
MKT0VfXOWR/bjVGzvThDrQkmuoTEz8L8DVh7mspfh8KFfuxoXGfGMPfwa1AB/5gJ
rn/RG0iyYjsEAdJmPe3hoyjxTFIO4aRAQlVO7bH/6fkfee4mJ6TKbFMuQgnXPG9D
pLA4dH+vE6FcFK0PQhZWVo9yTkGpt7V6NoVGDVK5sPnH2OjzUwYW8jfSaJBnX8Ix
AttcOP7tynSRLNOY+EHBMxh3o+Y2WZbneHcR2QbdQM84NMev//9yexLrDxT7Oigc
SM/iXx1hEjdFRQR87rCvoHnVrpRo7rnaRd9yvptuyk1/IyYeCO1xLvZPcy/eia2S
Q4CrAc9sSRI6Q9dq5/PhyBisEnxCLwN/3wJq03+upAWEWvM6tAH8/aBPEutyy0Nh
re8Yakasrovq5JO9aHsi462XobDnVlFQ5ObACcJxhL7OMBfrzqfB6AUzMbysQnH6
ZQNmbrxvrFO2aVrGEEJkwuC2yQQIRjqSY+G3T+gEYdqoQr5V27rS8qdMAbOJR0w/
ypzNwyL18m2+OZTCMj0amd0zoQpPr06QLxQKemR7+cBMoMaQSL4XqIQ/XdICO7rC
TJzjzOuCLrq/R6LxPSv2Q7+jTjhFOG3zFsiwMvMMyBMklOIBwJo=
=2HpS
-----END PGP SIGNATURE-----
PrevNode v18.20.8 (LTS)NextNode v20.19.0 (LTS)\n\n\n\nMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingMaking Node.js Downloads ReliableAnnouncementsMaking Node.js Downloads Reliableflakey5Apr 05, 2025Node v23.11.0 (Current)ReleasesNode v23.11.0 (Current)Antoine du HamelApr 01, 2025Node.js Test CI Security IncidentVulnerabilitiesNode.js Test CI Security IncidentNode.js Technical Steering CommitteeMar 31, 2025Node v18.20.8 (LTS)ReleasesNode v18.20.8 (LTS)Richard LauMar 27, 2025Node.js Launches Official Community Space on DiscordAnnouncementsNode.js Launches Official Community Space on DiscordCarl Vitullo, Claudio WunderMar 17, 2025Node v23.10.0 (Current)ReleasesNode v23.10.0 (Current)Antoine du HamelMar 13, 2025Previous12345...158Next\n\n\n\nNode v20.19.0 (LTS)ReleasesNode v20.19.0 (LTS)Marco IppolitoMar 13, 2025Updates on CVE for End-of-Life VersionsVulnerabilitiesUpdates on CVE for End-of-Life VersionsRafael GonzagaMar 07, 2025Node v23.9.0 (Current)ReleasesNode v23.9.0 (Current)Michaël ZassoFeb 26, 2025Node v18.20.7 (LTS)ReleasesNode v18.20.7 (LTS)Antoine du HamelFeb 20, 2025Node v23.8.0 (Current)ReleasesNode v23.8.0 (Current)Michaël ZassoFeb 13, 2025Node v22.14.0 (LTS)ReleasesNode v22.14.0 (LTS)Antoine du HamelFeb 11, 2025\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingNode v20.19.0 (LTS)ReleasesNode v20.19.0 (LTS)Marco IppolitoMar 13, 2025Updates on CVE for End-of-Life VersionsVulnerabilitiesUpdates on CVE for End-of-Life VersionsRafael GonzagaMar 07, 2025Node v23.9.0 (Current)ReleasesNode v23.9.0 (Current)Michaël ZassoFeb 26, 2025Node v18.20.7 (LTS)ReleasesNode v18.20.7 (LTS)Antoine du HamelFeb 20, 2025Node v23.8.0 (Current)ReleasesNode v23.8.0 (Current)Michaël ZassoFeb 13, 2025Node v22.14.0 (LTS)ReleasesNode v22.14.0 (LTS)Antoine du HamelFeb 11, 2025Previous12345...158Next\n\n\n\nNode v20.18.3 (LTS)ReleasesNode v20.18.3 (LTS)Marco IppolitoFeb 10, 2025Node v23.7.0 (Current)ReleasesNode v23.7.0 (Current)Antoine du HamelJan 30, 2025Node v23.6.1 (Current)ReleasesNode v23.6.1 (Current)Rafael GonzagaJan 21, 2025Node v22.13.1 (LTS)ReleasesNode v22.13.1 (LTS)Rafael GonzagaJan 21, 2025Node v20.18.2 (LTS)ReleasesNode v20.18.2 (LTS)Rafael GonzagaJan 21, 2025Node v18.20.6 (LTS)ReleasesNode v18.20.6 (LTS)Rafael GonzagaJan 21, 2025\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingNode v20.18.3 (LTS)ReleasesNode v20.18.3 (LTS)Marco IppolitoFeb 10, 2025Node v23.7.0 (Current)ReleasesNode v23.7.0 (Current)Antoine du HamelJan 30, 2025Node v23.6.1 (Current)ReleasesNode v23.6.1 (Current)Rafael GonzagaJan 21, 2025Node v22.13.1 (LTS)ReleasesNode v22.13.1 (LTS)Rafael GonzagaJan 21, 2025Node v20.18.2 (LTS)ReleasesNode v20.18.2 (LTS)Rafael GonzagaJan 21, 2025Node v18.20.6 (LTS)ReleasesNode v18.20.6 (LTS)Rafael GonzagaJan 21, 2025Previous12345...158Next\n\n\n\nTuesday, January 21, 2025 Security ReleasesVulnerabilitiesTuesday, January 21, 2025 Security ReleasesThe Node.js ProjectJan 21, 2025Node v22.13.0 (LTS)ReleasesNode v22.13.0 (LTS)Ruy AdornoJan 07, 2025Node v23.6.0 (Current)ReleasesNode v23.6.0 (Current)Marco IppolitoJan 07, 2025Upcoming CVE for End-of-Life Node.js VersionsVulnerabilitiesUpcoming CVE for End-of-Life Node.js VersionsThe Node.js ProjectJan 06, 2025Node v23.5.0 (Current)ReleasesNode v23.5.0 (Current)Antoine du HamelDec 19, 2024Node v23.4.0 (Current)ReleasesNode v23.4.0 (Current)Antoine du HamelDec 10, 2024\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingTuesday, January 21, 2025 Security ReleasesVulnerabilitiesTuesday, January 21, 2025 Security ReleasesThe Node.js ProjectJan 21, 2025Node v22.13.0 (LTS)ReleasesNode v22.13.0 (LTS)Ruy AdornoJan 07, 2025Node v23.6.0 (Current)ReleasesNode v23.6.0 (Current)Marco IppolitoJan 07, 2025Upcoming CVE for End-of-Life Node.js VersionsVulnerabilitiesUpcoming CVE for End-of-Life Node.js VersionsThe Node.js ProjectJan 06, 2025Node v23.5.0 (Current)ReleasesNode v23.5.0 (Current)Antoine du HamelDec 19, 2024Node v23.4.0 (Current)ReleasesNode v23.4.0 (Current)Antoine du HamelDec 10, 2024Previous12345...158Next\n\n\n\nNode v22.12.0 (LTS)ReleasesNode v22.12.0 (LTS)Ruy AdornoDec 03, 2024Node v23.3.0 (Current)ReleasesNode v23.3.0 (Current)Rafael GonzagaNov 20, 2024Node v20.18.1 (LTS)ReleasesNode v20.18.1 (LTS)Marco IppolitoNov 20, 2024Node v18.20.5 (LTS)ReleasesNode v18.20.5 (LTS)Antoine du HamelNov 12, 2024Node v23.2.0 (Current)ReleasesNode v23.2.0 (Current)Antoine du HamelNov 11, 2024Trip report: Node.js collaboration summit (2024 Dublin)EventsTrip report: Node.js collaboration summit (2024 Dublin)AugustinMauroyNov 11, 2024\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingNode v22.12.0 (LTS)ReleasesNode v22.12.0 (LTS)Ruy AdornoDec 03, 2024Node v23.3.0 (Current)ReleasesNode v23.3.0 (Current)Rafael GonzagaNov 20, 2024Node v20.18.1 (LTS)ReleasesNode v20.18.1 (LTS)Marco IppolitoNov 20, 2024Node v18.20.5 (LTS)ReleasesNode v18.20.5 (LTS)Antoine du HamelNov 12, 2024Node v23.2.0 (Current)ReleasesNode v23.2.0 (Current)Antoine du HamelNov 11, 2024Trip report: Node.js collaboration summit (2024 Dublin)EventsTrip report: Node.js collaboration summit (2024 Dublin)AugustinMauroyNov 11, 2024Previous1...456...158Next\n\n\n\nOffice HoursUncategorizedOffice HoursRyan DahlMar 24, 2011Node v0.4.3ReleasesNode v0.4.3Ryan DahlMar 19, 2011npm 1.0: The New 'ls'npmnpm 1.0: The New 'ls'Isaac SchlueterMar 18, 2011Welcome to the Node blogVideoWelcome to the Node blogRyan DahlMar 18, 2011\n\nBlogThe latest Node.js news, case studies, tutorials, and resources.EverythingAnnouncementsReleasesVulnerabilitiesEventsCategoriesEverythingOffice HoursUncategorizedOffice HoursRyan DahlMar 24, 2011Node v0.4.3ReleasesNode v0.4.3Ryan DahlMar 19, 2011npm 1.0: The New 'ls'npmnpm 1.0: The New 'ls'Isaac SchlueterMar 18, 2011Welcome to the Node blogVideoWelcome to the Node blogRyan DahlMar 18, 2011Previous1...154155156157158Next\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
            
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      

      
        


About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
About this documentation

Contributing
Stability index
Stability overview
JSON output
System calls and man pages



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
About this documentation

Contributing
Stability index
Stability overview
JSON output
System calls and man pages




      
        About this documentation#


Welcome to the official API reference documentation for Node.js!
Node.js is a JavaScript runtime built on the V8 JavaScript engine.
Contributing#
Report errors in this documentation in the issue tracker. See
the contributing guide for directions on how to submit pull requests.
Stability index#

Throughout the documentation are indications of a section's stability. Some APIs
are so proven and so relied upon that they are unlikely to ever change at all.
Others are brand new and experimental, or known to be hazardous.
The stability indexes are as follows:
Stability: 0 - Deprecated. The feature may emit warnings. Backward
compatibility is not guaranteed.

Stability: 1 - Experimental. The feature is not subject to
semantic versioning rules. Non-backward compatible changes or removal may
occur in any future release. Use of the feature is not recommended in
production environments.Experimental features are subdivided into stages:
1.0 - Early development. Experimental features at this stage are unfinished
and subject to substantial change.
1.1 - Active development. Experimental features at this stage are nearing
minimum viability.
1.2 - Release candidate. Experimental features at this stage are hopefully
ready to become stable. No further breaking changes are anticipated but may
still occur in response to user feedback. We encourage user testing and
feedback so that we can know that this feature is ready to be marked as
stable.
Experimental features leave the experimental status typically either by
graduating to stable, or are removed without a deprecation cycle.

Stability: 2 - Stable. Compatibility with the npm ecosystem is a high
priority.

Stability: 3 - Legacy. Although this feature is unlikely to be removed and is
still covered by semantic versioning guarantees, it is no longer actively
maintained, and other alternatives are available.
Features are marked as legacy rather than being deprecated if their use does no
harm, and they are widely relied upon within the npm ecosystem. Bugs found in
legacy features are unlikely to be fixed.
Use caution when making use of Experimental features, particularly when
authoring libraries. Users may not be aware that experimental features are being
used. Bugs or behavior changes may surprise users when Experimental API
modifications occur. To avoid surprises, use of an Experimental feature may need
a command-line flag. Experimental features may also emit a warning.
Stability overview#
APIStabilityAssert(2) StableAsync hooks(1) ExperimentalAsynchronous context tracking(2) StableBuffer(2) StableChild process(2) StableCluster(2) StableConsole(2) StableCrypto(2) StableDiagnostics Channel(2) StableDNS(2) StableDomain(0) DeprecatedFile system(2) StableHTTP(2) StableHTTP/2(2) StableHTTPS(2) StableInspector(2) StableModules: node:module API(1) .2 - Release candidate (asynchronous version) Stability: 1.1 - Active development (synchronous version)Modules: CommonJS modules(2) StableModules: TypeScript(1) .1 - Active developmentOS(2) StablePath(2) StablePerformance measurement APIs(2) StablePunycode(0) DeprecatedQuery string(2) StableReadline(2) StableREPL(2) StableSingle executable applications(1) .1 - Active developmentSQLite(1) .1 - Active development.Stream(2) StableString decoder(2) StableTest runner(2) StableTimers(2) StableTLS (SSL)(2) StableTrace events(1) ExperimentalTTY(2) StableUDP/datagram sockets(2) StableURL(2) StableUtil(2) StableVM (executing JavaScript)(2) StableWeb Crypto API(2) StableWeb Streams API(2) StableWebAssembly System Interface (WASI)(1) ExperimentalWorker threads(2) StableZlib(2) Stable
JSON output#

Added in: v0.6.12

Every .html document has a corresponding .json document. This is for IDEs
and other utilities that consume the documentation.
System calls and man pages#
Node.js functions which wrap a system call will document that. The docs link
to the corresponding man pages which describe how the system call works.
Most Unix system calls have Windows analogues. Still, behavior differences may
be unavoidable.\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
Usage and example

Usage
Example



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
Usage and example

Usage
Example




      
        Usage and example#
Usage#


node [options] [V8 options] [script.js | -e "script" | - ] [arguments]
Please see the Command-line options document for more information.
Example#
An example of a web server written with Node.js which responds with
'Hello, World!':
Commands in this document start with $ or > to replicate how they would
appear in a user's terminal. Do not include the $ and > characters. They are
there to show the start of each command.
Lines that don't start with $ or > character show the output of the previous
command.
First, make sure to have downloaded and installed Node.js. See
Installing Node.js via package manager for further install information.
Now, create an empty project folder called projects, then navigate into it.
Linux and Mac:
mkdir ~/projects
cd ~/projects copy
Windows CMD:
mkdir %USERPROFILE%\projects
cd %USERPROFILE%\projects copy
Windows PowerShell:
mkdir $env:USERPROFILE\projects
cd $env:USERPROFILE\projects copy
Next, create a new source file in the projects
folder and call it hello-world.js.
Open hello-world.js in any preferred text editor and
paste in the following content:
const http = require('node:http');

const hostname = '127.0.0.1';
const port = 3000;

const server = http.createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello, World!\n');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
}); copy
Save the file. Then, in the terminal window, to run the hello-world.js file,
enter:
node hello-world.js copy
Output like this should appear in the terminal:
Server running at http://127.0.0.1:3000/ copy
Now, open any preferred web browser and visit http://127.0.0.1:3000.
If the browser displays the string Hello, World!, that indicates
the server is working.\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
Assert

Strict assertion mode
Legacy assertion mode
Class: assert.AssertionError

new assert.AssertionError(options)


Class: assert.CallTracker

new assert.CallTracker()
tracker.calls([fn][, exact])
tracker.getCalls(fn)
tracker.report()
tracker.reset([fn])
tracker.verify()


assert(value[, message])
assert.deepEqual(actual, expected[, message])

Comparison details


assert.deepStrictEqual(actual, expected[, message])

Comparison details


assert.doesNotMatch(string, regexp[, message])
assert.doesNotReject(asyncFn[, error][, message])
assert.doesNotThrow(fn[, error][, message])
assert.equal(actual, expected[, message])
assert.fail([message])
assert.fail(actual, expected[, message[, operator[, stackStartFn]]])
assert.ifError(value)
assert.match(string, regexp[, message])
assert.notDeepEqual(actual, expected[, message])
assert.notDeepStrictEqual(actual, expected[, message])
assert.notEqual(actual, expected[, message])
assert.notStrictEqual(actual, expected[, message])
assert.ok(value[, message])
assert.rejects(asyncFn[, error][, message])
assert.strictEqual(actual, expected[, message])
assert.throws(fn[, error][, message])
assert.partialDeepStrictEqual(actual, expected[, message])

Comparison details





    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
Assert

Strict assertion mode
Legacy assertion mode
Class: assert.AssertionError

new assert.AssertionError(options)


Class: assert.CallTracker

new assert.CallTracker()
tracker.calls([fn][, exact])
tracker.getCalls(fn)
tracker.report()
tracker.reset([fn])
tracker.verify()


assert(value[, message])
assert.deepEqual(actual, expected[, message])

Comparison details


assert.deepStrictEqual(actual, expected[, message])

Comparison details


assert.doesNotMatch(string, regexp[, message])
assert.doesNotReject(asyncFn[, error][, message])
assert.doesNotThrow(fn[, error][, message])
assert.equal(actual, expected[, message])
assert.fail([message])
assert.fail(actual, expected[, message[, operator[, stackStartFn]]])
assert.ifError(value)
assert.match(string, regexp[, message])
assert.notDeepEqual(actual, expected[, message])
assert.notDeepStrictEqual(actual, expected[, message])
assert.notEqual(actual, expected[, message])
assert.notStrictEqual(actual, expected[, message])
assert.ok(value[, message])
assert.rejects(asyncFn[, error][, message])
assert.strictEqual(actual, expected[, message])
assert.throws(fn[, error][, message])
assert.partialDeepStrictEqual(actual, expected[, message])

Comparison details






      
        Assert#

Stability: 2 - Stable
Source Code: lib/assert.js
The node:assert module provides a set of assertion functions for verifying
invariants.
Strict assertion mode#

History

VersionChanges
v15.0.0
Exposed as require('node:assert/strict').
v13.9.0, v12.16.2
Changed "strict mode" to "strict assertion mode" and "legacy mode" to "legacy assertion mode" to avoid confusion with the more usual meaning of "strict mode".
v9.9.0
Added error diffs to the strict assertion mode.
v9.9.0
Added strict assertion mode to the assert module.
v9.9.0
Added in: v9.9.0



In strict assertion mode, non-strict methods behave like their corresponding
strict methods. For example, assert.deepEqual() will behave like
assert.deepStrictEqual().
In strict assertion mode, error messages for objects display a diff. In legacy
assertion mode, error messages for objects display the objects, often truncated.
To use strict assertion mode:

import { strict as assert } from 'node:assert';const assert = require('node:assert').strict;copy

import assert from 'node:assert/strict';const assert = require('node:assert/strict');copy
Example error diff:

import { strict as assert } from 'node:assert';

assert.deepEqual([[[1, 2, 3]], 4, 5], [[[1, 2, '3']], 4, 5]);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected ... Lines skipped
//
//   [
//     [
// ...
//       2,
// +     3
// -     '3'
//     ],
// ...
//     5
//   ]const assert = require('node:assert/strict');

assert.deepEqual([[[1, 2, 3]], 4, 5], [[[1, 2, '3']], 4, 5]);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected ... Lines skipped
//
//   [
//     [
// ...
//       2,
// +     3
// -     '3'
//     ],
// ...
//     5
//   ]copy
To deactivate the colors, use the NO_COLOR or NODE_DISABLE_COLORS
environment variables. This will also deactivate the colors in the REPL. For
more on color support in terminal environments, read the tty
getColorDepth() documentation.
Legacy assertion mode#
Legacy assertion mode uses the == operator in:

assert.deepEqual()
assert.equal()
assert.notDeepEqual()
assert.notEqual()

To use legacy assertion mode:

import assert from 'node:assert';const assert = require('node:assert');copy
Legacy assertion mode may have surprising results, especially when using
assert.deepEqual():
// WARNING: This does not throw an AssertionError in legacy assertion mode!
assert.deepEqual(/a/gi, new Date()); copy
Class: assert.AssertionError[src]#

Extends: <errors.Error>

Indicates the failure of an assertion. All errors thrown by the node:assert
module will be instances of the AssertionError class.

new assert.AssertionError(options)#

Added in: v0.1.21


options <Object>

message <string> If provided, the error message is set to this value.
actual <any> The actual property on the error instance.
expected <any> The expected property on the error instance.
operator <string> The operator property on the error instance.
stackStartFn <Function> If provided, the generated stack trace omits
frames before this function.



A subclass of <Error> that indicates the failure of an assertion.
All instances contain the built-in Error properties (message and name)
and:

actual <any> Set to the actual argument for methods such as
assert.strictEqual().
expected <any> Set to the expected value for methods such as
assert.strictEqual().
generatedMessage <boolean> Indicates if the message was auto-generated
(true) or not.
code <string> Value is always ERR_ASSERTION to show that the error is an
assertion error.
operator <string> Set to the passed in operator value.


import assert from 'node:assert';

// Generate an AssertionError to compare the error message later:
const { message } = new assert.AssertionError({
  actual: 1,
  expected: 2,
  operator: 'strictEqual',
});

// Verify error output:
try {
  assert.strictEqual(1, 2);
} catch (err) {
  assert(err instanceof assert.AssertionError);
  assert.strictEqual(err.message, message);
  assert.strictEqual(err.name, 'AssertionError');
  assert.strictEqual(err.actual, 1);
  assert.strictEqual(err.expected, 2);
  assert.strictEqual(err.code, 'ERR_ASSERTION');
  assert.strictEqual(err.operator, 'strictEqual');
  assert.strictEqual(err.generatedMessage, true);
}const assert = require('node:assert');

// Generate an AssertionError to compare the error message later:
const { message } = new assert.AssertionError({
  actual: 1,
  expected: 2,
  operator: 'strictEqual',
});

// Verify error output:
try {
  assert.strictEqual(1, 2);
} catch (err) {
  assert(err instanceof assert.AssertionError);
  assert.strictEqual(err.message, message);
  assert.strictEqual(err.name, 'AssertionError');
  assert.strictEqual(err.actual, 1);
  assert.strictEqual(err.expected, 2);
  assert.strictEqual(err.code, 'ERR_ASSERTION');
  assert.strictEqual(err.operator, 'strictEqual');
  assert.strictEqual(err.generatedMessage, true);
}copy

Class: assert.CallTracker#

History

VersionChanges
v20.1.0
the assert.CallTracker class has been deprecated and will be removed in a future version.
v14.2.0, v12.19.0
Added in: v14.2.0, v12.19.0



Stability: 0 - Deprecated
This feature is deprecated and will be removed in a future version.
Please consider using alternatives such as the
mock helper function.

new assert.CallTracker()#

Added in: v14.2.0, v12.19.0

Creates a new CallTracker object which can be used to track if functions
were called a specific number of times. The tracker.verify() must be called
for the verification to take place. The usual pattern would be to call it in a
process.on('exit') handler.

import assert from 'node:assert';
import process from 'node:process';

const tracker = new assert.CallTracker();

function func() {}

// callsfunc() must be called exactly 1 time before tracker.verify().
const callsfunc = tracker.calls(func, 1);

callsfunc();

// Calls tracker.verify() and verifies if all tracker.calls() functions have
// been called exact times.
process.on('exit', () => {
  tracker.verify();
});const assert = require('node:assert');
const process = require('node:process');

const tracker = new assert.CallTracker();

function func() {}

// callsfunc() must be called exactly 1 time before tracker.verify().
const callsfunc = tracker.calls(func, 1);

callsfunc();

// Calls tracker.verify() and verifies if all tracker.calls() functions have
// been called exact times.
process.on('exit', () => {
  tracker.verify();
});copy

tracker.calls([fn][, exact])#

Added in: v14.2.0, v12.19.0


fn <Function> Default: A no-op function.
exact <number> Default: 1.
Returns: <Function> A function that wraps fn.

The wrapper function is expected to be called exactly exact times. If the
function has not been called exactly exact times when
tracker.verify() is called, then tracker.verify() will throw an
error.

import assert from 'node:assert';

// Creates call tracker.
const tracker = new assert.CallTracker();

function func() {}

// Returns a function that wraps func() that must be called exact times
// before tracker.verify().
const callsfunc = tracker.calls(func);const assert = require('node:assert');

// Creates call tracker.
const tracker = new assert.CallTracker();

function func() {}

// Returns a function that wraps func() that must be called exact times
// before tracker.verify().
const callsfunc = tracker.calls(func);copy

tracker.getCalls(fn)#

Added in: v18.8.0, v16.18.0



fn <Function>


Returns: <Array> An array with all the calls to a tracked function.


Object <Object>

thisArg <Object>
arguments <Array> the arguments passed to the tracked function




import assert from 'node:assert';

const tracker = new assert.CallTracker();

function func() {}
const callsfunc = tracker.calls(func);
callsfunc(1, 2, 3);

assert.deepStrictEqual(tracker.getCalls(callsfunc),
                       [{ thisArg: undefined, arguments: [1, 2, 3] }]);const assert = require('node:assert');

// Creates call tracker.
const tracker = new assert.CallTracker();

function func() {}
const callsfunc = tracker.calls(func);
callsfunc(1, 2, 3);

assert.deepStrictEqual(tracker.getCalls(callsfunc),
                       [{ thisArg: undefined, arguments: [1, 2, 3] }]);copy

tracker.report()#

Added in: v14.2.0, v12.19.0


Returns: <Array> An array of objects containing information about the wrapper
functions returned by tracker.calls().
Object <Object>

message <string>
actual <number> The actual number of times the function was called.
expected <number> The number of times the function was expected to be
called.
operator <string> The name of the function that is wrapped.
stack <Object> A stack trace of the function.



The arrays contains information about the expected and actual number of calls of
the functions that have not been called the expected number of times.

import assert from 'node:assert';

// Creates call tracker.
const tracker = new assert.CallTracker();

function func() {}

// Returns a function that wraps func() that must be called exact times
// before tracker.verify().
const callsfunc = tracker.calls(func, 2);

// Returns an array containing information on callsfunc()
console.log(tracker.report());
// [
//  {
//    message: 'Expected the func function to be executed 2 time(s) but was
//    executed 0 time(s).',
//    actual: 0,
//    expected: 2,
//    operator: 'func',
//    stack: stack trace
//  }
// ]const assert = require('node:assert');

// Creates call tracker.
const tracker = new assert.CallTracker();

function func() {}

// Returns a function that wraps func() that must be called exact times
// before tracker.verify().
const callsfunc = tracker.calls(func, 2);

// Returns an array containing information on callsfunc()
console.log(tracker.report());
// [
//  {
//    message: 'Expected the func function to be executed 2 time(s) but was
//    executed 0 time(s).',
//    actual: 0,
//    expected: 2,
//    operator: 'func',
//    stack: stack trace
//  }
// ]copy

tracker.reset([fn])#

Added in: v18.8.0, v16.18.0


fn <Function> a tracked function to reset.

Reset calls of the call tracker.
If a tracked function is passed as an argument, the calls will be reset for it.
If no arguments are passed, all tracked functions will be reset.

import assert from 'node:assert';

const tracker = new assert.CallTracker();

function func() {}
const callsfunc = tracker.calls(func);

callsfunc();
// Tracker was called once
assert.strictEqual(tracker.getCalls(callsfunc).length, 1);

tracker.reset(callsfunc);
assert.strictEqual(tracker.getCalls(callsfunc).length, 0);const assert = require('node:assert');

const tracker = new assert.CallTracker();

function func() {}
const callsfunc = tracker.calls(func);

callsfunc();
// Tracker was called once
assert.strictEqual(tracker.getCalls(callsfunc).length, 1);

tracker.reset(callsfunc);
assert.strictEqual(tracker.getCalls(callsfunc).length, 0);copy

tracker.verify()#

Added in: v14.2.0, v12.19.0

Iterates through the list of functions passed to
tracker.calls() and will throw an error for functions that
have not been called the expected number of times.

import assert from 'node:assert';

// Creates call tracker.
const tracker = new assert.CallTracker();

function func() {}

// Returns a function that wraps func() that must be called exact times
// before tracker.verify().
const callsfunc = tracker.calls(func, 2);

callsfunc();

// Will throw an error since callsfunc() was only called once.
tracker.verify();const assert = require('node:assert');

// Creates call tracker.
const tracker = new assert.CallTracker();

function func() {}

// Returns a function that wraps func() that must be called exact times
// before tracker.verify().
const callsfunc = tracker.calls(func, 2);

callsfunc();

// Will throw an error since callsfunc() was only called once.
tracker.verify();copy

assert(value[, message])#

Added in: v0.5.9


value <any> The input that is checked for being truthy.
message <string> | <Error>

An alias of assert.ok().
assert.deepEqual(actual, expected[, message])#

History

VersionChanges
v22.2.0, v20.15.0
Error cause and errors properties are now compared as well.
v18.0.0
Regular expressions lastIndex property is now compared as well.
v16.0.0, v14.18.0
In Legacy assertion mode, changed status from Deprecated to Legacy.
v14.0.0
NaN is now treated as being identical if both sides are NaN.
v12.0.0
The type tags are now properly compared and there are a couple minor comparison adjustments to make the check less surprising.
v9.0.0
The Error names and messages are now properly compared.
v8.0.0
The Set and Map content is also compared.
v6.4.0, v4.7.1
Typed array slices are handled correctly now.
v6.1.0, v4.5.0
Objects with circular references can be used as inputs now.
v5.10.1, v4.4.3
Handle non-Uint8Array typed arrays correctly.
v0.1.21
Added in: v0.1.21




actual <any>
expected <any>
message <string> | <Error>

Strict assertion mode
An alias of assert.deepStrictEqual().
Legacy assertion mode
Stability: 3 - Legacy: Use assert.deepStrictEqual() instead.
Tests for deep equality between the actual and expected parameters. Consider
using assert.deepStrictEqual() instead. assert.deepEqual() can have
surprising results.
Deep equality means that the enumerable "own" properties of child objects
are also recursively evaluated by the following rules.

Comparison details#

Primitive values are compared with the == operator,
with the exception of <NaN>. It is treated as being identical in case
both sides are <NaN>.
Type tags of objects should be the same.
Only enumerable "own" properties are considered.
<Error> names, messages, causes, and errors are always compared,
even if these are not enumerable properties.
Object wrappers are compared both as objects and unwrapped values.
Object properties are compared unordered.
<Map> keys and <Set> items are compared unordered.
Recursion stops when both sides differ or both sides encounter a circular
reference.
Implementation does not test the [[Prototype]] of
objects.
<Symbol> properties are not compared.
<WeakMap> and <WeakSet> comparison does not rely on their values
but only on their instances.
<RegExp> lastIndex, flags, and source are always compared, even if these
are not enumerable properties.

The following example does not throw an AssertionError because the
primitives are compared using the == operator.

import assert from 'node:assert';
// WARNING: This does not throw an AssertionError!

assert.deepEqual('+00000000', false);const assert = require('node:assert');
// WARNING: This does not throw an AssertionError!

assert.deepEqual('+00000000', false);copy
"Deep" equality means that the enumerable "own" properties of child objects
are evaluated also:

import assert from 'node:assert';

const obj1 = {
  a: {
    b: 1,
  },
};
const obj2 = {
  a: {
    b: 2,
  },
};
const obj3 = {
  a: {
    b: 1,
  },
};
const obj4 = { __proto__: obj1 };

assert.deepEqual(obj1, obj1);
// OK

// Values of b are different:
assert.deepEqual(obj1, obj2);
// AssertionError: { a: { b: 1 } } deepEqual { a: { b: 2 } }

assert.deepEqual(obj1, obj3);
// OK

// Prototypes are ignored:
assert.deepEqual(obj1, obj4);
// AssertionError: { a: { b: 1 } } deepEqual {}const assert = require('node:assert');

const obj1 = {
  a: {
    b: 1,
  },
};
const obj2 = {
  a: {
    b: 2,
  },
};
const obj3 = {
  a: {
    b: 1,
  },
};
const obj4 = { __proto__: obj1 };

assert.deepEqual(obj1, obj1);
// OK

// Values of b are different:
assert.deepEqual(obj1, obj2);
// AssertionError: { a: { b: 1 } } deepEqual { a: { b: 2 } }

assert.deepEqual(obj1, obj3);
// OK

// Prototypes are ignored:
assert.deepEqual(obj1, obj4);
// AssertionError: { a: { b: 1 } } deepEqual {}copy
If the values are not equal, an AssertionError is thrown with a message
property set equal to the value of the message parameter. If the message
parameter is undefined, a default error message is assigned. If the message
parameter is an instance of <Error> then it will be thrown instead of the
AssertionError.

assert.deepStrictEqual(actual, expected[, message])#

History

VersionChanges
v22.2.0, v20.15.0
Error cause and errors properties are now compared as well.
v18.0.0
Regular expressions lastIndex property is now compared as well.
v9.0.0
Enumerable symbol properties are now compared.
v9.0.0
The NaN is now compared using the SameValueZero comparison.
v8.5.0
The Error names and messages are now properly compared.
v8.0.0
The Set and Map content is also compared.
v6.1.0
Objects with circular references can be used as inputs now.
v6.4.0, v4.7.1
Typed array slices are handled correctly now.
v5.10.1, v4.4.3
Handle non-Uint8Array typed arrays correctly.
v1.2.0
Added in: v1.2.0




actual <any>
expected <any>
message <string> | <Error>

Tests for deep equality between the actual and expected parameters.
"Deep" equality means that the enumerable "own" properties of child objects
are recursively evaluated also by the following rules.

Comparison details#

Primitive values are compared using Object.is().
Type tags of objects should be the same.
[[Prototype]] of objects are compared using
the === operator.
Only enumerable "own" properties are considered.
<Error> names, messages, causes, and errors are always compared,
even if these are not enumerable properties.
errors is also compared.
Enumerable own <Symbol> properties are compared as well.
Object wrappers are compared both as objects and unwrapped values.
Object properties are compared unordered.
<Map> keys and <Set> items are compared unordered.
Recursion stops when both sides differ or both sides encounter a circular
reference.
<WeakMap> and <WeakSet> instances are not compared structurally.
They are only equal if they reference the same object. Any comparison between
different WeakMap or WeakSet instances will result in inequality,
even if they contain the same entries.
<RegExp> lastIndex, flags, and source are always compared, even if these
are not enumerable properties.


import assert from 'node:assert/strict';

// This fails because 1 !== '1'.
assert.deepStrictEqual({ a: 1 }, { a: '1' });
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
//   {
// +   a: 1
// -   a: '1'
//   }

// The following objects don't have own properties
const date = new Date();
const object = {};
const fakeDate = {};
Object.setPrototypeOf(fakeDate, Date.prototype);

// Different [[Prototype]]:
assert.deepStrictEqual(object, fakeDate);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + {}
// - Date {}

// Different type tags:
assert.deepStrictEqual(date, fakeDate);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + 2018-04-26T00:49:08.604Z
// - Date {}

assert.deepStrictEqual(NaN, NaN);
// OK because Object.is(NaN, NaN) is true.

// Different unwrapped numbers:
assert.deepStrictEqual(new Number(1), new Number(2));
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + [Number: 1]
// - [Number: 2]

assert.deepStrictEqual(new String('foo'), Object('foo'));
// OK because the object and the string are identical when unwrapped.

assert.deepStrictEqual(-0, -0);
// OK

// Different zeros:
assert.deepStrictEqual(0, -0);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + 0
// - -0

const symbol1 = Symbol();
const symbol2 = Symbol();
assert.deepStrictEqual({ [symbol1]: 1 }, { [symbol1]: 1 });
// OK, because it is the same symbol on both objects.

assert.deepStrictEqual({ [symbol1]: 1 }, { [symbol2]: 1 });
// AssertionError [ERR_ASSERTION]: Inputs identical but not reference equal:
//
// {
//   [Symbol()]: 1
// }

const weakMap1 = new WeakMap();
const weakMap2 = new WeakMap();
const obj = {};

weakMap1.set(obj, 'value');
weakMap2.set(obj, 'value');

// Comparing different instances fails, even with same contents
assert.deepStrictEqual(weakMap1, weakMap2);
// AssertionError: Values have same structure but are not reference-equal:
//
// WeakMap {
//   <items unknown>
// }

// Comparing the same instance to itself succeeds
assert.deepStrictEqual(weakMap1, weakMap1);
// OK

const weakSet1 = new WeakSet();
const weakSet2 = new WeakSet();
weakSet1.add(obj);
weakSet2.add(obj);

// Comparing different instances fails, even with same contents
assert.deepStrictEqual(weakSet1, weakSet2);
// AssertionError: Values have same structure but are not reference-equal:
// + actual - expected
//
// WeakSet {
//   <items unknown>
// }

// Comparing the same instance to itself succeeds
assert.deepStrictEqual(weakSet1, weakSet1);
// OKconst assert = require('node:assert/strict');

// This fails because 1 !== '1'.
assert.deepStrictEqual({ a: 1 }, { a: '1' });
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
//   {
// +   a: 1
// -   a: '1'
//   }

// The following objects don't have own properties
const date = new Date();
const object = {};
const fakeDate = {};
Object.setPrototypeOf(fakeDate, Date.prototype);

// Different [[Prototype]]:
assert.deepStrictEqual(object, fakeDate);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + {}
// - Date {}

// Different type tags:
assert.deepStrictEqual(date, fakeDate);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + 2018-04-26T00:49:08.604Z
// - Date {}

assert.deepStrictEqual(NaN, NaN);
// OK because Object.is(NaN, NaN) is true.

// Different unwrapped numbers:
assert.deepStrictEqual(new Number(1), new Number(2));
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + [Number: 1]
// - [Number: 2]

assert.deepStrictEqual(new String('foo'), Object('foo'));
// OK because the object and the string are identical when unwrapped.

assert.deepStrictEqual(-0, -0);
// OK

// Different zeros:
assert.deepStrictEqual(0, -0);
// AssertionError: Expected inputs to be strictly deep-equal:
// + actual - expected
//
// + 0
// - -0

const symbol1 = Symbol();
const symbol2 = Symbol();
assert.deepStrictEqual({ [symbol1]: 1 }, { [symbol1]: 1 });
// OK, because it is the same symbol on both objects.

assert.deepStrictEqual({ [symbol1]: 1 }, { [symbol2]: 1 });
// AssertionError [ERR_ASSERTION]: Inputs identical but not reference equal:
//
// {
//   [Symbol()]: 1
// }

const weakMap1 = new WeakMap();
const weakMap2 = new WeakMap();
const obj = {};

weakMap1.set(obj, 'value');
weakMap2.set(obj, 'value');

// Comparing different instances fails, even with same contents
assert.deepStrictEqual(weakMap1, weakMap2);
// AssertionError: Values have same structure but are not reference-equal:
//
// WeakMap {
//   <items unknown>
// }

// Comparing the same instance to itself succeeds
assert.deepStrictEqual(weakMap1, weakMap1);
// OK

const weakSet1 = new WeakSet();
const weakSet2 = new WeakSet();
weakSet1.add(obj);
weakSet2.add(obj);

// Comparing different instances fails, even with same contents
assert.deepStrictEqual(weakSet1, weakSet2);
// AssertionError: Values have same structure but are not reference-equal:
// + actual - expected
//
// WeakSet {
//   <items unknown>
// }

// Comparing the same instance to itself succeeds
assert.deepStrictEqual(weakSet1, weakSet1);
// OKcopy
If the values are not equal, an AssertionError is thrown with a message
property set equal to the value of the message parameter. If the message
parameter is undefined, a default error message is assigned. If the message
parameter is an instance of <Error> then it will be thrown instead of the
AssertionError.

assert.doesNotMatch(string, regexp[, message])#

History

VersionChanges
v16.0.0
This API is no longer experimental.
v13.6.0, v12.16.0
Added in: v13.6.0, v12.16.0




string <string>
regexp <RegExp>
message <string> | <Error>

Expects the string input not to match the regular expression.

import assert from 'node:assert/strict';

assert.doesNotMatch('I will fail', /fail/);
// AssertionError [ERR_ASSERTION]: The input was expected to not match the ...

assert.doesNotMatch(123, /pass/);
// AssertionError [ERR_ASSERTION]: The "string" argument must be of type string.

assert.doesNotMatch('I will pass', /different/);
// OKconst assert = require('node:assert/strict');

assert.doesNotMatch('I will fail', /fail/);
// AssertionError [ERR_ASSERTION]: The input was expected to not match the ...

assert.doesNotMatch(123, /pass/);
// AssertionError [ERR_ASSERTION]: The "string" argument must be of type string.

assert.doesNotMatch('I will pass', /different/);
// OKcopy
If the values do match, or if the string argument is of another type than
string, an AssertionError is thrown with a message property set equal
to the value of the message parameter. If the message parameter is
undefined, a default error message is assigned. If the message parameter is an
instance of <Error> then it will be thrown instead of the
AssertionError.
assert.doesNotReject(asyncFn[, error][, message])#

Added in: v10.0.0


asyncFn <Function> | <Promise>
error <RegExp> | <Function>
message <string>
Returns: <Promise>

Awaits the asyncFn promise or, if asyncFn is a function, immediately
calls the function and awaits the returned promise to complete. It will then
check that the promise is not rejected.
If asyncFn is a function and it throws an error synchronously,
assert.doesNotReject() will return a rejected Promise with that error. If
the function does not return a promise, assert.doesNotReject() will return a
rejected Promise with an ERR_INVALID_RETURN_VALUE error. In both cases
the error handler is skipped.
Using assert.doesNotReject() is actually not useful because there is little
benefit in catching a rejection and then rejecting it again. Instead, consider
adding a comment next to the specific code path that should not reject and keep
error messages as expressive as possible.
If specified, error can be a Class, <RegExp> or a validation
function. See assert.throws() for more details.
Besides the async nature to await the completion behaves identically to
assert.doesNotThrow().

import assert from 'node:assert/strict';

await assert.doesNotReject(
  async () => {
    throw new TypeError('Wrong value');
  },
  SyntaxError,
);const assert = require('node:assert/strict');

(async () => {
  await assert.doesNotReject(
    async () => {
      throw new TypeError('Wrong value');
    },
    SyntaxError,
  );
})();copy

import assert from 'node:assert/strict';

assert.doesNotReject(Promise.reject(new TypeError('Wrong value')))
  .then(() => {
    // ...
  });const assert = require('node:assert/strict');

assert.doesNotReject(Promise.reject(new TypeError('Wrong value')))
  .then(() => {
    // ...
  });copy
assert.doesNotThrow(fn[, error][, message])#

History

VersionChanges
v5.11.0, v4.4.5
The message parameter is respected now.
v4.2.0
The error parameter can now be an arrow function.
v0.1.21
Added in: v0.1.21




fn <Function>
error <RegExp> | <Function>
message <string>

Asserts that the function fn does not throw an error.
Using assert.doesNotThrow() is actually not useful because there
is no benefit in catching an error and then rethrowing it. Instead, consider
adding a comment next to the specific code path that should not throw and keep
error messages as expressive as possible.
When assert.doesNotThrow() is called, it will immediately call the fn
function.
If an error is thrown and it is the same type as that specified by the error
parameter, then an AssertionError is thrown. If the error is of a
different type, or if the error parameter is undefined, the error is
propagated back to the caller.
If specified, error can be a Class, <RegExp>, or a validation
function. See assert.throws() for more details.
The following, for instance, will throw the <TypeError> because there is no
matching error type in the assertion:

import assert from 'node:assert/strict';

assert.doesNotThrow(
  () => {
    throw new TypeError('Wrong value');
  },
  SyntaxError,
);const assert = require('node:assert/strict');

assert.doesNotThrow(
  () => {
    throw new TypeError('Wrong value');
  },
  SyntaxError,
);copy
However, the following will result in an AssertionError with the message
'Got unwanted exception...':

import assert from 'node:assert/strict';

assert.doesNotThrow(
  () => {
    throw new TypeError('Wrong value');
  },
  TypeError,
);const assert = require('node:assert/strict');

assert.doesNotThrow(
  () => {
    throw new TypeError('Wrong value');
  },
  TypeError,
);copy
If an AssertionError is thrown and a value is provided for the message
parameter, the value of message will be appended to the AssertionError
message:

import assert from 'node:assert/strict';

assert.doesNotThrow(
  () => {
    throw new TypeError('Wrong value');
  },
  /Wrong value/,
  'Whoops',
);
// Throws: AssertionError: Got unwanted exception: Whoopsconst assert = require('node:assert/strict');

assert.doesNotThrow(
  () => {
    throw new TypeError('Wrong value');
  },
  /Wrong value/,
  'Whoops',
);
// Throws: AssertionError: Got unwanted exception: Whoopscopy
assert.equal(actual, expected[, message])#

History

VersionChanges
v16.0.0, v14.18.0
In Legacy assertion mode, changed status from Deprecated to Legacy.
v14.0.0
NaN is now treated as being identical if both sides are NaN.
v0.1.21
Added in: v0.1.21




actual <any>
expected <any>
message <string> | <Error>

Strict assertion mode
An alias of assert.strictEqual().
Legacy assertion mode
Stability: 3 - Legacy: Use assert.strictEqual() instead.
Tests shallow, coercive equality between the actual and expected parameters
using the == operator. NaN is specially handled
and treated as being identical if both sides are NaN.

import assert from 'node:assert';

assert.equal(1, 1);
// OK, 1 == 1
assert.equal(1, '1');
// OK, 1 == '1'
assert.equal(NaN, NaN);
// OK

assert.equal(1, 2);
// AssertionError: 1 == 2
assert.equal({ a: { b: 1 } }, { a: { b: 1 } });
// AssertionError: { a: { b: 1 } } == { a: { b: 1 } }const assert = require('node:assert');

assert.equal(1, 1);
// OK, 1 == 1
assert.equal(1, '1');
// OK, 1 == '1'
assert.equal(NaN, NaN);
// OK

assert.equal(1, 2);
// AssertionError: 1 == 2
assert.equal({ a: { b: 1 } }, { a: { b: 1 } });
// AssertionError: { a: { b: 1 } } == { a: { b: 1 } }copy
If the values are not equal, an AssertionError is thrown with a message
property set equal to the value of the message parameter. If the message
parameter is undefined, a default error message is assigned. If the message
parameter is an instance of <Error> then it will be thrown instead of the
AssertionError.
assert.fail([message])#

Added in: v0.1.21


message <string> | <Error> Default: 'Failed'

Throws an AssertionError with the provided error message or a default
error message. If the message parameter is an instance of <Error> then
it will be thrown instead of the AssertionError.

import assert from 'node:assert/strict';

assert.fail();
// AssertionError [ERR_ASSERTION]: Failed

assert.fail('boom');
// AssertionError [ERR_ASSERTION]: boom

assert.fail(new TypeError('need array'));
// TypeError: need arrayconst assert = require('node:assert/strict');

assert.fail();
// AssertionError [ERR_ASSERTION]: Failed

assert.fail('boom');
// AssertionError [ERR_ASSERTION]: boom

assert.fail(new TypeError('need array'));
// TypeError: need arraycopy
Using assert.fail() with more than two arguments is possible but deprecated.
See below for further details.
assert.fail(actual, expected[, message[, operator[, stackStartFn]]])#

History

VersionChanges
v10.0.0
Calling assert.fail() with more than one argument is deprecated and emits a warning.
v0.1.21
Added in: v0.1.21



Stability: 0 - Deprecated: Use assert.fail([message]) or other assert
functions instead.

actual <any>
expected <any>
message <string> | <Error>
operator <string> Default: '!='
stackStartFn <Function> Default: assert.fail

If message is falsy, the error message is set as the values of actual and
expected separated by the provided operator. If just the two actual and
expected arguments are provided, operator will default to '!='. If
message is provided as third argument it will be used as the error message and
the other arguments will be stored as properties on the thrown object. If
stackStartFn is provided, all stack frames above that function will be
removed from stacktrace (see Error.captureStackTrace). If no arguments are
given, the default message Failed will be used.

import assert from 'node:assert/strict';

assert.fail('a', 'b');
// AssertionError [ERR_ASSERTION]: 'a' != 'b'

assert.fail(1, 2, undefined, '>');
// AssertionError [ERR_ASSERTION]: 1 > 2

assert.fail(1, 2, 'fail');
// AssertionError [ERR_ASSERTION]: fail

assert.fail(1, 2, 'whoops', '>');
// AssertionError [ERR_ASSERTION]: whoops

assert.fail(1, 2, new TypeError('need array'));
// TypeError: need arrayconst assert = require('node:assert/strict');

assert.fail('a', 'b');
// AssertionError [ERR_ASSERTION]: 'a' != 'b'

assert.fail(1, 2, undefined, '>');
// AssertionError [ERR_ASSERTION]: 1 > 2

assert.fail(1, 2, 'fail');
// AssertionError [ERR_ASSERTION]: fail

assert.fail(1, 2, 'whoops', '>');
// AssertionError [ERR_ASSERTION]: whoops

assert.fail(1, 2, new TypeError('need array'));
// TypeError: need arraycopy
In the last three cases actual, expected, and operator have no
influence on the error message.
Example use of stackStartFn for truncating the exception's stacktrace:

import assert from 'node:assert/strict';

function suppressFrame() {
  assert.fail('a', 'b', undefined, '!==', suppressFrame);
}
suppressFrame();
// AssertionError [ERR_ASSERTION]: 'a' !== 'b'
//     at repl:1:1
//     at ContextifyScript.Script.runInThisContext (vm.js:44:33)
//     ...const assert = require('node:assert/strict');

function suppressFrame() {
  assert.fail('a', 'b', undefined, '!==', suppressFrame);
}
suppressFrame();
// AssertionError [ERR_ASSERTION]: 'a' !== 'b'
//     at repl:1:1
//     at ContextifyScript.Script.runInThisContext (vm.js:44:33)
//     ...copy
assert.ifError(value)#

History

VersionChanges
v10.0.0
Instead of throwing the original error it is now wrapped into an [AssertionError][] that contains the full stack trace.
v10.0.0
Value may now only be undefined or null. Before all falsy values were handled the same as null and did not throw.
v0.1.97
Added in: v0.1.97




value <any>

Throws value if value is not undefined or null. This is useful when
testing the error argument in callbacks. The stack trace contains all frames
from the error passed to ifError() including the potential new frames for
ifError() itself.

import assert from 'node:assert/strict';

assert.ifError(null);
// OK
assert.ifError(0);
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: 0
assert.ifError('error');
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: 'error'
assert.ifError(new Error());
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: Error

// Create some random error frames.
let err;
(function errorFrame() {
  err = new Error('test error');
})();

(function ifErrorFrame() {
  assert.ifError(err);
})();
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: test error
//     at ifErrorFrame
//     at errorFrameconst assert = require('node:assert/strict');

assert.ifError(null);
// OK
assert.ifError(0);
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: 0
assert.ifError('error');
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: 'error'
assert.ifError(new Error());
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: Error

// Create some random error frames.
let err;
(function errorFrame() {
  err = new Error('test error');
})();

(function ifErrorFrame() {
  assert.ifError(err);
})();
// AssertionError [ERR_ASSERTION]: ifError got unwanted exception: test error
//     at ifErrorFrame
//     at errorFramecopy
assert.match(string, regexp[, message])#

History

VersionChanges
v16.0.0
This API is no longer experimental.
v13.6.0, v12.16.0
Added in: v13.6.0, v12.16.0




string <string>
regexp <RegExp>
message <string> | <Error>

Expects the string input to match the regular expression.

import assert from 'node:assert/strict';

assert.match('I will fail', /pass/);
// AssertionError [ERR_ASSERTION]: The input did not match the regular ...

assert.match(123, /pass/);
// AssertionError [ERR_ASSERTION]: The "string" argument must be of type string.

assert.match('I will pass', /pass/);
// OKconst assert = require('node:assert/strict');

assert.match('I will fail', /pass/);
// AssertionError [ERR_ASSERTION]: The input did not match the regular ...

assert.match(123, /pass/);
// AssertionError [ERR_ASSERTION]: The "string" argument must be of type string.

assert.match('I will pass', /pass/);
// OKcopy
If the values do not match, or if the string argument is of another type than
string, an AssertionError is thrown with a message property set equal
to the value of the message parameter. If the message parameter is
undefined, a default error message is assigned. If the message parameter is an
instance of <Error> then it will be thrown instead of the
AssertionError.
assert.notDeepEqual(actual, expected[, message])#

History

VersionChanges
v16.0.0, v14.18.0
In Legacy assertion mode, changed status from Deprecated to Legacy.
v14.0.0
NaN is now treated as being identical if both sides are NaN.
v9.0.0
The Error names and messages are now properly compared.
v8.0.0
The Set and Map content is also compared.
v6.4.0, v4.7.1
Typed array slices are handled correctly now.
v6.1.0, v4.5.0
Objects with circular references can be used as inputs now.
v5.10.1, v4.4.3
Handle non-Uint8Array typed arrays correctly.
v0.1.21
Added in: v0.1.21




actual <any>
expected <any>
message <string> | <Error>

Strict assertion mode
An alias of assert.notDeepStrictEqual().
Legacy assertion mode
Stability: 3 - Legacy: Use assert.notDeepStrictEqual() instead.
Tests for any deep inequality. Opposite of assert.deepEqual().

import assert from 'node:assert';

const obj1 = {
  a: {
    b: 1,
  },
};
const obj2 = {
  a: {
    b: 2,
  },
};
const obj3 = {
  a: {
    b: 1,
  },
};
const obj4 = { __proto__: obj1 };

assert.notDeepEqual(obj1, obj1);
// AssertionError: { a: { b: 1 } } notDeepEqual { a: { b: 1 } }

assert.notDeepEqual(obj1, obj2);
// OK

assert.notDeepEqual(obj1, obj3);
// AssertionError: { a: { b: 1 } } notDeepEqual { a: { b: 1 } }

assert.notDeepEqual(obj1, obj4);
// OKconst assert = require('node:assert');

const obj1 = {
  a: {
    b: 1,
  },
};
const obj2 = {
  a: {
    b: 2,
  },
};
const obj3 = {
  a: {
    b: 1,
  },
};
const obj4 = { __proto__: obj1 };

assert.notDeepEqual(obj1, obj1);
// AssertionError: { a: { b: 1 } } notDeepEqual { a: { b: 1 } }

assert.notDeepEqual(obj1, obj2);
// OK

assert.notDeepEqual(obj1, obj3);
// AssertionError: { a: { b: 1 } } notDeepEqual { a: { b: 1 } }

assert.notDeepEqual(obj1, obj4);
// OKcopy
If the values are deeply equal, an AssertionError is thrown with a
message property set equal to the value of the message parameter. If the
message parameter is undefined, a default error message is assigned. If the
message parameter is an instance of <Error> then it will be thrown
instead of the AssertionError.
assert.notDeepStrictEqual(actual, expected[, message])#

History

VersionChanges
v9.0.0
The -0 and +0 are not considered equal anymore.
v9.0.0
The NaN is now compared using the SameValueZero comparison.
v9.0.0
The Error names and messages are now properly compared.
v8.0.0
The Set and Map content is also compared.
v6.1.0
Objects with circular references can be used as inputs now.
v6.4.0, v4.7.1
Typed array slices are handled correctly now.
v5.10.1, v4.4.3
Handle non-Uint8Array typed arrays correctly.
v1.2.0
Added in: v1.2.0




actual <any>
expected <any>
message <string> | <Error>

Tests for deep strict inequality. Opposite of assert.deepStrictEqual().

import assert from 'node:assert/strict';

assert.notDeepStrictEqual({ a: 1 }, { a: '1' });
// OKconst assert = require('node:assert/strict');

assert.notDeepStrictEqual({ a: 1 }, { a: '1' });
// OKcopy
If the values are deeply and strictly equal, an AssertionError is thrown
with a message property set equal to the value of the message parameter. If
the message parameter is undefined, a default error message is assigned. If
the message parameter is an instance of <Error> then it will be thrown
instead of the AssertionError.
assert.notEqual(actual, expected[, message])#

History

VersionChanges
v16.0.0, v14.18.0
In Legacy assertion mode, changed status from Deprecated to Legacy.
v14.0.0
NaN is now treated as being identical if both sides are NaN.
v0.1.21
Added in: v0.1.21




actual <any>
expected <any>
message <string> | <Error>

Strict assertion mode
An alias of assert.notStrictEqual().
Legacy assertion mode
Stability: 3 - Legacy: Use assert.notStrictEqual() instead.
Tests shallow, coercive inequality with the != operator. NaN is
specially handled and treated as being identical if both sides are NaN.

import assert from 'node:assert';

assert.notEqual(1, 2);
// OK

assert.notEqual(1, 1);
// AssertionError: 1 != 1

assert.notEqual(1, '1');
// AssertionError: 1 != '1'const assert = require('node:assert');

assert.notEqual(1, 2);
// OK

assert.notEqual(1, 1);
// AssertionError: 1 != 1

assert.notEqual(1, '1');
// AssertionError: 1 != '1'copy
If the values are equal, an AssertionError is thrown with a message
property set equal to the value of the message parameter. If the message
parameter is undefined, a default error message is assigned. If the message
parameter is an instance of <Error> then it will be thrown instead of the
AssertionError.
assert.notStrictEqual(actual, expected[, message])#

History

VersionChanges
v10.0.0
Used comparison changed from Strict Equality to Object.is().
v0.1.21
Added in: v0.1.21




actual <any>
expected <any>
message <string> | <Error>

Tests strict inequality between the actual and expected parameters as
determined by Object.is().

import assert from 'node:assert/strict';

assert.notStrictEqual(1, 2);
// OK

assert.notStrictEqual(1, 1);
// AssertionError [ERR_ASSERTION]: Expected "actual" to be strictly unequal to:
//
// 1

assert.notStrictEqual(1, '1');
// OKconst assert = require('node:assert/strict');

assert.notStrictEqual(1, 2);
// OK

assert.notStrictEqual(1, 1);
// AssertionError [ERR_ASSERTION]: Expected "actual" to be strictly unequal to:
//
// 1

assert.notStrictEqual(1, '1');
// OKcopy
If the values are strictly equal, an AssertionError is thrown with a
message property set equal to the value of the message parameter. If the
message parameter is undefined, a default error message is assigned. If the
message parameter is an instance of <Error> then it will be thrown
instead of the AssertionError.
assert.ok(value[, message])#

History

VersionChanges
v10.0.0
The assert.ok() (no arguments) will now use a predefined error message.
v0.1.21
Added in: v0.1.21




value <any>
message <string> | <Error>

Tests if value is truthy. It is equivalent to
assert.equal(!!value, true, message).
If value is not truthy, an AssertionError is thrown with a message
property set equal to the value of the message parameter. If the message
parameter is undefined, a default error message is assigned. If the message
parameter is an instance of <Error> then it will be thrown instead of the
AssertionError.
If no arguments are passed in at all message will be set to the string:
'No value argument passed to `assert.ok()`'.
Be aware that in the repl the error message will be different to the one
thrown in a file! See below for further details.

import assert from 'node:assert/strict';

assert.ok(true);
// OK
assert.ok(1);
// OK

assert.ok();
// AssertionError: No value argument passed to `assert.ok()`

assert.ok(false, 'it\'s false');
// AssertionError: it's false

// In the repl:
assert.ok(typeof 123 === 'string');
// AssertionError: false == true

// In a file (e.g. test.js):
assert.ok(typeof 123 === 'string');
// AssertionError: The expression evaluated to a falsy value:
//
//   assert.ok(typeof 123 === 'string')

assert.ok(false);
// AssertionError: The expression evaluated to a falsy value:
//
//   assert.ok(false)

assert.ok(0);
// AssertionError: The expression evaluated to a falsy value:
//
//   assert.ok(0)const assert = require('node:assert/strict');

assert.ok(true);
// OK
assert.ok(1);
// OK

assert.ok();
// AssertionError: No value argument passed to `assert.ok()`

assert.ok(false, 'it\'s false');
// AssertionError: it's false

// In the repl:
assert.ok(typeof 123 === 'string');
// AssertionError: false == true

// In a file (e.g. test.js):
assert.ok(typeof 123 === 'string');
// AssertionError: The expression evaluated to a falsy value:
//
//   assert.ok(typeof 123 === 'string')

assert.ok(false);
// AssertionError: The expression evaluated to a falsy value:
//
//   assert.ok(false)

assert.ok(0);
// AssertionError: The expression evaluated to a falsy value:
//
//   assert.ok(0)copy

import assert from 'node:assert/strict';

// Using `assert()` works the same:
assert(0);
// AssertionError: The expression evaluated to a falsy value:
//
//   assert(0)const assert = require('node:assert');

// Using `assert()` works the same:
assert(0);
// AssertionError: The expression evaluated to a falsy value:
//
//   assert(0)copy
assert.rejects(asyncFn[, error][, message])#

Added in: v10.0.0


asyncFn <Function> | <Promise>
error <RegExp> | <Function> | <Object> | <Error>
message <string>
Returns: <Promise>

Awaits the asyncFn promise or, if asyncFn is a function, immediately
calls the function and awaits the returned promise to complete. It will then
check that the promise is rejected.
If asyncFn is a function and it throws an error synchronously,
assert.rejects() will return a rejected Promise with that error. If the
function does not return a promise, assert.rejects() will return a rejected
Promise with an ERR_INVALID_RETURN_VALUE error. In both cases the error
handler is skipped.
Besides the async nature to await the completion behaves identically to
assert.throws().
If specified, error can be a Class, <RegExp>, a validation function,
an object where each property will be tested for, or an instance of error where
each property will be tested for including the non-enumerable message and
name properties.
If specified, message will be the message provided by the AssertionError
if the asyncFn fails to reject.

import assert from 'node:assert/strict';

await assert.rejects(
  async () => {
    throw new TypeError('Wrong value');
  },
  {
    name: 'TypeError',
    message: 'Wrong value',
  },
);const assert = require('node:assert/strict');

(async () => {
  await assert.rejects(
    async () => {
      throw new TypeError('Wrong value');
    },
    {
      name: 'TypeError',
      message: 'Wrong value',
    },
  );
})();copy

import assert from 'node:assert/strict';

await assert.rejects(
  async () => {
    throw new TypeError('Wrong value');
  },
  (err) => {
    assert.strictEqual(err.name, 'TypeError');
    assert.strictEqual(err.message, 'Wrong value');
    return true;
  },
);const assert = require('node:assert/strict');

(async () => {
  await assert.rejects(
    async () => {
      throw new TypeError('Wrong value');
    },
    (err) => {
      assert.strictEqual(err.name, 'TypeError');
      assert.strictEqual(err.message, 'Wrong value');
      return true;
    },
  );
})();copy

import assert from 'node:assert/strict';

assert.rejects(
  Promise.reject(new Error('Wrong value')),
  Error,
).then(() => {
  // ...
});const assert = require('node:assert/strict');

assert.rejects(
  Promise.reject(new Error('Wrong value')),
  Error,
).then(() => {
  // ...
});copy
error cannot be a string. If a string is provided as the second
argument, then error is assumed to be omitted and the string will be used for
message instead. This can lead to easy-to-miss mistakes. Please read the
example in assert.throws() carefully if using a string as the second
argument gets considered.
assert.strictEqual(actual, expected[, message])#

History

VersionChanges
v10.0.0
Used comparison changed from Strict Equality to Object.is().
v0.1.21
Added in: v0.1.21




actual <any>
expected <any>
message <string> | <Error>

Tests strict equality between the actual and expected parameters as
determined by Object.is().

import assert from 'node:assert/strict';

assert.strictEqual(1, 2);
// AssertionError [ERR_ASSERTION]: Expected inputs to be strictly equal:
//
// 1 !== 2

assert.strictEqual(1, 1);
// OK

assert.strictEqual('Hello foobar', 'Hello World!');
// AssertionError [ERR_ASSERTION]: Expected inputs to be strictly equal:
// + actual - expected
//
// + 'Hello foobar'
// - 'Hello World!'
//          ^

const apples = 1;
const oranges = 2;
assert.strictEqual(apples, oranges, `apples ${apples} !== oranges ${oranges}`);
// AssertionError [ERR_ASSERTION]: apples 1 !== oranges 2

assert.strictEqual(1, '1', new TypeError('Inputs are not identical'));
// TypeError: Inputs are not identicalconst assert = require('node:assert/strict');

assert.strictEqual(1, 2);
// AssertionError [ERR_ASSERTION]: Expected inputs to be strictly equal:
//
// 1 !== 2

assert.strictEqual(1, 1);
// OK

assert.strictEqual('Hello foobar', 'Hello World!');
// AssertionError [ERR_ASSERTION]: Expected inputs to be strictly equal:
// + actual - expected
//
// + 'Hello foobar'
// - 'Hello World!'
//          ^

const apples = 1;
const oranges = 2;
assert.strictEqual(apples, oranges, `apples ${apples} !== oranges ${oranges}`);
// AssertionError [ERR_ASSERTION]: apples 1 !== oranges 2

assert.strictEqual(1, '1', new TypeError('Inputs are not identical'));
// TypeError: Inputs are not identicalcopy
If the values are not strictly equal, an AssertionError is thrown with a
message property set equal to the value of the message parameter. If the
message parameter is undefined, a default error message is assigned. If the
message parameter is an instance of <Error> then it will be thrown
instead of the AssertionError.
assert.throws(fn[, error][, message])#

History

VersionChanges
v10.2.0
The error parameter can be an object containing regular expressions now.
v9.9.0
The error parameter can now be an object as well.
v4.2.0
The error parameter can now be an arrow function.
v0.1.21
Added in: v0.1.21




fn <Function>
error <RegExp> | <Function> | <Object> | <Error>
message <string>

Expects the function fn to throw an error.
If specified, error can be a Class, <RegExp>, a validation function,
a validation object where each property will be tested for strict deep equality,
or an instance of error where each property will be tested for strict deep
equality including the non-enumerable message and name properties. When
using an object, it is also possible to use a regular expression, when
validating against a string property. See below for examples.
If specified, message will be appended to the message provided by the
AssertionError if the fn call fails to throw or in case the error validation
fails.
Custom validation object/error instance:

import assert from 'node:assert/strict';

const err = new TypeError('Wrong value');
err.code = 404;
err.foo = 'bar';
err.info = {
  nested: true,
  baz: 'text',
};
err.reg = /abc/i;

assert.throws(
  () => {
    throw err;
  },
  {
    name: 'TypeError',
    message: 'Wrong value',
    info: {
      nested: true,
      baz: 'text',
    },
    // Only properties on the validation object will be tested for.
    // Using nested objects requires all properties to be present. Otherwise
    // the validation is going to fail.
  },
);

// Using regular expressions to validate error properties:
assert.throws(
  () => {
    throw err;
  },
  {
    // The `name` and `message` properties are strings and using regular
    // expressions on those will match against the string. If they fail, an
    // error is thrown.
    name: /^TypeError$/,
    message: /Wrong/,
    foo: 'bar',
    info: {
      nested: true,
      // It is not possible to use regular expressions for nested properties!
      baz: 'text',
    },
    // The `reg` property contains a regular expression and only if the
    // validation object contains an identical regular expression, it is going
    // to pass.
    reg: /abc/i,
  },
);

// Fails due to the different `message` and `name` properties:
assert.throws(
  () => {
    const otherErr = new Error('Not found');
    // Copy all enumerable properties from `err` to `otherErr`.
    for (const [key, value] of Object.entries(err)) {
      otherErr[key] = value;
    }
    throw otherErr;
  },
  // The error's `message` and `name` properties will also be checked when using
  // an error as validation object.
  err,
);const assert = require('node:assert/strict');

const err = new TypeError('Wrong value');
err.code = 404;
err.foo = 'bar';
err.info = {
  nested: true,
  baz: 'text',
};
err.reg = /abc/i;

assert.throws(
  () => {
    throw err;
  },
  {
    name: 'TypeError',
    message: 'Wrong value',
    info: {
      nested: true,
      baz: 'text',
    },
    // Only properties on the validation object will be tested for.
    // Using nested objects requires all properties to be present. Otherwise
    // the validation is going to fail.
  },
);

// Using regular expressions to validate error properties:
assert.throws(
  () => {
    throw err;
  },
  {
    // The `name` and `message` properties are strings and using regular
    // expressions on those will match against the string. If they fail, an
    // error is thrown.
    name: /^TypeError$/,
    message: /Wrong/,
    foo: 'bar',
    info: {
      nested: true,
      // It is not possible to use regular expressions for nested properties!
      baz: 'text',
    },
    // The `reg` property contains a regular expression and only if the
    // validation object contains an identical regular expression, it is going
    // to pass.
    reg: /abc/i,
  },
);

// Fails due to the different `message` and `name` properties:
assert.throws(
  () => {
    const otherErr = new Error('Not found');
    // Copy all enumerable properties from `err` to `otherErr`.
    for (const [key, value] of Object.entries(err)) {
      otherErr[key] = value;
    }
    throw otherErr;
  },
  // The error's `message` and `name` properties will also be checked when using
  // an error as validation object.
  err,
);copy
Validate instanceof using constructor:

import assert from 'node:assert/strict';

assert.throws(
  () => {
    throw new Error('Wrong value');
  },
  Error,
);const assert = require('node:assert/strict');

assert.throws(
  () => {
    throw new Error('Wrong value');
  },
  Error,
);copy
Validate error message using <RegExp>:
Using a regular expression runs .toString on the error object, and will
therefore also include the error name.

import assert from 'node:assert/strict';

assert.throws(
  () => {
    throw new Error('Wrong value');
  },
  /^Error: Wrong value$/,
);const assert = require('node:assert/strict');

assert.throws(
  () => {
    throw new Error('Wrong value');
  },
  /^Error: Wrong value$/,
);copy
Custom error validation:
The function must return true to indicate all internal validations passed.
It will otherwise fail with an AssertionError.

import assert from 'node:assert/strict';

assert.throws(
  () => {
    throw new Error('Wrong value');
  },
  (err) => {
    assert(err instanceof Error);
    assert(/value/.test(err));
    // Avoid returning anything from validation functions besides `true`.
    // Otherwise, it's not clear what part of the validation failed. Instead,
    // throw an error about the specific validation that failed (as done in this
    // example) and add as much helpful debugging information to that error as
    // possible.
    return true;
  },
  'unexpected error',
);const assert = require('node:assert/strict');

assert.throws(
  () => {
    throw new Error('Wrong value');
  },
  (err) => {
    assert(err instanceof Error);
    assert(/value/.test(err));
    // Avoid returning anything from validation functions besides `true`.
    // Otherwise, it's not clear what part of the validation failed. Instead,
    // throw an error about the specific validation that failed (as done in this
    // example) and add as much helpful debugging information to that error as
    // possible.
    return true;
  },
  'unexpected error',
);copy
error cannot be a string. If a string is provided as the second
argument, then error is assumed to be omitted and the string will be used for
message instead. This can lead to easy-to-miss mistakes. Using the same
message as the thrown error message is going to result in an
ERR_AMBIGUOUS_ARGUMENT error. Please read the example below carefully if using
a string as the second argument gets considered:

import assert from 'node:assert/strict';

function throwingFirst() {
  throw new Error('First');
}

function throwingSecond() {
  throw new Error('Second');
}

function notThrowing() {}

// The second argument is a string and the input function threw an Error.
// The first case will not throw as it does not match for the error message
// thrown by the input function!
assert.throws(throwingFirst, 'Second');
// In the next example the message has no benefit over the message from the
// error and since it is not clear if the user intended to actually match
// against the error message, Node.js throws an `ERR_AMBIGUOUS_ARGUMENT` error.
assert.throws(throwingSecond, 'Second');
// TypeError [ERR_AMBIGUOUS_ARGUMENT]

// The string is only used (as message) in case the function does not throw:
assert.throws(notThrowing, 'Second');
// AssertionError [ERR_ASSERTION]: Missing expected exception: Second

// If it was intended to match for the error message do this instead:
// It does not throw because the error messages match.
assert.throws(throwingSecond, /Second$/);

// If the error message does not match, an AssertionError is thrown.
assert.throws(throwingFirst, /Second$/);
// AssertionError [ERR_ASSERTION]const assert = require('node:assert/strict');

function throwingFirst() {
  throw new Error('First');
}

function throwingSecond() {
  throw new Error('Second');
}

function notThrowing() {}

// The second argument is a string and the input function threw an Error.
// The first case will not throw as it does not match for the error message
// thrown by the input function!
assert.throws(throwingFirst, 'Second');
// In the next example the message has no benefit over the message from the
// error and since it is not clear if the user intended to actually match
// against the error message, Node.js throws an `ERR_AMBIGUOUS_ARGUMENT` error.
assert.throws(throwingSecond, 'Second');
// TypeError [ERR_AMBIGUOUS_ARGUMENT]

// The string is only used (as message) in case the function does not throw:
assert.throws(notThrowing, 'Second');
// AssertionError [ERR_ASSERTION]: Missing expected exception: Second

// If it was intended to match for the error message do this instead:
// It does not throw because the error messages match.
assert.throws(throwingSecond, /Second$/);

// If the error message does not match, an AssertionError is thrown.
assert.throws(throwingFirst, /Second$/);
// AssertionError [ERR_ASSERTION]copy
Due to the confusing error-prone notation, avoid a string as the second
argument.
assert.partialDeepStrictEqual(actual, expected[, message])#

Added in: v23.4.0

Stability: 1.2 - Release candidate

actual <any>
expected <any>
message <string> | <Error>

Tests for partial deep equality between the actual and expected parameters.
"Deep" equality means that the enumerable "own" properties of child objects
are recursively evaluated also by the following rules. "Partial" equality means
that only properties that exist on the expected parameter are going to be
compared.
This method always passes the same test cases as assert.deepStrictEqual(),
behaving as a super set of it.

Comparison details#

Primitive values are compared using Object.is().
Type tags of objects should be the same.
[[Prototype]] of objects are not compared.
Only enumerable "own" properties are considered.
<Error> names, messages, causes, and errors are always compared,
even if these are not enumerable properties.
errors is also compared.
Enumerable own <Symbol> properties are compared as well.
Object wrappers are compared both as objects and unwrapped values.
Object properties are compared unordered.
<Map> keys and <Set> items are compared unordered.
Recursion stops when both sides differ or both sides encounter a circular
reference.
<WeakMap> and <WeakSet> instances are not compared structurally.
They are only equal if they reference the same object. Any comparison between
different WeakMap or WeakSet instances will result in inequality,
even if they contain the same entries.
<RegExp> lastIndex, flags, and source are always compared, even if these
are not enumerable properties.
Holes in sparse arrays are ignored.


import assert from 'node:assert';

assert.partialDeepStrictEqual(
  { a: { b: { c: 1 } } },
  { a: { b: { c: 1 } } },
);
// OK

assert.partialDeepStrictEqual(
  { a: 1, b: 2, c: 3 },
  { b: 2 },
);
// OK

assert.partialDeepStrictEqual(
  [1, 2, 3, 4, 5, 6, 7, 8, 9],
  [4, 5, 8],
);
// OK

assert.partialDeepStrictEqual(
  new Set([{ a: 1 }, { b: 1 }]),
  new Set([{ a: 1 }]),
);
// OK

assert.partialDeepStrictEqual(
  new Map([['key1', 'value1'], ['key2', 'value2']]),
  new Map([['key2', 'value2']]),
);
// OK

assert.partialDeepStrictEqual(123n, 123n);
// OK

assert.partialDeepStrictEqual(
  [1, 2, 3, 4, 5, 6, 7, 8, 9],
  [5, 4, 8],
);
// AssertionError

assert.partialDeepStrictEqual(
  { a: 1 },
  { a: 1, b: 2 },
);
// AssertionError

assert.partialDeepStrictEqual(
  { a: { b: 2 } },
  { a: { b: '2' } },
);
// AssertionErrorconst assert = require('node:assert');

assert.partialDeepStrictEqual(
  { a: { b: { c: 1 } } },
  { a: { b: { c: 1 } } },
);
// OK

assert.partialDeepStrictEqual(
  { a: 1, b: 2, c: 3 },
  { b: 2 },
);
// OK

assert.partialDeepStrictEqual(
  [1, 2, 3, 4, 5, 6, 7, 8, 9],
  [4, 5, 8],
);
// OK

assert.partialDeepStrictEqual(
  new Set([{ a: 1 }, { b: 1 }]),
  new Set([{ a: 1 }]),
);
// OK

assert.partialDeepStrictEqual(
  new Map([['key1', 'value1'], ['key2', 'value2']]),
  new Map([['key2', 'value2']]),
);
// OK

assert.partialDeepStrictEqual(123n, 123n);
// OK

assert.partialDeepStrictEqual(
  [1, 2, 3, 4, 5, 6, 7, 8, 9],
  [5, 4, 8],
);
// AssertionError

assert.partialDeepStrictEqual(
  { a: 1 },
  { a: 1, b: 2 },
);
// AssertionError

assert.partialDeepStrictEqual(
  { a: { b: 2 } },
  { a: { b: '2' } },
);
// AssertionErrorcopy\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
Asynchronous context tracking

Introduction
Class: AsyncLocalStorage

new AsyncLocalStorage()
Static method: AsyncLocalStorage.bind(fn)
Static method: AsyncLocalStorage.snapshot()
asyncLocalStorage.disable()
asyncLocalStorage.getStore()
asyncLocalStorage.enterWith(store)
asyncLocalStorage.run(store, callback[, ...args])
asyncLocalStorage.exit(callback[, ...args])
Usage with async/await
Troubleshooting: Context loss


Class: AsyncResource

new AsyncResource(type[, options])
Static method: AsyncResource.bind(fn[, type[, thisArg]])
asyncResource.bind(fn[, thisArg])
asyncResource.runInAsyncScope(fn[, thisArg, ...args])
asyncResource.emitDestroy()
asyncResource.asyncId()
asyncResource.triggerAsyncId()
Using AsyncResource for a Worker thread pool
Integrating AsyncResource with EventEmitter





    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
Asynchronous context tracking

Introduction
Class: AsyncLocalStorage

new AsyncLocalStorage()
Static method: AsyncLocalStorage.bind(fn)
Static method: AsyncLocalStorage.snapshot()
asyncLocalStorage.disable()
asyncLocalStorage.getStore()
asyncLocalStorage.enterWith(store)
asyncLocalStorage.run(store, callback[, ...args])
asyncLocalStorage.exit(callback[, ...args])
Usage with async/await
Troubleshooting: Context loss


Class: AsyncResource

new AsyncResource(type[, options])
Static method: AsyncResource.bind(fn[, type[, thisArg]])
asyncResource.bind(fn[, thisArg])
asyncResource.runInAsyncScope(fn[, thisArg, ...args])
asyncResource.emitDestroy()
asyncResource.asyncId()
asyncResource.triggerAsyncId()
Using AsyncResource for a Worker thread pool
Integrating AsyncResource with EventEmitter






      
        Asynchronous context tracking#

Stability: 2 - Stable
Source Code: lib/async_hooks.js
Introduction#
These classes are used to associate state and propagate it throughout
callbacks and promise chains.
They allow storing data throughout the lifetime of a web request
or any other asynchronous duration. It is similar to thread-local storage
in other languages.
The AsyncLocalStorage and AsyncResource classes are part of the
node:async_hooks module:

import { AsyncLocalStorage, AsyncResource } from 'node:async_hooks';const { AsyncLocalStorage, AsyncResource } = require('node:async_hooks');copy
Class: AsyncLocalStorage#

History

VersionChanges
v16.4.0
AsyncLocalStorage is now Stable. Previously, it had been Experimental.
v13.10.0, v12.17.0
Added in: v13.10.0, v12.17.0



This class creates stores that stay coherent through asynchronous operations.
While you can create your own implementation on top of the node:async_hooks
module, AsyncLocalStorage should be preferred as it is a performant and memory
safe implementation that involves significant optimizations that are non-obvious
to implement.
The following example uses AsyncLocalStorage to build a simple logger
that assigns IDs to incoming HTTP requests and includes them in messages
logged within each request.

import http from 'node:http';
import { AsyncLocalStorage } from 'node:async_hooks';

const asyncLocalStorage = new AsyncLocalStorage();

function logWithId(msg) {
  const id = asyncLocalStorage.getStore();
  console.log(`${id !== undefined ? id : '-'}:`, msg);
}

let idSeq = 0;
http.createServer((req, res) => {
  asyncLocalStorage.run(idSeq++, () => {
    logWithId('start');
    // Imagine any chain of async operations here
    setImmediate(() => {
      logWithId('finish');
      res.end();
    });
  });
}).listen(8080);

http.get('http://localhost:8080');
http.get('http://localhost:8080');
// Prints:
//   0: start
//   1: start
//   0: finish
//   1: finishconst http = require('node:http');
const { AsyncLocalStorage } = require('node:async_hooks');

const asyncLocalStorage = new AsyncLocalStorage();

function logWithId(msg) {
  const id = asyncLocalStorage.getStore();
  console.log(`${id !== undefined ? id : '-'}:`, msg);
}

let idSeq = 0;
http.createServer((req, res) => {
  asyncLocalStorage.run(idSeq++, () => {
    logWithId('start');
    // Imagine any chain of async operations here
    setImmediate(() => {
      logWithId('finish');
      res.end();
    });
  });
}).listen(8080);

http.get('http://localhost:8080');
http.get('http://localhost:8080');
// Prints:
//   0: start
//   1: start
//   0: finish
//   1: finishcopy
Each instance of AsyncLocalStorage maintains an independent storage context.
Multiple instances can safely exist simultaneously without risk of interfering
with each other's data.

new AsyncLocalStorage()#

History

VersionChanges
v19.7.0, v18.16.0
Removed experimental onPropagate option.
v19.2.0, v18.13.0
Add option onPropagate.
v13.10.0, v12.17.0
Added in: v13.10.0, v12.17.0



Creates a new instance of AsyncLocalStorage. Store is only provided within a
run() call or after an enterWith() call.

Static method: AsyncLocalStorage.bind(fn)#

History

VersionChanges
v23.11.0
Marking the API stable.
v19.8.0, v18.16.0
Added in: v19.8.0, v18.16.0




fn <Function> The function to bind to the current execution context.
Returns: <Function> A new function that calls fn within the captured
execution context.

Binds the given function to the current execution context.

Static method: AsyncLocalStorage.snapshot()#

History

VersionChanges
v23.11.0
Marking the API stable.
v19.8.0, v18.16.0
Added in: v19.8.0, v18.16.0




Returns: <Function> A new function with the signature
(fn: (...args) : R, ...args) : R.

Captures the current execution context and returns a function that accepts a
function as an argument. Whenever the returned function is called, it
calls the function passed to it within the captured context.
const asyncLocalStorage = new AsyncLocalStorage();
const runInAsyncScope = asyncLocalStorage.run(123, () => AsyncLocalStorage.snapshot());
const result = asyncLocalStorage.run(321, () => runInAsyncScope(() => asyncLocalStorage.getStore()));
console.log(result);  // returns 123 copy
AsyncLocalStorage.snapshot() can replace the use of AsyncResource for simple
async context tracking purposes, for example:
class Foo {
  #runInAsyncScope = AsyncLocalStorage.snapshot();

  get() { return this.#runInAsyncScope(() => asyncLocalStorage.getStore()); }
}

const foo = asyncLocalStorage.run(123, () => new Foo());
console.log(asyncLocalStorage.run(321, () => foo.get())); // returns 123 copy

asyncLocalStorage.disable()#

Added in: v13.10.0, v12.17.0

Stability: 1 - Experimental
Disables the instance of AsyncLocalStorage. All subsequent calls
to asyncLocalStorage.getStore() will return undefined until
asyncLocalStorage.run() or asyncLocalStorage.enterWith() is called again.
When calling asyncLocalStorage.disable(), all current contexts linked to the
instance will be exited.
Calling asyncLocalStorage.disable() is required before the
asyncLocalStorage can be garbage collected. This does not apply to stores
provided by the asyncLocalStorage, as those objects are garbage collected
along with the corresponding async resources.
Use this method when the asyncLocalStorage is not in use anymore
in the current process.

asyncLocalStorage.getStore()#

Added in: v13.10.0, v12.17.0


Returns: <any>

Returns the current store.
If called outside of an asynchronous context initialized by
calling asyncLocalStorage.run() or asyncLocalStorage.enterWith(), it
returns undefined.

asyncLocalStorage.enterWith(store)#

Added in: v13.11.0, v12.17.0

Stability: 1 - Experimental

store <any>

Transitions into the context for the remainder of the current
synchronous execution and then persists the store through any following
asynchronous calls.
Example:
const store = { id: 1 };
// Replaces previous store with the given store object
asyncLocalStorage.enterWith(store);
asyncLocalStorage.getStore(); // Returns the store object
someAsyncOperation(() => {
  asyncLocalStorage.getStore(); // Returns the same object
}); copy
This transition will continue for the entire synchronous execution.
This means that if, for example, the context is entered within an event
handler subsequent event handlers will also run within that context unless
specifically bound to another context with an AsyncResource. That is why
run() should be preferred over enterWith() unless there are strong reasons
to use the latter method.
const store = { id: 1 };

emitter.on('my-event', () => {
  asyncLocalStorage.enterWith(store);
});
emitter.on('my-event', () => {
  asyncLocalStorage.getStore(); // Returns the same object
});

asyncLocalStorage.getStore(); // Returns undefined
emitter.emit('my-event');
asyncLocalStorage.getStore(); // Returns the same object copy

asyncLocalStorage.run(store, callback[, ...args])#

Added in: v13.10.0, v12.17.0


store <any>
callback <Function>
...args <any>

Runs a function synchronously within a context and returns its
return value. The store is not accessible outside of the callback function.
The store is accessible to any asynchronous operations created within the
callback.
The optional args are passed to the callback function.
If the callback function throws an error, the error is thrown by run() too.
The stacktrace is not impacted by this call and the context is exited.
Example:
const store = { id: 2 };
try {
  asyncLocalStorage.run(store, () => {
    asyncLocalStorage.getStore(); // Returns the store object
    setTimeout(() => {
      asyncLocalStorage.getStore(); // Returns the store object
    }, 200);
    throw new Error();
  });
} catch (e) {
  asyncLocalStorage.getStore(); // Returns undefined
  // The error will be caught here
} copy

asyncLocalStorage.exit(callback[, ...args])#

Added in: v13.10.0, v12.17.0

Stability: 1 - Experimental

callback <Function>
...args <any>

Runs a function synchronously outside of a context and returns its
return value. The store is not accessible within the callback function or
the asynchronous operations created within the callback. Any getStore()
call done within the callback function will always return undefined.
The optional args are passed to the callback function.
If the callback function throws an error, the error is thrown by exit() too.
The stacktrace is not impacted by this call and the context is re-entered.
Example:
// Within a call to run
try {
  asyncLocalStorage.getStore(); // Returns the store object or value
  asyncLocalStorage.exit(() => {
    asyncLocalStorage.getStore(); // Returns undefined
    throw new Error();
  });
} catch (e) {
  asyncLocalStorage.getStore(); // Returns the same object or value
  // The error will be caught here
} copy

Usage with async/await#
If, within an async function, only one await call is to run within a context,
the following pattern should be used:
async function fn() {
  await asyncLocalStorage.run(new Map(), () => {
    asyncLocalStorage.getStore().set('key', value);
    return foo(); // The return value of foo will be awaited
  });
} copy
In this example, the store is only available in the callback function and the
functions called by foo. Outside of run, calling getStore will return
undefined.

Troubleshooting: Context loss#
In most cases, AsyncLocalStorage works without issues. In rare situations, the
current store is lost in one of the asynchronous operations.
If your code is callback-based, it is enough to promisify it with
util.promisify() so it starts working with native promises.
If you need to use a callback-based API or your code assumes
a custom thenable implementation, use the AsyncResource class
to associate the asynchronous operation with the correct execution context.
Find the function call responsible for the context loss by logging the content
of asyncLocalStorage.getStore() after the calls you suspect are responsible
for the loss. When the code logs undefined, the last callback called is
probably responsible for the context loss.

Class: AsyncResource#

History

VersionChanges
v16.4.0
AsyncResource is now Stable. Previously, it had been Experimental.



The class AsyncResource is designed to be extended by the embedder's async
resources. Using this, users can easily trigger the lifetime events of their
own resources.
The init hook will trigger when an AsyncResource is instantiated.
The following is an overview of the AsyncResource API.

import { AsyncResource, executionAsyncId } from 'node:async_hooks';

// AsyncResource() is meant to be extended. Instantiating a
// new AsyncResource() also triggers init. If triggerAsyncId is omitted then
// async_hook.executionAsyncId() is used.
const asyncResource = new AsyncResource(
  type, { triggerAsyncId: executionAsyncId(), requireManualDestroy: false },
);

// Run a function in the execution context of the resource. This will
// * establish the context of the resource
// * trigger the AsyncHooks before callbacks
// * call the provided function `fn` with the supplied arguments
// * trigger the AsyncHooks after callbacks
// * restore the original execution context
asyncResource.runInAsyncScope(fn, thisArg, ...args);

// Call AsyncHooks destroy callbacks.
asyncResource.emitDestroy();

// Return the unique ID assigned to the AsyncResource instance.
asyncResource.asyncId();

// Return the trigger ID for the AsyncResource instance.
asyncResource.triggerAsyncId();const { AsyncResource, executionAsyncId } = require('node:async_hooks');

// AsyncResource() is meant to be extended. Instantiating a
// new AsyncResource() also triggers init. If triggerAsyncId is omitted then
// async_hook.executionAsyncId() is used.
const asyncResource = new AsyncResource(
  type, { triggerAsyncId: executionAsyncId(), requireManualDestroy: false },
);

// Run a function in the execution context of the resource. This will
// * establish the context of the resource
// * trigger the AsyncHooks before callbacks
// * call the provided function `fn` with the supplied arguments
// * trigger the AsyncHooks after callbacks
// * restore the original execution context
asyncResource.runInAsyncScope(fn, thisArg, ...args);

// Call AsyncHooks destroy callbacks.
asyncResource.emitDestroy();

// Return the unique ID assigned to the AsyncResource instance.
asyncResource.asyncId();

// Return the trigger ID for the AsyncResource instance.
asyncResource.triggerAsyncId();copy

new AsyncResource(type[, options])#

type <string> The type of async event.
options <Object>

triggerAsyncId <number> The ID of the execution context that created this
async event. Default: executionAsyncId().
requireManualDestroy <boolean> If set to true, disables emitDestroy
when the object is garbage collected. This usually does not need to be set
(even if emitDestroy is called manually), unless the resource's asyncId
is retrieved and the sensitive API's emitDestroy is called with it.
When set to false, the emitDestroy call on garbage collection
will only take place if there is at least one active destroy hook.
Default: false.



Example usage:
class DBQuery extends AsyncResource {
  constructor(db) {
    super('DBQuery');
    this.db = db;
  }

  getInfo(query, callback) {
    this.db.get(query, (err, data) => {
      this.runInAsyncScope(callback, null, err, data);
    });
  }

  close() {
    this.db = null;
    this.emitDestroy();
  }
} copy

Static method: AsyncResource.bind(fn[, type[, thisArg]])#

History

VersionChanges
v20.0.0
The asyncResource property added to the bound function has been deprecated and will be removed in a future version.
v17.8.0, v16.15.0
Changed the default when thisArg is undefined to use this from the caller.
v16.0.0
Added optional thisArg.
v14.8.0, v12.19.0
Added in: v14.8.0, v12.19.0




fn <Function> The function to bind to the current execution context.
type <string> An optional name to associate with the underlying
AsyncResource.
thisArg <any>

Binds the given function to the current execution context.

asyncResource.bind(fn[, thisArg])#

History

VersionChanges
v20.0.0
The asyncResource property added to the bound function has been deprecated and will be removed in a future version.
v17.8.0, v16.15.0
Changed the default when thisArg is undefined to use this from the caller.
v16.0.0
Added optional thisArg.
v14.8.0, v12.19.0
Added in: v14.8.0, v12.19.0




fn <Function> The function to bind to the current AsyncResource.
thisArg <any>

Binds the given function to execute to this AsyncResource's scope.

asyncResource.runInAsyncScope(fn[, thisArg, ...args])#

Added in: v9.6.0


fn <Function> The function to call in the execution context of this async
resource.
thisArg <any> The receiver to be used for the function call.
...args <any> Optional arguments to pass to the function.

Call the provided function with the provided arguments in the execution context
of the async resource. This will establish the context, trigger the AsyncHooks
before callbacks, call the function, trigger the AsyncHooks after callbacks, and
then restore the original execution context.

asyncResource.emitDestroy()#

Returns: <AsyncResource> A reference to asyncResource.

Call all destroy hooks. This should only ever be called once. An error will
be thrown if it is called more than once. This must be manually called. If
the resource is left to be collected by the GC then the destroy hooks will
never be called.

asyncResource.asyncId()#

Returns: <number> The unique asyncId assigned to the resource.


asyncResource.triggerAsyncId()#

Returns: <number> The same triggerAsyncId that is passed to the
AsyncResource constructor.



Using AsyncResource for a Worker thread pool#
The following example shows how to use the AsyncResource class to properly
provide async tracking for a Worker pool. Other resource pools, such as
database connection pools, can follow a similar model.
Assuming that the task is adding two numbers, using a file named
task_processor.js with the following content:

import { parentPort } from 'node:worker_threads';
parentPort.on('message', (task) => {
  parentPort.postMessage(task.a + task.b);
});const { parentPort } = require('node:worker_threads');
parentPort.on('message', (task) => {
  parentPort.postMessage(task.a + task.b);
});copy
a Worker pool around it could use the following structure:

import { AsyncResource } from 'node:async_hooks';
import { EventEmitter } from 'node:events';
import { Worker } from 'node:worker_threads';

const kTaskInfo = Symbol('kTaskInfo');
const kWorkerFreedEvent = Symbol('kWorkerFreedEvent');

class WorkerPoolTaskInfo extends AsyncResource {
  constructor(callback) {
    super('WorkerPoolTaskInfo');
    this.callback = callback;
  }

  done(err, result) {
    this.runInAsyncScope(this.callback, null, err, result);
    this.emitDestroy();  // `TaskInfo`s are used only once.
  }
}

export default class WorkerPool extends EventEmitter {
  constructor(numThreads) {
    super();
    this.numThreads = numThreads;
    this.workers = [];
    this.freeWorkers = [];
    this.tasks = [];

    for (let i = 0; i < numThreads; i++)
      this.addNewWorker();

    // Any time the kWorkerFreedEvent is emitted, dispatch
    // the next task pending in the queue, if any.
    this.on(kWorkerFreedEvent, () => {
      if (this.tasks.length > 0) {
        const { task, callback } = this.tasks.shift();
        this.runTask(task, callback);
      }
    });
  }

  addNewWorker() {
    const worker = new Worker(new URL('task_processor.js', import.meta.url));
    worker.on('message', (result) => {
      // In case of success: Call the callback that was passed to `runTask`,
      // remove the `TaskInfo` associated with the Worker, and mark it as free
      // again.
      worker[kTaskInfo].done(null, result);
      worker[kTaskInfo] = null;
      this.freeWorkers.push(worker);
      this.emit(kWorkerFreedEvent);
    });
    worker.on('error', (err) => {
      // In case of an uncaught exception: Call the callback that was passed to
      // `runTask` with the error.
      if (worker[kTaskInfo])
        worker[kTaskInfo].done(err, null);
      else
        this.emit('error', err);
      // Remove the worker from the list and start a new Worker to replace the
      // current one.
      this.workers.splice(this.workers.indexOf(worker), 1);
      this.addNewWorker();
    });
    this.workers.push(worker);
    this.freeWorkers.push(worker);
    this.emit(kWorkerFreedEvent);
  }

  runTask(task, callback) {
    if (this.freeWorkers.length === 0) {
      // No free threads, wait until a worker thread becomes free.
      this.tasks.push({ task, callback });
      return;
    }

    const worker = this.freeWorkers.pop();
    worker[kTaskInfo] = new WorkerPoolTaskInfo(callback);
    worker.postMessage(task);
  }

  close() {
    for (const worker of this.workers) worker.terminate();
  }
}const { AsyncResource } = require('node:async_hooks');
const { EventEmitter } = require('node:events');
const path = require('node:path');
const { Worker } = require('node:worker_threads');

const kTaskInfo = Symbol('kTaskInfo');
const kWorkerFreedEvent = Symbol('kWorkerFreedEvent');

class WorkerPoolTaskInfo extends AsyncResource {
  constructor(callback) {
    super('WorkerPoolTaskInfo');
    this.callback = callback;
  }

  done(err, result) {
    this.runInAsyncScope(this.callback, null, err, result);
    this.emitDestroy();  // `TaskInfo`s are used only once.
  }
}

class WorkerPool extends EventEmitter {
  constructor(numThreads) {
    super();
    this.numThreads = numThreads;
    this.workers = [];
    this.freeWorkers = [];
    this.tasks = [];

    for (let i = 0; i < numThreads; i++)
      this.addNewWorker();

    // Any time the kWorkerFreedEvent is emitted, dispatch
    // the next task pending in the queue, if any.
    this.on(kWorkerFreedEvent, () => {
      if (this.tasks.length > 0) {
        const { task, callback } = this.tasks.shift();
        this.runTask(task, callback);
      }
    });
  }

  addNewWorker() {
    const worker = new Worker(path.resolve(__dirname, 'task_processor.js'));
    worker.on('message', (result) => {
      // In case of success: Call the callback that was passed to `runTask`,
      // remove the `TaskInfo` associated with the Worker, and mark it as free
      // again.
      worker[kTaskInfo].done(null, result);
      worker[kTaskInfo] = null;
      this.freeWorkers.push(worker);
      this.emit(kWorkerFreedEvent);
    });
    worker.on('error', (err) => {
      // In case of an uncaught exception: Call the callback that was passed to
      // `runTask` with the error.
      if (worker[kTaskInfo])
        worker[kTaskInfo].done(err, null);
      else
        this.emit('error', err);
      // Remove the worker from the list and start a new Worker to replace the
      // current one.
      this.workers.splice(this.workers.indexOf(worker), 1);
      this.addNewWorker();
    });
    this.workers.push(worker);
    this.freeWorkers.push(worker);
    this.emit(kWorkerFreedEvent);
  }

  runTask(task, callback) {
    if (this.freeWorkers.length === 0) {
      // No free threads, wait until a worker thread becomes free.
      this.tasks.push({ task, callback });
      return;
    }

    const worker = this.freeWorkers.pop();
    worker[kTaskInfo] = new WorkerPoolTaskInfo(callback);
    worker.postMessage(task);
  }

  close() {
    for (const worker of this.workers) worker.terminate();
  }
}

module.exports = WorkerPool;copy
Without the explicit tracking added by the WorkerPoolTaskInfo objects,
it would appear that the callbacks are associated with the individual Worker
objects. However, the creation of the Workers is not associated with the
creation of the tasks and does not provide information about when tasks
were scheduled.
This pool could be used as follows:

import WorkerPool from './worker_pool.js';
import os from 'node:os';

const pool = new WorkerPool(os.availableParallelism());

let finished = 0;
for (let i = 0; i < 10; i++) {
  pool.runTask({ a: 42, b: 100 }, (err, result) => {
    console.log(i, err, result);
    if (++finished === 10)
      pool.close();
  });
}const WorkerPool = require('./worker_pool.js');
const os = require('node:os');

const pool = new WorkerPool(os.availableParallelism());

let finished = 0;
for (let i = 0; i < 10; i++) {
  pool.runTask({ a: 42, b: 100 }, (err, result) => {
    console.log(i, err, result);
    if (++finished === 10)
      pool.close();
  });
}copy

Integrating AsyncResource with EventEmitter#
Event listeners triggered by an EventEmitter may be run in a different
execution context than the one that was active when eventEmitter.on() was
called.
The following example shows how to use the AsyncResource class to properly
associate an event listener with the correct execution context. The same
approach can be applied to a Stream or a similar event-driven class.

import { createServer } from 'node:http';
import { AsyncResource, executionAsyncId } from 'node:async_hooks';

const server = createServer((req, res) => {
  req.on('close', AsyncResource.bind(() => {
    // Execution context is bound to the current outer scope.
  }));
  req.on('close', () => {
    // Execution context is bound to the scope that caused 'close' to emit.
  });
  res.end();
}).listen(3000);const { createServer } = require('node:http');
const { AsyncResource, executionAsyncId } = require('node:async_hooks');

const server = createServer((req, res) => {
  req.on('close', AsyncResource.bind(() => {
    // Execution context is bound to the current outer scope.
  }));
  req.on('close', () => {
    // Execution context is bound to the scope that caused 'close' to emit.
  });
  res.end();
}).listen(3000);copy\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
Async hooks

Terminology
Overview
async_hooks.createHook(callbacks)

Error handling
Printing in AsyncHook callbacks


Class: AsyncHook

asyncHook.enable()
asyncHook.disable()
Hook callbacks

init(asyncId, type, triggerAsyncId, resource)

type
triggerAsyncId
resource
Asynchronous context example


before(asyncId)
after(asyncId)
destroy(asyncId)
promiseResolve(asyncId)


async_hooks.executionAsyncResource()
async_hooks.executionAsyncId()
async_hooks.triggerAsyncId()
async_hooks.asyncWrapProviders


Promise execution tracking
JavaScript embedder API

Class: AsyncResource


Class: AsyncLocalStorage



    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
Async hooks

Terminology
Overview
async_hooks.createHook(callbacks)

Error handling
Printing in AsyncHook callbacks


Class: AsyncHook

asyncHook.enable()
asyncHook.disable()
Hook callbacks

init(asyncId, type, triggerAsyncId, resource)

type
triggerAsyncId
resource
Asynchronous context example


before(asyncId)
after(asyncId)
destroy(asyncId)
promiseResolve(asyncId)


async_hooks.executionAsyncResource()
async_hooks.executionAsyncId()
async_hooks.triggerAsyncId()
async_hooks.asyncWrapProviders


Promise execution tracking
JavaScript embedder API

Class: AsyncResource


Class: AsyncLocalStorage




      
        Async hooks#

Stability: 1 - Experimental. Please migrate away from this API, if you can.
We do not recommend using the createHook, AsyncHook, and
executionAsyncResource APIs as they have usability issues, safety risks,
and performance implications. Async context tracking use cases are better
served by the stable AsyncLocalStorage API. If you have a use case for
createHook, AsyncHook, or executionAsyncResource beyond the context
tracking need solved by AsyncLocalStorage or diagnostics data currently
provided by Diagnostics Channel, please open an issue at
https://github.com/nodejs/node/issues describing your use case so we can
create a more purpose-focused API.
Source Code: lib/async_hooks.js
We strongly discourage the use of the async_hooks API.
Other APIs that can cover most of its use cases include:

AsyncLocalStorage tracks async context
process.getActiveResourcesInfo() tracks active resources

The node:async_hooks module provides an API to track asynchronous resources.
It can be accessed using:

import async_hooks from 'node:async_hooks';const async_hooks = require('node:async_hooks');copy
Terminology#
An asynchronous resource represents an object with an associated callback.
This callback may be called multiple times, such as the 'connection'
event in net.createServer(), or just a single time like in fs.open().
A resource can also be closed before the callback is called. AsyncHook does
not explicitly distinguish between these different cases but will represent them
as the abstract concept that is a resource.
If Workers are used, each thread has an independent async_hooks
interface, and each thread will use a new set of async IDs.
Overview#
Following is a simple overview of the public API.

import async_hooks from 'node:async_hooks';

// Return the ID of the current execution context.
const eid = async_hooks.executionAsyncId();

// Return the ID of the handle responsible for triggering the callback of the
// current execution scope to call.
const tid = async_hooks.triggerAsyncId();

// Create a new AsyncHook instance. All of these callbacks are optional.
const asyncHook =
    async_hooks.createHook({ init, before, after, destroy, promiseResolve });

// Allow callbacks of this AsyncHook instance to call. This is not an implicit
// action after running the constructor, and must be explicitly run to begin
// executing callbacks.
asyncHook.enable();

// Disable listening for new asynchronous events.
asyncHook.disable();

//
// The following are the callbacks that can be passed to createHook().
//

// init() is called during object construction. The resource may not have
// completed construction when this callback runs. Therefore, all fields of the
// resource referenced by "asyncId" may not have been populated.
function init(asyncId, type, triggerAsyncId, resource) { }

// before() is called just before the resource's callback is called. It can be
// called 0-N times for handles (such as TCPWrap), and will be called exactly 1
// time for requests (such as FSReqCallback).
function before(asyncId) { }

// after() is called just after the resource's callback has finished.
function after(asyncId) { }

// destroy() is called when the resource is destroyed.
function destroy(asyncId) { }

// promiseResolve() is called only for promise resources, when the
// resolve() function passed to the Promise constructor is invoked
// (either directly or through other means of resolving a promise).
function promiseResolve(asyncId) { }const async_hooks = require('node:async_hooks');

// Return the ID of the current execution context.
const eid = async_hooks.executionAsyncId();

// Return the ID of the handle responsible for triggering the callback of the
// current execution scope to call.
const tid = async_hooks.triggerAsyncId();

// Create a new AsyncHook instance. All of these callbacks are optional.
const asyncHook =
    async_hooks.createHook({ init, before, after, destroy, promiseResolve });

// Allow callbacks of this AsyncHook instance to call. This is not an implicit
// action after running the constructor, and must be explicitly run to begin
// executing callbacks.
asyncHook.enable();

// Disable listening for new asynchronous events.
asyncHook.disable();

//
// The following are the callbacks that can be passed to createHook().
//

// init() is called during object construction. The resource may not have
// completed construction when this callback runs. Therefore, all fields of the
// resource referenced by "asyncId" may not have been populated.
function init(asyncId, type, triggerAsyncId, resource) { }

// before() is called just before the resource's callback is called. It can be
// called 0-N times for handles (such as TCPWrap), and will be called exactly 1
// time for requests (such as FSReqCallback).
function before(asyncId) { }

// after() is called just after the resource's callback has finished.
function after(asyncId) { }

// destroy() is called when the resource is destroyed.
function destroy(asyncId) { }

// promiseResolve() is called only for promise resources, when the
// resolve() function passed to the Promise constructor is invoked
// (either directly or through other means of resolving a promise).
function promiseResolve(asyncId) { }copy
async_hooks.createHook(callbacks)#

Added in: v8.1.0


callbacks <Object> The Hook Callbacks to register

init <Function> The init callback.
before <Function> The before callback.
after <Function> The after callback.
destroy <Function> The destroy callback.
promiseResolve <Function> The promiseResolve callback.


Returns: <AsyncHook> Instance used for disabling and enabling hooks

Registers functions to be called for different lifetime events of each async
operation.
The callbacks init()/before()/after()/destroy() are called for the
respective asynchronous event during a resource's lifetime.
All callbacks are optional. For example, if only resource cleanup needs to
be tracked, then only the destroy callback needs to be passed. The
specifics of all functions that can be passed to callbacks is in the
Hook Callbacks section.

import { createHook } from 'node:async_hooks';

const asyncHook = createHook({
  init(asyncId, type, triggerAsyncId, resource) { },
  destroy(asyncId) { },
});const async_hooks = require('node:async_hooks');

const asyncHook = async_hooks.createHook({
  init(asyncId, type, triggerAsyncId, resource) { },
  destroy(asyncId) { },
});copy
The callbacks will be inherited via the prototype chain:
class MyAsyncCallbacks {
  init(asyncId, type, triggerAsyncId, resource) { }
  destroy(asyncId) {}
}

class MyAddedCallbacks extends MyAsyncCallbacks {
  before(asyncId) { }
  after(asyncId) { }
}

const asyncHook = async_hooks.createHook(new MyAddedCallbacks()); copy
Because promises are asynchronous resources whose lifecycle is tracked
via the async hooks mechanism, the init(), before(), after(), and
destroy() callbacks must not be async functions that return promises.

Error handling#
If any AsyncHook callbacks throw, the application will print the stack trace
and exit. The exit path does follow that of an uncaught exception, but
all 'uncaughtException' listeners are removed, thus forcing the process to
exit. The 'exit' callbacks will still be called unless the application is run
with --abort-on-uncaught-exception, in which case a stack trace will be
printed and the application exits, leaving a core file.
The reason for this error handling behavior is that these callbacks are running
at potentially volatile points in an object's lifetime, for example during
class construction and destruction. Because of this, it is deemed necessary to
bring down the process quickly in order to prevent an unintentional abort in the
future. This is subject to change in the future if a comprehensive analysis is
performed to ensure an exception can follow the normal control flow without
unintentional side effects.

Printing in AsyncHook callbacks#
Because printing to the console is an asynchronous operation, console.log()
will cause AsyncHook callbacks to be called. Using console.log() or
similar asynchronous operations inside an AsyncHook callback function will
cause an infinite recursion. An easy solution to this when debugging is to use a
synchronous logging operation such as fs.writeFileSync(file, msg, flag).
This will print to the file and will not invoke AsyncHook recursively because
it is synchronous.

import { writeFileSync } from 'node:fs';
import { format } from 'node:util';

function debug(...args) {
  // Use a function like this one when debugging inside an AsyncHook callback
  writeFileSync('log.out', `${format(...args)}\n`, { flag: 'a' });
}const fs = require('node:fs');
const util = require('node:util');

function debug(...args) {
  // Use a function like this one when debugging inside an AsyncHook callback
  fs.writeFileSync('log.out', `${util.format(...args)}\n`, { flag: 'a' });
}copy
If an asynchronous operation is needed for logging, it is possible to keep
track of what caused the asynchronous operation using the information
provided by AsyncHook itself. The logging should then be skipped when
it was the logging itself that caused the AsyncHook callback to be called. By
doing this, the otherwise infinite recursion is broken.

Class: AsyncHook#
The class AsyncHook exposes an interface for tracking lifetime events
of asynchronous operations.

asyncHook.enable()#

Returns: <AsyncHook> A reference to asyncHook.

Enable the callbacks for a given AsyncHook instance. If no callbacks are
provided, enabling is a no-op.
The AsyncHook instance is disabled by default. If the AsyncHook instance
should be enabled immediately after creation, the following pattern can be used.

import { createHook } from 'node:async_hooks';

const hook = createHook(callbacks).enable();const async_hooks = require('node:async_hooks');

const hook = async_hooks.createHook(callbacks).enable();copy

asyncHook.disable()#

Returns: <AsyncHook> A reference to asyncHook.

Disable the callbacks for a given AsyncHook instance from the global pool of
AsyncHook callbacks to be executed. Once a hook has been disabled it will not
be called again until enabled.
For API consistency disable() also returns the AsyncHook instance.

Hook callbacks#
Key events in the lifetime of asynchronous events have been categorized into
four areas: instantiation, before/after the callback is called, and when the
instance is destroyed.

init(asyncId, type, triggerAsyncId, resource)#

asyncId <number> A unique ID for the async resource.
type <string> The type of the async resource.
triggerAsyncId <number> The unique ID of the async resource in whose
execution context this async resource was created.
resource <Object> Reference to the resource representing the async
operation, needs to be released during destroy.

Called when a class is constructed that has the possibility to emit an
asynchronous event. This does not mean the instance must call
before/after before destroy is called, only that the possibility
exists.
This behavior can be observed by doing something like opening a resource then
closing it before the resource can be used. The following snippet demonstrates
this.

import { createServer } from 'node:net';

createServer().listen(function() { this.close(); });
// OR
clearTimeout(setTimeout(() => {}, 10));require('node:net').createServer().listen(function() { this.close(); });
// OR
clearTimeout(setTimeout(() => {}, 10));copy
Every new resource is assigned an ID that is unique within the scope of the
current Node.js instance.

type#
The type is a string identifying the type of resource that caused
init to be called. Generally, it will correspond to the name of the
resource's constructor.
The type of resources created by Node.js itself can change in any Node.js
release. Valid values include TLSWRAP,
TCPWRAP, TCPSERVERWRAP, GETADDRINFOREQWRAP, FSREQCALLBACK,
Microtask, and Timeout. Inspect the source code of the Node.js version used
to get the full list.
Furthermore users of AsyncResource create async resources independent
of Node.js itself.
There is also the PROMISE resource type, which is used to track Promise
instances and asynchronous work scheduled by them.
Users are able to define their own type when using the public embedder API.
It is possible to have type name collisions. Embedders are encouraged to use
unique prefixes, such as the npm package name, to prevent collisions when
listening to the hooks.

triggerAsyncId#
triggerAsyncId is the asyncId of the resource that caused (or "triggered")
the new resource to initialize and that caused init to call. This is different
from async_hooks.executionAsyncId() that only shows when a resource was
created, while triggerAsyncId shows why a resource was created.
The following is a simple demonstration of triggerAsyncId:

import { createHook, executionAsyncId } from 'node:async_hooks';
import { stdout } from 'node:process';
import net from 'node:net';
import fs from 'node:fs';

createHook({
  init(asyncId, type, triggerAsyncId) {
    const eid = executionAsyncId();
    fs.writeSync(
      stdout.fd,
      `${type}(${asyncId}): trigger: ${triggerAsyncId} execution: ${eid}\n`);
  },
}).enable();

net.createServer((conn) => {}).listen(8080);const { createHook, executionAsyncId } = require('node:async_hooks');
const { stdout } = require('node:process');
const net = require('node:net');
const fs = require('node:fs');

createHook({
  init(asyncId, type, triggerAsyncId) {
    const eid = executionAsyncId();
    fs.writeSync(
      stdout.fd,
      `${type}(${asyncId}): trigger: ${triggerAsyncId} execution: ${eid}\n`);
  },
}).enable();

net.createServer((conn) => {}).listen(8080);copy
Output when hitting the server with nc localhost 8080:
TCPSERVERWRAP(5): trigger: 1 execution: 1
TCPWRAP(7): trigger: 5 execution: 0 copy
The TCPSERVERWRAP is the server which receives the connections.
The TCPWRAP is the new connection from the client. When a new
connection is made, the TCPWrap instance is immediately constructed. This
happens outside of any JavaScript stack. (An executionAsyncId() of 0 means
that it is being executed from C++ with no JavaScript stack above it.) With only
that information, it would be impossible to link resources together in
terms of what caused them to be created, so triggerAsyncId is given the task
of propagating what resource is responsible for the new resource's existence.

resource#
resource is an object that represents the actual async resource that has
been initialized. The API to access the object may be specified by the
creator of the resource. Resources created by Node.js itself are internal
and may change at any time. Therefore no API is specified for these.
In some cases the resource object is reused for performance reasons, it is
thus not safe to use it as a key in a WeakMap or add properties to it.

Asynchronous context example#
The context tracking use case is covered by the stable API AsyncLocalStorage.
This example only illustrates async hooks operation but AsyncLocalStorage
fits better to this use case.
The following is an example with additional information about the calls to
init between the before and after calls, specifically what the
callback to listen() will look like. The output formatting is slightly more
elaborate to make calling context easier to see.

import async_hooks from 'node:async_hooks';
import fs from 'node:fs';
import net from 'node:net';
import { stdout } from 'node:process';
const { fd } = stdout;

let indent = 0;
async_hooks.createHook({
  init(asyncId, type, triggerAsyncId) {
    const eid = async_hooks.executionAsyncId();
    const indentStr = ' '.repeat(indent);
    fs.writeSync(
      fd,
      `${indentStr}${type}(${asyncId}):` +
      ` trigger: ${triggerAsyncId} execution: ${eid}\n`);
  },
  before(asyncId) {
    const indentStr = ' '.repeat(indent);
    fs.writeSync(fd, `${indentStr}before:  ${asyncId}\n`);
    indent += 2;
  },
  after(asyncId) {
    indent -= 2;
    const indentStr = ' '.repeat(indent);
    fs.writeSync(fd, `${indentStr}after:  ${asyncId}\n`);
  },
  destroy(asyncId) {
    const indentStr = ' '.repeat(indent);
    fs.writeSync(fd, `${indentStr}destroy:  ${asyncId}\n`);
  },
}).enable();

net.createServer(() => {}).listen(8080, () => {
  // Let's wait 10ms before logging the server started.
  setTimeout(() => {
    console.log('>>>', async_hooks.executionAsyncId());
  }, 10);
});const async_hooks = require('node:async_hooks');
const fs = require('node:fs');
const net = require('node:net');
const { fd } = process.stdout;

let indent = 0;
async_hooks.createHook({
  init(asyncId, type, triggerAsyncId) {
    const eid = async_hooks.executionAsyncId();
    const indentStr = ' '.repeat(indent);
    fs.writeSync(
      fd,
      `${indentStr}${type}(${asyncId}):` +
      ` trigger: ${triggerAsyncId} execution: ${eid}\n`);
  },
  before(asyncId) {
    const indentStr = ' '.repeat(indent);
    fs.writeSync(fd, `${indentStr}before:  ${asyncId}\n`);
    indent += 2;
  },
  after(asyncId) {
    indent -= 2;
    const indentStr = ' '.repeat(indent);
    fs.writeSync(fd, `${indentStr}after:  ${asyncId}\n`);
  },
  destroy(asyncId) {
    const indentStr = ' '.repeat(indent);
    fs.writeSync(fd, `${indentStr}destroy:  ${asyncId}\n`);
  },
}).enable();

net.createServer(() => {}).listen(8080, () => {
  // Let's wait 10ms before logging the server started.
  setTimeout(() => {
    console.log('>>>', async_hooks.executionAsyncId());
  }, 10);
});copy
Output from only starting the server:
TCPSERVERWRAP(5): trigger: 1 execution: 1
TickObject(6): trigger: 5 execution: 1
before:  6
  Timeout(7): trigger: 6 execution: 6
after:   6
destroy: 6
before:  7
>>> 7
  TickObject(8): trigger: 7 execution: 7
after:   7
before:  8
after:   8 copy
As illustrated in the example, executionAsyncId() and execution each specify
the value of the current execution context; which is delineated by calls to
before and after.
Only using execution to graph resource allocation results in the following:
  root(1)
     ^
     |
TickObject(6)
     ^
     |
 Timeout(7) copy
The TCPSERVERWRAP is not part of this graph, even though it was the reason for
console.log() being called. This is because binding to a port without a host
name is a synchronous operation, but to maintain a completely asynchronous
API the user's callback is placed in a process.nextTick(). Which is why
TickObject is present in the output and is a 'parent' for .listen()
callback.
The graph only shows when a resource was created, not why, so to track
the why use triggerAsyncId. Which can be represented with the following
graph:
 bootstrap(1)
     |
     ˅
TCPSERVERWRAP(5)
     |
     ˅
 TickObject(6)
     |
     ˅
  Timeout(7) copy

before(asyncId)#

asyncId <number>

When an asynchronous operation is initiated (such as a TCP server receiving a
new connection) or completes (such as writing data to disk) a callback is
called to notify the user. The before callback is called just before said
callback is executed. asyncId is the unique identifier assigned to the
resource about to execute the callback.
The before callback will be called 0 to N times. The before callback
will typically be called 0 times if the asynchronous operation was cancelled
or, for example, if no connections are received by a TCP server. Persistent
asynchronous resources like a TCP server will typically call the before
callback multiple times, while other operations like fs.open() will call
it only once.

after(asyncId)#

asyncId <number>

Called immediately after the callback specified in before is completed.
If an uncaught exception occurs during execution of the callback, then after
will run after the 'uncaughtException' event is emitted or a domain's
handler runs.

destroy(asyncId)#

asyncId <number>

Called after the resource corresponding to asyncId is destroyed. It is also
called asynchronously from the embedder API emitDestroy().
Some resources depend on garbage collection for cleanup, so if a reference is
made to the resource object passed to init it is possible that destroy
will never be called, causing a memory leak in the application. If the resource
does not depend on garbage collection, then this will not be an issue.
Using the destroy hook results in additional overhead because it enables
tracking of Promise instances via the garbage collector.

promiseResolve(asyncId)#

Added in: v8.6.0


asyncId <number>

Called when the resolve function passed to the Promise constructor is
invoked (either directly or through other means of resolving a promise).
resolve() does not do any observable synchronous work.
The Promise is not necessarily fulfilled or rejected at this point if the
Promise was resolved by assuming the state of another Promise.
new Promise((resolve) => resolve(true)).then((a) => {}); copy
calls the following callbacks:
init for PROMISE with id 5, trigger id: 1
  promise resolve 5      # corresponds to resolve(true)
init for PROMISE with id 6, trigger id: 5  # the Promise returned by then()
  before 6               # the then() callback is entered
  promise resolve 6      # the then() callback resolves the promise by returning
  after 6 copy

async_hooks.executionAsyncResource()#

Added in: v13.9.0, v12.17.0


Returns: <Object> The resource representing the current execution.
Useful to store data within the resource.

Resource objects returned by executionAsyncResource() are most often internal
Node.js handle objects with undocumented APIs. Using any functions or properties
on the object is likely to crash your application and should be avoided.
Using executionAsyncResource() in the top-level execution context will
return an empty object as there is no handle or request object to use,
but having an object representing the top-level can be helpful.

import { open } from 'node:fs';
import { executionAsyncId, executionAsyncResource } from 'node:async_hooks';

console.log(executionAsyncId(), executionAsyncResource());  // 1 {}
open(new URL(import.meta.url), 'r', (err, fd) => {
  console.log(executionAsyncId(), executionAsyncResource());  // 7 FSReqWrap
});const { open } = require('node:fs');
const { executionAsyncId, executionAsyncResource } = require('node:async_hooks');

console.log(executionAsyncId(), executionAsyncResource());  // 1 {}
open(__filename, 'r', (err, fd) => {
  console.log(executionAsyncId(), executionAsyncResource());  // 7 FSReqWrap
});copy
This can be used to implement continuation local storage without the
use of a tracking Map to store the metadata:

import { createServer } from 'node:http';
import {
  executionAsyncId,
  executionAsyncResource,
  createHook,
} from 'node:async_hooks';
const sym = Symbol('state'); // Private symbol to avoid pollution

createHook({
  init(asyncId, type, triggerAsyncId, resource) {
    const cr = executionAsyncResource();
    if (cr) {
      resource[sym] = cr[sym];
    }
  },
}).enable();

const server = createServer((req, res) => {
  executionAsyncResource()[sym] = { state: req.url };
  setTimeout(function() {
    res.end(JSON.stringify(executionAsyncResource()[sym]));
  }, 100);
}).listen(3000);const { createServer } = require('node:http');
const {
  executionAsyncId,
  executionAsyncResource,
  createHook,
} = require('node:async_hooks');
const sym = Symbol('state'); // Private symbol to avoid pollution

createHook({
  init(asyncId, type, triggerAsyncId, resource) {
    const cr = executionAsyncResource();
    if (cr) {
      resource[sym] = cr[sym];
    }
  },
}).enable();

const server = createServer((req, res) => {
  executionAsyncResource()[sym] = { state: req.url };
  setTimeout(function() {
    res.end(JSON.stringify(executionAsyncResource()[sym]));
  }, 100);
}).listen(3000);copy

async_hooks.executionAsyncId()#

History

VersionChanges
v8.2.0
Renamed from currentId.
v8.1.0
Added in: v8.1.0




Returns: <number> The asyncId of the current execution context. Useful to
track when something calls.


import { executionAsyncId } from 'node:async_hooks';
import fs from 'node:fs';

console.log(executionAsyncId());  // 1 - bootstrap
const path = '.';
fs.open(path, 'r', (err, fd) => {
  console.log(executionAsyncId());  // 6 - open()
});const async_hooks = require('node:async_hooks');
const fs = require('node:fs');

console.log(async_hooks.executionAsyncId());  // 1 - bootstrap
const path = '.';
fs.open(path, 'r', (err, fd) => {
  console.log(async_hooks.executionAsyncId());  // 6 - open()
});copy
The ID returned from executionAsyncId() is related to execution timing, not
causality (which is covered by triggerAsyncId()):
const server = net.createServer((conn) => {
  // Returns the ID of the server, not of the new connection, because the
  // callback runs in the execution scope of the server's MakeCallback().
  async_hooks.executionAsyncId();

}).listen(port, () => {
  // Returns the ID of a TickObject (process.nextTick()) because all
  // callbacks passed to .listen() are wrapped in a nextTick().
  async_hooks.executionAsyncId();
}); copy
Promise contexts may not get precise executionAsyncIds by default.
See the section on promise execution tracking.

async_hooks.triggerAsyncId()#

Returns: <number> The ID of the resource responsible for calling the callback
that is currently being executed.

const server = net.createServer((conn) => {
  // The resource that caused (or triggered) this callback to be called
  // was that of the new connection. Thus the return value of triggerAsyncId()
  // is the asyncId of "conn".
  async_hooks.triggerAsyncId();

}).listen(port, () => {
  // Even though all callbacks passed to .listen() are wrapped in a nextTick()
  // the callback itself exists because the call to the server's .listen()
  // was made. So the return value would be the ID of the server.
  async_hooks.triggerAsyncId();
}); copy
Promise contexts may not get valid triggerAsyncIds by default. See
the section on promise execution tracking.

async_hooks.asyncWrapProviders#

Added in: v17.2.0, v16.14.0


Returns: A map of provider types to the corresponding numeric id.
This map contains all the event types that might be emitted by the async_hooks.init() event.

This feature suppresses the deprecated usage of process.binding('async_wrap').Providers.
See: DEP0111

Promise execution tracking#
By default, promise executions are not assigned asyncIds due to the relatively
expensive nature of the promise introspection API provided by
V8. This means that programs using promises or async/await will not get
correct execution and trigger ids for promise callback contexts by default.

import { executionAsyncId, triggerAsyncId } from 'node:async_hooks';

Promise.resolve(1729).then(() => {
  console.log(`eid ${executionAsyncId()} tid ${triggerAsyncId()}`);
});
// produces:
// eid 1 tid 0const { executionAsyncId, triggerAsyncId } = require('node:async_hooks');

Promise.resolve(1729).then(() => {
  console.log(`eid ${executionAsyncId()} tid ${triggerAsyncId()}`);
});
// produces:
// eid 1 tid 0copy
Observe that the then() callback claims to have executed in the context of the
outer scope even though there was an asynchronous hop involved. Also,
the triggerAsyncId value is 0, which means that we are missing context about
the resource that caused (triggered) the then() callback to be executed.
Installing async hooks via async_hooks.createHook enables promise execution
tracking:

import { createHook, executionAsyncId, triggerAsyncId } from 'node:async_hooks';
createHook({ init() {} }).enable(); // forces PromiseHooks to be enabled.
Promise.resolve(1729).then(() => {
  console.log(`eid ${executionAsyncId()} tid ${triggerAsyncId()}`);
});
// produces:
// eid 7 tid 6const { createHook, executionAsyncId, triggerAsyncId } = require('node:async_hooks');

createHook({ init() {} }).enable(); // forces PromiseHooks to be enabled.
Promise.resolve(1729).then(() => {
  console.log(`eid ${executionAsyncId()} tid ${triggerAsyncId()}`);
});
// produces:
// eid 7 tid 6copy
In this example, adding any actual hook function enabled the tracking of
promises. There are two promises in the example above; the promise created by
Promise.resolve() and the promise returned by the call to then(). In the
example above, the first promise got the asyncId 6 and the latter got
asyncId 7. During the execution of the then() callback, we are executing
in the context of promise with asyncId 7. This promise was triggered by
async resource 6.
Another subtlety with promises is that before and after callbacks are run
only on chained promises. That means promises not created by then()/catch()
will not have the before and after callbacks fired on them. For more details
see the details of the V8 PromiseHooks API.
JavaScript embedder API#
Library developers that handle their own asynchronous resources performing tasks
like I/O, connection pooling, or managing callback queues may use the
AsyncResource JavaScript API so that all the appropriate callbacks are called.

Class: AsyncResource#
The documentation for this class has moved AsyncResource.

Class: AsyncLocalStorage#
The documentation for this class has moved AsyncLocalStorage.\n\n\n\nNode.js
        
      
      
About this documentation
Usage and example



Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    

    
      
        
          Node.js v23.11.0 documentation
          
            
            
          
        
        
          
            Node.js v23.11.0
            
    
      
        
        Table of contents
      

      
Buffer

Buffers and character encodings
Buffers and TypedArrays
Buffers and iteration
Class: Blob

new buffer.Blob([sources[, options]])
blob.arrayBuffer()

blob.bytes()


blob.size
blob.slice([start[, end[, type]]])
blob.stream()
blob.text()
blob.type
Blob objects and MessageChannel


Class: Buffer

Static method: Buffer.alloc(size[, fill[, encoding]])
Static method: Buffer.allocUnsafe(size)
Static method: Buffer.allocUnsafeSlow(size)
Static method: Buffer.byteLength(string[, encoding])
Static method: Buffer.compare(buf1, buf2)
Static method: Buffer.concat(list[, totalLength])
Static method: Buffer.copyBytesFrom(view[, offset[, length]])
Static method: Buffer.from(array)
Static method: Buffer.from(arrayBuffer[, byteOffset[, length]])
Static method: Buffer.from(buffer)
Static method: Buffer.from(object[, offsetOrEncoding[, length]])
Static method: Buffer.from(string[, encoding])
Static method: Buffer.isBuffer(obj)
Static method: Buffer.isEncoding(encoding)
Class property: Buffer.poolSize
buf[index]
buf.buffer
buf.byteOffset
buf.compare(target[, targetStart[, targetEnd[, sourceStart[, sourceEnd]]]])
buf.copy(target[, targetStart[, sourceStart[, sourceEnd]]])
buf.entries()
buf.equals(otherBuffer)
buf.fill(value[, offset[, end]][, encoding])
buf.includes(value[, byteOffset][, encoding])
buf.indexOf(value[, byteOffset][, encoding])
buf.keys()
buf.lastIndexOf(value[, byteOffset][, encoding])
buf.length
buf.parent
buf.readBigInt64BE([offset])
buf.readBigInt64LE([offset])
buf.readBigUInt64BE([offset])
buf.readBigUInt64LE([offset])
buf.readDoubleBE([offset])
buf.readDoubleLE([offset])
buf.readFloatBE([offset])
buf.readFloatLE([offset])
buf.readInt8([offset])
buf.readInt16BE([offset])
buf.readInt16LE([offset])
buf.readInt32BE([offset])
buf.readInt32LE([offset])
buf.readIntBE(offset, byteLength)
buf.readIntLE(offset, byteLength)
buf.readUInt8([offset])
buf.readUInt16BE([offset])
buf.readUInt16LE([offset])
buf.readUInt32BE([offset])
buf.readUInt32LE([offset])
buf.readUIntBE(offset, byteLength)
buf.readUIntLE(offset, byteLength)
buf.subarray([start[, end]])
buf.slice([start[, end]])
buf.swap16()
buf.swap32()
buf.swap64()
buf.toJSON()
buf.toString([encoding[, start[, end]]])
buf.values()
buf.write(string[, offset[, length]][, encoding])
buf.writeBigInt64BE(value[, offset])
buf.writeBigInt64LE(value[, offset])
buf.writeBigUInt64BE(value[, offset])
buf.writeBigUInt64LE(value[, offset])
buf.writeDoubleBE(value[, offset])
buf.writeDoubleLE(value[, offset])
buf.writeFloatBE(value[, offset])
buf.writeFloatLE(value[, offset])
buf.writeInt8(value[, offset])
buf.writeInt16BE(value[, offset])
buf.writeInt16LE(value[, offset])
buf.writeInt32BE(value[, offset])
buf.writeInt32LE(value[, offset])
buf.writeIntBE(value, offset, byteLength)
buf.writeIntLE(value, offset, byteLength)
buf.writeUInt8(value[, offset])
buf.writeUInt16BE(value[, offset])
buf.writeUInt16LE(value[, offset])
buf.writeUInt32BE(value[, offset])
buf.writeUInt32LE(value[, offset])
buf.writeUIntBE(value, offset, byteLength)
buf.writeUIntLE(value, offset, byteLength)
new Buffer(array)
new Buffer(arrayBuffer[, byteOffset[, length]])
new Buffer(buffer)
new Buffer(size)
new Buffer(string[, encoding])


Class: File

new buffer.File(sources, fileName[, options])
file.name
file.lastModified


node:buffer module APIs

buffer.atob(data)
buffer.btoa(data)
buffer.isAscii(input)
buffer.isUtf8(input)
buffer.INSPECT_MAX_BYTES
buffer.kMaxLength
buffer.kStringMaxLength
buffer.resolveObjectURL(id)
buffer.transcode(source, fromEnc, toEnc)
Class: SlowBuffer

new SlowBuffer(size)


Buffer constants

buffer.constants.MAX_LENGTH
buffer.constants.MAX_STRING_LENGTH




Buffer.from(), Buffer.alloc(), and Buffer.allocUnsafe()

The --zero-fill-buffers command-line option
What makes Buffer.allocUnsafe() and Buffer.allocUnsafeSlow() "unsafe"?





    
  
            
    
      
        
        Index
      

      
About this documentation
Usage and example

      
        Index
      
    
  


Assertion testing
Asynchronous context tracking
Async hooks
Buffer
C++ addons
C/C++ addons with Node-API
C++ embedder API
Child processes
Cluster
Command-line options
Console
Crypto
Debugger
Deprecated APIs
Diagnostics Channel
DNS
Domain
Errors
Events
File system
Globals
HTTP
HTTP/2
HTTPS
Inspector
Internationalization
Modules: CommonJS modules
Modules: ECMAScript modules
Modules: node:module API
Modules: Packages
Modules: TypeScript
Net
OS
Path
Performance hooks
Permissions
Process
Punycode
Query strings
QUIC
Readline
REPL
Report
Single executable applications
SQLite
Stream
String decoder
Test runner
Timers
TLS/SSL
Trace events
TTY
UDP/datagram
URL
Utilities
V8
VM
WASI
Web Crypto API
Web Streams API
Worker threads
Zlib



Code repository and issue tracker

    
  
            
    
      
        
        Other versions
      
      23.x
22.x LTS
21.x
20.x LTS
19.x
18.x LTS
17.x
16.x
15.x
14.x
13.x
12.x
11.x
10.x
9.x
8.x
7.x
6.x
5.x
4.x
0.12.x
0.10.x
    
  
            
              
                
                Options
              
        
              
                
                  
                    View on single page
                  
                  
                    View as JSON
                  
                  Edit on GitHub    
                
              
            
          
        
        
      

      Table of contents
Buffer

Buffers and character encodings
Buffers and TypedArrays
Buffers and iteration
Class: Blob

new buffer.Blob([sources[, options]])
blob.arrayBuffer()

blob.bytes()


blob.size
blob.slice([start[, end[, type]]])
blob.stream()
blob.text()
blob.type
Blob objects and MessageChannel


Class: Buffer

Static method: Buffer.alloc(size[, fill[, encoding]])
Static method: Buffer.allocUnsafe(size)
Static method: Buffer.allocUnsafeSlow(size)
Static method: Buffer.byteLength(string[, encoding])
Static method: Buffer.compare(buf1, buf2)
Static method: Buffer.concat(list[, totalLength])
Static method: Buffer.copyBytesFrom(view[, offset[, length]])
Static method: Buffer.from(array)
Static method: Buffer.from(arrayBuffer[, byteOffset[, length]])
Static method: Buffer.from(buffer)
Static method: Buffer.from(object[, offsetOrEncoding[, length]])
Static method: Buffer.from(string[, encoding])
Static method: Buffer.isBuffer(obj)
Static method: Buffer.isEncoding(encoding)
Class property: Buffer.poolSize
buf[index]
buf.buffer
buf.byteOffset
buf.compare(target[, targetStart[, targetEnd[, sourceStart[, sourceEnd]]]])
buf.copy(target[, targetStart[, sourceStart[, sourceEnd]]])
buf.entries()
buf.equals(otherBuffer)
buf.fill(value[, offset[, end]][, encoding])
buf.includes(value[, byteOffset][, encoding])
buf.indexOf(value[, byteOffset][, encoding])
buf.keys()
buf.lastIndexOf(value[, byteOffset][, encoding])
buf.length
buf.parent
buf.readBigInt64BE([offset])
buf.readBigInt64LE([offset])
buf.readBigUInt64BE([offset])
buf.readBigUInt64LE([offset])
buf.readDoubleBE([offset])
buf.readDoubleLE([offset])
buf.readFloatBE([offset])
buf.readFloatLE([offset])
buf.readInt8([offset])
buf.readInt16BE([offset])
buf.readInt16LE([offset])
buf.readInt32BE([offset])
buf.readInt32LE([offset])
buf.readIntBE(offset, byteLength)
buf.readIntLE(offset, byteLength)
buf.readUInt8([offset])
buf.readUInt16BE([offset])
buf.readUInt16LE([offset])
buf.readUInt32BE([offset])
buf.readUInt32LE([offset])
buf.readUIntBE(offset, byteLength)
buf.readUIntLE(offset, byteLength)
buf.subarray([start[, end]])
buf.slice([start[, end]])
buf.swap16()
buf.swap32()
buf.swap64()
buf.toJSON()
buf.toString([encoding[, start[, end]]])
buf.values()
buf.write(string[, offset[, length]][, encoding])
buf.writeBigInt64BE(value[, offset])
buf.writeBigInt64LE(value[, offset])
buf.writeBigUInt64BE(value[, offset])
buf.writeBigUInt64LE(value[, offset])
buf.writeDoubleBE(value[, offset])
buf.writeDoubleLE(value[, offset])
buf.writeFloatBE(value[, offset])
buf.writeFloatLE(value[, offset])
buf.writeInt8(value[, offset])
buf.writeInt16BE(value[, offset])
buf.writeInt16LE(value[, offset])
buf.writeInt32BE(value[, offset])
buf.writeInt32LE(value[, offset])
buf.writeIntBE(value, offset, byteLength)
buf.writeIntLE(value, offset, byteLength)
buf.writeUInt8(value[, offset])
buf.writeUInt16BE(value[, offset])
buf.writeUInt16LE(value[, offset])
buf.writeUInt32BE(value[, offset])
buf.writeUInt32LE(value[, offset])
buf.writeUIntBE(value, offset, byteLength)
buf.writeUIntLE(value, offset, byteLength)
new Buffer(array)
new Buffer(arrayBuffer[, byteOffset[, length]])
new Buffer(buffer)
new Buffer(size)
new Buffer(string[, encoding])


Class: File

new buffer.File(sources, fileName[, options])
file.name
file.lastModified


node:buffer module APIs

buffer.atob(data)
buffer.btoa(data)
buffer.isAscii(input)
buffer.isUtf8(input)
buffer.INSPECT_MAX_BYTES
buffer.kMaxLength
buffer.kStringMaxLength
buffer.resolveObjectURL(id)
buffer.transcode(source, fromEnc, toEnc)
Class: SlowBuffer

new SlowBuffer(size)


Buffer constants

buffer.constants.MAX_LENGTH
buffer.constants.MAX_STRING_LENGTH




Buffer.from(), Buffer.alloc(), and Buffer.allocUnsafe()

The --zero-fill-buffers command-line option
What makes Buffer.allocUnsafe() and Buffer.allocUnsafeSlow() "unsafe"?






      
        Buffer#

Stability: 2 - Stable
Source Code: lib/buffer.js
Buffer objects are used to represent a fixed-length sequence of bytes. Many
Node.js APIs support Buffers.
The Buffer class is a subclass of JavaScript's <Uint8Array> class and
extends it with methods that cover additional use cases. Node.js APIs accept
plain <Uint8Array>s wherever Buffers are supported as well.
While the Buffer class is available within the global scope, it is still
recommended to explicitly reference it via an import or require statement.

import { Buffer } from 'node:buffer';

// Creates a zero-filled Buffer of length 10.
const buf1 = Buffer.alloc(10);

// Creates a Buffer of length 10,
// filled with bytes which all have the value `1`.
const buf2 = Buffer.alloc(10, 1);

// Creates an uninitialized buffer of length 10.
// This is faster than calling Buffer.alloc() but the returned
// Buffer instance might contain old data that needs to be
// overwritten using fill(), write(), or other functions that fill the Buffer's
// contents.
const buf3 = Buffer.allocUnsafe(10);

// Creates a Buffer containing the bytes [1, 2, 3].
const buf4 = Buffer.from([1, 2, 3]);

// Creates a Buffer containing the bytes [1, 1, 1, 1] – the entries
// are all truncated using `(value & 255)` to fit into the range 0–255.
const buf5 = Buffer.from([257, 257.5, -255, '1']);

// Creates a Buffer containing the UTF-8-encoded bytes for the string 'tést':
// [0x74, 0xc3, 0xa9, 0x73, 0x74] (in hexadecimal notation)
// [116, 195, 169, 115, 116] (in decimal notation)
const buf6 = Buffer.from('tést');

// Creates a Buffer containing the Latin-1 bytes [0x74, 0xe9, 0x73, 0x74].
const buf7 = Buffer.from('tést', 'latin1');const { Buffer } = require('node:buffer');

// Creates a zero-filled Buffer of length 10.
const buf1 = Buffer.alloc(10);

// Creates a Buffer of length 10,
// filled with bytes which all have the value `1`.
const buf2 = Buffer.alloc(10, 1);

// Creates an uninitialized buffer of length 10.
// This is faster than calling Buffer.alloc() but the returned
// Buffer instance might contain old data that needs to be
// overwritten using fill(), write(), or other functions that fill the Buffer's
// contents.
const buf3 = Buffer.allocUnsafe(10);

// Creates a Buffer containing the bytes [1, 2, 3].
const buf4 = Buffer.from([1, 2, 3]);

// Creates a Buffer containing the bytes [1, 1, 1, 1] – the entries
// are all truncated using `(value & 255)` to fit into the range 0–255.
const buf5 = Buffer.from([257, 257.5, -255, '1']);

// Creates a Buffer containing the UTF-8-encoded bytes for the string 'tést':
// [0x74, 0xc3, 0xa9, 0x73, 0x74] (in hexadecimal notation)
// [116, 195, 169, 115, 116] (in decimal notation)
const buf6 = Buffer.from('tést');

// Creates a Buffer containing the Latin-1 bytes [0x74, 0xe9, 0x73, 0x74].
const buf7 = Buffer.from('tést', 'latin1');copy
Buffers and character encodings#

History

VersionChanges
v15.7.0, v14.18.0
Introduced base64url encoding.
v6.4.0
Introduced latin1 as an alias for binary.
v5.0.0
Removed the deprecated raw and raws encodings.



When converting between Buffers and strings, a character encoding may be
specified. If no character encoding is specified, UTF-8 will be used as the
default.

import { Buffer } from 'node:buffer';

const buf = Buffer.from('hello world', 'utf8');

console.log(buf.toString('hex'));
// Prints: 68656c6c6f20776f726c64
console.log(buf.toString('base64'));
// Prints: aGVsbG8gd29ybGQ=

console.log(Buffer.from('fhqwhgads', 'utf8'));
// Prints: <Buffer 66 68 71 77 68 67 61 64 73>
console.log(Buffer.from('fhqwhgads', 'utf16le'));
// Prints: <Buffer 66 00 68 00 71 00 77 00 68 00 67 00 61 00 64 00 73 00>const { Buffer } = require('node:buffer');

const buf = Buffer.from('hello world', 'utf8');

console.log(buf.toString('hex'));
// Prints: 68656c6c6f20776f726c64
console.log(buf.toString('base64'));
// Prints: aGVsbG8gd29ybGQ=

console.log(Buffer.from('fhqwhgads', 'utf8'));
// Prints: <Buffer 66 68 71 77 68 67 61 64 73>
console.log(Buffer.from('fhqwhgads', 'utf16le'));
// Prints: <Buffer 66 00 68 00 71 00 77 00 68 00 67 00 61 00 64 00 73 00>copy
Node.js buffers accept all case variations of encoding strings that they
receive. For example, UTF-8 can be specified as 'utf8', 'UTF8', or 'uTf8'.
The character encodings currently supported by Node.js are the following:


'utf8' (alias: 'utf-8'): Multi-byte encoded Unicode characters. Many web
pages and other document formats use UTF-8. This is the default character
encoding. When decoding a Buffer into a string that does not exclusively
contain valid UTF-8 data, the Unicode replacement character U+FFFD � will be
used to represent those errors.


'utf16le' (alias: 'utf-16le'): Multi-byte encoded Unicode characters.
Unlike 'utf8', each character in the string will be encoded using either 2
or 4 bytes. Node.js only supports the little-endian variant of
UTF-16.


'latin1': Latin-1 stands for ISO-8859-1. This character encoding only
supports the Unicode characters from U+0000 to U+00FF. Each character is
encoded using a single byte. Characters that do not fit into that range are
truncated and will be mapped to characters in that range.


Converting a Buffer into a string using one of the above is referred to as
decoding, and converting a string into a Buffer is referred to as encoding.
Node.js also supports the following binary-to-text encodings. For
binary-to-text encodings, the naming convention is reversed: Converting a
Buffer into a string is typically referred to as encoding, and converting a
string into a Buffer as decoding.


'base64': Base64 encoding. When creating a Buffer from a string,
this encoding will also correctly accept "URL and Filename Safe Alphabet" as
specified in RFC 4648, Section 5. Whitespace characters such as spaces,
tabs, and new lines contained within the base64-encoded string are ignored.


'base64url': base64url encoding as specified in
RFC 4648, Section 5. When creating a Buffer from a string, this
encoding will also correctly accept regular base64-encoded strings. When
encoding a Buffer to a string, this encoding will omit padding.


'hex': Encode each byte as two hexadecimal characters. Data truncation
may occur when decoding strings that do not exclusively consist of an even
number of hexadecimal characters. See below for an example.


The following legacy character encodings are also supported:


'ascii': For 7-bit ASCII data only. When encoding a string into a
Buffer, this is equivalent to using 'latin1'. When decoding a Buffer
into a string, using this encoding will additionally unset the highest bit of
each byte before decoding as 'latin1'.
Generally, there should be no reason to use this encoding, as 'utf8'
(or, if the data is known to always be ASCII-only, 'latin1') will be a
better choice when encoding or decoding ASCII-only text. It is only provided
for legacy compatibility.


'binary': Alias for 'latin1'.
The name of this encoding can be very misleading, as all of the
encodings listed here convert between strings and binary data. For converting
between strings and Buffers, typically 'utf8' is the right choice.


'ucs2', 'ucs-2': Aliases of 'utf16le'. UCS-2 used to refer to a variant
of UTF-16 that did not support characters that had code points larger than
U+FFFF. In Node.js, these code points are always supported.



import { Buffer } from 'node:buffer';

Buffer.from('1ag123', 'hex');
// Prints <Buffer 1a>, data truncated when first non-hexadecimal value
// ('g') encountered.

Buffer.from('1a7', 'hex');
// Prints <Buffer 1a>, data truncated when data ends in single digit ('7').

Buffer.from('1634', 'hex');
// Prints <Buffer 16 34>, all data represented.const { Buffer } = require('node:buffer');

Buffer.from('1ag123', 'hex');
// Prints <Buffer 1a>, data truncated when first non-hexadecimal value
// ('g') encountered.

Buffer.from('1a7', 'hex');
// Prints <Buffer 1a>, data truncated when data ends in single digit ('7').

Buffer.from('1634', 'hex');
// Prints <Buffer 16 34>, all data represented.copy
Modern Web browsers follow the WHATWG Encoding Standard which aliases
both 'latin1' and 'ISO-8859-1' to 'win-1252'. This means that while doing
something like http.get(), if the returned charset is one of those listed in
the WHATWG specification it is possible that the server actually returned
'win-1252'-encoded data, and using 'latin1' encoding may incorrectly decode
the characters.
Buffers and TypedArrays#

History

VersionChanges
v3.0.0
The Buffers class now inherits from Uint8Array.



Buffer instances are also JavaScript <Uint8Array> and <TypedArray>
instances. All <TypedArray> methods are available on Buffers. There are,
however, subtle incompatibilities between the Buffer API and the
<TypedArray> API.
In particular:

While TypedArray.prototype.slice() creates a copy of part of the TypedArray,
Buffer.prototype.slice() creates a view over the existing Buffer
without copying. This behavior can be surprising, and only exists for legacy
compatibility. TypedArray.prototype.subarray() can be used to achieve
the behavior of Buffer.prototype.slice() on both Buffers
and other TypedArrays and should be preferred.
buf.toString() is incompatible with its TypedArray equivalent.
A number of methods, e.g. buf.indexOf(), support additional arguments.

There are two ways to create new <TypedArray> instances from a Buffer:

Passing a Buffer to a <TypedArray> constructor will copy the Buffers
contents, interpreted as an array of integers, and not as a byte sequence
of the target type.


import { Buffer } from 'node:buffer';

const buf = Buffer.from([1, 2, 3, 4]);
const uint32array = new Uint32Array(buf);

console.log(uint32array);

// Prints: Uint32Array(4) [ 1, 2, 3, 4 ]const { Buffer } = require('node:buffer');

const buf = Buffer.from([1, 2, 3, 4]);
const uint32array = new Uint32Array(buf);

console.log(uint32array);

// Prints: Uint32Array(4) [ 1, 2, 3, 4 ]copy

Passing the Buffers underlying <ArrayBuffer> will create a
<TypedArray> that shares its memory with the Buffer.


import { Buffer } from 'node:buffer';

const buf = Buffer.from('hello', 'utf16le');
const uint16array = new Uint16Array(
  buf.buffer,
  buf.byteOffset,
  buf.length / Uint16Array.BYTES_PER_ELEMENT);

console.log(uint16array);

// Prints: Uint16Array(5) [ 104, 101, 108, 108, 111 ]const { Buffer } = require('node:buffer');

const buf = Buffer.from('hello', 'utf16le');
const uint16array = new Uint16Array(
  buf.buffer,
  buf.byteOffset,
  buf.length / Uint16Array.BYTES_PER_ELEMENT);

console.log(uint16array);

// Prints: Uint16Array(5) [ 104, 101, 108, 108, 111 ]copy
It is possible to create a new Buffer that shares the same allocated
memory as a <TypedArray> instance by using the TypedArray object's
.buffer property in the same way. Buffer.from()
behaves like new Uint8Array() in this context.

import { Buffer } from 'node:buffer';

const arr = new Uint16Array(2);

arr[0] = 5000;
arr[1] = 4000;

// Copies the contents of `arr`.
const buf1 = Buffer.from(arr);

// Shares memory with `arr`.
const buf2 = Buffer.from(arr.buffer);

console.log(buf1);
// Prints: <Buffer 88 a0>
console.log(buf2);
// Prints: <Buffer 88 13 a0 0f>

arr[1] = 6000;

console.log(buf1);
// Prints: <Buffer 88 a0>
console.log(buf2);
// Prints: <Buffer 88 13 70 17>const { Buffer } = require('node:buffer');

const arr = new Uint16Array(2);

arr[0] = 5000;
arr[1] = 4000;

// Copies the contents of `arr`.
const buf1 = Buffer.from(arr);

// Shares memory with `arr`.
const buf2 = Buffer.from(arr.buffer);

console.log(buf1);
// Prints: <Buffer 88 a0>
console.log(buf2);
// Prints: <Buffer 88 13 a0 0f>

arr[1] = 6000;

console.log(buf1);
// Prints: <Buffer 88 a0>
console.log(buf2);
// Prints: <Buffer 88 13 70 17>copy
When creating a Buffer using a <TypedArray>'s .buffer, it is
possible to use only a portion of the underlying <ArrayBuffer> by passing in
byteOffset and length parameters.

import { Buffer } from 'node:buffer';

const arr = new Uint16Array(20);
const buf = Buffer.from(arr.buffer, 0, 16);

console.log(buf.length);
// Prints: 16const { Buffer } = require('node:buffer');

const arr = new Uint16Array(20);
const buf = Buffer.from(arr.buffer, 0, 16);

console.log(buf.length);
// Prints: 16copy
The Buffer.from() and TypedArray.from() have different signatures and
implementations. Specifically, the <TypedArray> variants accept a second
argument that is a mapping function that is invoked on every element of the
typed array:

TypedArray.from(source[, mapFn[, thisArg]])

The Buffer.from() method, however, does not support the use of a mapping
function:

Buffer.from(array)
Buffer.from(buffer)
Buffer.from(arrayBuffer[, byteOffset[, length]])
Buffer.from(string[, encoding])

Buffers and iteration#
Buffer instances can be iterated over using for..of syntax:

import { Buffer } from 'node:buffer';

const buf = Buffer.from([1, 2, 3]);

for (const b of buf) {
  console.log(b);
}
// Prints:
//   1
//   2
//   3const { Buffer } = require('node:buffer');

const buf = Buffer.from([1, 2, 3]);

for (const b of buf) {
  console.log(b);
}
// Prints:
//   1
//   2
//   3copy
Additionally, the buf.values(), buf.keys(), and
buf.entries() methods can be used to create iterators.
Class: Blob#

History

VersionChanges
v18.0.0, v16.17.0
No longer experimental.
v15.7.0, v14.18.0
Added in: v15.7.0, v14.18.0



A Blob encapsulates immutable, raw data that can be safely shared across
multiple worker threads.

new buffer.Blob([sources[, options]])#

History

VersionChanges
v16.7.0
Added the standard endings option to replace line-endings, and removed the non-standard encoding option.
v15.7.0, v14.18.0
Added in: v15.7.0, v14.18.0




sources <string[]> | <ArrayBuffer[]> | <TypedArray[]> | <DataView[]> | <Blob[]> An
array of string, <ArrayBuffer>, <TypedArray>, <DataView>, or <Blob> objects,
or any mix of such objects, that will be stored within the Blob.
options <Object>

endings <string> One of either 'transparent' or 'native'. When set
to 'native', line endings in string source parts will be converted to
the platform native line-ending as specified by require('node:os').EOL.
type <string> The Blob content-type. The intent is for type to convey
the MIME media type of the data, however no validation of the type format
is performed.



Creates a new Blob object containing a concatenation of the given sources.
<ArrayBuffer>, <TypedArray>, <DataView>, and <Buffer> sources are copied into
the 'Blob' and can therefore be safely modified after the 'Blob' is created.
String sources are encoded as UTF-8 byte sequences and copied into the Blob.
Unmatched surrogate pairs within each string part will be replaced by Unicode
U+FFFD replacement characters.

blob.arrayBuffer()#

Added in: v15.7.0, v14.18.0


Returns: <Promise>

Returns a promise that fulfills with an <ArrayBuffer> containing a copy of
the Blob data.

blob.bytes()#

Added in: v22.3.0, v20.16.0

The blob.bytes() method returns the byte of the Blob object as a Promise<Uint8Array>.
const blob = new Blob(['hello']);
blob.bytes().then((bytes) => {
  console.log(bytes); // Outputs: Uint8Array(5) [ 104, 101, 108, 108, 111 ]
}); copy

blob.size#

Added in: v15.7.0, v14.18.0

The total size of the Blob in bytes.

blob.slice([start[, end[, type]]])#

Added in: v15.7.0, v14.18.0


start <number> The starting index.
end <number> The ending index.
type <string> The content-type for the new Blob

Creates and returns a new Blob containing a subset of this Blob objects
data. The original Blob is not altered.

blob.stream()#

Added in: v16.7.0


Returns: <ReadableStream>

Returns a new ReadableStream that allows the content of the Blob to be read.

blob.text()#

Added in: v15.7.0, v14.18.0


Returns: <Promise>

Returns a promise that fulfills with the contents of the Blob decoded as a
UTF-8 string.

blob.type#

Added in: v15.7.0, v14.18.0


Type: <string>

The content-type of the Blob.

Blob objects and MessageChannel#
Once a <Blob> object is created, it can be sent via MessagePort to multiple
destinations without transferring or immediately copying the data. The data
contained by the Blob is copied only when the arrayBuffer() or text()
methods are called.

import { Blob } from 'node:buffer';
import { setTimeout as delay } from 'node:timers/promises';

const blob = new Blob(['hello there']);

const mc1 = new MessageChannel();
const mc2 = new MessageChannel();

mc1.port1.onmessage = async ({ data }) => {
  console.log(await data.arrayBuffer());
  mc1.port1.close();
};

mc2.port1.onmessage = async ({ data }) => {
  await delay(1000);
  console.log(await data.arrayBuffer());
  mc2.port1.close();
};

mc1.port2.postMessage(blob);
mc2.port2.postMessage(blob);

// The Blob is still usable after posting.
blob.text().then(console.log);const { Blob } = require('node:buffer');
const { setTimeout: delay } = require('node:timers/promises');

const blob = new Blob(['hello there']);

const mc1 = new MessageChannel();
const mc2 = new MessageChannel();

mc1.port1.onmessage = async ({ data }) => {
  console.log(await data.arrayBuffer());
  mc1.port1.close();
};

mc2.port1.onmessage = async ({ data }) => {
  await delay(1000);
  console.log(await data.arrayBuffer());
  mc2.port1.close();
};

mc1.port2.postMessage(blob);
mc2.port2.postMessage(blob);

// The Blob is still usable after posting.
blob.text().then(console.log);copy

Class: Buffer#
The Buffer class is a global type for dealing with binary data directly.
It can be constructed in a variety of ways.

Static method: Buffer.alloc(size[, fill[, encoding]])#

History

VersionChanges
v20.0.0
Throw ERR_INVALID_ARG_TYPE or ERR_OUT_OF_RANGE instead of ERR_INVALID_ARG_VALUE for invalid input arguments.
v15.0.0
Throw ERR_INVALID_ARG_VALUE instead of ERR_INVALID_OPT_VALUE for invalid input arguments.
v10.0.0
Attempting to fill a non-zero length buffer with a zero length buffer triggers a thrown exception.
v10.0.0
Specifying an invalid string for fill triggers a thrown exception.
v8.9.3
Specifying an invalid string for fill now results in a zero-filled buffer.
v5.10.0
Added in: v5.10.0




size <integer> The desired length of the new Buffer.
fill <string> | <Buffer> | <Uint8Array> | <integer> A value to pre-fill the new Buffer
with. Default: 0.
encoding <string> If fill is a string, this is its encoding.
Default: 'utf8'.
Returns: <Buffer>

Allocates a new Buffer of size bytes. If fill is undefined, the
Buffer will be zero-filled.

import { Buffer } from 'node:buffer';

const buf = Buffer.alloc(5);

console.log(buf);
// Prints: <Buffer 00 00 00 00 00>const { Buffer } = require('node:buffer');

const buf = Buffer.alloc(5);

console.log(buf);
// Prints: <Buffer 00 00 00 00 00>copy
If size is larger than
buffer.constants.MAX_LENGTH or smaller than 0, ERR_OUT_OF_RANGE
is thrown.
If fill is specified, the allocated Buffer will be initialized by calling
buf.fill(fill).

import { Buffer } from 'node:buffer';

const buf = Buffer.alloc(5, 'a');

console.log(buf);
// Prints: <Buffer 61 61 61 61 61>const { Buffer } = require('node:buffer');

const buf = Buffer.alloc(5, 'a');

console.log(buf);
// Prints: <Buffer 61 61 61 61 61>copy
If both fill and encoding are specified, the allocated Buffer will be
initialized by calling buf.fill(fill, encoding).

import { Buffer } from 'node:buffer';

const buf = Buffer.alloc(11, 'aGVsbG8gd29ybGQ=', 'base64');

console.log(buf);
// Prints: <Buffer 68 65 6c 6c 6f 20 77 6f 72 6c 64>const { Buffer } = require('node:buffer');

const buf = Buffer.alloc(11, 'aGVsbG8gd29ybGQ=', 'base64');

console.log(buf);
// Prints: <Buffer 68 65 6c 6c 6f 20 77 6f 72 6c 64>copy
Calling Buffer.alloc() can be measurably slower than the alternative
Buffer.allocUnsafe() but ensures that the newly created Buffer instance
contents will never contain sensitive data from previous allocations, including
data that might not have been allocated for Buffers.
A TypeError will be thrown if size is not a number.

Static method: Buffer.allocUnsafe(size)#

History

VersionChanges
v20.0.0
Throw ERR_INVALID_ARG_TYPE or ERR_OUT_OF_RANGE instead of ERR_INVALID_ARG_VALUE for invalid input arguments.
v15.0.0
Throw ERR_INVALID_ARG_VALUE instead of ERR_INVALID_OPT_VALUE for invalid input arguments.
v7.0.0
Passing a negative size will now throw an error.
v5.10.0
Added in: v5.10.0




size <integer> The desired length of the new Buffer.
Returns: <Buffer>

Allocates a new Buffer of size bytes. If size is larger than
buffer.constants.MAX_LENGTH or smaller than 0, ERR_OUT_OF_RANGE
is thrown.
The underlying memory for Buffer instances created in this way is not
initialized. The contents of the newly created Buffer are unknown and
may contain sensitive data. Use Buffer.alloc() instead to initialize
Buffer instances with zeroes.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(10);

console.log(buf);
// Prints (contents may vary): <Buffer a0 8b 28 3f 01 00 00 00 50 32>

buf.fill(0);

console.log(buf);
// Prints: <Buffer 00 00 00 00 00 00 00 00 00 00>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(10);

console.log(buf);
// Prints (contents may vary): <Buffer a0 8b 28 3f 01 00 00 00 50 32>

buf.fill(0);

console.log(buf);
// Prints: <Buffer 00 00 00 00 00 00 00 00 00 00>copy
A TypeError will be thrown if size is not a number.
The Buffer module pre-allocates an internal Buffer instance of
size Buffer.poolSize that is used as a pool for the fast allocation of new
Buffer instances created using Buffer.allocUnsafe(), Buffer.from(array),
Buffer.from(string), and Buffer.concat() only when size is less than
Buffer.poolSize >>> 1 (floor of Buffer.poolSize divided by two).
Use of this pre-allocated internal memory pool is a key difference between
calling Buffer.alloc(size, fill) vs. Buffer.allocUnsafe(size).fill(fill).
Specifically, Buffer.alloc(size, fill) will never use the internal Buffer
pool, while Buffer.allocUnsafe(size).fill(fill) will use the internal
Buffer pool if size is less than or equal to half Buffer.poolSize. The
difference is subtle but can be important when an application requires the
additional performance that Buffer.allocUnsafe() provides.

Static method: Buffer.allocUnsafeSlow(size)#

History

VersionChanges
v20.0.0
Throw ERR_INVALID_ARG_TYPE or ERR_OUT_OF_RANGE instead of ERR_INVALID_ARG_VALUE for invalid input arguments.
v15.0.0
Throw ERR_INVALID_ARG_VALUE instead of ERR_INVALID_OPT_VALUE for invalid input arguments.
v5.12.0
Added in: v5.12.0




size <integer> The desired length of the new Buffer.
Returns: <Buffer>

Allocates a new Buffer of size bytes. If size is larger than
buffer.constants.MAX_LENGTH or smaller than 0, ERR_OUT_OF_RANGE
is thrown. A zero-length Buffer is created if size is 0.
The underlying memory for Buffer instances created in this way is not
initialized. The contents of the newly created Buffer are unknown and
may contain sensitive data. Use buf.fill(0) to initialize
such Buffer instances with zeroes.
When using Buffer.allocUnsafe() to allocate new Buffer instances,
allocations less than Buffer.poolSize >>> 1 (4KiB when default poolSize is used) are sliced
from a single pre-allocated Buffer. This allows applications to avoid the
garbage collection overhead of creating many individually allocated Buffer
instances. This approach improves both performance and memory usage by
eliminating the need to track and clean up as many individual ArrayBuffer objects.
However, in the case where a developer may need to retain a small chunk of
memory from a pool for an indeterminate amount of time, it may be appropriate
to create an un-pooled Buffer instance using Buffer.allocUnsafeSlow() and
then copying out the relevant bits.

import { Buffer } from 'node:buffer';

// Need to keep around a few small chunks of memory.
const store = [];

socket.on('readable', () => {
  let data;
  while (null !== (data = readable.read())) {
    // Allocate for retained data.
    const sb = Buffer.allocUnsafeSlow(10);

    // Copy the data into the new allocation.
    data.copy(sb, 0, 0, 10);

    store.push(sb);
  }
});const { Buffer } = require('node:buffer');

// Need to keep around a few small chunks of memory.
const store = [];

socket.on('readable', () => {
  let data;
  while (null !== (data = readable.read())) {
    // Allocate for retained data.
    const sb = Buffer.allocUnsafeSlow(10);

    // Copy the data into the new allocation.
    data.copy(sb, 0, 0, 10);

    store.push(sb);
  }
});copy
A TypeError will be thrown if size is not a number.

Static method: Buffer.byteLength(string[, encoding])#

History

VersionChanges
v7.0.0
Passing invalid input will now throw an error.
v5.10.0
The string parameter can now be any TypedArray, DataView or ArrayBuffer.
v0.1.90
Added in: v0.1.90




string <string> | <Buffer> | <TypedArray> | <DataView> | <ArrayBuffer> | <SharedArrayBuffer> A
value to calculate the length of.
encoding <string> If string is a string, this is its encoding.
Default: 'utf8'.
Returns: <integer> The number of bytes contained within string.

Returns the byte length of a string when encoded using encoding.
This is not the same as String.prototype.length, which does not account
for the encoding that is used to convert the string into bytes.
For 'base64', 'base64url', and 'hex', this function assumes valid input.
For strings that contain non-base64/hex-encoded data (e.g. whitespace), the
return value might be greater than the length of a Buffer created from the
string.

import { Buffer } from 'node:buffer';

const str = '\u00bd + \u00bc = \u00be';

console.log(`${str}: ${str.length} characters, ` +
            `${Buffer.byteLength(str, 'utf8')} bytes`);
// Prints: ½ + ¼ = ¾: 9 characters, 12 bytesconst { Buffer } = require('node:buffer');

const str = '\u00bd + \u00bc = \u00be';

console.log(`${str}: ${str.length} characters, ` +
            `${Buffer.byteLength(str, 'utf8')} bytes`);
// Prints: ½ + ¼ = ¾: 9 characters, 12 bytescopy
When string is a <Buffer> | <DataView> | <TypedArray> | <ArrayBuffer> | <SharedArrayBuffer>,
the byte length as reported by .byteLength is returned.

Static method: Buffer.compare(buf1, buf2)#

History

VersionChanges
v8.0.0
The arguments can now be Uint8Arrays.
v0.11.13
Added in: v0.11.13




buf1 <Buffer> | <Uint8Array>
buf2 <Buffer> | <Uint8Array>
Returns: <integer> Either -1, 0, or 1, depending on the result of the
comparison. See buf.compare() for details.

Compares buf1 to buf2, typically for the purpose of sorting arrays of
Buffer instances. This is equivalent to calling
buf1.compare(buf2).

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from('1234');
const buf2 = Buffer.from('0123');
const arr = [buf1, buf2];

console.log(arr.sort(Buffer.compare));
// Prints: [ <Buffer 30 31 32 33>, <Buffer 31 32 33 34> ]
// (This result is equal to: [buf2, buf1].)const { Buffer } = require('node:buffer');

const buf1 = Buffer.from('1234');
const buf2 = Buffer.from('0123');
const arr = [buf1, buf2];

console.log(arr.sort(Buffer.compare));
// Prints: [ <Buffer 30 31 32 33>, <Buffer 31 32 33 34> ]
// (This result is equal to: [buf2, buf1].)copy

Static method: Buffer.concat(list[, totalLength])#

History

VersionChanges
v8.0.0
The elements of list can now be Uint8Arrays.
v0.7.11
Added in: v0.7.11




list <Buffer[]> | <Uint8Array[]> List of Buffer or <Uint8Array>
instances to concatenate.
totalLength <integer> Total length of the Buffer instances in list
when concatenated.
Returns: <Buffer>

Returns a new Buffer which is the result of concatenating all the Buffer
instances in the list together.
If the list has no items, or if the totalLength is 0, then a new zero-length
Buffer is returned.
If totalLength is not provided, it is calculated from the Buffer instances
in list by adding their lengths.
If totalLength is provided, it is coerced to an unsigned integer. If the
combined length of the Buffers in list exceeds totalLength, the result is
truncated to totalLength. If the combined length of the Buffers in list is
less than totalLength, the remaining space is filled with zeros.

import { Buffer } from 'node:buffer';

// Create a single `Buffer` from a list of three `Buffer` instances.

const buf1 = Buffer.alloc(10);
const buf2 = Buffer.alloc(14);
const buf3 = Buffer.alloc(18);
const totalLength = buf1.length + buf2.length + buf3.length;

console.log(totalLength);
// Prints: 42

const bufA = Buffer.concat([buf1, buf2, buf3], totalLength);

console.log(bufA);
// Prints: <Buffer 00 00 00 00 ...>
console.log(bufA.length);
// Prints: 42const { Buffer } = require('node:buffer');

// Create a single `Buffer` from a list of three `Buffer` instances.

const buf1 = Buffer.alloc(10);
const buf2 = Buffer.alloc(14);
const buf3 = Buffer.alloc(18);
const totalLength = buf1.length + buf2.length + buf3.length;

console.log(totalLength);
// Prints: 42

const bufA = Buffer.concat([buf1, buf2, buf3], totalLength);

console.log(bufA);
// Prints: <Buffer 00 00 00 00 ...>
console.log(bufA.length);
// Prints: 42copy
Buffer.concat() may also use the internal Buffer pool like
Buffer.allocUnsafe() does.

Static method: Buffer.copyBytesFrom(view[, offset[, length]])#

Added in: v19.8.0, v18.16.0


view <TypedArray> The <TypedArray> to copy.
offset <integer> The starting offset within view. Default: 0.
length <integer> The number of elements from view to copy.
Default: view.length - offset.
Returns: <Buffer>

Copies the underlying memory of view into a new Buffer.
const u16 = new Uint16Array([0, 0xffff]);
const buf = Buffer.copyBytesFrom(u16, 1, 1);
u16[1] = 0;
console.log(buf.length); // 2
console.log(buf[0]); // 255
console.log(buf[1]); // 255 copy

Static method: Buffer.from(array)#

Added in: v5.10.0


array <integer[]>
Returns: <Buffer>

Allocates a new Buffer using an array of bytes in the range 0 – 255.
Array entries outside that range will be truncated to fit into it.

import { Buffer } from 'node:buffer';

// Creates a new Buffer containing the UTF-8 bytes of the string 'buffer'.
const buf = Buffer.from([0x62, 0x75, 0x66, 0x66, 0x65, 0x72]);const { Buffer } = require('node:buffer');

// Creates a new Buffer containing the UTF-8 bytes of the string 'buffer'.
const buf = Buffer.from([0x62, 0x75, 0x66, 0x66, 0x65, 0x72]);copy
If array is an Array-like object (that is, one with a length property of
type number), it is treated as if it is an array, unless it is a Buffer or
a Uint8Array. This means all other TypedArray variants get treated as an
Array. To create a Buffer from the bytes backing a TypedArray, use
Buffer.copyBytesFrom().
A TypeError will be thrown if array is not an Array or another type
appropriate for Buffer.from() variants.
Buffer.from(array) and Buffer.from(string) may also use the internal
Buffer pool like Buffer.allocUnsafe() does.

Static method: Buffer.from(arrayBuffer[, byteOffset[, length]])#

Added in: v5.10.0


arrayBuffer <ArrayBuffer> | <SharedArrayBuffer> An <ArrayBuffer>,
<SharedArrayBuffer>, for example the .buffer property of a
<TypedArray>.
byteOffset <integer> Index of first byte to expose. Default: 0.
length <integer> Number of bytes to expose.
Default: arrayBuffer.byteLength - byteOffset.
Returns: <Buffer>

This creates a view of the <ArrayBuffer> without copying the underlying
memory. For example, when passed a reference to the .buffer property of a
<TypedArray> instance, the newly created Buffer will share the same
allocated memory as the <TypedArray>'s underlying ArrayBuffer.

import { Buffer } from 'node:buffer';

const arr = new Uint16Array(2);

arr[0] = 5000;
arr[1] = 4000;

// Shares memory with `arr`.
const buf = Buffer.from(arr.buffer);

console.log(buf);
// Prints: <Buffer 88 13 a0 0f>

// Changing the original Uint16Array changes the Buffer also.
arr[1] = 6000;

console.log(buf);
// Prints: <Buffer 88 13 70 17>const { Buffer } = require('node:buffer');

const arr = new Uint16Array(2);

arr[0] = 5000;
arr[1] = 4000;

// Shares memory with `arr`.
const buf = Buffer.from(arr.buffer);

console.log(buf);
// Prints: <Buffer 88 13 a0 0f>

// Changing the original Uint16Array changes the Buffer also.
arr[1] = 6000;

console.log(buf);
// Prints: <Buffer 88 13 70 17>copy
The optional byteOffset and length arguments specify a memory range within
the arrayBuffer that will be shared by the Buffer.

import { Buffer } from 'node:buffer';

const ab = new ArrayBuffer(10);
const buf = Buffer.from(ab, 0, 2);

console.log(buf.length);
// Prints: 2const { Buffer } = require('node:buffer');

const ab = new ArrayBuffer(10);
const buf = Buffer.from(ab, 0, 2);

console.log(buf.length);
// Prints: 2copy
A TypeError will be thrown if arrayBuffer is not an <ArrayBuffer> or a
<SharedArrayBuffer> or another type appropriate for Buffer.from()
variants.
It is important to remember that a backing ArrayBuffer can cover a range
of memory that extends beyond the bounds of a TypedArray view. A new
Buffer created using the buffer property of a TypedArray may extend
beyond the range of the TypedArray:

import { Buffer } from 'node:buffer';

const arrA = Uint8Array.from([0x63, 0x64, 0x65, 0x66]); // 4 elements
const arrB = new Uint8Array(arrA.buffer, 1, 2); // 2 elements
console.log(arrA.buffer === arrB.buffer); // true

const buf = Buffer.from(arrB.buffer);
console.log(buf);
// Prints: <Buffer 63 64 65 66>const { Buffer } = require('node:buffer');

const arrA = Uint8Array.from([0x63, 0x64, 0x65, 0x66]); // 4 elements
const arrB = new Uint8Array(arrA.buffer, 1, 2); // 2 elements
console.log(arrA.buffer === arrB.buffer); // true

const buf = Buffer.from(arrB.buffer);
console.log(buf);
// Prints: <Buffer 63 64 65 66>copy

Static method: Buffer.from(buffer)#

Added in: v5.10.0


buffer <Buffer> | <Uint8Array> An existing Buffer or <Uint8Array> from
which to copy data.
Returns: <Buffer>

Copies the passed buffer data onto a new Buffer instance.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from('buffer');
const buf2 = Buffer.from(buf1);

buf1[0] = 0x61;

console.log(buf1.toString());
// Prints: auffer
console.log(buf2.toString());
// Prints: bufferconst { Buffer } = require('node:buffer');

const buf1 = Buffer.from('buffer');
const buf2 = Buffer.from(buf1);

buf1[0] = 0x61;

console.log(buf1.toString());
// Prints: auffer
console.log(buf2.toString());
// Prints: buffercopy
A TypeError will be thrown if buffer is not a Buffer or another type
appropriate for Buffer.from() variants.

Static method: Buffer.from(object[, offsetOrEncoding[, length]])#

Added in: v8.2.0


object <Object> An object supporting Symbol.toPrimitive or valueOf().
offsetOrEncoding <integer> | <string> A byte-offset or encoding.
length <integer> A length.
Returns: <Buffer>

For objects whose valueOf() function returns a value not strictly equal to
object, returns Buffer.from(object.valueOf(), offsetOrEncoding, length).

import { Buffer } from 'node:buffer';

const buf = Buffer.from(new String('this is a test'));
// Prints: <Buffer 74 68 69 73 20 69 73 20 61 20 74 65 73 74>const { Buffer } = require('node:buffer');

const buf = Buffer.from(new String('this is a test'));
// Prints: <Buffer 74 68 69 73 20 69 73 20 61 20 74 65 73 74>copy
For objects that support Symbol.toPrimitive, returns
Buffer.from(object[Symbol.toPrimitive]('string'), offsetOrEncoding).

import { Buffer } from 'node:buffer';

class Foo {
  [Symbol.toPrimitive]() {
    return 'this is a test';
  }
}

const buf = Buffer.from(new Foo(), 'utf8');
// Prints: <Buffer 74 68 69 73 20 69 73 20 61 20 74 65 73 74>const { Buffer } = require('node:buffer');

class Foo {
  [Symbol.toPrimitive]() {
    return 'this is a test';
  }
}

const buf = Buffer.from(new Foo(), 'utf8');
// Prints: <Buffer 74 68 69 73 20 69 73 20 61 20 74 65 73 74>copy
A TypeError will be thrown if object does not have the mentioned methods or
is not of another type appropriate for Buffer.from() variants.

Static method: Buffer.from(string[, encoding])#

Added in: v5.10.0


string <string> A string to encode.
encoding <string> The encoding of string. Default: 'utf8'.
Returns: <Buffer>

Creates a new Buffer containing string. The encoding parameter identifies
the character encoding to be used when converting string into bytes.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from('this is a tést');
const buf2 = Buffer.from('7468697320697320612074c3a97374', 'hex');

console.log(buf1.toString());
// Prints: this is a tést
console.log(buf2.toString());
// Prints: this is a tést
console.log(buf1.toString('latin1'));
// Prints: this is a tÃ©stconst { Buffer } = require('node:buffer');

const buf1 = Buffer.from('this is a tést');
const buf2 = Buffer.from('7468697320697320612074c3a97374', 'hex');

console.log(buf1.toString());
// Prints: this is a tést
console.log(buf2.toString());
// Prints: this is a tést
console.log(buf1.toString('latin1'));
// Prints: this is a tÃ©stcopy
A TypeError will be thrown if string is not a string or another type
appropriate for Buffer.from() variants.
Buffer.from(string) may also use the internal Buffer pool like
Buffer.allocUnsafe() does.

Static method: Buffer.isBuffer(obj)#

Added in: v0.1.101


obj <Object>
Returns: <boolean>

Returns true if obj is a Buffer, false otherwise.

import { Buffer } from 'node:buffer';

Buffer.isBuffer(Buffer.alloc(10)); // true
Buffer.isBuffer(Buffer.from('foo')); // true
Buffer.isBuffer('a string'); // false
Buffer.isBuffer([]); // false
Buffer.isBuffer(new Uint8Array(1024)); // falseconst { Buffer } = require('node:buffer');

Buffer.isBuffer(Buffer.alloc(10)); // true
Buffer.isBuffer(Buffer.from('foo')); // true
Buffer.isBuffer('a string'); // false
Buffer.isBuffer([]); // false
Buffer.isBuffer(new Uint8Array(1024)); // falsecopy

Static method: Buffer.isEncoding(encoding)#

Added in: v0.9.1


encoding <string> A character encoding name to check.
Returns: <boolean>

Returns true if encoding is the name of a supported character encoding,
or false otherwise.

import { Buffer } from 'node:buffer';

console.log(Buffer.isEncoding('utf8'));
// Prints: true

console.log(Buffer.isEncoding('hex'));
// Prints: true

console.log(Buffer.isEncoding('utf/8'));
// Prints: false

console.log(Buffer.isEncoding(''));
// Prints: falseconst { Buffer } = require('node:buffer');

console.log(Buffer.isEncoding('utf8'));
// Prints: true

console.log(Buffer.isEncoding('hex'));
// Prints: true

console.log(Buffer.isEncoding('utf/8'));
// Prints: false

console.log(Buffer.isEncoding(''));
// Prints: falsecopy

Class property: Buffer.poolSize#

Added in: v0.11.3


<integer> Default: 8192

This is the size (in bytes) of pre-allocated internal Buffer instances used
for pooling. This value may be modified.

buf[index]#

index <integer>

The index operator [index] can be used to get and set the octet at position
index in buf. The values refer to individual bytes, so the legal value
range is between 0x00 and 0xFF (hex) or 0 and 255 (decimal).
This operator is inherited from Uint8Array, so its behavior on out-of-bounds
access is the same as Uint8Array. In other words, buf[index] returns
undefined when index is negative or greater or equal to buf.length, and
buf[index] = value does not modify the buffer if index is negative or
>= buf.length.

import { Buffer } from 'node:buffer';

// Copy an ASCII string into a `Buffer` one byte at a time.
// (This only works for ASCII-only strings. In general, one should use
// `Buffer.from()` to perform this conversion.)

const str = 'Node.js';
const buf = Buffer.allocUnsafe(str.length);

for (let i = 0; i < str.length; i++) {
  buf[i] = str.charCodeAt(i);
}

console.log(buf.toString('utf8'));
// Prints: Node.jsconst { Buffer } = require('node:buffer');

// Copy an ASCII string into a `Buffer` one byte at a time.
// (This only works for ASCII-only strings. In general, one should use
// `Buffer.from()` to perform this conversion.)

const str = 'Node.js';
const buf = Buffer.allocUnsafe(str.length);

for (let i = 0; i < str.length; i++) {
  buf[i] = str.charCodeAt(i);
}

console.log(buf.toString('utf8'));
// Prints: Node.jscopy

buf.buffer#

<ArrayBuffer> The underlying ArrayBuffer object based on which this Buffer
object is created.

This ArrayBuffer is not guaranteed to correspond exactly to the original
Buffer. See the notes on buf.byteOffset for details.

import { Buffer } from 'node:buffer';

const arrayBuffer = new ArrayBuffer(16);
const buffer = Buffer.from(arrayBuffer);

console.log(buffer.buffer === arrayBuffer);
// Prints: trueconst { Buffer } = require('node:buffer');

const arrayBuffer = new ArrayBuffer(16);
const buffer = Buffer.from(arrayBuffer);

console.log(buffer.buffer === arrayBuffer);
// Prints: truecopy

buf.byteOffset#

<integer> The byteOffset of the Buffers underlying ArrayBuffer object.

When setting byteOffset in Buffer.from(ArrayBuffer, byteOffset, length),
or sometimes when allocating a Buffer smaller than Buffer.poolSize, the
buffer does not start from a zero offset on the underlying ArrayBuffer.
This can cause problems when accessing the underlying ArrayBuffer directly
using buf.buffer, as other parts of the ArrayBuffer may be unrelated
to the Buffer object itself.
A common issue when creating a TypedArray object that shares its memory with
a Buffer is that in this case one needs to specify the byteOffset correctly:

import { Buffer } from 'node:buffer';

// Create a buffer smaller than `Buffer.poolSize`.
const nodeBuffer = Buffer.from([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]);

// When casting the Node.js Buffer to an Int8Array, use the byteOffset
// to refer only to the part of `nodeBuffer.buffer` that contains the memory
// for `nodeBuffer`.
new Int8Array(nodeBuffer.buffer, nodeBuffer.byteOffset, nodeBuffer.length);const { Buffer } = require('node:buffer');

// Create a buffer smaller than `Buffer.poolSize`.
const nodeBuffer = Buffer.from([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]);

// When casting the Node.js Buffer to an Int8Array, use the byteOffset
// to refer only to the part of `nodeBuffer.buffer` that contains the memory
// for `nodeBuffer`.
new Int8Array(nodeBuffer.buffer, nodeBuffer.byteOffset, nodeBuffer.length);copy

buf.compare(target[, targetStart[, targetEnd[, sourceStart[, sourceEnd]]]])#

History

VersionChanges
v8.0.0
The target parameter can now be a Uint8Array.
v5.11.0
Additional parameters for specifying offsets are supported now.
v0.11.13
Added in: v0.11.13




target <Buffer> | <Uint8Array> A Buffer or <Uint8Array> with which to
compare buf.
targetStart <integer> The offset within target at which to begin
comparison. Default: 0.
targetEnd <integer> The offset within target at which to end comparison
(not inclusive). Default: target.length.
sourceStart <integer> The offset within buf at which to begin comparison.
Default: 0.
sourceEnd <integer> The offset within buf at which to end comparison
(not inclusive). Default: buf.length.
Returns: <integer>

Compares buf with target and returns a number indicating whether buf
comes before, after, or is the same as target in sort order.
Comparison is based on the actual sequence of bytes in each Buffer.

0 is returned if target is the same as buf
1 is returned if target should come before buf when sorted.
-1 is returned if target should come after buf when sorted.


import { Buffer } from 'node:buffer';

const buf1 = Buffer.from('ABC');
const buf2 = Buffer.from('BCD');
const buf3 = Buffer.from('ABCD');

console.log(buf1.compare(buf1));
// Prints: 0
console.log(buf1.compare(buf2));
// Prints: -1
console.log(buf1.compare(buf3));
// Prints: -1
console.log(buf2.compare(buf1));
// Prints: 1
console.log(buf2.compare(buf3));
// Prints: 1
console.log([buf1, buf2, buf3].sort(Buffer.compare));
// Prints: [ <Buffer 41 42 43>, <Buffer 41 42 43 44>, <Buffer 42 43 44> ]
// (This result is equal to: [buf1, buf3, buf2].)const { Buffer } = require('node:buffer');

const buf1 = Buffer.from('ABC');
const buf2 = Buffer.from('BCD');
const buf3 = Buffer.from('ABCD');

console.log(buf1.compare(buf1));
// Prints: 0
console.log(buf1.compare(buf2));
// Prints: -1
console.log(buf1.compare(buf3));
// Prints: -1
console.log(buf2.compare(buf1));
// Prints: 1
console.log(buf2.compare(buf3));
// Prints: 1
console.log([buf1, buf2, buf3].sort(Buffer.compare));
// Prints: [ <Buffer 41 42 43>, <Buffer 41 42 43 44>, <Buffer 42 43 44> ]
// (This result is equal to: [buf1, buf3, buf2].)copy
The optional targetStart, targetEnd, sourceStart, and sourceEnd
arguments can be used to limit the comparison to specific ranges within target
and buf respectively.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from([1, 2, 3, 4, 5, 6, 7, 8, 9]);
const buf2 = Buffer.from([5, 6, 7, 8, 9, 1, 2, 3, 4]);

console.log(buf1.compare(buf2, 5, 9, 0, 4));
// Prints: 0
console.log(buf1.compare(buf2, 0, 6, 4));
// Prints: -1
console.log(buf1.compare(buf2, 5, 6, 5));
// Prints: 1const { Buffer } = require('node:buffer');

const buf1 = Buffer.from([1, 2, 3, 4, 5, 6, 7, 8, 9]);
const buf2 = Buffer.from([5, 6, 7, 8, 9, 1, 2, 3, 4]);

console.log(buf1.compare(buf2, 5, 9, 0, 4));
// Prints: 0
console.log(buf1.compare(buf2, 0, 6, 4));
// Prints: -1
console.log(buf1.compare(buf2, 5, 6, 5));
// Prints: 1copy
ERR_OUT_OF_RANGE is thrown if targetStart < 0, sourceStart < 0,
targetEnd > target.byteLength, or sourceEnd > source.byteLength.

buf.copy(target[, targetStart[, sourceStart[, sourceEnd]]])#

Added in: v0.1.90


target <Buffer> | <Uint8Array> A Buffer or <Uint8Array> to copy into.
targetStart <integer> The offset within target at which to begin
writing. Default: 0.
sourceStart <integer> The offset within buf from which to begin copying.
Default: 0.
sourceEnd <integer> The offset within buf at which to stop copying (not
inclusive). Default: buf.length.
Returns: <integer> The number of bytes copied.

Copies data from a region of buf to a region in target, even if the target
memory region overlaps with buf.
TypedArray.prototype.set() performs the same operation, and is available
for all TypedArrays, including Node.js Buffers, although it takes
different function arguments.

import { Buffer } from 'node:buffer';

// Create two `Buffer` instances.
const buf1 = Buffer.allocUnsafe(26);
const buf2 = Buffer.allocUnsafe(26).fill('!');

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf1[i] = i + 97;
}

// Copy `buf1` bytes 16 through 19 into `buf2` starting at byte 8 of `buf2`.
buf1.copy(buf2, 8, 16, 20);
// This is equivalent to:
// buf2.set(buf1.subarray(16, 20), 8);

console.log(buf2.toString('ascii', 0, 25));
// Prints: !!!!!!!!qrst!!!!!!!!!!!!!const { Buffer } = require('node:buffer');

// Create two `Buffer` instances.
const buf1 = Buffer.allocUnsafe(26);
const buf2 = Buffer.allocUnsafe(26).fill('!');

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf1[i] = i + 97;
}

// Copy `buf1` bytes 16 through 19 into `buf2` starting at byte 8 of `buf2`.
buf1.copy(buf2, 8, 16, 20);
// This is equivalent to:
// buf2.set(buf1.subarray(16, 20), 8);

console.log(buf2.toString('ascii', 0, 25));
// Prints: !!!!!!!!qrst!!!!!!!!!!!!!copy

import { Buffer } from 'node:buffer';

// Create a `Buffer` and copy data from one region to an overlapping region
// within the same `Buffer`.

const buf = Buffer.allocUnsafe(26);

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf[i] = i + 97;
}

buf.copy(buf, 0, 4, 10);

console.log(buf.toString());
// Prints: efghijghijklmnopqrstuvwxyzconst { Buffer } = require('node:buffer');

// Create a `Buffer` and copy data from one region to an overlapping region
// within the same `Buffer`.

const buf = Buffer.allocUnsafe(26);

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf[i] = i + 97;
}

buf.copy(buf, 0, 4, 10);

console.log(buf.toString());
// Prints: efghijghijklmnopqrstuvwxyzcopy

buf.entries()#

Added in: v1.1.0


Returns: <Iterator>

Creates and returns an iterator of [index, byte] pairs from the contents
of buf.

import { Buffer } from 'node:buffer';

// Log the entire contents of a `Buffer`.

const buf = Buffer.from('buffer');

for (const pair of buf.entries()) {
  console.log(pair);
}
// Prints:
//   [0, 98]
//   [1, 117]
//   [2, 102]
//   [3, 102]
//   [4, 101]
//   [5, 114]const { Buffer } = require('node:buffer');

// Log the entire contents of a `Buffer`.

const buf = Buffer.from('buffer');

for (const pair of buf.entries()) {
  console.log(pair);
}
// Prints:
//   [0, 98]
//   [1, 117]
//   [2, 102]
//   [3, 102]
//   [4, 101]
//   [5, 114]copy

buf.equals(otherBuffer)#

History

VersionChanges
v8.0.0
The arguments can now be Uint8Arrays.
v0.11.13
Added in: v0.11.13




otherBuffer <Buffer> | <Uint8Array> A Buffer or <Uint8Array> with which to
compare buf.
Returns: <boolean>

Returns true if both buf and otherBuffer have exactly the same bytes,
false otherwise. Equivalent to
buf.compare(otherBuffer) === 0.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from('ABC');
const buf2 = Buffer.from('414243', 'hex');
const buf3 = Buffer.from('ABCD');

console.log(buf1.equals(buf2));
// Prints: true
console.log(buf1.equals(buf3));
// Prints: falseconst { Buffer } = require('node:buffer');

const buf1 = Buffer.from('ABC');
const buf2 = Buffer.from('414243', 'hex');
const buf3 = Buffer.from('ABCD');

console.log(buf1.equals(buf2));
// Prints: true
console.log(buf1.equals(buf3));
// Prints: falsecopy

buf.fill(value[, offset[, end]][, encoding])#

History

VersionChanges
v11.0.0
Throws ERR_OUT_OF_RANGE instead of ERR_INDEX_OUT_OF_RANGE.
v10.0.0
Negative end values throw an ERR_INDEX_OUT_OF_RANGE error.
v10.0.0
Attempting to fill a non-zero length buffer with a zero length buffer triggers a thrown exception.
v10.0.0
Specifying an invalid string for value triggers a thrown exception.
v5.7.0
The encoding parameter is supported now.
v0.5.0
Added in: v0.5.0




value <string> | <Buffer> | <Uint8Array> | <integer> The value with which to fill buf.
Empty value (string, Uint8Array, Buffer) is coerced to 0.
offset <integer> Number of bytes to skip before starting to fill buf.
Default: 0.
end <integer> Where to stop filling buf (not inclusive). Default:
buf.length.
encoding <string> The encoding for value if value is a string.
Default: 'utf8'.
Returns: <Buffer> A reference to buf.

Fills buf with the specified value. If the offset and end are not given,
the entire buf will be filled:

import { Buffer } from 'node:buffer';

// Fill a `Buffer` with the ASCII character 'h'.

const b = Buffer.allocUnsafe(50).fill('h');

console.log(b.toString());
// Prints: hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh

// Fill a buffer with empty string
const c = Buffer.allocUnsafe(5).fill('');

console.log(c.fill(''));
// Prints: <Buffer 00 00 00 00 00>const { Buffer } = require('node:buffer');

// Fill a `Buffer` with the ASCII character 'h'.

const b = Buffer.allocUnsafe(50).fill('h');

console.log(b.toString());
// Prints: hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh

// Fill a buffer with empty string
const c = Buffer.allocUnsafe(5).fill('');

console.log(c.fill(''));
// Prints: <Buffer 00 00 00 00 00>copy
value is coerced to a uint32 value if it is not a string, Buffer, or
integer. If the resulting integer is greater than 255 (decimal), buf will be
filled with value & 255.
If the final write of a fill() operation falls on a multi-byte character,
then only the bytes of that character that fit into buf are written:

import { Buffer } from 'node:buffer';

// Fill a `Buffer` with character that takes up two bytes in UTF-8.

console.log(Buffer.allocUnsafe(5).fill('\u0222'));
// Prints: <Buffer c8 a2 c8 a2 c8>const { Buffer } = require('node:buffer');

// Fill a `Buffer` with character that takes up two bytes in UTF-8.

console.log(Buffer.allocUnsafe(5).fill('\u0222'));
// Prints: <Buffer c8 a2 c8 a2 c8>copy
If value contains invalid characters, it is truncated; if no valid
fill data remains, an exception is thrown:

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(5);

console.log(buf.fill('a'));
// Prints: <Buffer 61 61 61 61 61>
console.log(buf.fill('aazz', 'hex'));
// Prints: <Buffer aa aa aa aa aa>
console.log(buf.fill('zz', 'hex'));
// Throws an exception.const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(5);

console.log(buf.fill('a'));
// Prints: <Buffer 61 61 61 61 61>
console.log(buf.fill('aazz', 'hex'));
// Prints: <Buffer aa aa aa aa aa>
console.log(buf.fill('zz', 'hex'));
// Throws an exception.copy

buf.includes(value[, byteOffset][, encoding])#

Added in: v5.3.0


value <string> | <Buffer> | <Uint8Array> | <integer> What to search for.
byteOffset <integer> Where to begin searching in buf. If negative, then
offset is calculated from the end of buf. Default: 0.
encoding <string> If value is a string, this is its encoding.
Default: 'utf8'.
Returns: <boolean> true if value was found in buf, false otherwise.

Equivalent to buf.indexOf() !== -1.

import { Buffer } from 'node:buffer';

const buf = Buffer.from('this is a buffer');

console.log(buf.includes('this'));
// Prints: true
console.log(buf.includes('is'));
// Prints: true
console.log(buf.includes(Buffer.from('a buffer')));
// Prints: true
console.log(buf.includes(97));
// Prints: true (97 is the decimal ASCII value for 'a')
console.log(buf.includes(Buffer.from('a buffer example')));
// Prints: false
console.log(buf.includes(Buffer.from('a buffer example').slice(0, 8)));
// Prints: true
console.log(buf.includes('this', 4));
// Prints: falseconst { Buffer } = require('node:buffer');

const buf = Buffer.from('this is a buffer');

console.log(buf.includes('this'));
// Prints: true
console.log(buf.includes('is'));
// Prints: true
console.log(buf.includes(Buffer.from('a buffer')));
// Prints: true
console.log(buf.includes(97));
// Prints: true (97 is the decimal ASCII value for 'a')
console.log(buf.includes(Buffer.from('a buffer example')));
// Prints: false
console.log(buf.includes(Buffer.from('a buffer example').slice(0, 8)));
// Prints: true
console.log(buf.includes('this', 4));
// Prints: falsecopy

buf.indexOf(value[, byteOffset][, encoding])#

History

VersionChanges
v8.0.0
The value can now be a Uint8Array.
v5.7.0, v4.4.0
When encoding is being passed, the byteOffset parameter is no longer required.
v1.5.0
Added in: v1.5.0




value <string> | <Buffer> | <Uint8Array> | <integer> What to search for.
byteOffset <integer> Where to begin searching in buf. If negative, then
offset is calculated from the end of buf. Default: 0.
encoding <string> If value is a string, this is the encoding used to
determine the binary representation of the string that will be searched for in
buf. Default: 'utf8'.
Returns: <integer> The index of the first occurrence of value in buf, or
-1 if buf does not contain value.

If value is:

a string, value is interpreted according to the character encoding in
encoding.
a Buffer or <Uint8Array>, value will be used in its entirety.
To compare a partial Buffer, use buf.subarray.
a number, value will be interpreted as an unsigned 8-bit integer
value between 0 and 255.


import { Buffer } from 'node:buffer';

const buf = Buffer.from('this is a buffer');

console.log(buf.indexOf('this'));
// Prints: 0
console.log(buf.indexOf('is'));
// Prints: 2
console.log(buf.indexOf(Buffer.from('a buffer')));
// Prints: 8
console.log(buf.indexOf(97));
// Prints: 8 (97 is the decimal ASCII value for 'a')
console.log(buf.indexOf(Buffer.from('a buffer example')));
// Prints: -1
console.log(buf.indexOf(Buffer.from('a buffer example').slice(0, 8)));
// Prints: 8

const utf16Buffer = Buffer.from('\u039a\u0391\u03a3\u03a3\u0395', 'utf16le');

console.log(utf16Buffer.indexOf('\u03a3', 0, 'utf16le'));
// Prints: 4
console.log(utf16Buffer.indexOf('\u03a3', -4, 'utf16le'));
// Prints: 6const { Buffer } = require('node:buffer');

const buf = Buffer.from('this is a buffer');

console.log(buf.indexOf('this'));
// Prints: 0
console.log(buf.indexOf('is'));
// Prints: 2
console.log(buf.indexOf(Buffer.from('a buffer')));
// Prints: 8
console.log(buf.indexOf(97));
// Prints: 8 (97 is the decimal ASCII value for 'a')
console.log(buf.indexOf(Buffer.from('a buffer example')));
// Prints: -1
console.log(buf.indexOf(Buffer.from('a buffer example').slice(0, 8)));
// Prints: 8

const utf16Buffer = Buffer.from('\u039a\u0391\u03a3\u03a3\u0395', 'utf16le');

console.log(utf16Buffer.indexOf('\u03a3', 0, 'utf16le'));
// Prints: 4
console.log(utf16Buffer.indexOf('\u03a3', -4, 'utf16le'));
// Prints: 6copy
If value is not a string, number, or Buffer, this method will throw a
TypeError. If value is a number, it will be coerced to a valid byte value,
an integer between 0 and 255.
If byteOffset is not a number, it will be coerced to a number. If the result
of coercion is NaN or 0, then the entire buffer will be searched. This
behavior matches String.prototype.indexOf().

import { Buffer } from 'node:buffer';

const b = Buffer.from('abcdef');

// Passing a value that's a number, but not a valid byte.
// Prints: 2, equivalent to searching for 99 or 'c'.
console.log(b.indexOf(99.9));
console.log(b.indexOf(256 + 99));

// Passing a byteOffset that coerces to NaN or 0.
// Prints: 1, searching the whole buffer.
console.log(b.indexOf('b', undefined));
console.log(b.indexOf('b', {}));
console.log(b.indexOf('b', null));
console.log(b.indexOf('b', []));const { Buffer } = require('node:buffer');

const b = Buffer.from('abcdef');

// Passing a value that's a number, but not a valid byte.
// Prints: 2, equivalent to searching for 99 or 'c'.
console.log(b.indexOf(99.9));
console.log(b.indexOf(256 + 99));

// Passing a byteOffset that coerces to NaN or 0.
// Prints: 1, searching the whole buffer.
console.log(b.indexOf('b', undefined));
console.log(b.indexOf('b', {}));
console.log(b.indexOf('b', null));
console.log(b.indexOf('b', []));copy
If value is an empty string or empty Buffer and byteOffset is less
than buf.length, byteOffset will be returned. If value is empty and
byteOffset is at least buf.length, buf.length will be returned.

buf.keys()#

Added in: v1.1.0


Returns: <Iterator>

Creates and returns an iterator of buf keys (indexes).

import { Buffer } from 'node:buffer';

const buf = Buffer.from('buffer');

for (const key of buf.keys()) {
  console.log(key);
}
// Prints:
//   0
//   1
//   2
//   3
//   4
//   5const { Buffer } = require('node:buffer');

const buf = Buffer.from('buffer');

for (const key of buf.keys()) {
  console.log(key);
}
// Prints:
//   0
//   1
//   2
//   3
//   4
//   5copy

buf.lastIndexOf(value[, byteOffset][, encoding])#

History

VersionChanges
v8.0.0
The value can now be a Uint8Array.
v6.0.0
Added in: v6.0.0




value <string> | <Buffer> | <Uint8Array> | <integer> What to search for.
byteOffset <integer> Where to begin searching in buf. If negative, then
offset is calculated from the end of buf. Default:
buf.length - 1.
encoding <string> If value is a string, this is the encoding used to
determine the binary representation of the string that will be searched for in
buf. Default: 'utf8'.
Returns: <integer> The index of the last occurrence of value in buf, or
-1 if buf does not contain value.

Identical to buf.indexOf(), except the last occurrence of value is found
rather than the first occurrence.

import { Buffer } from 'node:buffer';

const buf = Buffer.from('this buffer is a buffer');

console.log(buf.lastIndexOf('this'));
// Prints: 0
console.log(buf.lastIndexOf('buffer'));
// Prints: 17
console.log(buf.lastIndexOf(Buffer.from('buffer')));
// Prints: 17
console.log(buf.lastIndexOf(97));
// Prints: 15 (97 is the decimal ASCII value for 'a')
console.log(buf.lastIndexOf(Buffer.from('yolo')));
// Prints: -1
console.log(buf.lastIndexOf('buffer', 5));
// Prints: 5
console.log(buf.lastIndexOf('buffer', 4));
// Prints: -1

const utf16Buffer = Buffer.from('\u039a\u0391\u03a3\u03a3\u0395', 'utf16le');

console.log(utf16Buffer.lastIndexOf('\u03a3', undefined, 'utf16le'));
// Prints: 6
console.log(utf16Buffer.lastIndexOf('\u03a3', -5, 'utf16le'));
// Prints: 4const { Buffer } = require('node:buffer');

const buf = Buffer.from('this buffer is a buffer');

console.log(buf.lastIndexOf('this'));
// Prints: 0
console.log(buf.lastIndexOf('buffer'));
// Prints: 17
console.log(buf.lastIndexOf(Buffer.from('buffer')));
// Prints: 17
console.log(buf.lastIndexOf(97));
// Prints: 15 (97 is the decimal ASCII value for 'a')
console.log(buf.lastIndexOf(Buffer.from('yolo')));
// Prints: -1
console.log(buf.lastIndexOf('buffer', 5));
// Prints: 5
console.log(buf.lastIndexOf('buffer', 4));
// Prints: -1

const utf16Buffer = Buffer.from('\u039a\u0391\u03a3\u03a3\u0395', 'utf16le');

console.log(utf16Buffer.lastIndexOf('\u03a3', undefined, 'utf16le'));
// Prints: 6
console.log(utf16Buffer.lastIndexOf('\u03a3', -5, 'utf16le'));
// Prints: 4copy
If value is not a string, number, or Buffer, this method will throw a
TypeError. If value is a number, it will be coerced to a valid byte value,
an integer between 0 and 255.
If byteOffset is not a number, it will be coerced to a number. Any arguments
that coerce to NaN, like {} or undefined, will search the whole buffer.
This behavior matches String.prototype.lastIndexOf().

import { Buffer } from 'node:buffer';

const b = Buffer.from('abcdef');

// Passing a value that's a number, but not a valid byte.
// Prints: 2, equivalent to searching for 99 or 'c'.
console.log(b.lastIndexOf(99.9));
console.log(b.lastIndexOf(256 + 99));

// Passing a byteOffset that coerces to NaN.
// Prints: 1, searching the whole buffer.
console.log(b.lastIndexOf('b', undefined));
console.log(b.lastIndexOf('b', {}));

// Passing a byteOffset that coerces to 0.
// Prints: -1, equivalent to passing 0.
console.log(b.lastIndexOf('b', null));
console.log(b.lastIndexOf('b', []));const { Buffer } = require('node:buffer');

const b = Buffer.from('abcdef');

// Passing a value that's a number, but not a valid byte.
// Prints: 2, equivalent to searching for 99 or 'c'.
console.log(b.lastIndexOf(99.9));
console.log(b.lastIndexOf(256 + 99));

// Passing a byteOffset that coerces to NaN.
// Prints: 1, searching the whole buffer.
console.log(b.lastIndexOf('b', undefined));
console.log(b.lastIndexOf('b', {}));

// Passing a byteOffset that coerces to 0.
// Prints: -1, equivalent to passing 0.
console.log(b.lastIndexOf('b', null));
console.log(b.lastIndexOf('b', []));copy
If value is an empty string or empty Buffer, byteOffset will be returned.

buf.length#

Added in: v0.1.90


<integer>

Returns the number of bytes in buf.

import { Buffer } from 'node:buffer';

// Create a `Buffer` and write a shorter string to it using UTF-8.

const buf = Buffer.alloc(1234);

console.log(buf.length);
// Prints: 1234

buf.write('some string', 0, 'utf8');

console.log(buf.length);
// Prints: 1234const { Buffer } = require('node:buffer');

// Create a `Buffer` and write a shorter string to it using UTF-8.

const buf = Buffer.alloc(1234);

console.log(buf.length);
// Prints: 1234

buf.write('some string', 0, 'utf8');

console.log(buf.length);
// Prints: 1234copy

buf.parent#

Deprecated since: v8.0.0

Stability: 0 - Deprecated: Use buf.buffer instead.
The buf.parent property is a deprecated alias for buf.buffer.

buf.readBigInt64BE([offset])#

Added in: v12.0.0, v10.20.0


offset <integer> Number of bytes to skip before starting to read. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <bigint>

Reads a signed, big-endian 64-bit integer from buf at the specified offset.
Integers read from a Buffer are interpreted as two's complement signed
values.

buf.readBigInt64LE([offset])#

Added in: v12.0.0, v10.20.0


offset <integer> Number of bytes to skip before starting to read. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <bigint>

Reads a signed, little-endian 64-bit integer from buf at the specified
offset.
Integers read from a Buffer are interpreted as two's complement signed
values.

buf.readBigUInt64BE([offset])#

History

VersionChanges
v14.10.0, v12.19.0
This function is also available as buf.readBigUint64BE().
v12.0.0, v10.20.0
Added in: v12.0.0, v10.20.0




offset <integer> Number of bytes to skip before starting to read. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <bigint>

Reads an unsigned, big-endian 64-bit integer from buf at the specified
offset.
This function is also available under the readBigUint64BE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x00, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff]);

console.log(buf.readBigUInt64BE(0));
// Prints: 4294967295nconst { Buffer } = require('node:buffer');

const buf = Buffer.from([0x00, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff]);

console.log(buf.readBigUInt64BE(0));
// Prints: 4294967295ncopy

buf.readBigUInt64LE([offset])#

History

VersionChanges
v14.10.0, v12.19.0
This function is also available as buf.readBigUint64LE().
v12.0.0, v10.20.0
Added in: v12.0.0, v10.20.0




offset <integer> Number of bytes to skip before starting to read. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <bigint>

Reads an unsigned, little-endian 64-bit integer from buf at the specified
offset.
This function is also available under the readBigUint64LE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x00, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff]);

console.log(buf.readBigUInt64LE(0));
// Prints: 18446744069414584320nconst { Buffer } = require('node:buffer');

const buf = Buffer.from([0x00, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff]);

console.log(buf.readBigUInt64LE(0));
// Prints: 18446744069414584320ncopy

buf.readDoubleBE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 8. Default: 0.
Returns: <number>

Reads a 64-bit, big-endian double from buf at the specified offset.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([1, 2, 3, 4, 5, 6, 7, 8]);

console.log(buf.readDoubleBE(0));
// Prints: 8.20788039913184e-304const { Buffer } = require('node:buffer');

const buf = Buffer.from([1, 2, 3, 4, 5, 6, 7, 8]);

console.log(buf.readDoubleBE(0));
// Prints: 8.20788039913184e-304copy

buf.readDoubleLE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 8. Default: 0.
Returns: <number>

Reads a 64-bit, little-endian double from buf at the specified offset.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([1, 2, 3, 4, 5, 6, 7, 8]);

console.log(buf.readDoubleLE(0));
// Prints: 5.447603722011605e-270
console.log(buf.readDoubleLE(1));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([1, 2, 3, 4, 5, 6, 7, 8]);

console.log(buf.readDoubleLE(0));
// Prints: 5.447603722011605e-270
console.log(buf.readDoubleLE(1));
// Throws ERR_OUT_OF_RANGE.copy

buf.readFloatBE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <number>

Reads a 32-bit, big-endian float from buf at the specified offset.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([1, 2, 3, 4]);

console.log(buf.readFloatBE(0));
// Prints: 2.387939260590663e-38const { Buffer } = require('node:buffer');

const buf = Buffer.from([1, 2, 3, 4]);

console.log(buf.readFloatBE(0));
// Prints: 2.387939260590663e-38copy

buf.readFloatLE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <number>

Reads a 32-bit, little-endian float from buf at the specified offset.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([1, 2, 3, 4]);

console.log(buf.readFloatLE(0));
// Prints: 1.539989614439558e-36
console.log(buf.readFloatLE(1));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([1, 2, 3, 4]);

console.log(buf.readFloatLE(0));
// Prints: 1.539989614439558e-36
console.log(buf.readFloatLE(1));
// Throws ERR_OUT_OF_RANGE.copy

buf.readInt8([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.0
Added in: v0.5.0




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 1. Default: 0.
Returns: <integer>

Reads a signed 8-bit integer from buf at the specified offset.
Integers read from a Buffer are interpreted as two's complement signed values.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([-1, 5]);

console.log(buf.readInt8(0));
// Prints: -1
console.log(buf.readInt8(1));
// Prints: 5
console.log(buf.readInt8(2));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([-1, 5]);

console.log(buf.readInt8(0));
// Prints: -1
console.log(buf.readInt8(1));
// Prints: 5
console.log(buf.readInt8(2));
// Throws ERR_OUT_OF_RANGE.copy

buf.readInt16BE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer>

Reads a signed, big-endian 16-bit integer from buf at the specified offset.
Integers read from a Buffer are interpreted as two's complement signed values.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0, 5]);

console.log(buf.readInt16BE(0));
// Prints: 5const { Buffer } = require('node:buffer');

const buf = Buffer.from([0, 5]);

console.log(buf.readInt16BE(0));
// Prints: 5copy

buf.readInt16LE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer>

Reads a signed, little-endian 16-bit integer from buf at the specified
offset.
Integers read from a Buffer are interpreted as two's complement signed values.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0, 5]);

console.log(buf.readInt16LE(0));
// Prints: 1280
console.log(buf.readInt16LE(1));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([0, 5]);

console.log(buf.readInt16LE(0));
// Prints: 1280
console.log(buf.readInt16LE(1));
// Throws ERR_OUT_OF_RANGE.copy

buf.readInt32BE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer>

Reads a signed, big-endian 32-bit integer from buf at the specified offset.
Integers read from a Buffer are interpreted as two's complement signed values.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0, 0, 0, 5]);

console.log(buf.readInt32BE(0));
// Prints: 5const { Buffer } = require('node:buffer');

const buf = Buffer.from([0, 0, 0, 5]);

console.log(buf.readInt32BE(0));
// Prints: 5copy

buf.readInt32LE([offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer>

Reads a signed, little-endian 32-bit integer from buf at the specified
offset.
Integers read from a Buffer are interpreted as two's complement signed values.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0, 0, 0, 5]);

console.log(buf.readInt32LE(0));
// Prints: 83886080
console.log(buf.readInt32LE(1));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([0, 0, 0, 5]);

console.log(buf.readInt32LE(0));
// Prints: 83886080
console.log(buf.readInt32LE(1));
// Throws ERR_OUT_OF_RANGE.copy

buf.readIntBE(offset, byteLength)#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to read. Must satisfy
0 < byteLength <= 6.
Returns: <integer>

Reads byteLength number of bytes from buf at the specified offset
and interprets the result as a big-endian, two's complement signed value
supporting up to 48 bits of accuracy.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readIntBE(0, 6).toString(16));
// Prints: 1234567890ab
console.log(buf.readIntBE(1, 6).toString(16));
// Throws ERR_OUT_OF_RANGE.
console.log(buf.readIntBE(1, 0).toString(16));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readIntBE(0, 6).toString(16));
// Prints: 1234567890ab
console.log(buf.readIntBE(1, 6).toString(16));
// Throws ERR_OUT_OF_RANGE.
console.log(buf.readIntBE(1, 0).toString(16));
// Throws ERR_OUT_OF_RANGE.copy

buf.readIntLE(offset, byteLength)#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to read. Must satisfy
0 < byteLength <= 6.
Returns: <integer>

Reads byteLength number of bytes from buf at the specified offset
and interprets the result as a little-endian, two's complement signed value
supporting up to 48 bits of accuracy.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readIntLE(0, 6).toString(16));
// Prints: -546f87a9cbeeconst { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readIntLE(0, 6).toString(16));
// Prints: -546f87a9cbeecopy

buf.readUInt8([offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.readUint8().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.0
Added in: v0.5.0




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 1. Default: 0.
Returns: <integer>

Reads an unsigned 8-bit integer from buf at the specified offset.
This function is also available under the readUint8 alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([1, -2]);

console.log(buf.readUInt8(0));
// Prints: 1
console.log(buf.readUInt8(1));
// Prints: 254
console.log(buf.readUInt8(2));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([1, -2]);

console.log(buf.readUInt8(0));
// Prints: 1
console.log(buf.readUInt8(1));
// Prints: 254
console.log(buf.readUInt8(2));
// Throws ERR_OUT_OF_RANGE.copy

buf.readUInt16BE([offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.readUint16BE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer>

Reads an unsigned, big-endian 16-bit integer from buf at the specified
offset.
This function is also available under the readUint16BE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56]);

console.log(buf.readUInt16BE(0).toString(16));
// Prints: 1234
console.log(buf.readUInt16BE(1).toString(16));
// Prints: 3456const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56]);

console.log(buf.readUInt16BE(0).toString(16));
// Prints: 1234
console.log(buf.readUInt16BE(1).toString(16));
// Prints: 3456copy

buf.readUInt16LE([offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.readUint16LE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer>

Reads an unsigned, little-endian 16-bit integer from buf at the specified
offset.
This function is also available under the readUint16LE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56]);

console.log(buf.readUInt16LE(0).toString(16));
// Prints: 3412
console.log(buf.readUInt16LE(1).toString(16));
// Prints: 5634
console.log(buf.readUInt16LE(2).toString(16));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56]);

console.log(buf.readUInt16LE(0).toString(16));
// Prints: 3412
console.log(buf.readUInt16LE(1).toString(16));
// Prints: 5634
console.log(buf.readUInt16LE(2).toString(16));
// Throws ERR_OUT_OF_RANGE.copy

buf.readUInt32BE([offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.readUint32BE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer>

Reads an unsigned, big-endian 32-bit integer from buf at the specified
offset.
This function is also available under the readUint32BE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78]);

console.log(buf.readUInt32BE(0).toString(16));
// Prints: 12345678const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78]);

console.log(buf.readUInt32BE(0).toString(16));
// Prints: 12345678copy

buf.readUInt32LE([offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.readUint32LE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer>

Reads an unsigned, little-endian 32-bit integer from buf at the specified
offset.
This function is also available under the readUint32LE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78]);

console.log(buf.readUInt32LE(0).toString(16));
// Prints: 78563412
console.log(buf.readUInt32LE(1).toString(16));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78]);

console.log(buf.readUInt32LE(0).toString(16));
// Prints: 78563412
console.log(buf.readUInt32LE(1).toString(16));
// Throws ERR_OUT_OF_RANGE.copy

buf.readUIntBE(offset, byteLength)#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.readUintBE().
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to read. Must satisfy
0 < byteLength <= 6.
Returns: <integer>

Reads byteLength number of bytes from buf at the specified offset
and interprets the result as an unsigned big-endian integer supporting
up to 48 bits of accuracy.
This function is also available under the readUintBE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readUIntBE(0, 6).toString(16));
// Prints: 1234567890ab
console.log(buf.readUIntBE(1, 6).toString(16));
// Throws ERR_OUT_OF_RANGE.const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readUIntBE(0, 6).toString(16));
// Prints: 1234567890ab
console.log(buf.readUIntBE(1, 6).toString(16));
// Throws ERR_OUT_OF_RANGE.copy

buf.readUIntLE(offset, byteLength)#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.readUintLE().
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.11.15
Added in: v0.11.15




offset <integer> Number of bytes to skip before starting to read. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to read. Must satisfy
0 < byteLength <= 6.
Returns: <integer>

Reads byteLength number of bytes from buf at the specified offset
and interprets the result as an unsigned, little-endian integer supporting
up to 48 bits of accuracy.
This function is also available under the readUintLE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readUIntLE(0, 6).toString(16));
// Prints: ab9078563412const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x12, 0x34, 0x56, 0x78, 0x90, 0xab]);

console.log(buf.readUIntLE(0, 6).toString(16));
// Prints: ab9078563412copy

buf.subarray([start[, end]])#

Added in: v3.0.0


start <integer> Where the new Buffer will start. Default: 0.
end <integer> Where the new Buffer will end (not inclusive).
Default: buf.length.
Returns: <Buffer>

Returns a new Buffer that references the same memory as the original, but
offset and cropped by the start and end indexes.
Specifying end greater than buf.length will return the same result as
that of end equal to buf.length.
This method is inherited from TypedArray.prototype.subarray().
Modifying the new Buffer slice will modify the memory in the original Buffer
because the allocated memory of the two objects overlap.

import { Buffer } from 'node:buffer';

// Create a `Buffer` with the ASCII alphabet, take a slice, and modify one byte
// from the original `Buffer`.

const buf1 = Buffer.allocUnsafe(26);

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf1[i] = i + 97;
}

const buf2 = buf1.subarray(0, 3);

console.log(buf2.toString('ascii', 0, buf2.length));
// Prints: abc

buf1[0] = 33;

console.log(buf2.toString('ascii', 0, buf2.length));
// Prints: !bcconst { Buffer } = require('node:buffer');

// Create a `Buffer` with the ASCII alphabet, take a slice, and modify one byte
// from the original `Buffer`.

const buf1 = Buffer.allocUnsafe(26);

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf1[i] = i + 97;
}

const buf2 = buf1.subarray(0, 3);

console.log(buf2.toString('ascii', 0, buf2.length));
// Prints: abc

buf1[0] = 33;

console.log(buf2.toString('ascii', 0, buf2.length));
// Prints: !bccopy
Specifying negative indexes causes the slice to be generated relative to the
end of buf rather than the beginning.

import { Buffer } from 'node:buffer';

const buf = Buffer.from('buffer');

console.log(buf.subarray(-6, -1).toString());
// Prints: buffe
// (Equivalent to buf.subarray(0, 5).)

console.log(buf.subarray(-6, -2).toString());
// Prints: buff
// (Equivalent to buf.subarray(0, 4).)

console.log(buf.subarray(-5, -2).toString());
// Prints: uff
// (Equivalent to buf.subarray(1, 4).)const { Buffer } = require('node:buffer');

const buf = Buffer.from('buffer');

console.log(buf.subarray(-6, -1).toString());
// Prints: buffe
// (Equivalent to buf.subarray(0, 5).)

console.log(buf.subarray(-6, -2).toString());
// Prints: buff
// (Equivalent to buf.subarray(0, 4).)

console.log(buf.subarray(-5, -2).toString());
// Prints: uff
// (Equivalent to buf.subarray(1, 4).)copy

buf.slice([start[, end]])#

History

VersionChanges
v17.5.0, v16.15.0
The buf.slice() method has been deprecated.
v7.0.0
All offsets are now coerced to integers before doing any calculations with them.
v7.1.0, v6.9.2
Coercing the offsets to integers now handles values outside the 32-bit integer range properly.
v0.3.0
Added in: v0.3.0




start <integer> Where the new Buffer will start. Default: 0.
end <integer> Where the new Buffer will end (not inclusive).
Default: buf.length.
Returns: <Buffer>

Stability: 0 - Deprecated: Use buf.subarray instead.
Returns a new Buffer that references the same memory as the original, but
offset and cropped by the start and end indexes.
This method is not compatible with the Uint8Array.prototype.slice(),
which is a superclass of Buffer. To copy the slice, use
Uint8Array.prototype.slice().

import { Buffer } from 'node:buffer';

const buf = Buffer.from('buffer');

const copiedBuf = Uint8Array.prototype.slice.call(buf);
copiedBuf[0]++;
console.log(copiedBuf.toString());
// Prints: cuffer

console.log(buf.toString());
// Prints: buffer

// With buf.slice(), the original buffer is modified.
const notReallyCopiedBuf = buf.slice();
notReallyCopiedBuf[0]++;
console.log(notReallyCopiedBuf.toString());
// Prints: cuffer
console.log(buf.toString());
// Also prints: cuffer (!)const { Buffer } = require('node:buffer');

const buf = Buffer.from('buffer');

const copiedBuf = Uint8Array.prototype.slice.call(buf);
copiedBuf[0]++;
console.log(copiedBuf.toString());
// Prints: cuffer

console.log(buf.toString());
// Prints: buffer

// With buf.slice(), the original buffer is modified.
const notReallyCopiedBuf = buf.slice();
notReallyCopiedBuf[0]++;
console.log(notReallyCopiedBuf.toString());
// Prints: cuffer
console.log(buf.toString());
// Also prints: cuffer (!)copy

buf.swap16()#

Added in: v5.10.0


Returns: <Buffer> A reference to buf.

Interprets buf as an array of unsigned 16-bit integers and swaps the
byte order in-place. Throws ERR_INVALID_BUFFER_SIZE if buf.length
is not a multiple of 2.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5, 0x6, 0x7, 0x8]);

console.log(buf1);
// Prints: <Buffer 01 02 03 04 05 06 07 08>

buf1.swap16();

console.log(buf1);
// Prints: <Buffer 02 01 04 03 06 05 08 07>

const buf2 = Buffer.from([0x1, 0x2, 0x3]);

buf2.swap16();
// Throws ERR_INVALID_BUFFER_SIZE.const { Buffer } = require('node:buffer');

const buf1 = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5, 0x6, 0x7, 0x8]);

console.log(buf1);
// Prints: <Buffer 01 02 03 04 05 06 07 08>

buf1.swap16();

console.log(buf1);
// Prints: <Buffer 02 01 04 03 06 05 08 07>

const buf2 = Buffer.from([0x1, 0x2, 0x3]);

buf2.swap16();
// Throws ERR_INVALID_BUFFER_SIZE.copy
One convenient use of buf.swap16() is to perform a fast in-place conversion
between UTF-16 little-endian and UTF-16 big-endian:

import { Buffer } from 'node:buffer';

const buf = Buffer.from('This is little-endian UTF-16', 'utf16le');
buf.swap16(); // Convert to big-endian UTF-16 text.const { Buffer } = require('node:buffer');

const buf = Buffer.from('This is little-endian UTF-16', 'utf16le');
buf.swap16(); // Convert to big-endian UTF-16 text.copy

buf.swap32()#

Added in: v5.10.0


Returns: <Buffer> A reference to buf.

Interprets buf as an array of unsigned 32-bit integers and swaps the
byte order in-place. Throws ERR_INVALID_BUFFER_SIZE if buf.length
is not a multiple of 4.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5, 0x6, 0x7, 0x8]);

console.log(buf1);
// Prints: <Buffer 01 02 03 04 05 06 07 08>

buf1.swap32();

console.log(buf1);
// Prints: <Buffer 04 03 02 01 08 07 06 05>

const buf2 = Buffer.from([0x1, 0x2, 0x3]);

buf2.swap32();
// Throws ERR_INVALID_BUFFER_SIZE.const { Buffer } = require('node:buffer');

const buf1 = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5, 0x6, 0x7, 0x8]);

console.log(buf1);
// Prints: <Buffer 01 02 03 04 05 06 07 08>

buf1.swap32();

console.log(buf1);
// Prints: <Buffer 04 03 02 01 08 07 06 05>

const buf2 = Buffer.from([0x1, 0x2, 0x3]);

buf2.swap32();
// Throws ERR_INVALID_BUFFER_SIZE.copy

buf.swap64()#

Added in: v6.3.0


Returns: <Buffer> A reference to buf.

Interprets buf as an array of 64-bit numbers and swaps byte order in-place.
Throws ERR_INVALID_BUFFER_SIZE if buf.length is not a multiple of 8.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5, 0x6, 0x7, 0x8]);

console.log(buf1);
// Prints: <Buffer 01 02 03 04 05 06 07 08>

buf1.swap64();

console.log(buf1);
// Prints: <Buffer 08 07 06 05 04 03 02 01>

const buf2 = Buffer.from([0x1, 0x2, 0x3]);

buf2.swap64();
// Throws ERR_INVALID_BUFFER_SIZE.const { Buffer } = require('node:buffer');

const buf1 = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5, 0x6, 0x7, 0x8]);

console.log(buf1);
// Prints: <Buffer 01 02 03 04 05 06 07 08>

buf1.swap64();

console.log(buf1);
// Prints: <Buffer 08 07 06 05 04 03 02 01>

const buf2 = Buffer.from([0x1, 0x2, 0x3]);

buf2.swap64();
// Throws ERR_INVALID_BUFFER_SIZE.copy

buf.toJSON()#

Added in: v0.9.2


Returns: <Object>

Returns a JSON representation of buf. JSON.stringify() implicitly calls
this function when stringifying a Buffer instance.
Buffer.from() accepts objects in the format returned from this method.
In particular, Buffer.from(buf.toJSON()) works like Buffer.from(buf).

import { Buffer } from 'node:buffer';

const buf = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5]);
const json = JSON.stringify(buf);

console.log(json);
// Prints: {"type":"Buffer","data":[1,2,3,4,5]}

const copy = JSON.parse(json, (key, value) => {
  return value && value.type === 'Buffer' ?
    Buffer.from(value) :
    value;
});

console.log(copy);
// Prints: <Buffer 01 02 03 04 05>const { Buffer } = require('node:buffer');

const buf = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5]);
const json = JSON.stringify(buf);

console.log(json);
// Prints: {"type":"Buffer","data":[1,2,3,4,5]}

const copy = JSON.parse(json, (key, value) => {
  return value && value.type === 'Buffer' ?
    Buffer.from(value) :
    value;
});

console.log(copy);
// Prints: <Buffer 01 02 03 04 05>copy

buf.toString([encoding[, start[, end]]])#

Added in: v0.1.90


encoding <string> The character encoding to use. Default: 'utf8'.
start <integer> The byte offset to start decoding at. Default: 0.
end <integer> The byte offset to stop decoding at (not inclusive).
Default: buf.length.
Returns: <string>

Decodes buf to a string according to the specified character encoding in
encoding. start and end may be passed to decode only a subset of buf.
If encoding is 'utf8' and a byte sequence in the input is not valid UTF-8,
then each invalid byte is replaced with the replacement character U+FFFD.
The maximum length of a string instance (in UTF-16 code units) is available
as buffer.constants.MAX_STRING_LENGTH.

import { Buffer } from 'node:buffer';

const buf1 = Buffer.allocUnsafe(26);

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf1[i] = i + 97;
}

console.log(buf1.toString('utf8'));
// Prints: abcdefghijklmnopqrstuvwxyz
console.log(buf1.toString('utf8', 0, 5));
// Prints: abcde

const buf2 = Buffer.from('tést');

console.log(buf2.toString('hex'));
// Prints: 74c3a97374
console.log(buf2.toString('utf8', 0, 3));
// Prints: té
console.log(buf2.toString(undefined, 0, 3));
// Prints: téconst { Buffer } = require('node:buffer');

const buf1 = Buffer.allocUnsafe(26);

for (let i = 0; i < 26; i++) {
  // 97 is the decimal ASCII value for 'a'.
  buf1[i] = i + 97;
}

console.log(buf1.toString('utf8'));
// Prints: abcdefghijklmnopqrstuvwxyz
console.log(buf1.toString('utf8', 0, 5));
// Prints: abcde

const buf2 = Buffer.from('tést');

console.log(buf2.toString('hex'));
// Prints: 74c3a97374
console.log(buf2.toString('utf8', 0, 3));
// Prints: té
console.log(buf2.toString(undefined, 0, 3));
// Prints: técopy

buf.values()#

Added in: v1.1.0


Returns: <Iterator>

Creates and returns an iterator for buf values (bytes). This function is
called automatically when a Buffer is used in a for..of statement.

import { Buffer } from 'node:buffer';

const buf = Buffer.from('buffer');

for (const value of buf.values()) {
  console.log(value);
}
// Prints:
//   98
//   117
//   102
//   102
//   101
//   114

for (const value of buf) {
  console.log(value);
}
// Prints:
//   98
//   117
//   102
//   102
//   101
//   114const { Buffer } = require('node:buffer');

const buf = Buffer.from('buffer');

for (const value of buf.values()) {
  console.log(value);
}
// Prints:
//   98
//   117
//   102
//   102
//   101
//   114

for (const value of buf) {
  console.log(value);
}
// Prints:
//   98
//   117
//   102
//   102
//   101
//   114copy

buf.write(string[, offset[, length]][, encoding])#

Added in: v0.1.90


string <string> String to write to buf.
offset <integer> Number of bytes to skip before starting to write string.
Default: 0.
length <integer> Maximum number of bytes to write (written bytes will not
exceed buf.length - offset). Default: buf.length - offset.
encoding <string> The character encoding of string. Default: 'utf8'.
Returns: <integer> Number of bytes written.

Writes string to buf at offset according to the character encoding in
encoding. The length parameter is the number of bytes to write. If buf did
not contain enough space to fit the entire string, only part of string will be
written. However, partially encoded characters will not be written.

import { Buffer } from 'node:buffer';

const buf = Buffer.alloc(256);

const len = buf.write('\u00bd + \u00bc = \u00be', 0);

console.log(`${len} bytes: ${buf.toString('utf8', 0, len)}`);
// Prints: 12 bytes: ½ + ¼ = ¾

const buffer = Buffer.alloc(10);

const length = buffer.write('abcd', 8);

console.log(`${length} bytes: ${buffer.toString('utf8', 8, 10)}`);
// Prints: 2 bytes : abconst { Buffer } = require('node:buffer');

const buf = Buffer.alloc(256);

const len = buf.write('\u00bd + \u00bc = \u00be', 0);

console.log(`${len} bytes: ${buf.toString('utf8', 0, len)}`);
// Prints: 12 bytes: ½ + ¼ = ¾

const buffer = Buffer.alloc(10);

const length = buffer.write('abcd', 8);

console.log(`${length} bytes: ${buffer.toString('utf8', 8, 10)}`);
// Prints: 2 bytes : abcopy

buf.writeBigInt64BE(value[, offset])#

Added in: v12.0.0, v10.20.0


value <bigint> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian.
value is interpreted and written as a two's complement signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(8);

buf.writeBigInt64BE(0x0102030405060708n, 0);

console.log(buf);
// Prints: <Buffer 01 02 03 04 05 06 07 08>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(8);

buf.writeBigInt64BE(0x0102030405060708n, 0);

console.log(buf);
// Prints: <Buffer 01 02 03 04 05 06 07 08>copy

buf.writeBigInt64LE(value[, offset])#

Added in: v12.0.0, v10.20.0


value <bigint> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian.
value is interpreted and written as a two's complement signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(8);

buf.writeBigInt64LE(0x0102030405060708n, 0);

console.log(buf);
// Prints: <Buffer 08 07 06 05 04 03 02 01>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(8);

buf.writeBigInt64LE(0x0102030405060708n, 0);

console.log(buf);
// Prints: <Buffer 08 07 06 05 04 03 02 01>copy

buf.writeBigUInt64BE(value[, offset])#

History

VersionChanges
v14.10.0, v12.19.0
This function is also available as buf.writeBigUint64BE().
v12.0.0, v10.20.0
Added in: v12.0.0, v10.20.0




value <bigint> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian.
This function is also available under the writeBigUint64BE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(8);

buf.writeBigUInt64BE(0xdecafafecacefaden, 0);

console.log(buf);
// Prints: <Buffer de ca fa fe ca ce fa de>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(8);

buf.writeBigUInt64BE(0xdecafafecacefaden, 0);

console.log(buf);
// Prints: <Buffer de ca fa fe ca ce fa de>copy

buf.writeBigUInt64LE(value[, offset])#

History

VersionChanges
v14.10.0, v12.19.0
This function is also available as buf.writeBigUint64LE().
v12.0.0, v10.20.0
Added in: v12.0.0, v10.20.0




value <bigint> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy: 0 <= offset <= buf.length - 8. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(8);

buf.writeBigUInt64LE(0xdecafafecacefaden, 0);

console.log(buf);
// Prints: <Buffer de fa ce ca fe fa ca de>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(8);

buf.writeBigUInt64LE(0xdecafafecacefaden, 0);

console.log(buf);
// Prints: <Buffer de fa ce ca fe fa ca de>copy
This function is also available under the writeBigUint64LE alias.

buf.writeDoubleBE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




value <number> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 8. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian. The value
must be a JavaScript number. Behavior is undefined when value is anything
other than a JavaScript number.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(8);

buf.writeDoubleBE(123.456, 0);

console.log(buf);
// Prints: <Buffer 40 5e dd 2f 1a 9f be 77>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(8);

buf.writeDoubleBE(123.456, 0);

console.log(buf);
// Prints: <Buffer 40 5e dd 2f 1a 9f be 77>copy

buf.writeDoubleLE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




value <number> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 8. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian. The value
must be a JavaScript number. Behavior is undefined when value is anything
other than a JavaScript number.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(8);

buf.writeDoubleLE(123.456, 0);

console.log(buf);
// Prints: <Buffer 77 be 9f 1a 2f dd 5e 40>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(8);

buf.writeDoubleLE(123.456, 0);

console.log(buf);
// Prints: <Buffer 77 be 9f 1a 2f dd 5e 40>copy

buf.writeFloatBE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




value <number> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian. Behavior is
undefined when value is anything other than a JavaScript number.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeFloatBE(0xcafebabe, 0);

console.log(buf);
// Prints: <Buffer 4f 4a fe bb>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeFloatBE(0xcafebabe, 0);

console.log(buf);
// Prints: <Buffer 4f 4a fe bb>copy

buf.writeFloatLE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.11.15
Added in: v0.11.15




value <number> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian. Behavior is
undefined when value is anything other than a JavaScript number.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeFloatLE(0xcafebabe, 0);

console.log(buf);
// Prints: <Buffer bb fe 4a 4f>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeFloatLE(0xcafebabe, 0);

console.log(buf);
// Prints: <Buffer bb fe 4a 4f>copy

buf.writeInt8(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.0
Added in: v0.5.0




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 1. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset. value must be a valid
signed 8-bit integer. Behavior is undefined when value is anything other than
a signed 8-bit integer.
value is interpreted and written as a two's complement signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(2);

buf.writeInt8(2, 0);
buf.writeInt8(-2, 1);

console.log(buf);
// Prints: <Buffer 02 fe>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(2);

buf.writeInt8(2, 0);
buf.writeInt8(-2, 1);

console.log(buf);
// Prints: <Buffer 02 fe>copy

buf.writeInt16BE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian.  The value
must be a valid signed 16-bit integer. Behavior is undefined when value is
anything other than a signed 16-bit integer.
The value is interpreted and written as a two's complement signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(2);

buf.writeInt16BE(0x0102, 0);

console.log(buf);
// Prints: <Buffer 01 02>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(2);

buf.writeInt16BE(0x0102, 0);

console.log(buf);
// Prints: <Buffer 01 02>copy

buf.writeInt16LE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian.  The value
must be a valid signed 16-bit integer. Behavior is undefined when value is
anything other than a signed 16-bit integer.
The value is interpreted and written as a two's complement signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(2);

buf.writeInt16LE(0x0304, 0);

console.log(buf);
// Prints: <Buffer 04 03>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(2);

buf.writeInt16LE(0x0304, 0);

console.log(buf);
// Prints: <Buffer 04 03>copy

buf.writeInt32BE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian. The value
must be a valid signed 32-bit integer. Behavior is undefined when value is
anything other than a signed 32-bit integer.
The value is interpreted and written as a two's complement signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeInt32BE(0x01020304, 0);

console.log(buf);
// Prints: <Buffer 01 02 03 04>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeInt32BE(0x01020304, 0);

console.log(buf);
// Prints: <Buffer 01 02 03 04>copy

buf.writeInt32LE(value[, offset])#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian. The value
must be a valid signed 32-bit integer. Behavior is undefined when value is
anything other than a signed 32-bit integer.
The value is interpreted and written as a two's complement signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeInt32LE(0x05060708, 0);

console.log(buf);
// Prints: <Buffer 08 07 06 05>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeInt32LE(0x05060708, 0);

console.log(buf);
// Prints: <Buffer 08 07 06 05>copy

buf.writeIntBE(value, offset, byteLength)#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.11.15
Added in: v0.11.15




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to write. Must satisfy
0 < byteLength <= 6.
Returns: <integer> offset plus the number of bytes written.

Writes byteLength bytes of value to buf at the specified offset
as big-endian. Supports up to 48 bits of accuracy. Behavior is undefined when
value is anything other than a signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(6);

buf.writeIntBE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer 12 34 56 78 90 ab>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(6);

buf.writeIntBE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer 12 34 56 78 90 ab>copy

buf.writeIntLE(value, offset, byteLength)#

History

VersionChanges
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.11.15
Added in: v0.11.15




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to write. Must satisfy
0 < byteLength <= 6.
Returns: <integer> offset plus the number of bytes written.

Writes byteLength bytes of value to buf at the specified offset
as little-endian. Supports up to 48 bits of accuracy. Behavior is undefined
when value is anything other than a signed integer.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(6);

buf.writeIntLE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer ab 90 78 56 34 12>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(6);

buf.writeIntLE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer ab 90 78 56 34 12>copy

buf.writeUInt8(value[, offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.writeUint8().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.0
Added in: v0.5.0




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 1. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset. value must be a
valid unsigned 8-bit integer. Behavior is undefined when value is anything
other than an unsigned 8-bit integer.
This function is also available under the writeUint8 alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeUInt8(0x3, 0);
buf.writeUInt8(0x4, 1);
buf.writeUInt8(0x23, 2);
buf.writeUInt8(0x42, 3);

console.log(buf);
// Prints: <Buffer 03 04 23 42>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeUInt8(0x3, 0);
buf.writeUInt8(0x4, 1);
buf.writeUInt8(0x23, 2);
buf.writeUInt8(0x42, 3);

console.log(buf);
// Prints: <Buffer 03 04 23 42>copy

buf.writeUInt16BE(value[, offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.writeUint16BE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian. The value
must be a valid unsigned 16-bit integer. Behavior is undefined when value
is anything other than an unsigned 16-bit integer.
This function is also available under the writeUint16BE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeUInt16BE(0xdead, 0);
buf.writeUInt16BE(0xbeef, 2);

console.log(buf);
// Prints: <Buffer de ad be ef>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeUInt16BE(0xdead, 0);
buf.writeUInt16BE(0xbeef, 2);

console.log(buf);
// Prints: <Buffer de ad be ef>copy

buf.writeUInt16LE(value[, offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.writeUint16LE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 2. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian. The value
must be a valid unsigned 16-bit integer. Behavior is undefined when value is
anything other than an unsigned 16-bit integer.
This function is also available under the writeUint16LE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeUInt16LE(0xdead, 0);
buf.writeUInt16LE(0xbeef, 2);

console.log(buf);
// Prints: <Buffer ad de ef be>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeUInt16LE(0xdead, 0);
buf.writeUInt16LE(0xbeef, 2);

console.log(buf);
// Prints: <Buffer ad de ef be>copy

buf.writeUInt32BE(value[, offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.writeUint32BE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as big-endian. The value
must be a valid unsigned 32-bit integer. Behavior is undefined when value
is anything other than an unsigned 32-bit integer.
This function is also available under the writeUint32BE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeUInt32BE(0xfeedface, 0);

console.log(buf);
// Prints: <Buffer fe ed fa ce>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeUInt32BE(0xfeedface, 0);

console.log(buf);
// Prints: <Buffer fe ed fa ce>copy

buf.writeUInt32LE(value[, offset])#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.writeUint32LE().
v10.0.0
Removed noAssert and no implicit coercion of the offset to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - 4. Default: 0.
Returns: <integer> offset plus the number of bytes written.

Writes value to buf at the specified offset as little-endian. The value
must be a valid unsigned 32-bit integer. Behavior is undefined when value is
anything other than an unsigned 32-bit integer.
This function is also available under the writeUint32LE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(4);

buf.writeUInt32LE(0xfeedface, 0);

console.log(buf);
// Prints: <Buffer ce fa ed fe>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(4);

buf.writeUInt32LE(0xfeedface, 0);

console.log(buf);
// Prints: <Buffer ce fa ed fe>copy

buf.writeUIntBE(value, offset, byteLength)#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.writeUintBE().
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to write. Must satisfy
0 < byteLength <= 6.
Returns: <integer> offset plus the number of bytes written.

Writes byteLength bytes of value to buf at the specified offset
as big-endian. Supports up to 48 bits of accuracy. Behavior is undefined
when value is anything other than an unsigned integer.
This function is also available under the writeUintBE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(6);

buf.writeUIntBE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer 12 34 56 78 90 ab>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(6);

buf.writeUIntBE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer 12 34 56 78 90 ab>copy

buf.writeUIntLE(value, offset, byteLength)#

History

VersionChanges
v14.9.0, v12.19.0
This function is also available as buf.writeUintLE().
v10.0.0
Removed noAssert and no implicit coercion of the offset and byteLength to uint32 anymore.
v0.5.5
Added in: v0.5.5




value <integer> Number to be written to buf.
offset <integer> Number of bytes to skip before starting to write. Must
satisfy 0 <= offset <= buf.length - byteLength.
byteLength <integer> Number of bytes to write. Must satisfy
0 < byteLength <= 6.
Returns: <integer> offset plus the number of bytes written.

Writes byteLength bytes of value to buf at the specified offset
as little-endian. Supports up to 48 bits of accuracy. Behavior is undefined
when value is anything other than an unsigned integer.
This function is also available under the writeUintLE alias.

import { Buffer } from 'node:buffer';

const buf = Buffer.allocUnsafe(6);

buf.writeUIntLE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer ab 90 78 56 34 12>const { Buffer } = require('node:buffer');

const buf = Buffer.allocUnsafe(6);

buf.writeUIntLE(0x1234567890ab, 0, 6);

console.log(buf);
// Prints: <Buffer ab 90 78 56 34 12>copy

new Buffer(array)#

History

VersionChanges
v10.0.0
Calling this constructor emits a deprecation warning when run from code outside the node_modules directory.
v7.2.1
Calling this constructor no longer emits a deprecation warning.
v7.0.0
Calling this constructor emits a deprecation warning now.
v6.0.0
Deprecated since: v6.0.0



Stability: 0 - Deprecated: Use Buffer.from(array) instead.

array <integer[]> An array of bytes to copy from.

See Buffer.from(array).

new Buffer(arrayBuffer[, byteOffset[, length]])#

History

VersionChanges
v10.0.0
Calling this constructor emits a deprecation warning when run from code outside the node_modules directory.
v7.2.1
Calling this constructor no longer emits a deprecation warning.
v7.0.0
Calling this constructor emits a deprecation warning now.
v6.0.0
The byteOffset and length parameters are supported now.
v6.0.0
Deprecated since: v6.0.0
v3.0.0
Added in: v3.0.0



Stability: 0 - Deprecated: Use
Buffer.from(arrayBuffer[, byteOffset[, length]])
instead.

arrayBuffer <ArrayBuffer> | <SharedArrayBuffer> An <ArrayBuffer>,
<SharedArrayBuffer> or the .buffer property of a <TypedArray>.
byteOffset <integer> Index of first byte to expose. Default: 0.
length <integer> Number of bytes to expose.
Default: arrayBuffer.byteLength - byteOffset.

See
Buffer.from(arrayBuffer[, byteOffset[, length]]).

new Buffer(buffer)#

History

VersionChanges
v10.0.0
Calling this constructor emits a deprecation warning when run from code outside the node_modules directory.
v7.2.1
Calling this constructor no longer emits a deprecation warning.
v7.0.0
Calling this constructor emits a deprecation warning now.
v6.0.0
Deprecated since: v6.0.0



Stability: 0 - Deprecated: Use Buffer.from(buffer) instead.

buffer <Buffer> | <Uint8Array> An existing Buffer or <Uint8Array> from
which to copy data.

See Buffer.from(buffer).

new Buffer(size)#

History

VersionChanges
v10.0.0
Calling this constructor emits a deprecation warning when run from code outside the node_modules directory.
v8.0.0
The new Buffer(size) will return zero-filled memory by default.
v7.2.1
Calling this constructor no longer emits a deprecation warning.
v7.0.0
Calling this constructor emits a deprecation warning now.
v6.0.0
Deprecated since: v6.0.0



Stability: 0 - Deprecated: Use Buffer.alloc() instead (also see
Buffer.allocUnsafe()).

size <integer> The desired length of the new Buffer.

See Buffer.alloc() and Buffer.allocUnsafe(). This variant of the
constructor is equivalent to Buffer.alloc().

new Buffer(string[, encoding])#

History

VersionChanges
v10.0.0
Calling this constructor emits a deprecation warning when run from code outside the node_modules directory.
v7.2.1
Calling this constructor no longer emits a deprecation warning.
v7.0.0
Calling this constructor emits a deprecation warning now.
v6.0.0
Deprecated since: v6.0.0



Stability: 0 - Deprecated:
Use Buffer.from(string[, encoding]) instead.

string <string> String to encode.
encoding <string> The encoding of string. Default: 'utf8'.

See Buffer.from(string[, encoding]).

Class: File#

History

VersionChanges
v23.0.0
Makes File instances cloneable.
v20.0.0
No longer experimental.
v19.2.0, v18.13.0
Added in: v19.2.0, v18.13.0




Extends: <Blob>

A <File> provides information about files.

new buffer.File(sources, fileName[, options])#

Added in: v19.2.0, v18.13.0


sources <string[]> | <ArrayBuffer[]> | <TypedArray[]> | <DataView[]> | <Blob[]> | <File[]>
An array of string, <ArrayBuffer>, <TypedArray>, <DataView>, <File>, or <Blob>
objects, or any mix of such objects, that will be stored within the File.
fileName <string> The name of the file.
options <Object>

endings <string> One of either 'transparent' or 'native'. When set
to 'native', line endings in string source parts will be converted to
the platform native line-ending as specified by require('node:os').EOL.
type <string> The File content-type.
lastModified <number> The last modified date of the file.
Default: Date.now().




file.name#

Added in: v19.2.0, v18.13.0


Type: <string>

The name of the File.

file.lastModified#

Added in: v19.2.0, v18.13.0


Type: <number>

The last modified date of the File.

node:buffer module APIs#
While, the Buffer object is available as a global, there are additional
Buffer-related APIs that are available only via the node:buffer module
accessed using require('node:buffer').

buffer.atob(data)#

Added in: v15.13.0, v14.17.0

Stability: 3 - Legacy. Use Buffer.from(data, 'base64') instead.

data <any> The Base64-encoded input string.

Decodes a string of Base64-encoded data into bytes, and encodes those bytes
into a string using Latin-1 (ISO-8859-1).
The data may be any JavaScript-value that can be coerced into a string.
This function is only provided for compatibility with legacy web platform APIs
and should never be used in new code, because they use strings to represent
binary data and predate the introduction of typed arrays in JavaScript.
For code running using Node.js APIs, converting between base64-encoded strings
and binary data should be performed using Buffer.from(str, 'base64') and
buf.toString('base64').

buffer.btoa(data)#

Added in: v15.13.0, v14.17.0

Stability: 3 - Legacy. Use buf.toString('base64') instead.

data <any> An ASCII (Latin1) string.

Decodes a string into bytes using Latin-1 (ISO-8859), and encodes those bytes
into a string using Base64.
The data may be any JavaScript-value that can be coerced into a string.
This function is only provided for compatibility with legacy web platform APIs
and should never be used in new code, because they use strings to represent
binary data and predate the introduction of typed arrays in JavaScript.
For code running using Node.js APIs, converting between base64-encoded strings
and binary data should be performed using Buffer.from(str, 'base64') and
buf.toString('base64').

buffer.isAscii(input)#

Added in: v19.6.0, v18.15.0


input <Buffer> | <ArrayBuffer> | <TypedArray> The input to validate.
Returns: <boolean>

This function returns true if input contains only valid ASCII-encoded data,
including the case in which input is empty.
Throws if the input is a detached array buffer.

buffer.isUtf8(input)#

Added in: v19.4.0, v18.14.0


input <Buffer> | <ArrayBuffer> | <TypedArray> The input to validate.
Returns: <boolean>

This function returns true if input contains only valid UTF-8-encoded data,
including the case in which input is empty.
Throws if the input is a detached array buffer.

buffer.INSPECT_MAX_BYTES#

Added in: v0.5.4


<integer> Default: 50

Returns the maximum number of bytes that will be returned when
buf.inspect() is called. This can be overridden by user modules. See
util.inspect() for more details on buf.inspect() behavior.

buffer.kMaxLength#

Added in: v3.0.0


<integer> The largest size allowed for a single Buffer instance.

An alias for buffer.constants.MAX_LENGTH.

buffer.kStringMaxLength#

Added in: v3.0.0


<integer> The largest length allowed for a single string instance.

An alias for buffer.constants.MAX_STRING_LENGTH.

buffer.resolveObjectURL(id)#

Added in: v16.7.0

Stability: 1 - Experimental

id <string> A 'blob:nodedata:... URL string returned by a prior call to
URL.createObjectURL().
Returns: <Blob>

Resolves a 'blob:nodedata:...' an associated <Blob> object registered using
a prior call to URL.createObjectURL().

buffer.transcode(source, fromEnc, toEnc)#

History

VersionChanges
v8.0.0
The source parameter can now be a Uint8Array.
v7.1.0
Added in: v7.1.0




source <Buffer> | <Uint8Array> A Buffer or Uint8Array instance.
fromEnc <string> The current encoding.
toEnc <string> To target encoding.
Returns: <Buffer>

Re-encodes the given Buffer or Uint8Array instance from one character
encoding to another. Returns a new Buffer instance.
Throws if the fromEnc or toEnc specify invalid character encodings or if
conversion from fromEnc to toEnc is not permitted.
Encodings supported by buffer.transcode() are: 'ascii', 'utf8',
'utf16le', 'ucs2', 'latin1', and 'binary'.
The transcoding process will use substitution characters if a given byte
sequence cannot be adequately represented in the target encoding. For instance:

import { Buffer, transcode } from 'node:buffer';

const newBuf = transcode(Buffer.from('€'), 'utf8', 'ascii');
console.log(newBuf.toString('ascii'));
// Prints: '?'const { Buffer, transcode } = require('node:buffer');

const newBuf = transcode(Buffer.from('€'), 'utf8', 'ascii');
console.log(newBuf.toString('ascii'));
// Prints: '?'copy
Because the Euro (€) sign is not representable in US-ASCII, it is replaced
with ? in the transcoded Buffer.

Class: SlowBuffer#

Deprecated since: v6.0.0

Stability: 0 - Deprecated: Use Buffer.allocUnsafeSlow() instead.
See Buffer.allocUnsafeSlow(). This was never a class in the sense that
the constructor always returned a Buffer instance, rather than a SlowBuffer
instance.

new SlowBuffer(size)#

Deprecated since: v6.0.0


size <integer> The desired length of the new SlowBuffer.

See Buffer.allocUnsafeSlow().

Buffer constants#

Added in: v8.2.0


buffer.constants.MAX_LENGTH#

History

VersionChanges
v22.0.0
Value is changed to 253 - 1 on 64-bit architectures.
v15.0.0
Value is changed to 232 on 64-bit architectures.
v14.0.0
Value is changed from 231 - 1 to 232 - 1 on 64-bit architectures.
v8.2.0
Added in: v8.2.0




<integer> The largest size allowed for a single Buffer instance.

On 32-bit architectures, this value currently is 230 - 1 (about 1
GiB).
On 64-bit architectures, this value currently is 253 - 1 (about 8 PiB).
It reflects v8::TypedArray::kMaxLength under the hood.
This value is also available as buffer.kMaxLength.

buffer.constants.MAX_STRING_LENGTH#

Added in: v8.2.0


<integer> The largest length allowed for a single string instance.

Represents the largest length that a string primitive can have, counted
in UTF-16 code units.
This value may depend on the JS engine that is being used.

Buffer.from(), Buffer.alloc(), and Buffer.allocUnsafe()#
In versions of Node.js prior to 6.0.0, Buffer instances were created using the
Buffer constructor function, which allocates the returned Buffer
differently based on what arguments are provided:

Passing a number as the first argument to Buffer() (e.g. new Buffer(10))
allocates a new Buffer object of the specified size. Prior to Node.js 8.0.0,
the memory allocated for such Buffer instances is not initialized and
can contain sensitive data. Such Buffer instances must be subsequently
initialized by using either buf.fill(0) or by writing to the
entire Buffer before reading data from the Buffer.
While this behavior is intentional to improve performance,
development experience has demonstrated that a more explicit distinction is
required between creating a fast-but-uninitialized Buffer versus creating a
slower-but-safer Buffer. Since Node.js 8.0.0, Buffer(num) and new Buffer(num) return a Buffer with initialized memory.
Passing a string, array, or Buffer as the first argument copies the
passed object's data into the Buffer.
Passing an <ArrayBuffer> or a <SharedArrayBuffer> returns a Buffer
that shares allocated memory with the given array buffer.

Because the behavior of new Buffer() is different depending on the type of the
first argument, security and reliability issues can be inadvertently introduced
into applications when argument validation or Buffer initialization is not
performed.
For example, if an attacker can cause an application to receive a number where
a string is expected, the application may call new Buffer(100)
instead of new Buffer("100"), leading it to allocate a 100 byte buffer instead
of allocating a 3 byte buffer with content "100". This is commonly possible
using JSON API calls. Since JSON distinguishes between numeric and string types,
it allows injection of numbers where a naively written application that does not
validate its input sufficiently might expect to always receive a string.
Before Node.js 8.0.0, the 100 byte buffer might contain
arbitrary pre-existing in-memory data, so may be used to expose in-memory
secrets to a remote attacker. Since Node.js 8.0.0, exposure of memory cannot
occur because the data is zero-filled. However, other attacks are still
possible, such as causing very large buffers to be allocated by the server,
leading to performance degradation or crashing on memory exhaustion.
To make the creation of Buffer instances more reliable and less error-prone,
the various forms of the new Buffer() constructor have been deprecated
and replaced by separate Buffer.from(), Buffer.alloc(), and
Buffer.allocUnsafe() methods.
Developers should migrate all existing uses of the new Buffer() constructors
to one of these new APIs.

Buffer.from(array) returns a new Buffer that contains a copy of the
provided octets.
Buffer.from(arrayBuffer[, byteOffset[, length]])
returns a new Buffer that shares the same allocated memory as the given
<ArrayBuffer>.
Buffer.from(buffer) returns a new Buffer that contains a copy of the
contents of the given Buffer.
Buffer.from(string[, encoding]) returns a new
Buffer that contains a copy of the provided string.
Buffer.alloc(size[, fill[, encoding]]) returns a new
initialized Buffer of the specified size. This method is slower than
Buffer.allocUnsafe(size) but guarantees that newly
created Buffer instances never contain old data that is potentially
sensitive. A TypeError will be thrown if size is not a number.
Buffer.allocUnsafe(size) and
Buffer.allocUnsafeSlow(size) each return a
new uninitialized Buffer of the specified size. Because the Buffer is
uninitialized, the allocated segment of memory might contain old data that is
potentially sensitive.

Buffer instances returned by Buffer.allocUnsafe(), Buffer.from(string),
Buffer.concat() and Buffer.from(array) may be allocated off a shared
internal memory pool if size is less than or equal to half Buffer.poolSize.
Instances returned by Buffer.allocUnsafeSlow() never use the shared internal
memory pool.

The --zero-fill-buffers command-line option#

Added in: v5.10.0

Node.js can be started using the --zero-fill-buffers command-line option to
cause all newly-allocated Buffer instances to be zero-filled upon creation by
default. Without the option, buffers created with Buffer.allocUnsafe(),
Buffer.allocUnsafeSlow(), and new SlowBuffer(size) are not zero-filled.
Use of this flag can have a measurable negative impact on performance. Use the
--zero-fill-buffers option only when necessary to enforce that newly allocated
Buffer instances cannot contain old data that is potentially sensitive.
$ node --zero-fill-buffers
> Buffer.allocUnsafe(5);
<Buffer 00 00 00 00 00> copy

What makes Buffer.allocUnsafe() and Buffer.allocUnsafeSlow() "unsafe"?#
When calling Buffer.allocUnsafe() and Buffer.allocUnsafeSlow(), the
segment of allocated memory is uninitialized (it is not zeroed-out). While
this design makes the allocation of memory quite fast, the allocated segment of
memory might contain old data that is potentially sensitive. Using a Buffer
created by Buffer.allocUnsafe() without completely overwriting the
memory can allow this old data to be leaked when the Buffer memory is read.
While there are clear performance advantages to using
Buffer.allocUnsafe(), extra care must be taken in order to avoid
introducing security vulnerabilities into an application.\n\n\n\n