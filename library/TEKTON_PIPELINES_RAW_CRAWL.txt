Documentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Tasks and Pipelines
  Building Blocks of Tekton CI/CD Workflow
	
		

  
    


  
    


  

		
	
	Tekton Pipelines
Tekton Pipelines is a Kubernetes extension that installs and runs on your Kubernetes cluster.
It defines a set of Kubernetes Custom Resources that act as building blocks from which you can assemble CI/CD pipelines. Once installed,
Tekton Pipelines becomes available via the Kubernetes CLI (kubectl) and via API calls, just
like pods and other resources. Tekton is open-source and part of the CD Foundation,
a Linux Foundation project.
Tekton Pipelines entities
Tekton Pipelines defines the following entities:

  
    Entity
    Description
  
  
    Task
    Defines a series of steps which launch specific build or delivery tools that ingest specific inputs and produce specific outputs.
  
  
    TaskRun
    Instantiates a Task for execution with specific inputs, outputs, and execution parameters. Can be invoked on its own or as part of a Pipeline.
  
  
    Pipeline
    Defines a series of Tasks that accomplish a specific build or delivery goal. Can be triggered by an event or invoked from a PipelineRun.
  
  
    PipelineRun
    Instantiates a Pipeline for execution with specific inputs, outputs, and execution parameters.
  
  
    PipelineResource (Deprecated)
    Defines locations for inputs ingested and outputs produced by the steps in Tasks.
  
  
    Run (alpha)
    Instantiates a Custom Task for execution when specific inputs.
  

Getting started
To get started, complete the Tekton Pipelines Tutorial and go through our
examples.
Understanding Tekton Pipelines
See the following topics to learn how to use Tekton Pipelines in your project:

Creating a Task
Running a standalone Task
Creating a Pipeline
Running a Pipeline
Defining Workspaces
Configuring authentication
Using labels
Viewing logs
Pipelines metrics
Variable Substitutions
Running a Custom Task
Remote resolution of Pipelines and Tasks
Trusted Resources

Contributing to Tekton Pipelines
If youâ€™d like to contribute to the Tekton Pipelines project, see the Tekton Pipeline Contributorâ€™s Guide.

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

        
    
    
    
    
    
    
    
    
    
        
            
            
                
                    Install Tekton Pipelines
                
                Install Tekton Pipelines on your cluster
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    Additional Configuration Options
                
                Additional configurations when installing Tekton Pipelines
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    Pipeline API
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
    


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified October 28, 2020: Add the past 4 versions for pipeline, triggers and CLI (4f7f217)\n\nInstall Tekton Pipelines
                
                Install Tekton Pipelines on your cluster
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    Additional Configuration Options
                
                Additional configurations when installing Tekton Pipelines
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    Pipeline API\n\n\n\nCloud Native CI/CD
          
          
            
	
		Documentation 
	
	
		GitHub 
	


          
        
      
    
  









	
		
			
				
					
						Tekton is a powerful and flexible 
						open-source framework for creating CI/CD systems,
						allowing developers to 
						build, test, and deploy 
						across cloud providers and on-premise systems.
						Get started with Tekton.
					
				
				
					
					
					
					
				
			
		
  	






 


	
		
			
				


  

Standardization

Tekton standardizes CI/CD tooling and processes across vendors, languages,
and deployment environments. It works well with Jenkins, Jenkins X, Skaffold,
Knative, and many other popular CI/CD tools.









  

Built-in best practices

Tekton lets you create CI/CD systems quickly,
giving you scalable, serverless, cloud native execution out of the box.









  

Maximum flexibility

Tekton abstracts the underlying implementation so that you can choose the build,
test, and deploy workflow based on your teamâ€™s requirements.








			
		
	









    
		
			
				
			
		
    
    
		
			
                Tekton is a collaborative project where members of the ecosystem contribute together to make CI/CD easier for everyone.
                    See the individuals and organizations
                    that are involved in the Tekton project.
			
		
  	






 


	
		
			
				


  

Install Tekton

Get Tekton here









  

Contributions welcome

Join the Tekton Community









  

Try Tekton

Get started with Tekton








			
		
	









	
		
			
				
			
		
	
	
		
			
				Tekton is a Graduated Continuous Delivery
            Foundation project and follows 
            the OpenSSF best practices.\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Tasks and Pipelines
  Building Blocks of Tekton CI/CD Workflow
	
		

  
    


  
    


  

		
	
	Tekton Pipelines
Tekton Pipelines is a Kubernetes extension that installs and runs on your Kubernetes cluster.
It defines a set of Kubernetes Custom Resources that act as building blocks from which you can assemble CI/CD pipelines. Once installed,
Tekton Pipelines becomes available via the Kubernetes CLI (kubectl) and via API calls, just
like pods and other resources. Tekton is open-source and part of the CD Foundation,
a Linux Foundation project.
Tekton Pipelines entities
Tekton Pipelines defines the following entities:

  
    Entity
    Description
  
  
    Task
    Defines a series of steps which launch specific build or delivery tools that ingest specific inputs and produce specific outputs.
  
  
    TaskRun
    Instantiates a Task for execution with specific inputs, outputs, and execution parameters. Can be invoked on its own or as part of a Pipeline.
  
  
    Pipeline
    Defines a series of Tasks that accomplish a specific build or delivery goal. Can be triggered by an event or invoked from a PipelineRun.
  
  
    PipelineRun
    Instantiates a Pipeline for execution with specific inputs, outputs, and execution parameters.
  
  
    PipelineResource (Deprecated)
    Defines locations for inputs ingested and outputs produced by the steps in Tasks.
  
  
    Run (alpha)
    Instantiates a Custom Task for execution when specific inputs.
  

Getting started
To get started, complete the Tekton Pipelines Tutorial and go through our
examples.
Understanding Tekton Pipelines
See the following topics to learn how to use Tekton Pipelines in your project:

Creating a Task
Running a standalone Task
Creating a Pipeline
Running a Pipeline
Defining Workspaces
Configuring authentication
Using labels
Viewing logs
Pipelines metrics
Variable Substitutions
Running a Custom Task
Remote resolution of Pipelines and Tasks
Trusted Resources

Contributing to Tekton Pipelines
If youâ€™d like to contribute to the Tekton Pipelines project, see the Tekton Pipeline Contributorâ€™s Guide.

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

        
    
    
    
    
    
    
    
    
    
        
            
            
                
                    Install Tekton Pipelines
                
                Install Tekton Pipelines on your cluster
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    Additional Configuration Options
                
                Additional configurations when installing Tekton Pipelines
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    Pipeline API
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
            
            
                
                    
                
                
            
        
    


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified October 28, 2020: Add the past 4 versions for pipeline, triggers and CLI (4f7f217)\n\nInstall Tekton Pipelines
                
                Install Tekton Pipelines on your cluster
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    Additional Configuration Options
                
                Additional configurations when installing Tekton Pipelines
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    
                
                
            
                
                    Pipeline API\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Welcome to Tekton
  
	
		

  
    


  
    


  

		
	
	Tekton is a cloud-native solution for building CI/CD systems. It consists of
Tekton Pipelines, which provides the building blocks, and of supporting
components, such as Tekton CLI and Tekton Catalog, that make Tekton a complete
ecosystem.  Tekton is part of the CD Foundation, a
Linux Foundation project. For more
information, see the Overview of Tekton.

        
    
    
    
    
    
    
    
    
    
        
            
            
                
                    Getting Started
                
                Get started with Tekton
            
        
            
            
                
                    Installation
                
                Installation instructions for Tekton components
            
        
            
            
                
                    Results
                
                Result storage for Tekton CI/CD data.
            
        
            
            
                
                    Tasks and Pipelines
                
                Building Blocks of Tekton CI/CD Workflow
            
        
            
            
                
                    Concepts
                
                Conceptual and technical information about Tekton
            
        
            
            
                
                    Triggers and EventListeners
                
                Event Based Triggers for Tekton Pipelines
            
        
            
            
                
                    How-to Guides
                
                Guides to help you complete a specific goal
            
        
            
            
                
                    CLI
                
                Command-Line Interface
            
        
            
            
                
                    Dashboard
                
                Web-based UI for Tekton Pipelines and Tekton Triggers resources
            
        
            
            
                
                    Catalog
                
                Reusable Task and Pipeline Resources
            
        
            
            
                
                    Operator
                
                Manage Tekton CI/CD Building Blocks
            
        
            
            
                
                    Supply Chain Security
                
                Artifact signatures and attestations for Tekton
            
        
            
            
                
                    Contribute to Documentation
                
                Contribution guidelines
            
        
    


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 29, 2023: Rework left-hand nav order (dbe0a93)\n\nGetting Started
                
                Get started with Tekton
            
                
                    Installation
                
                Installation instructions for Tekton components
            
                
                    Results
                
                Result storage for Tekton CI/CD data.
            
                
                    Tasks and Pipelines
                
                Building Blocks of Tekton CI/CD Workflow
            
                
                    Concepts
                
                Conceptual and technical information about Tekton
            
                
                    Triggers and EventListeners
                
                Event Based Triggers for Tekton Pipelines
            
                
                    How-to Guides
                
                Guides to help you complete a specific goal
            
                
                    CLI
                
                Command-Line Interface
            
                
                    Dashboard
                
                Web-based UI for Tekton Pipelines and Tekton Triggers resources
            
                
                    Catalog
                
                Reusable Task and Pipeline Resources
            
                
                    Operator
                
                Manage Tekton CI/CD Building Blocks
            
                
                    Supply Chain Security
                
                Artifact signatures and attestations for Tekton
            
                
                    Contribute to Documentation
                
                Contribution guidelines\n\n\n\nWelcome to the Tekton Community!
            Our mission is to be the industry-standard, cloud-native CI/CD
            platform components and ecosystem.
        
    



    
        Get involved
        Read our contribution standards:
        
          
            
              Code of conduct
          
          
            
              Design principles
          
          
            
              Commits standards
          
          
            
              Code standards
          
          
            
              User profiles
          
          
            
              Release cycles
          
        
    
    
        Reach out
        Join our community spaces:
        
          
	    
              Join the Tekton Slack!
          
          
            
              Follow us on Twitter
          

          
            
              Tekton developers mailing list
          
          
            
              Tekton users mailing list
          
          
            
              Working groups and meetings
          
          
            
              The Tekton Developer's shared Drive
          
        
    
    
        Contribution process
        Check out our processes:
        
          
            
              Find something to work on
          
          
            
              Propose new features
          
          
            
              Contributor ladder
          
          
            
              Pull requests reviews
          
          
            
              Propose new projects
          
          
            
              GitHub Org management
          
          
            
              Contributor's Licence Agreement
          
        
    
    
        Learn about Tekton users
        Open source projects, cloud vendors, and end users are building
        amazing things with Tekton. Check out the
        
          list of Tekton Adopters to learn more.
    




    Find more information on the Tekton Community
repository.\n\n\n\nPosts in 2025
			
				
				
					
						Migration to Github Container Registry
						Thursday, April 03, 2025 in Blog
						
							

  
    


  
    


  

							
						
						





						Why and How Dear Tekton users and contributors, to reduce costs, weâ€™ve migrated all our releases to the free tier on ghcr.io/tektoncd.
All new Tekton releases are exclusively on ghcr.io/tektoncd. Old releases are also now available on â€¦
						Read more
					
				
				
			
			
			Posts in 2023
			
				
				
					
						Speeding Up Container Image Builds in Tekton Pipelines
						Thursday, November 02, 2023 in Blog
						
							

  
    


  
    


  

							
						
						





						Overview When DevOps or developer teams evaluate the move from a persistent CI server (or a CI server using persistent workers) to a system using non-persistent workers backed by Kubernetes pods, such as Tekton Pipelines, they might also wonder how â€¦
						Read more
					
				
				
				
					
						Distributed Traces for Testing with Tekton Pipelines and Tracetest
						Tuesday, August 15, 2023 in Blog
						
							

  
    


  
    


  

							
						
						





						Tekton is an open-source framework for creating efficient CI/CD systems. This empowers developers to seamlessly construct, test, and deploy applications across various cloud environments and on-premise setups.
Tracetest, an open-source testing tool â€¦
						Read more
					
				
				
				
					
						Getting To SLSA Level 2 with Tekton and Tekton Chains
						Wednesday, April 19, 2023 in Blog
						
							

  
    


  
    


  

							
						
						





						Overview As application developers, we achieve amazing results quickly by leveraging a rich ecosystem of freely available libraries, modules and frameworks that provide ready-to-use capabilities and abstract away from underlying complexity. This is â€¦
						Read more
					
				
				
				
					
						Meet with Tekton Community at cdCon + GitOpsCon 2023
						Friday, April 14, 2023 in Blog
						
							

  
    


  
    


  

							
						
						





						 The Continuous Delivery Foundation (CDF) is happy to host its fourth flagship event, cdCon, taking place on May 8â€“9, 2023 in Vancouver, Canada as cdCon + GitOpsCon, co-organized with the Cloud Native Computing Foundation (CNCF), making it the â€¦
						Read more
					
				
				
			
			
			Posts in 2022
			
				
				
					
						Tekton Graduation
						Wednesday, October 26, 2022 in Blog
						
							

  
    


  
    


  

							
						
						





						Weâ€™re very happy to announce that Tekton has reached graduated status within the Continuous Delivery Foundation (CDF). The CDF Technical Oversight Committee (TOC) conducted public voting to decide on the graduation status for Tekton, and the â€¦
						Read more\n\n\n\n\n\n\n\nMigration to Github Container Registry
	How to migrate your images from gcr.io to ghcr.io and why we migrated
	
		By Stanislav Jakuschevskij, IBM |
		Thursday, April 03, 2025
	
	
		

  
    


  
    


  

		
	
	Why and How
Dear Tekton users and contributors, to reduce costs, weâ€™ve migrated all our releases to the free tier on ghcr.io/tektoncd.
All new Tekton releases are exclusively on ghcr.io/tektoncd. Old releases are also now available on ghcr.io/tektoncd.
Please migrate your old releases to ghcr.io immediately since we have limited funding to allocate to gcr.io egress.
To migrate you need to replace gcr.io/tekton-releases with ghcr.io/tektoncd in all your images. e.g.
# old
gcr.io/tekton-releases/github/tektoncd/pipeline/cmd/entrypoint:v0.9.2

# new
ghcr.io/tektoncd/github.com/tektoncd/pipeline/cmd/entrypoint:v0.9.2
You could run:
sed -e 's,gcr.io/tekton-releases,ghcr.io/tektoncd,g' 
End of Life Releases
We started enforcing the download of Tekton images from ghcr.io.
Here are the details of what changed:

Tekton LTS releases originally released to gcr.io will remain available on gcr.io until the EOL date.
The same images are available on ghcr.io, so we ask users to please update their manifests and download images from ghcr.io to save egress bandwidth costs.
Any EOL Tekton release will only be available on ghcr.io.
Images on gcr.io will be removed from public access over the next few days.
New releases will only be made on ghcr.io, so no action is required.

Please feel free to reach out to us via Slack or our mailing list if you have any questions.
The original info mails were sent on our mailing list and if youâ€™re in our google group you can read them here.

	

	
  
    â†Previous
  
  
    Nextâ†’\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Getting Started
  Get started with Tekton
	
		

  
    


  
    


  

		
	
	Welcome to Tekton. Tekton is an open-source cloud native CICD (Continuous
Integration and Continuous Delivery/Deployment) solution. Check the Concepts
section to learn more about how Tekton works.
Letâ€™s get started! You can go ahead and create your first task with
Tekton. If you prefer, watch the following
videos to learn the basics of how Tekton works before your first hands-on
experience:

  
  
    
    
    
      
        
            
    
  

        
      
    
  
  
    
      
        Tekton for Kubernetes explained
      
    
  



  
  
    
    
    
      
        
            
    
  

        
      
    
  
  
    
      
        Tekton CI/CD Pipelines overview
      
    
  



  
  
    
    
    
      
        
            
    
  

        
      
    
  
  
    
      
        Tekton Chains: extensibility, automation, and security
      
    
  



        
    
    
    
    
    
    
    
    
    
        
            
            
                
                    Getting started with Tasks
                
                Set up and run your first Tekton Task
            
        
            
            
                
                    Getting Started with Pipelines
                
                Create and run your first Tekton Pipeline
            
        
            
            
                
                    Getting Started with Triggers
                
                Create and run your first Tekton Trigger.
            
        
            
            
                
                    Getting Started Supply Chain Security
                
                Create and sign artifact provenance with Tekton Chains
            
        
    


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 29, 2023: Rework left-hand nav order (dbe0a93)\n\nGetting started with Tasks
                
                Set up and run your first Tekton Task
            
                
                    Getting Started with Pipelines
                
                Create and run your first Tekton Pipeline
            
                
                    Getting Started with Triggers
                
                Create and run your first Tekton Trigger.
            
                
                    Getting Started Supply Chain Security
                
                Create and sign artifact provenance with Tekton Chains\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Getting started with Tasks
	Set up and run your first Tekton Task
	
		

  
    


  
    


  

		
	    
	This tutorial shows you how to

Create a Kubernetes cluster with minikube.
Install Tekton pipelines.
Create a Task.
Use TaskRun to instantiate and run the Task.

Prerequisites


Install minikube. You only have
to complete the step 1, â€œInstallationâ€.


Install kubectl.


Create a Kubernetes cluster
Create a cluster:
minikube start --kubernetes-version v1.30.2
The process takes a few seconds, you see an output similar to the following,
depending on the minikube driver
that you are using:
ğŸ˜„  minikube v1.34.0 on Darwin 15.1.1 (arm64)
âœ¨  Automatically selected the docker driver
ğŸ“Œ  Using Docker Desktop driver with root privileges
ğŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
ğŸšœ  Pulling base image v0.0.45 ...
ğŸ’¾  Downloading Kubernetes v1.30.2 preload ...
    > preloaded-images-k8s-v18-v1...:  319.86 MiB / 319.86 MiB  100.00% 25.70 M
    > gcr.io/k8s-minikube/kicbase...:  441.45 MiB / 441.45 MiB  100.00% 19.30 M
        ğŸ”¥  Creating docker container (CPUs=2, Memory=4000MB) ...
ğŸ³  Preparing Kubernetes v1.30.2 on Docker 27.2.0 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring bridge CNI (Container Networking Interface) ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
You can check that the cluster was successfully created with kubectl:
kubectl cluster-info
The output confirms that Kubernetes is running:
Kubernetes control plane is running at https://127.0.0.1:39509
CoreDNS is running at
https://127.0.0.1:39509/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
Install Tekton Pipelines


To install the latest version of Tekton Pipelines, use kubectl:
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml


Monitor the installation:
kubectl get pods --namespace tekton-pipelines --watch
When both tekton-pipelines-controller and tekton-pipelines-webhook show
1/1 under the READY column, you are ready to continue. For example:
NAME                                           READY   STATUS              RESTARTS   AGE
tekton-pipelines-controller-6d989cc968-j57cs   0/1     Pending             0          3s
tekton-pipelines-webhook-69744499d9-t58s5      0/1     ContainerCreating   0          3s
tekton-pipelines-controller-6d989cc968-j57cs   0/1     ContainerCreating   0          3s
tekton-pipelines-controller-6d989cc968-j57cs   0/1     Running             0          5s
tekton-pipelines-webhook-69744499d9-t58s5      0/1     Running             0          6s
tekton-pipelines-controller-6d989cc968-j57cs   1/1     Running             0          10s
tekton-pipelines-webhook-69744499d9-t58s5      1/1     Running             0          20s
Hit Ctrl + C to stop monitoring.


Create and run a basic Task
A Task, represented in the API as an object of kind Task, defines a
series of Steps that run sequentially to perform logic that the Task
requires. Every Task runs as a pod on the Kubernetes cluster, with each step
running in its own container.


To create a Task, open your favorite editor and create a file named
hello-world.yaml with the following content:
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: hello
spec:
  steps:
    - name: echo
      image: alpine
      script: |
        #!/bin/sh
        echo "Hello World"        


Apply the changes to your cluster:
kubectl apply --filename hello-world.yaml
The output confirms that the Task was created successfully:
task.tekton.dev/hello created


A TaskRun object instantiates and executes this Task. Create another
file named hello-world-run.yaml with the following content:
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: hello-task-run
spec:
  taskRef:
    name: hello


Apply the changes to your cluster to launch the Task:
kubectl apply --filename hello-world-run.yaml


Verify that everything worked correctly:
kubectl get taskrun hello-task-run
The output of this command shows the status of the Task:
 NAME                    SUCCEEDED    REASON       STARTTIME   COMPLETIONTIME
 hello-task-run          True         Succeeded    22h         22h
The value True under SUCCEEDED confirms that TaskRun completed with no errors.


Take a look at the logs:
kubectl logs --selector=tekton.dev/taskRun=hello-task-run
The output displays the message:
Hello World


Cleanup
To learn about Tekton Pipelines, skip this section and proceed to the next
tutorial.
To delete the cluster that you created for this guide run:
minikube delete
The output confirms that the cluster was deleted:
ğŸ”¥  Deleting "minikube" in docker ...
ğŸ”¥  Deleting container "minikube" ...
ğŸ”¥  Removing /home/user/.minikube/machines/minikube ...
ğŸ’€  Removed all traces of the "minikube" cluster.
Further Reading:
We recommend that you complete Getting started with Pipelines.
For more complex examples see:

Clone a git repository with Tekton.
Build and push a container image with Tekton.


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified December 10, 2024: getting started with tasks: update log messages to use latest version (e6b6d31)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Getting Started with Pipelines
	Create and run your first Tekton Pipeline
	
		

  
    


  
    


  

		
	    
	This tutorial shows you how to:

Create two Tasks.
Create a Pipeline containing your Tasks.
Use PipelineRun to instantiate and run the Pipeline containing your Tasks.

This guide uses a local cluster with minikube.
Prerequisites


Complete the Getting started with Tasks
tutorial. Do not clean up your resources, skip the last section.


Install tkn, the Tekton CLI.


Create and run a second Task
You already have a â€œHello World!â€ Task. To create a second â€œGoodbye!â€
Task:


Create a new file named  goodbye-world.yaml and add the following
content:
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: goodbye
spec:
  params:
  - name: username
    type: string
  steps:
    - name: goodbye
      image: ubuntu
      script: |
        #!/bin/bash
        echo "Goodbye $(params.username)!"        
This Task takes one parameter, username. Whenever this Task is used a
value for that parameter must be passed to the Task.


Apply the Task file:
kubectl apply --filename goodbye-world.yaml


When a Task is part of a Pipeline, Tekton creates a TaskRun object for every
task in the Pipeline.
Create and run a Pipeline
A Pipeline defines an ordered series of Tasks arranged in a specific
execution order as part of the CI/CD workflow.
In this section you are going to create your first Pipeline, that will include
both the â€œHello World!â€ and â€œGoodbye!â€ Tasks.


Create a new file named  hello-goodbye-pipeline.yaml and add the following
content:
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: hello-goodbye
spec:
  params:
  - name: username
    type: string
  tasks:
    - name: hello
      taskRef:
        name: hello
    - name: goodbye
      runAfter:
        - hello
      taskRef:
        name: goodbye
      params:
      - name: username
        value: $(params.username)
The Pipeline defines the parameter username, which is then passed to the
goodbye Task.


Apply the Pipeline configuration to your cluster:
kubectl apply --filename hello-goodbye-pipeline.yaml


A PipelineRun, represented in the API as an object of kind
PipelineRun, sets the value for the parameters and executes a Pipeline. To
create  PipelineRun, create a new file named
hello-goodbye-pipeline-run.yaml with the following:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: hello-goodbye-run
spec:
  pipelineRef:
    name: hello-goodbye
  params:
  - name: username
    value: "Tekton"
This sets the actual value for the username parameter: "Tekton".


Start the Pipeline by applying the PipelineRun configuration to your
cluster:
kubectl apply --filename hello-goodbye-pipeline-run.yaml
You see the following output:
pipelinerun.tekton.dev/hello-goodbye-run created
Tekton now starts running the Pipeline.


To see the logs of the PipelineRun, use the following command:
tkn pipelinerun logs hello-goodbye-run -f -n default
The output shows both Tasks completed successfully:
[hello : echo] Hello World!

[goodbye : goodbye] Goodbye Tekton!



Cleanup
To learn about Tekton Triggers, skip this section and proceed to the
next tutorial.
To delete the cluster that you created for this guide run:
minikube delete
The output confirms that the cluster was deleted:
ğŸ”¥  Deleting "minikube" in docker ...
ğŸ”¥  Deleting container "minikube" ...
ğŸ”¥  Removing /home/user/.minikube/machines/minikube ...
ğŸ’€  Removed all traces of the "minikube" cluster.

Further reading
We recommend that you complete Getting started with Triggers.
For more complex examples check:

Clone a git repository with Tekton.
Build and push a container image with Tekton.


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified October 12, 2023: Update pipelines.md to reflect actual observed pipeline output (b73edbb)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Getting Started with Triggers
	Create and run your first Tekton Trigger.
	
		

  
    


  
    


  

		
	    
	This tutorial shows you how to

Install Tekton Triggers.
Create a TriggerTemplate.
Create a TriggerBind.
Create an EventListener.

This guide uses a local cluster with minikube.
Before you begin


Complete the two previous Getting Started tutorials: Tasks and
Pipelines. Do not clean up your resources, skip the last
section.


Install curl if itâ€™s not already available on your system.


Overview
You can use Tekton Triggers to modify the behavior of your CI/CD Pipelines
depending on external events. The basic implementation you are going to create
in this guide comprises three main components:


An EventListener object that listens to the world waiting for â€œsomethingâ€
to happen.


A TriggerTemplate object, which configures a PipelineRun when this
event occurs.


A TriggerBinding object, that passes the data to the PipelineRun created
by the TriggerTemplate object.



An optional ClusterInterceptor object can be added to validate and process
event data.
You are going to create a Tekton Trigger to run the hello-goodbye Pipeline
when the EventListener detects an event.
Install Tekton Triggers


Use kubectl to install Tekton Triggers:
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.yaml


Monitor the installation:
kubectl get pods --namespace tekton-pipelines --watch
When tekton-triggers-controller, tekton-triggers-webhook, and
tekton-triggers-core-interceptors show 1/1 under the READY column, you
are ready to continue. For example:
NAME                                                 READY   STATUS    RESTARTS        AGE
tekton-pipelines-controller-68b8d87687-8mzvt         1/1     Running   2 (4d13h ago)   4d19h
tekton-pipelines-webhook-6fb6dd6d75-7jfz6            1/1     Running   2 (104s ago)    4d19h
tekton-triggers-controller-74b654c6bc-24ds7          1/1     Running   2 (104s ago)    4d19h
tekton-triggers-core-interceptors-79f4dbb969-sk2dk   1/1     Running   3 (104s ago)    4d19h
tekton-triggers-webhook-56885c9875-nx499             1/1     Running   2 (104s ago)    4d19h
Hit Ctrl + C to stop monitoring.


Create a TriggerTemplate
A TriggerTemplate defines what happens when an event is detected.


Create a new file named trigger-template.yaml and add the following:
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerTemplate
metadata:
  name: hello-template
spec:
  params:
  - name: username
    default: "Kubernetes"
  resourcetemplates:
  - apiVersion: tekton.dev/v1beta1
    kind: PipelineRun
    metadata:
      generateName: hello-goodbye-run-
    spec:
      pipelineRef:
        name: hello-goodbye
      params:
      - name: username
        value: $(tt.params.username)
The PipelineRun object that you created in the previous tutorial is now
included in the template declaration. This trigger expects the username
parameter to be available; if itâ€™s not, it assigns a default value:
â€œKubernetesâ€.


Apply the TriggerTemplate to your cluster:
kubectl apply -f trigger-template.yaml


Create a TriggerBinding
A TriggerBinding executes the TriggerTemplate, the same way you had to create a
PipelineRun to execute the Pipeline.


Create a file named trigger-binding.yaml with the following content:
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerBinding
metadata:
  name: hello-binding
spec: 
  params:
  - name: username
    value: $(body.username)
This TriggerBinding gets some information and saves it in the username
variable.


Apply the TriggerBinding:
kubectl apply -f trigger-binding.yaml


Create an EventListener
The EventListener object encompasses both the TriggerTemplate and the
TriggerBinding.


Create a file named event-listener.yaml and add the following:
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: hello-listener
spec:
  serviceAccountName: tekton-robot
  triggers:
    - name: hello-trigger 
      bindings:
      - ref: hello-binding
      template:
        ref: hello-template
This declares that when an event is detected, it will run the TriggerBinding
and the TriggerTemplate.


The EventListener requires a service account to run. To create the service
account for this example create a file named rbac.yaml and add the
following:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tekton-robot
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: triggers-example-eventlistener-binding
subjects:
- kind: ServiceAccount
  name: tekton-robot
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tekton-triggers-eventlistener-roles
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: triggers-example-eventlistener-clusterbinding
subjects:
- kind: ServiceAccount
  name: tekton-robot
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tekton-triggers-eventlistener-clusterroles
This service account allows the EventListener to create PipelineRuns.


Apply the file to your cluster:
kubectl apply -f rbac.yaml


Running the Trigger
You have everything you need to run this Trigger and start listening for events.


Create the EventListener:
kubectl apply -f event-listener.yaml


To communicate outside the cluster, enable port-forwarding:
kubectl port-forward service/el-hello-listener 8080
The output confirms that port-forwarding is working:
kubectl port-forward service/el-hello-listener 8080
Forwarding from 127.0.0.1:8080 -> 8080
Forwarding from [::1]:8080 -> 8080    
Keep this service running, donâ€™t close the terminal.


Monitor the Trigger
Now that the EventListener is running, you can send an event and see what
happens:


Open a new terminal and submit a payload to the cluster:
curl -v \
   -H 'content-Type: application/json' \
   -d '{"username": "Tekton"}' \
   http://localhost:8080
You can change â€œTektonâ€ for any string you want. This value will be
ultimately read by the goodbye-world Task.
The response is successful:
< HTTP/1.1 202 Accepted
< Content-Type: application/json
< Date: Fri, 30 Sep 2022 00:11:19 GMT
< Content-Length: 164
<
{"eventListener":"hello-listener","namespace":"default","eventListenerUID":"35dd0858-3692-4bb5-8c4f-1bf6d705bb73","eventID":"1a0a1120-7833-4078-9f30-0e3688f27dde"}
* Connection #0 to host localhost left intact


This event triggers a PipelineRun, check the PipelineRuns on your
cluster :
kubectl get pipelineruns
The output confirms the pipeline is working:
NAME                      SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME
hello-goodbye-run         True        Succeeded   24m         24m
hello-goodbye-run-8hckl   True        Succeeded   81s         72s
You see two PipelineRuns, the first one created in the previous guide,
the last one was created by the Trigger.


Check the PipelineRun logs. The name is auto-generated adding a suffix
for every run, in this case itâ€™s hello-goodbye-run-8hckl. Use your own
PiepelineRun name in the following command to see the logs:
tkn pipelinerun logs <my-pipeline-run> -f
And you get the expected output:
[hello : echo] Hello World

[goodbye : goodbye] Goodbye Tekton!
Both Tasks completed successfuly. Congratulations!


Clean up


Press Ctrl + C in the terminal running the port-forwarding process to stop
it.


Delete the cluster:
minikube delete
The output confirms that the cluster was deleted:
ğŸ”¥  Deleting "minikube" in docker ...
ğŸ”¥  Deleting container "minikube" ...
ğŸ”¥  Removing /home/user/.minikube/machines/minikube ...
ğŸ’€  Removed all traces of the "minikube" cluster.


Further reading
For more complex Pipelines examples check:

Clone a git repository with Tekton.
Build and push a container image with Tekton.

You can find more Tekton Triggers examples on the Triggers GitHub
repository.

	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 29, 2023: Rework left-hand nav order (dbe0a93)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Getting Started Supply Chain Security
	Create and sign artifact provenance with Tekton Chains
	
		

  
    


  
    


  

		
	    
	This guide shows you how to:

Create a Pipeline to build and push a container image to a local registry.
Record and sign provenance of the image.
Read back the provenance information.
Verify the signature.

Prerequisites

Install minikube. Only complete the step 1, â€œInstallationâ€.
Install kubectl.
Install tkn, the Tekton CLI.
Install jq.
Install cosign.

Start minikube with a local registry enabled


Delete any previous clusters:
minikube delete


Start up minikube with insecure registry enabled:
minikube start --insecure-registry "10.0.0.0/24"
The process takes a few seconds, you see an output similar to the following,
depending on the minikube driver
that you are using:
ğŸ˜„  minikube v1.29.0
âœ¨  Automatically selected the docker driver. Other choices: none, ssh
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”¥  Creating docker container (CPUs=2, Memory=7900MB) ...
ğŸ³  Preparing Kubernetes v1.26.1 on Docker 20.10.23 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring bridge CNI (Container Networking Interface) ...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ”  Verifying Kubernetes components...
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


Enable the local registry plugin:
minikube addons enable registry 
The output confirms that the registry plugin is enabled:
ğŸ’¡  registry is an addon maintained by Google. For any concerns contact minikube on GitHub.
You can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS
    â–ª Using image docker.io/registry:2.8.1
    â–ª Using image gcr.io/google_containers/kube-registry-proxy:0.4
ğŸ”  Verifying registry addon...
ğŸŒŸ  The 'registry' addon is enabled


Now you can push images to a registry within your minikube cluster.
Install and configure the necessary Tekton components


Install Tekton Pipelines:
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml


Monitor the installation:
kubectl get po -n tekton-pipelines -w
When both tekton-pipelines-controller and tekton-pipelines-webhook show 1/1
under the READY column, you are ready to continue. For example:
NAME                                          READY   STATUS    RESTARTS   AGE
tekton-pipelines-controller-9675574d7-sxtm4   1/1     Running   0          2m28s
tekton-pipelines-webhook-58b5cbb7dd-s6lfs     1/1     Running   0          2m28s
Hit Ctrl + C to stop monitoring.


Install Tekton Chains:
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/chains/latest/release.yaml


Monitor the installation
kubectl get po -n tekton-chains -w
When tekton-chains-controller shows 1/1 under the READY column, you
are ready to continue. For example:
NAME                                        READY   STATUS    RESTARTS   AGE
tekton-chains-controller-57dcc994b9-vs2f2   1/1     Running   0          2m23s
Hit Ctrl + C to stop monitoring.


Configure Tekton Chains to store the provenance metadata locally:
kubectl patch configmap chains-config -n tekton-chains \
-p='{"data":{"artifacts.oci.storage": "", "artifacts.taskrun.format":"in-toto", "artifacts.taskrun.storage": "tekton"}}'
The output confirms that the configuration was updated successfully:
configmap/chains-config patched


Generate a key pair to sign the artifact provenance:
cosign generate-key-pair k8s://tekton-chains/signing-secrets
You are prompted to enter a password for the private key. For this guide,
leave the password empty and press Enter twice. A public key, cosign.pub,
is created in your current directory.


Build and push a container image


Create a file called pipeline.yaml and add the following:
  apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: build-push
spec:
  params:
  - name: image-reference
    type: string
  results:
  - name: image-ARTIFACT_OUTPUTS
    description: Built artifact.
    value:
      uri: $(tasks.kaniko-build.results.IMAGE_URL)
      digest: sha1:$(tasks.kaniko-build.results.IMAGE_DIGEST)
  workspaces:
  - name: shared-data
  tasks: 
  - name: dockerfile
    taskRef:
      name: create-dockerfile
    workspaces:
    - name: source
      workspace: shared-data
  - name: kaniko-build
    runAfter: ["dockerfile"]
    taskRef:
      name: kaniko
    workspaces:
    - name: source
      workspace: shared-data
    params:
    - name: IMAGE
      value: $(params.image-reference)
---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: create-dockerfile
spec:
  workspaces:
  - name: source
  steps:
  - name: add-dockerfile
    workingDir: $(workspaces.source.path)
    image: bash
    script: |
      cat <<EOF > $(workspaces.source.path)/Dockerfile
      FROM alpine:3.16
      RUN echo "hello world" > hello.log
      EOF      
---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: kaniko
  labels:
    app.kubernetes.io/version: "0.6"
  annotations:
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/categories: Image Build
    tekton.dev/tags: image-build
    tekton.dev/displayName: "Build and upload container image using Kaniko"
    tekton.dev/platforms: "linux/amd64,linux/arm64,linux/ppc64le"
spec:
  description: >-
    This Task builds a simple Dockerfile with kaniko and pushes to a registry.
    This Task stores the image name and digest as results, allowing Tekton Chains to pick up
    that an image was built & sign it.    
  params:
    - name: IMAGE
      description: Name (reference) of the image to build.
    - name: DOCKERFILE
      description: Path to the Dockerfile to build.
      default: ./Dockerfile
    - name: CONTEXT
      description: The build context used by Kaniko.
      default: ./
    - name: EXTRA_ARGS
      type: array
      default: []
    - name: BUILDER_IMAGE
      description: The image on which builds will run (default is v1.5.1)
      default: gcr.io/kaniko-project/executor:v1.5.1@sha256:c6166717f7fe0b7da44908c986137ecfeab21f31ec3992f6e128fff8a94be8a5
  workspaces:
    - name: source
      description: Holds the context and Dockerfile
    - name: dockerconfig
      description: Includes a docker `config.json`
      optional: true
      mountPath: /kaniko/.docker
  results:
    - name: IMAGE_DIGEST
      description: Digest of the image just built.
    - name: IMAGE_URL
      description: URL of the image just built.
  steps:
    - name: build-and-push
      workingDir: $(workspaces.source.path)
      image: $(params.BUILDER_IMAGE)
      args:
        - $(params.EXTRA_ARGS)
        - --dockerfile=$(params.DOCKERFILE)
        - --context=$(workspaces.source.path)/$(params.CONTEXT) # The user does not need to care the workspace and the source.
        - --destination=$(params.IMAGE)
        - --digest-file=$(results.IMAGE_DIGEST.path)
      # kaniko assumes it is running as root, which means this example fails on platforms
      # that default to run containers as random uid (like OpenShift). Adding this securityContext
      # makes it explicit that it needs to run as root.
      securityContext:
        runAsUser: 0
    - name: write-url
      image: docker.io/library/bash:5.1.4@sha256:c523c636b722339f41b6a431b44588ab2f762c5de5ec3bd7964420ff982fb1d9
      script: |
        set -e
        image="$(params.IMAGE)"
        echo -n "${image}" | tee "$(results.IMAGE_URL.path)"        



Get your cluster IPs:
kubectl get service --namespace kube-system
This shows the IPs of the services on your cluster:
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   48m
registry   ClusterIP   10.101.134.48   <none>        80/TCP,443/TCP           47m
Save your registry IP, in this case 10.101.134.48, for the next step.


Create a file called pipelinerun.yaml and add the following:
  apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: build-push-run-
spec: 
  pipelineRef:
    name: build-push
  params:
  - name: image-reference
    value: <registry-ip>/tekton-test
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi



Replace <registry-ip> with the value from the previous step.


Apply the Pipeline to your cluster:
kubectl apply -f pipeline.yaml
You see the following output:
pipeline.tekton.dev/build-push created
task.tekton.dev/create-dockerfile created
task.tekton.dev/kaniko created


Run the Pipeline:
kubectl create -f pipelinerun.yaml
A new PipelineRun with a unique name is created:
pipelinerun.tekton.dev/build-push-run-q22b5 created 


Use the PipelineRun name, build-push-run-q22b5 , to monitor the
execution:
tkn pr logs build-push-run-q22b5 -f
The output shows the Pipeline completed successfully:
[kaniko-build : build-and-push] INFO[0000] Retrieving image manifest alpine:3.16
[kaniko-build : build-and-push] INFO[0000] Retrieving image alpine:3.16 from registry index.docker.io
[kaniko-build : build-and-push] INFO[0000] Built cross stage deps: map[]
[kaniko-build : build-and-push] INFO[0000] Retrieving image manifest alpine:3.16
[kaniko-build : build-and-push] INFO[0000] Returning cached image manifest
[kaniko-build : build-and-push] INFO[0000] Executing 0 build triggers
[kaniko-build : build-and-push] INFO[0000] Unpacking rootfs as cmd RUN echo "hello world" > hello.log requires it.
[kaniko-build : build-and-push] INFO[0000] RUN echo "hello world" > hello.log
[kaniko-build : build-and-push] INFO[0000] Taking snapshot of full filesystem...
[kaniko-build : build-and-push] INFO[0000] cmd: /bin/sh
[kaniko-build : build-and-push] INFO[0000] args: [-c echo "hello world" > hello.log]
[kaniko-build : build-and-push] INFO[0000] Running: [/bin/sh -c echo "hello world" > hello.log]
[kaniko-build : build-and-push] INFO[0000] Taking snapshot of full filesystem...
[kaniko-build : build-and-push] INFO[0000] Pushing image to 10.101.134.48/tekton-test
[kaniko-build : build-and-push] INFO[0001] Pushed image to 1 destinations

[kaniko-build : write-url] 10.101.134.48/tekton-test


Retrieve and verify the artifact provenance
Tekton Chains silently monitored the execution of the PipelineRun. It recorded
and signed the provenance metadata, information about the container that the
PipelineRun built and pushed.


Get the PipelineRun UID:
export PR_UID=$(tkn pr describe --last -o  jsonpath='{.metadata.uid}')


Fetch the metadata and store it in a JSON file:
tkn pr describe --last \
-o jsonpath="{.metadata.annotations.chains\.tekton\.dev/signature-pipelinerun-$PR_UID}" \
| base64 -d > metadata.json


View the provenance:
cat metadata.json | jq -r '.payload' | base64 -d | jq .
The output contains a detailed description of the build:
  {
  "_type": "https://in-toto.io/Statement/v0.1",
  "predicateType": "https://slsa.dev/provenance/v0.2",
  "subject": null,
  "predicate": {
    "builder": {
      "id": "https://tekton.dev/chains/v2"
    },
    "buildType": "tekton.dev/v1beta1/PipelineRun",
    "invocation": {
      "configSource": {},
      "parameters": {
        "image-reference": "10.101.124.48/tekton-test"
      },
      "environment": {
        "labels": {
          "tekton.dev/pipeline": "build-push"
        }
      }
    },
    "buildConfig": {
      "tasks": [
        {
          "name": "dockerfile",
          "ref": {
            "name": "create-dockerfile",
            "kind": "Task"
          },
          "startedOn": "2023-04-18T04:23:16Z",
          "finishedOn": "2023-04-18T04:23:24Z",
          "status": "Succeeded",
          "steps": [
            {
              "entryPoint": "cat <<EOF > Dockerfile\nFROM alpine:3.16\nRUN echo \"hello world\" > hello.log\nEOF\n",
              "arguments": null,
              "environment": {
                "container": "add-dockerfile",
                "image": "docker-pullable://bash@sha256:e0acf0b8fb59c01b6a2b66de360c86bcad5c3cd114db325155970e6bab9663a0"
              },
              "annotations": null
            }
          ],
          "invocation": {
            "configSource": {},
            "parameters": {},
            "environment": {
              "annotations": {
                "pipeline.tekton.dev/affinity-assistant": "affinity-assistant-fd643aaebc",
                "pipeline.tekton.dev/release": "086e76a"
              },
              "labels": {
                "app.kubernetes.io/managed-by": "tekton-pipelines",
                "tekton.dev/memberOf": "tasks",
                "tekton.dev/pipeline": "build-push",
                "tekton.dev/pipelineRun": "build-push-run-7gn4d",
                "tekton.dev/pipelineTask": "dockerfile",
                "tekton.dev/task": "create-dockerfile"
              }
            }
          }
        },
        {
          "name": "kaniko-build",
          "after": [
            "dockerfile"
          ],
          "ref": {
            "name": "kaniko",
            "kind": "Task"
          },
          "startedOn": "2023-04-18T04:23:24Z",
          "finishedOn": "2023-04-18T04:23:32Z",
          "status": "Succeeded",
          "steps": [
            {
              "entryPoint": "",
              "arguments": [
                "--dockerfile=./Dockerfile",
                "--context=/workspace/source/./",
                "--destination=10.101.124.48/tekton-test",
                "--digest-file=/tekton/results/IMAGE_DIGEST"
              ],
              "environment": {
                "container": "build-and-push",
                "image": "docker-pullable://gcr.io/kaniko-project/executor@sha256:c6166717f7fe0b7da44908c986137ecfeab21f31ec3992f6e128fff8a94be8a5"
              },
              "annotations": null
            },
            {
              "entryPoint": "set -e\nimage=\"10.101.124.48/tekton-test\"\necho -n \"${image}\" | tee \"/tekton/results/IMAGE_URL\"\n",
              "arguments": null,
              "environment": {
                "container": "write-url",
                "image": "docker-pullable://bash@sha256:c523c636b722339f41b6a431b44588ab2f762c5de5ec3bd7964420ff982fb1d9"
              },
              "annotations": null
            }
          ],
          "invocation": {
            "configSource": {},
            "parameters": {
              "BUILDER_IMAGE": "gcr.io/kaniko-project/executor:v1.5.1@sha256:c6166717f7fe0b7da44908c986137ecfeab21f31ec3992f6e128fff8a94be8a5",
              "CONTEXT": "./",
              "DOCKERFILE": "./Dockerfile",
              "EXTRA_ARGS": [],
              "IMAGE": "10.101.124.48/tekton-test"
            },
            "environment": {
              "annotations": {
                "pipeline.tekton.dev/affinity-assistant": "affinity-assistant-fd643aaebc",
                "pipeline.tekton.dev/release": "086e76a",
                "tekton.dev/categories": "Image Build",
                "tekton.dev/displayName": "Build and upload container image using Kaniko",
                "tekton.dev/pipelines.minVersion": "0.17.0",
                "tekton.dev/platforms": "linux/amd64,linux/arm64,linux/ppc64le",
                "tekton.dev/tags": "image-build"
              },
              "labels": {
                "app.kubernetes.io/managed-by": "tekton-pipelines",
                "app.kubernetes.io/version": "0.6",
                "tekton.dev/memberOf": "tasks",
                "tekton.dev/pipeline": "build-push",
                "tekton.dev/pipelineRun": "build-push-run-7gn4d",
                "tekton.dev/pipelineTask": "kaniko-build",
                "tekton.dev/task": "kaniko"
              }
            }
          },
          "results": [
            {
              "name": "IMAGE_DIGEST",
              "type": "string",
              "value": "sha256:423d2382809c377fcf3a890316769852a6d298e760b34d784dc0222ec7630de3"
            },
            {
              "name": "IMAGE_URL",
              "type": "string",
              "value": "10.101.124.48/tekton-test"
            }
          ]
        }
      ]
    },
    "metadata": {
      "buildStartedOn": "2023-04-18T04:23:16Z",
      "buildFinishedOn": "2023-04-18T04:23:32Z",
      "completeness": {
        "parameters": false,
        "environment": false,
        "materials": false
      },
      "reproducible": false
    },
    "materials": [
      {
        "uri": "docker-pullable://bash",
        "digest": {
          "sha256": "c523c636b722339f41b6a431b44588ab2f762c5de5ec3bd7964420ff982fb1d9"
        }
      },
      {
        "uri": "docker-pullable://bash",
        "digest": {
          "sha256": "e0acf0b8fb59c01b6a2b66de360c86bcad5c3cd114db325155970e6bab9663a0"
        }
      },
      {
        "uri": "docker-pullable://gcr.io/kaniko-project/executor",
        "digest": {
          "sha256": "c6166717f7fe0b7da44908c986137ecfeab21f31ec3992f6e128fff8a94be8a5"
        }
      }
    ]
  }
}



To verify that the metadata hasnâ€™t been tampered with, check the signature
with cosign:
cosign verify-blob-attestation --insecure-ignore-tlog \
--key k8s://tekton-chains/signing-secrets --signature metadata.json \
--type slsaprovenance --check-claims=false /dev/null
The output confirms that the signature is valid:
Verified OK


Further reading

Learn about Tekton Chains and Supply Chain Security.
Getting To SLSA Level 2 with Tekton and Tekton Chains blog post.
Check more examples on the Tekton Chains repository.


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 27, 2025: Fix doc error in Getting Started Supply Chain Security guide (1353cee)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Installation
  Installation instructions for Tekton components
	
		

  
    


  
    


  

		
	
	This section contains guides to install the latest version of the Tekton
components. For other versions, select the component and version from the
drop-down menu in the top right corner, then navigate to the corresponding
installation document.

        
    
    
    
    
    
    
    
    
    
        
            
            
                
                    Local Kubernetes cluster
                
                Set up a Kubernetes cluster on your computer to test Tekton.
            
        
            
            
                
                    Install Tekton Pipelines
                
                Install Tekton Pipelines on your cluster
            
        
            
            
                
                    Install and set up Tekton Triggers
                
                Install Tekton Triggers on your cluster
            
        
            
            
                
                    Additional Configuration Options
                
                Additional configurations when installing Tekton Pipelines
            
        
    


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 29, 2023: Rework left-hand nav order (dbe0a93)\n\nLocal Kubernetes cluster
                
                Set up a Kubernetes cluster on your computer to test Tekton.
            
                
                    Install Tekton Pipelines
                
                Install Tekton Pipelines on your cluster
            
                
                    Install and set up Tekton Triggers
                
                Install Tekton Triggers on your cluster
            
                
                    Additional Configuration Options
                
                Additional configurations when installing Tekton Pipelines\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Local Kubernetes cluster
	Set up a Kubernetes cluster on your computer to test Tekton.
	
		

  
    


  
    


  

		
	    
	There are several tools to run a local Kubernetes cluster on your computer. The
Tekton documentation often includes instructions for either minikube or
kind.
See the corresponding minikube and kind documentation to learn how to
install and set up a cluster on your computer.
You can find some additional resources on the Tekton repositories:


Instructions to run both tools with a local registry are available on the
Pipelines repository.


You can find some convenience scripts to run Tekton components with kind on
the plumbing repository.


Further reading

Getting started with Tasks


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified September 27, 2022: Create installation section (e76e5b8)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Install Tekton Pipelines
	Install Tekton Pipelines on your cluster
	
		

  
    


  
    


  

		
	    
	

  




	






  






  



   
This page includes content about running Tekton with specific platforms and
cloud providers. The accuracy and freshness of this vendor documentation varies
by vendor.
If you want to contribute with platform-specific documentation, follow the
vendor contributions guidelines.



This guide explains how to install Tekton Pipelines.
Prerequisites

A Kubernetes cluster running version 1.28 or later.
Kubectl.
Grant cluster-admin privileges to the current user. See the Kubernetes
role-based access control (RBAC) docs for more information.
(Optional) Install a Metrics Server if you need support for high
availability use cases.

See the local installation guide if you want to test Tekton on
your computer.
Installation










      Kubernetes
    

    
      
    
      Google Cloud
    

    
      
    
      OpenShift
    

    
      
    





        To install Tekton Pipelines on a Kubernetes cluster:


Run one of the following commands depending on which version of Tekton
Pipelines you want to install:


Latest official release:
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
Note: These instructions are ideal as a quick start installation guide with Tekton Pipelines and not meant for the production use. Please refer to the operator to install, upgrade and manage Tekton projects.


Nightly release:
kubectl apply --filename https://storage.googleapis.com/tekton-releases-nightly/pipeline/latest/release.yaml


Specific release:
 kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/previous/<version_number>/release.yaml
Replace <version_number> with the numbered version you want to install.
For example, v0.26.0.


Untagged release:
If your container runtime does not support image-reference:tag@digest:
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.notags.yaml




Multi-tenant installation is only partially supported today, read the guide
for reference.


Monitor the installation:
kubectl get pods --namespace tekton-pipelines --watch
When all components show 1/1 under the READY column, the installation is
complete. Hit Ctrl + C to stop monitoring.


Congratulations! You have successfully installed Tekton Pipelines on your
Kubernetes cluster.


    
      
    

  
          Before you proceed, create or select a project on Google Cloud and install the
gcloud CLI on your computer.
To install Tekton Pipelines:


Enable the Google Kubernetes Engine (GKE) API:
gcloud services enable container.googleapis.com


Create a cluster with Workload Identity enabled. For example:
gcloud container clusters create tekton-cluster \
  --num-nodes=<nodes> \
  --region=<location> \
  --workload-pool=<project-id>.svc.id.goog
Where:


<location> is the cluster location. For example, us-central1.
See the documentation about regional and zonal
clusters for more information.


<project-id> is the project ID.


<nodes> is the number of nodes.


Workload Identity allows your GKE cluster to access Google Cloud services
using an Identity Access Management (IAM) service account.
For example, the Tekton build and push guide explains how to
authenticate to Artifact Registry on a cluster with Workload Identity
enabled.
You can also enable Workload Idenitity on an existing cluster.


Follow the regular Kubernetes installation steps.


Private clusters
If you are running a private cluster and experience problems
with GKE DNS resolution, allow the port 8443 in your firewall
rules.
gcloud compute firewall-rules update <firewall_rule_name> --allow tcp:8443
See the documentation about firewall rules for private
clusters for more information.
Autopilot
If you are using Autopilot mode on your GKE cluster and
experience some problems, try the following:


Allow port 8443 in your firewall rules.
gcloud compute firewall-rules update <firewall_rule_name> --allow tcp:8443


Disable the affinity assistant.
kubectl patch cm feature-flags -n tekton-pipelines \
  -p '{"data":{"disable-affinity-assistant":"true"}}'


Increase the ephemeral storage.




    
      
    

  
          To install Tekton Pipelines on OpenShift, you must first apply the anyuid
security context constraint to the tekton-pipelines-controller service
account. This is required to run the webhook Pod.  See Security Context
Constraints for more information.


Log on as a user with cluster-admin privileges. The following example
uses the default system:admin user:
oc login -u system:admin


Set up the namespace (project) and configure the service account:
oc new-project tekton-pipelines
oc adm policy add-scc-to-user anyuid -z tekton-pipelines-controller
oc adm policy add-scc-to-user anyuid -z tekton-pipelines-webhook


Install Tekton Pipelines:
Because OpenShift uses random user id (and user id range per namespace) for pods, we need to remove the securityContext.runAsUser and securityContext.runAsGroup from any container from the release.yaml.
You will need to have yq installed for this to work. Another way would be to download the yaml, search and replace (here replace with nothing) in your favourite editor.
curl https://storage.googleapis.com/tekton-releases/pipeline/latest/release.notags.yaml | yq 'del(.spec.template.spec.containers[].securityContext.runAsUser, .spec.template.spec.containers[].securityContext.runAsGroup)' | oc apply -f -
See the OpenShift CLI documentation for more information on
the oc command.


Monitor the installation using the following command until all components
show a Running status:
oc get pods --namespace tekton-pipelines --watch
Note: Hit CTRL + C to stop monitoring.


Congratulations! You have successfully installed Tekton Pipelines on your
OpenShift environment.
To run OpenShift 4.x on your laptop (or desktop), take a look at Red Hat
CodeReady Containers.


    
      
    

  


Additional configuration options
You can enable additional alpha and beta features, customize execution
parameters, configure availability, and many more options. See the
addition configurations options for more information.
Next steps
To get started with Tekton check the Introductory tutorials,
the how-to guides, and the examples folder.






  
  
    
    
    
      
        
          Source for this document available on GitHub.
        
      
    
  
  



	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified February 15, 2023: Add release selector to docs (17ee7b8)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Install and set up Tekton Triggers
	Install Tekton Triggers on your cluster
	
		

  
    


  
    


  

		
	    
	  
This document shows you how to install and set up Tekton Triggers.
Prerequisites

Kubernetes cluster version 1.18 or later.
Kubectl.
Tekton Pipelines.
Grant cluster-admin privileges to the user that installed Tekton Pipelines. See
the kubernetes role-based access control (RBAC) docs.

Installation


Log on to your Kubernetes cluster with the same user account that installed
Tekton Pipelines.


Depending on which version of Tekton Triggers you want to install, run one
of the following commands:


Latest official release
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.yaml


Nightly release
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases-nightly/triggers/latest/release.yaml
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases-nightly/triggers/latest/interceptors.yaml


Specific Release
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/previous/VERSION_NUMBER/release.yaml
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/previous/VERSION_NUMBER/interceptors.yaml
Replace VERSION_NUMBER with the numbered version you want to install.
For example, v0.19.1.


Untagged Release
If your container runtime does not support image-reference:tag@digest (for
example, cri-o used in OpenShift 4.x):
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/latest/release.notags.yaml
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.notags.yaml




To monitor the installation, run:
kubectl get pods --namespace tekton-pipelines --watch
When all components show 1/1 under the READY column, the installation is
complete. Hit Ctrl + C to stop monitoring.


Customization options
You can customize the behavior of the Triggers Controller changing some values
in the config/feature-flags-triggers.yaml file.


Enable alpha features. Set the value of enable-api-fields: to "alpha", the
default value is "stable". This flag only applies to the v1beta1 API
version.


Exclude labels. Set the labels-exclusion-pattern: field to a regex  pattern.
Labels that match this pattern are excluded from getting added to the
resources created by the EventListener. By default this field is empty, so all
labels added to the EventListener are propagated down.


Further reading

Get started with Tekton Triggers
Explore Tekton Triggers code examples



  
  
    
    
    
      
        
          Source for this document available on GitHub.
        
      
    
  
  



	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified September 27, 2022: Create installation section (e76e5b8)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Additional Configuration Options
	Additional configurations when installing Tekton Pipelines
	
		

  
    


  
    


  

		
	    
	  
This document describes additional options to configure your Tekton Pipelines
installation.
Table of Contents

Configuring built-in remote Task and Pipeline resolution
Configuring CloudEvents notifications
Configuring self-signed cert for private registry
Configuring environment variables
Customizing basic execution parameters

Customizing the Pipelines Controller behavior
Alpha Features
Beta Features


Enabling larger results using sidecar logs
Configuring High Availability
Configuring tekton pipeline controller performance
Platform Support
Creating a custom release of Tekton Pipelines
Verify Tekton Pipelines Release

Verify signatures using cosign
Verify the transparency logs using rekor-cli


Verify Tekton Resources
Pipelinerun with Affinity Assistant
TaskRuns with imagePullBackOff Timeout
Disabling Inline Spec in TaskRun and PipelineRun
Next steps

Configuring built-in remote Task and Pipeline resolution
Four remote resolvers are currently provided as part of the Tekton Pipelines installation.
By default, these remote resolvers are enabled. Each resolver can be disabled by setting
the appropriate feature flag in the resolvers-feature-flags ConfigMap in the tekton-pipelines-resolvers
namespace:

The bundles resolver, disabled by setting the enable-bundles-resolver
feature flag to false.
The git resolver, disabled by setting the enable-git-resolver
feature flag to false.
The hub resolver, disabled by setting the enable-hub-resolver
feature flag to false.
The cluster resolver, disabled by setting the enable-cluster-resolver
feature flag to false.

Configuring CloudEvents notifications
When configured so, Tekton can generate CloudEvents for TaskRun,
PipelineRun and CustomRunlifecycle events. The main configuration parameter is the
URL of the sink. When not set, no notification is generated.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-events
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  formats: tektonv1
  sink: https://my-sink-url
The sink used to be configured in the config-defaults config map.
This option is still available, but deprecated, and will be removed.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  default-cloud-events-sink: https://my-sink-url
Additionally, CloudEvents for CustomRuns require an extra configuration to be
enabled. This setting exists to avoid collisions with CloudEvents that might
be sent by custom task controllers:
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  send-cloudevents-for-runs: true
Configuring self-signed cert for private registry
The SSL_CERT_DIR is set to /etc/ssl/certs as the default cert directory. If you are using a self-signed cert for private registry and the cert file is not under the default cert directory, configure your registry cert in the config-registry-cert ConfigMap with the key cert.
Configuring environment variables
Environment variables can be configured in the following ways, mentioned in order of precedence from lowest to highest.

Implicit environment variables
Step/StepTemplate environment variables
Environment variables specified via a default PodTemplate.
Environment variables specified via a PodTemplate.

The environment variables specified by a PodTemplate supercedes all other ways of specifying environment variables. However, there exists a configuration i.e. default-forbidden-env, the environment variable specified in this list cannot be updated via a PodTemplate.
For example:
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
data:
  default-timeout-minutes: "50"
  default-service-account: "tekton"
  default-forbidden-env: "TEST_TEKTON"
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: mytask
  namespace: default
spec:
  steps:
    - name: echo-env
      image: ubuntu
      command: ["bash", "-c"]
      args: ["echo $TEST_TEKTON "]
      env:
          - name: "TEST_TEKTON"
            value: "true"
---
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: mytaskrun
  namespace: default
spec:
  taskRef:
    name: mytask
  podTemplate:
    env:
        - name: "TEST_TEKTON"
          value: "false"
In the above example the environment variable TEST_TEKTON will not be overriden by value specified in podTemplate, because the config-default option default-forbidden-env is configured with value TEST_TEKTON.
Configuring default resources requirements
Resource requirements of containers created by the controller can be assigned default values. This allows to fully control the resources requirement of TaskRun.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
data:
  default-container-resource-requirements: |
    place-scripts: # updates resource requirements of a 'place-scripts' container
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"    
  
    prepare: # updates resource requirements of a 'prepare' container
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "256Mi"
        cpu: "500m"
  
    working-dir-initializer: # updates resource requirements of a 'working-dir-initializer' container
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "512Mi"
        cpu: "500m"
  
    prefix-scripts: # updates resource requirements of containers which starts with 'scripts-'
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  
    prefix-sidecar-scripts: # updates resource requirements of containers which starts with 'sidecar-scripts-'
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  
    default: # updates resource requirements of init-containers and containers which has empty resource resource requirements
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "256Mi"
        cpu: "500m"
Any resource requirements set at the Task and TaskRun levels will overidde the default one specified in the config-defaults configmap.
Customizing basic execution parameters
You can specify your own values that replace the default service account (ServiceAccount), timeout (Timeout), resolver (Resolver), and Pod template (PodTemplate) values used by Tekton Pipelines in TaskRun and PipelineRun definitions. To do so, modify the ConfigMap config-defaults with your desired values.
The example below customizes the following:

the default service account from default to tekton.
the default timeout from 60 minutes to 20 minutes.
the default app.kubernetes.io/managed-by label is applied to all Pods created to execute TaskRuns.
the default Pod template to include a node selector to select the node where the Pod will be scheduled by default. A list of supported fields is available here.
For more information, see PodTemplate in TaskRuns or PodTemplate in PipelineRuns.
the default Workspace configuration can be set for any Workspaces that a Task declares but that a TaskRun does not explicitly provide.
the default maximum combinations of Parameters in a Matrix that can be used to fan out a PipelineTask. For
more information, see Matrix.
the default resolver type to git.

apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
data:
  default-service-account: "tekton"
  default-timeout-minutes: "20"
  default-pod-template: |
    nodeSelector:
      kops.k8s.io/instancegroup: build-instance-group    
  default-managed-by-label-value: "my-tekton-installation"
  default-task-run-workspace-binding: |
        emptyDir: {}
  default-max-matrix-combinations-count: "1024"
  default-resolver-type: "git"
Note: The _example key in the provided config-defaults.yaml
file lists the keys you can customize along with their default values.
Customizing the Pipelines Controller behavior
To customize the behavior of the Pipelines Controller, modify the ConfigMap feature-flags via
kubectl edit configmap feature-flags -n tekton-pipelines.
Note: Changing feature flags may result in undefined behavior for TaskRuns and PipelineRuns
that are running while the change occurs.
The flags in this ConfigMap are as follows:


disable-affinity-assistant - set this flag to true to disable the Affinity Assistant
that is used to provide Node Affinity for TaskRun pods that share workspace volume.
The Affinity Assistant is incompatible with other affinity rules
configured for TaskRun pods.
Note: This feature flag is deprecated and will be removed in release v0.60. Consider using coschedule feature flag to configure Affinity Assistant behavior.
Note: Affinity Assistant use Inter-pod affinity and anti-affinity
that require substantial amount of processing which can slow down scheduling in large clusters
significantly. We do not recommend using them in clusters larger than several hundred nodes
Note: Pod anti-affinity requires nodes to be consistently labelled, in other words every
node in the cluster must have an appropriate label matching topologyKey. If some or all nodes
are missing the specified topologyKey label, it can lead to unintended behavior.


coschedule: set this flag determines how PipelineRun Pods are scheduled with Affinity Assistant.
Acceptable values are â€œworkspacesâ€ (default), â€œpipelinerunsâ€, â€œisolate-pipelinerunâ€, or â€œdisabledâ€.
Setting it to â€œworkspacesâ€ will schedule all the taskruns sharing the same PVC-based workspace in a pipelinerun to the same node.
Setting it to â€œpipelinerunsâ€ will schedule all the taskruns in a pipelinerun to the same node.
Setting it to â€œisolate-pipelinerunâ€ will schedule all the taskruns in a pipelinerun to the same node,
and only allows one pipelinerun to run on a node at a time. Setting it to â€œdisabledâ€ will not apply any coschedule policy.


await-sidecar-readiness: set this flag to "false" to allow the Tekton controller to start a
TasksRunâ€™s first step immediately without waiting for sidecar containers to be running first. Using
this option should decrease the time it takes for a TaskRun to start running, and will allow TaskRun
pods to be scheduled in environments that donâ€™t support Downward API
volumes (e.g. some virtual kubelet implementations). However, this may lead to unexpected behaviour
with Tasks that use sidecars, or in clusters that use injected sidecars (e.g. Istio). Setting this flag
to "false" will mean the running-in-environment-with-injected-sidecars flag has no effect.


running-in-environment-with-injected-sidecars: set this flag to "false" to allow the
Tekton controller to start a TasksRunâ€™s first step immediately if it has no Sidecars specified.
Using this option should decrease the time it takes for a TaskRun to start running.
However, for clusters that use injected sidecars (e.g. Istio) this can lead to unexpected behavior.


require-git-ssh-secret-known-hosts: set this flag to "true" to require that
Git SSH Secrets include a known_hosts field. This ensures that a git remote serverâ€™s
key is validated before data is accepted from it when authenticating over SSH. Secrets
that donâ€™t include a known_hosts will result in the TaskRun failing validation and
not running.


enable-tekton-oci-bundles: set this flag to "true" to enable the
tekton OCI bundle usage (see the tekton bundle
contract). Enabling this option
allows the use of bundle field in taskRef and pipelineRef for
Pipeline, PipelineRun and TaskRun. By default, this option is
disabled ("false"), which means it is disallowed to use the
bundle field.


disable-creds-init - set this flag to "true" to disable Tektonâ€™s built-in credential initialization
and use Workspaces to mount credentials from Secrets instead.
The default is false. For more information, see the associated issue.


enable-api-fields: When using v1beta1 APIs, setting this field to â€œstableâ€ or â€œbetaâ€
enables beta features. When using v1 APIs, setting this field to â€œstableâ€
allows only stable features, and setting it to â€œbetaâ€ allows only beta features.
Set this field to â€œalphaâ€ to allow alpha features to be used.


enable-kubernetes-sidecar: Set this flag to "true" to enable native kubernetes sidecar support. This will allow Tekton sidecars to run as Kubernetes sidecars. Must be using Kubernetes v1.29 or greater.


For example:
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  enable-api-fields: "alpha" # Allow alpha fields to be used in Tasks and Pipelines.


trusted-resources-verification-no-match-policy: Setting this flag to fail will fail the taskrun/pipelinerun if no matching policies found. Setting to warn will skip verification and log a warning if no matching policies are found, but not fail the taskrun/pipelinerun. Setting to ignore will skip verification if no matching policies found.
Defaults to â€œignoreâ€.


results-from: set this flag to â€œtermination-messageâ€ to use the containerâ€™s termination message to fetch results from. This is the default method of extracting results. Set it to â€œsidecar-logsâ€ to enable use of a results sidecar logs to extract results instead of termination message.


enable-provenance-in-status: Set this flag to "true" to enable populating
the provenance field in TaskRun and PipelineRun status. The provenance
field contains metadata about resources used in the TaskRun/PipelineRun such as the
source from where a remote Task/Pipeline definition was fetched. By default, this is set to true.
To disable populating this field, set this flag to "false".


set-security-context: Set this flag to true to set a security context for containers injected by Tekton that will allow TaskRun pods
to run in namespaces with restricted pod security admission. By default, this is set to false.


Alpha Features
Alpha features in the following table are still in development and their syntax is subject to change.

To enable the features without an individual flag:
set the enable-api-fields feature flag to "alpha" in the feature-flags ConfigMap alongside your Tekton Pipelines deployment via kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"enable-api-fields":"alpha"}}'.
To enable the features with an individual flag:
set the individual flag accordingly in the feature-flag ConfigMap alongside your Tekton Pipelines deployment. Example: kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"<FLAG-NAME>":"<FLAG-VALUE>"}}'.

Features currently in â€œalphaâ€ are:



Feature
Proposal
Release
Individual Flag




Bundles 
TEP-0005
v0.18.0
enable-tekton-oci-bundles


Hermetic Execution Mode
TEP-0025
v0.25.0



Windows Scripts
TEP-0057
v0.28.0



Debug
TEP-0042
v0.26.0



StdoutConfig and StderrConfig
TEP-0011
v0.38.0



Trusted Resources
TEP-0091
v0.49.0
trusted-resources-verification-no-match-policy


Configure Default Resolver
TEP-0133
v0.46.0



Coschedule
TEP-0135
v0.51.0
coschedule


keep pod on cancel
N/A
v0.52.0
keep-pod-on-cancel


CEL in WhenExpression
TEP-0145
v0.53.0
enable-cel-in-whenexpression


Param Enum
TEP-0144
v0.54.0
enable-param-enum



Beta Features
Beta features are fields of stable CRDs that follow our â€œbetaâ€ compatibility policy.
To enable these features, set the enable-api-fields feature flag to "beta" in
the feature-flags ConfigMap alongside your Tekton Pipelines deployment via
kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"enable-api-fields":"beta"}}'.
Features currently in â€œbetaâ€ are:



Feature
Proposal
Alpha Release
Beta Release
Individual Flag




Remote Tasks and Remote Pipelines
TEP-0060

v0.41.0



Provenance field in Status
issue#5550
v0.41.0
v0.48.0
enable-provenance-in-status


Isolated Step & Sidecar Workspaces
TEP-0029
v0.24.0
v0.50.0



Matrix
TEP-0090
v0.38.0
v0.53.0



Task-level Resource Requirements
TEP-0104
v0.39.0
v0.53.0



Reusable Steps via StepActions
TEP-0142
v0.54.0
enable-step-actions



Larger Results via Sidecar Logs
TEP-0127
v0.43.0
v0.61.0
results-from


Step and Sidecar Overrides
TEP-0094
v0.34.0

v0.61.0


Ignore Task Failure
TEP-0050
v0.55.0
v0.62.0
N/A



Enabling larger results using sidecar logs
Note: The maximum size of a Taskâ€™s results is limited by the container termination message feature of Kubernetes,
as results are passed back to the controller via this mechanism. At present, the limit is per task is â€œ4096 bytesâ€. All
results produced by the task share this upper limit.
To exceed this limit of 4096 bytes, you can enable larger results using sidecar logs. By enabling this feature, you will
have a configurable limit (with a default of 4096 bytes) per result with no restriction on the number of results. The
results are still stored in the taskRun CRD, so they should not exceed the 1.5MB CRD size limit.
Note: to enable this feature, you need to grant get access to all pods/log to the tekton-pipelines-controller.
This means that the tekton pipeline controller has the ability to access the pod logs.

Create a cluster role and rolebinding by applying the following spec to provide log access to tekton-pipelines-controller.

kubectl apply -f optional_config/enable-log-access-to-controller/

Set the results-from feature flag to use sidecar logs by setting results-from: sidecar-logs in the
configMap.

kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"results-from":"sidecar-logs"}}'

If you want the size per result to be something other than 4096 bytes, you can set the max-result-size feature flag
in bytes by setting max-result-size: 8192(whatever you need here). Note: The value you can set here cannot exceed
the size of the CRD limit of 1.5 MB.

kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"max-result-size":"<VALUE-IN-BYTES>"}}'
Configuring High Availability
If you want to run Tekton Pipelines in a way so that webhooks are resiliant against failures and support
high concurrency scenarios, you need to run a Metrics Server in
your Kubernetes cluster. This is required by the Horizontal Pod Autoscalers
to compute replica count.
See HA Support for Tekton Pipeline Controllers for instructions on configuring
High Availability in the Tekton Pipelines Controller.
The default configuration is defined in webhook-hpa.yaml which can be customized
to better fit specific usecases.
Configuring tekton pipeline controller performance
Out-of-the-box, Tekton Pipelines Controller is configured for relatively small-scale deployments but there have several options for configuring Pipelinesâ€™ performance are available. See the Performance Configuration document which describes how to change the default ThreadsPerController, QPS and Burst settings to meet your requirements.
Running TaskRuns and PipelineRuns with restricted pod security standards
To allow TaskRuns and PipelineRuns to run in namespaces with restricted pod security standards,
set the â€œset-security-contextâ€ feature flag to â€œtrueâ€ in the feature-flags configMap. This configuration option applies a SecurityContext
to any containers injected into TaskRuns by the Pipelines controller. This SecurityContext may not be supported in all Kubernetes implementations (for example, OpenShift).
Note: running TaskRuns and PipelineRuns in the â€œtekton-pipelinesâ€ namespace is discouraged.
Platform Support
The Tekton project provides support for running on x86 Linux Kubernetes nodes.
The project produces images capable of running on other architectures and operating systems, but may not be able to help debug issues specific to those platforms as readily as those that affect Linux on x86.
The controller and webhook components are currently built for:

linux/amd64
linux/arm64
linux/arm (Arm v7)
linux/ppc64le (PowerPC)
linux/s390x (IBM Z)

The entrypoint component is also built for Windows, which enables TaskRun workloads to execute on Windows nodes.
See Windows documentation for more information.
Creating a custom release of Tekton Pipelines
You can create a custom release of Tekton Pipelines by following and customizing the steps in Creating an official release. For example, you might want to customize the container images built and used by Tekton Pipelines.
Verify Tekton Pipelines Release

We will refine this process over time to be more streamlined. For now, please follow the steps listed in this section
to verify Tekton pipeline release.

Tekton Pipelineâ€™s images are being signed by Tekton Chains since 0.27.1. You can verify the images with
cosign using the Tektonâ€™s public key.
Verify signatures using cosign
With Go 1.16+, you can install cosign by running:
go install github.com/sigstore/cosign/cmd/cosign@latest
You can verify Tekton Pipelines official images using the Tekton public key:
cosign verify -key https://raw.githubusercontent.com/tektoncd/chains/main/tekton.pub gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1
which results in:
Verification for gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The signatures were verified against the specified public key
  - Any certificates were verified against the Fulcio roots.
{
  "Critical": {
    "Identity": {
      "docker-reference": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller"
    },
    "Image": {
      "Docker-manifest-digest": "sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8"
    },
    "Type": "Tekton container signature"
  },
  "Optional": {}
}
The verification shows a list of checks performed and returns the digest in Critical.Image.Docker-manifest-digest
which can be used to retrieve the provenance from the transparency logs for that image using rekor-cli.
Verify the transparency logs using rekor-cli
Install the rekor-cli by running:
go install -v github.com/sigstore/rekor/cmd/rekor-cli@latest
Now, use the digest collected from the previous section in
Critical.Image.Docker-manifest-digest, for example,
sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8.
Search the transparency log with the digest just collected:
rekor-cli search --sha sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8
which results in:
Found matching entries (listed by UUID):
68a53d0e75463d805dc9437dda5815171502475dd704459a5ce3078edba96226
Tekton Chains generates provenance based on the custom format
in which the subject holds the list of artifacts which were built as part of the release. For the Pipeline release,
subject includes a list of images including pipeline controller, pipeline webhook, etc. Use the UUID to get the provenance:
rekor-cli get --uuid 68a53d0e75463d805dc9437dda5815171502475dd704459a5ce3078edba96226 --format json | jq -r .Attestation | base64 --decode | jq
which results in:
{
  "_type": "https://in-toto.io/Statement/v0.1",
  "predicateType": "https://tekton.dev/chains/provenance",
  "subject": [
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller",
      "digest": {
        "sha256": "0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint",
      "digest": {
        "sha256": "2fa7f7c3408f52ff21b2d8c4271374dac4f5b113b1c4dbc7d5189131e71ce721"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init",
      "digest": {
        "sha256": "83d5ec6addece4aac79898c9631ee669f5fee5a710a2ed1f98a6d40c19fb88f7"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/imagedigestexporter",
      "digest": {
        "sha256": "e4d77b5b8902270f37812f85feb70d57d6d0e1fed2f3b46f86baf534f19cd9c0"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/nop",
      "digest": {
        "sha256": "59b5304bcfdd9834150a2701720cf66e3ebe6d6e4d361ae1612d9430089591f8"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/pullrequest-init",
      "digest": {
        "sha256": "4992491b2714a73c0a84553030e6056e6495b3d9d5cc6b20cf7bc8c51be779bb"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook",
      "digest": {
        "sha256": "bf0ef565b301a1981cb2e0d11eb6961c694f6d2401928dccebe7d1e9d8c914de"
      }
    }
  ],
  ...
Now, verify the digest in the release.yaml by matching it with the provenance, for example, the digest for the release v0.28.1:
curl -s https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.28.1/release.yaml | grep github.com/tektoncd/pipeline/cmd/controller:v0.28.1 | awk -F"github.com/tektoncd/pipeline/cmd/controller:v0.28.1@" '{print $2}'
which results in:
sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8
Now, you can verify the deployment specifications in the release.yaml to match each of these images and their digest.
The tekton-pipelines-controller deployment specification has a container named tekton-pipeline-controller and a
list of image references with their digest as part of the args:
      containers:
        - name: tekton-pipelines-controller
          image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1@sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8
          args: [
            # These images are built on-demand by `ko resolve` and are replaced
            # by image references by digest.
              "-git-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init:v0.28.1@sha256:83d5ec6addece4aac79898c9631ee669f5fee5a710a2ed1f98a6d40c19fb88f7",
              "-entrypoint-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint:v0.28.1@sha256:2fa7f7c3408f52ff21b2d8c4271374dac4f5b113b1c4dbc7d5189131e71ce721",
              "-nop-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/nop:v0.28.1@sha256:59b5304bcfdd9834150a2701720cf66e3ebe6d6e4d361ae1612d9430089591f8",
              "-imagedigest-exporter-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/imagedigestexporter:v0.28.1@sha256:e4d77b5b8902270f37812f85feb70d57d6d0e1fed2f3b46f86baf534f19cd9c0",
              "-pr-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/pullrequest-init:v0.28.1@sha256:4992491b2714a73c0a84553030e6056e6495b3d9d5cc6b20cf7bc8c51be779bb",
Similarly, you can verify the rest of the images which were published as part of the Tekton Pipelines release:
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/nop
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/imagedigestexporter
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/pullrequest-init
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook
Verify Tekton Resources
Trusted Resources is a feature to verify Tekton Tasks and Pipelines. The current
version of feature supports v1beta1 Task and Pipeline. For more details
please take a look at Trusted Resources.
Pipelineruns with Affinity Assistant
The cluster operators can review the guidelines to cordon a node in the cluster
with the tekton controller and the affinity assistant is enabled.
TaskRuns with imagePullBackOff Timeout
Tekton pipelines has adopted a fail fast strategy with a taskRun failing with TaskRunImagePullFailed in case of an
imagePullBackOff. This can be limited in some cases, and it generally depends on the infrastructure. To allow the
cluster operators to decide whether to wait in case of an imagePullBackOff, a setting is available to configure
the wait time such that the controller will wait for the specified duration before declaring a failure.
For example, with the following config-defaults, the controller does not mark the taskRun as failure for 5 minutes since
the pod is scheduled in case the image pull fails with imagePullBackOff. The default-imagepullbackoff-timeout is
of type time.Duration and can be set to a duration such as â€œ1mâ€, â€œ5mâ€, â€œ10sâ€, â€œ1hâ€, etc.
See issue https://github.com/tektoncd/pipeline/issues/5987 for more details.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
data:
  default-imagepullbackoff-timeout: "5m"
Disabling Inline Spec in Pipeline, TaskRun and PipelineRun
Tekton users may embed the specification of a Task (via taskSpec) or a Pipeline (via pipelineSpec) as an alternative to referring to an external resource via taskRef and pipelineRef respectively.  This behaviour can be selectively disabled for three Tekton resources: TaskRun, PipelineRun and Pipeline.
In certain clusters and scenarios, an admin might want to disable the customisation of Tasks and Pipelines and only allow users to run pre-defined resources. To achieve that the admin should disable embedded specification via the disable-inline-spec flag, and remote resolvers too.
To disable inline specification, set the disable-inline-spec flag to "pipeline,pipelinerun,taskrun"
in the feature-flags configmap.
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  disable-inline-spec: "pipeline,pipelinerun,taskrun"
Inline specifications can be disabled for specific resources only. To achieve that, set the disable-inline-spec flag to a comma-separated list of the desired resources. Valid values are pipeline, pipelinerun and taskrun.
The default value of disable-inline-spec is â€œâ€, which means inline specification is enabled in all cases.
Next steps
To get started with Tekton check the Introductory tutorials,
the how-to guides, and the examples folder.



  
  
    
    
    
      
        
          Source for this document available on GitHub.
        
      
    
  
  



	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified November 28, 2022: Refactor installation and authentication docs (a474765)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Concepts
  Conceptual and technical information about Tekton
	
		

  
    


  
    


  

		
	
	This section presents an overview of what the Tekton project is and how some of
its building blocks work.

        
    
    
    
    
    
    
    
    
    
        
            
            
                
                    Overview
                
                Components, benefits and caveats, common usage.
            
        
            
            
                
                    Concept model
                
                Basic Tekton components and data model
            
        
            
            
                
                    Supply Chain Security
                
                Overview of Supply Chain Security
            
        
    


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 29, 2023: Rework left-hand nav order (dbe0a93)\n\nOverview
                
                Components, benefits and caveats, common usage.
            
                
                    Concept model
                
                Basic Tekton components and data model
            
                
                    Supply Chain Security
                
                Overview of Supply Chain Security\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Overview
	Components, benefits and caveats, common usage.
	
		

  
    


  
    


  

		
	    
	What is Tekton?
Tekton is a cloud-native solution for building CI/CD pipelines. It consists of Tekton Pipelines, which provides
the building blocks, and of supporting components, such as Tekton CLI and Tekton Catalog, that make Tekton a complete
ecosystem. Tekton is part of the CD Foundation, a
Linux Foundation project.
Tekton installs and runs as an extension on a Kubernetes cluster and comprises a set of Kubernetes Custom Resources
that define the building blocks you can create and reuse for your pipelines. Once installed, Tekton Pipelines becomes
available via the Kubernetes CLI (kubectl) and via API calls, just like pods and other resources.
Who uses Tekton?
Tekton users typically fall into the following categories:

Platform engineers who build CI/CD systems for the developers in their organization.
Developers who use those CI/CD systems to do their work.

What are the benefits of Tekton?
Tekton provides the following benefits to builders and users of CI/CD systems:

Customizable. Tekton entities are fully customizable, allowing for a high degree of flexibility. Platform engineers
can define a highly detailed catalog of building blocks for developers to use in a wide variety of scenarios.
Reusable. Tekton entities are fully portable, so once defined, anyone within the organization can use a given
pipeline and reuse its building blocks. This allows developers to quickly build complex pipelines without â€œreinventing
the wheel.â€
Expandable. Tekton Catalog is a community-driven repository of Tekton building blocks. You can quickly create new and
expand existing pipelines using pre-made components from the Tekton Catalog.
Standardized. Tekton installs and runs as an extension on your Kubernetes cluster and uses the well-established
Kubernetes resource model. Tekton workloads execute inside Kubernetes containers.
Scalable. To increase your workload capacity, you can simply add nodes to your cluster. Tekton scales with your cluster
without the need to redefine your resource allocations or any other modifications to your pipelines.

What are the components of Tekton?
Tekton consists of the following components:


Tekton Pipelines is the foundation of Tekton. It defines a
set of Kubernetes Custom Resources that act as building blocks
from which you can assemble CI/CD pipelines.


Tekton Triggers allows you to instantiate pipelines based on events.
For example, you can trigger the instantiation and execution of a pipeline
every time a PR is merged against a GitHub repository. You can also build a user
interface that launches specific Tekton triggers.


Tekton CLI provides a command-line interface called tkn, built on top
of the Kubernetes CLI, that allows you to interact with Tekton.


Tekton Dashboard is a Web-based graphical interface for
Tekton Pipelines that displays information about the execution of your
pipelines. It is currently a work-in-progress.


Tekton Catalog is a repository of high-quality, community-contributed
Tekton building blocks - Tasks, Pipelines, and so on - that are ready for
use in your own pipelines.


Tekton Hub is a Web-based graphical interface for accessing the Tekton Catalog.


Tekton Operator is a Kubernetes Operator
pattern that allows you to
install, update, and remove Tekton projects on your Kubernetes cluster.


Tekton Chain provides tools to generate, store, and sign
provenance for artifacts built with Tekton Pipelines.


How do I work with Tekton?
To install Tekton, you need a Kubernetes cluster running a
version of Kubernetes specified for the current Tekton release.
Once installed, you can interact with Tekton using one of the following:

The tkn CLI, also known as the Tekton CLI, is the preferred command-line method
for interacting with Tekton. tkn provides a quick and streamlined experience, including high-level commands and color coding. To use it,
you only need to be familiar with Tekton.
The kubectl CLI, also known as the Kubernetes CLI, provides substantially more
granularity for controlling Tekton at the expense of higher complexity. Interacting with Tekton via kubectl is typically reserved for debugging
your pipelines and troubleshooting your builds.
The Tekton APIs, currently
available for Pipelines and
Triggers, allow you to programmatically interact
with Tekton components. This is typically reserved for highly customized CI/CD systems. In most scenarios, tkn and kubectl are the preferred
methods of controlling Tekton.

We also recommend having the following items configured on your Kubernetes cluster:

Persistent volume claims for specifying inputs and outputs.
Permissions appropriate to your environment and business needs.
Storage for building and pushing images (if applicable).

What can I do with Tekton?
Tekton introduces the concept of Tasks, which specify the workloads you want to run:


Task - defines a series of ordered Steps, and each Step invokes a specific
build tool on a specific set of inputs and produces a specific set of outputs, which can be used as inputs in the next Step.


Pipeline - defines a series of ordered Tasks, and just like Steps in a
Task, a Task in a Pipeline can use the output of a previously executed Task as its input.


TaskRun - instantiates a specific Task to execute on a particular set of
inputs and produce a particular set of outputs. In other words, the Task tells Tekton what to do, and a TaskRun tells Tekton what to do it on,
as well as any additional details on how to exactly do it, such as build flags.


PipelineRun - instantiates a specific Pipeline to execute on a particular
set of inputs and produce a particular set of outputs to particular destinations.


Each Task executes in its own Kubernetes Pod. Thus, by default, Tasks within a Pipeline do not share data. To share data among Tasks,
you must explicitly configure each Task to make its outputs available to the next Task and to ingest the outputs of a previously executed
Task as its inputs, whichever is applicable.
When to use which?

Task - useful for simpler workloads such as running a test, a lint, or building a Kaniko cache. A single Task executes in a single
Kubernetes Pod, uses a single disk, and generally keeps things simple.
Pipeline - useful for complex workloads, such as static analysis, as well as testing, building, and deploying complex projects.

I want to learn more!


See the Tekton concept model to learn more
about the basics of how Tekton tasks and pipelines interact.


Run your first pipeline following the Getting
Started guide.



	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified March 9, 2023: Get started with chains (02f4674)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Concept model
	Basic Tekton components and data model
	
		

  
    


  
    


  

		
	    
	Steps, Tasks, and Pipelines
A step is an operation in a CI/CD workflow, such as running some unit tests
for a Python web app, or the compilation of a Java program. Tekton performs
each step with a container image you provide. For example, you may use the
official Go image to compile a Go program
in the same manner as you would on your local workstation (go build).
A task is a collection of steps in order. Tekton runs a task in
the form of a Kubernetes pod,
where each step becomes a running container in the pod. This design allows you
to set up a shared environment for a number of related steps; for example,
you may mount a Kubernetes volume
in a task, which will be accessible inside each step of the task.
A pipeline is a collection of tasks in order. Tekton collects all the
tasks, connects them in a directed acyclic graph (DAG), and executes the graph
in sequence. In other words, it creates a number of Kubernetes pods and
ensures that each pod completes running successfully as desired. Tekton grants
developers full control of the process: one may set up a fan-in/fan-out
scenario of task completion, ask Tekton to retry automatically should
a flaky test exists, or specify a condition that a task must meet before
proceeding.
Tasks and pipelines are specified as custom resources
in a Kubernetes cluster.

TaskRuns and PipelineRuns
A pipelineRun, as its name implies, is a specific execution of a pipeline.
For example, you may ask Tekton to run your CI/CD workflow twice a day, and
each execution will become a pipelineRun resource trackable in your
Kubernetes cluster. You can view the status of your CI/CD workflow, including
the specifics of each task execution with pipelineRuns.
Similarly, a taskRun is a specific execution of a task. TaskRuns
are also available when you choose to run a task outside a pipeline, with
which you may view the specifics of each step execution in a task.
TaskRuns and pipelineRuns connect resources with tasks and
pipelines. A run must include the actual addresses of resources, such as
the URLs of repositories, its task or pipeline needs. This design allows
developers to reuse tasks and pipelines for different inputs and outputs.
You may create taskRuns or pipelineRuns manually, which triggers
Tekton to run a task or a pipeline immediately. Alternately, one may ask a
Tekton component, such as Tekton Triggers, to create a run automatically on
demand; for example, you may want to run a pipeline every time a new pull
request is checked into your git repository.

TaskRuns and pipelineRuns are specified as custom resources
in a Kubernetes cluster.
How Tekton works
Loosely speaking, at its core, Tekton Pipelines functions by wrapping each
of your steps. More specifically, Tekton Pipelines injects an entrypoint
binary in step containers, which executes the command you specify when
the system is ready.
Tekton Pipelines tracks the state of your pipeline using
Kubernetes Annotations.
These annotations are projected inside each step container in the form
of files with the
Kubernetes Downward API.
The entrypoint binary watches the projected files closely, and will only
start the provided command if a specific annotation appears as files. For
example, when you ask Tekton to run two steps consecutively in a task,
the entrypoint binary injected into the second step container will
wait idly until the annotations report that the first step container
has successfully completed.
In addition, Tekton Pipelines schedules some containers to run automatically
before and after your step containers, so as to support specific built-in
features, such as the retrieval of input resources and the uploading of
outputs to blob storage solutions. You can track their running statuses as
well via taskRuns and pipelineRuns. The system also performs a number
of other operations to set up the environment before running the step
containers; for more information, see Tasks and Pipelines.
Whatâ€™s next
Learn more about Tekton Pipelines in Tasks and Pipelines.

	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified April 25, 2022: Update concepts (c8d84b9)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Supply Chain Security
	Overview of Supply Chain Security
	
		

  
    


  
    


  

		
	    
	Given the increasing complexity of the CI/CD space, with projects that often
have dozens or even hundreds of dependencies, the supply chain has become a
common vector of attacks. Tekton Chains is a security-oriented part of the
Tekton portfolio to help you mitigate security risks.
Tekton Chains is a tool to generate, store, and sign provenance for artifacts
built with Tekton Pipelines. Provenance is metadata containing verifiable
information about software artifacts, describing where, when and how something
is built.
How to secure your Supply Chain
Supply chain Levels for Software Artifacts (SLSA) provides a set of
guidelines you can follow to make your software more secure. SLSA is organized
into a series of levels, where level 4 represents the ideal state. Go to
slsa.dev for more information.
Tekton Chains implements the SLSA guidelines to help you accomplish SLSA level
2, by documenting the build process in a tamper resistant format.
How does Tekton Chains work?
Tekton Chains works by deploying a controller that runs in the background and
monitors TaskRuns. While Tekton Pipelines executes your Tasks, Tekton Chains
watches the operation, once the operation is successfully completed, the Chains
controller generates the provenance for the artifacts produced.
The provenance records the inputs of your Tasks: source repositories, branches,
other artifacts; and the outputs: container images, packages, etc. This
information is recorded as in-toto metadata and signed. You can store
the keys to sign the provenance in a Kubernetes secret or by using a supported
key management system: GCP, AWS, Azure, or Vault. You can then upload the
provenance  to a specified location. Getting To SLSA Level 2 with Tekton and
Tekton Chains on the Google Open Source Blog provides more details.
graph LR
  subgraph TOP[Kubernetes]
    direction TB

    subgraph C[Tekton Chains controller]
      direction TB
      c1(Observe Runs)
      c2(Generate Provenance) 
      c3(Sign Metadata)
    end

    subgraph B[Pipelines]
      direction LR
      subgraph B1[Pipeline]
        direction TB
        i1[Task] --> f1[Task]
      end

      subgraph B2[Pipeline]
        direction TB
        i2[Task] --> f2[Task]
      end
      B1 --> B2
    end
  end

  A[Sources] -.-> B -.-> D[Artifacts]


Where can I try it?

For a hands-on experience, follow the Getting started with Tekton Chains
tutorial.
Check the examples available on the Chain repository.


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified March 9, 2023: Get started with chains (02f4674)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	How-to Guides
  Guides to help you complete a specific goal
	
		

  
    


  
    


  

		
	
	This section includes common practical applications of Tekton.

        
    
    
    
    
    
    
    
    
    
        
            
            
                
                    Clone a git repository with Tekton
                
                How to clone source code from git with Tekton Pipelines.
            
        
            
            
                
                    Build and push an image with Tekton
                
                Create a Pipeline to fetch the source code, build, and push an image with Kaniko and Tekton.
            
        
    


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 29, 2023: Rework left-hand nav order (dbe0a93)\n\nClone a git repository with Tekton
                
                How to clone source code from git with Tekton Pipelines.
            
                
                    Build and push an image with Tekton
                
                Create a Pipeline to fetch the source code, build, and push an image with Kaniko and Tekton.\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Clone a git repository with Tekton
	How to clone source code from git with Tekton Pipelines.
	
		

  
    


  
    


  

		
	    
	This guide shows you how to:

Create a Task to clone source code from a git repository.
Create a second Task to read the source code from a shared Workspace.

If you are already familiar with Tekton and just want to see the example, you
can skip to the full code samples.
Prerequisites


To follow this How-to you must have a Kubernetes cluster up and running and
kubectl properly configured to issue commands to your cluster.


Install Tekton Pipelines:
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
See the Pipelines installation documentation for other
installation options.


Install the Tekton CLI, tkn, on your machine.


If this is your first time using Tekton Pipelines, we recommend that you
complete the Getting Started tutorials before proceeding with
this guide.
Pull source code from git
In this section you are going to create a Pipeline containing a Task to pull
code from a git repository.
Create the Pipeline
One practical aspect of Tekton Tasks and Pipelines is that they are reusable.
Thereâ€™s a community hub with a curated list of Pipelines and Tasks
that you can include in your own CI/CD workflow. You are going to reuse one of
those Tasks in this guide.


Create a new Pipeline, pipeline.yaml:
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: clone-read
The name clone-read is going to be used by the PipelineRun to refer to
this Pipeline. This name can also be used with the CLI, to check logs,
delete the pipeline, etc.


Add the repository URL to the list of Pipeline parameters:
spec:
  description: | 
    This pipeline clones a git repo, then echoes the README file to the stdout.
  params:
  - name: repo-url
    type: string
    description: The git repo URL to clone from.
The params section contains a list of parameters to be used by the Tasks
in this Pipeline. For now there is only one, repo-url.


Add a Workspace, a shared volume to store the code this task is
going to download:
workspaces:
- name: shared-data
  description: | 
    This workspace contains the cloned repo files, so they can be read by the
    next task.


Create the Task that is going to use the parameter and the
Workplace that you just defined:
tasks:
- name: fetch-source
  taskRef:
    name: git-clone
  workspaces:
  - name: output
    workspace: shared-data
  params:
  - name: url
    value: $(params.repo-url)
This Task, fetch-source, refers to another Task, git-clone; to be
installed from the community hub. A Task has its own params and
workspaces passed down from the ones defined at Pipeline level. In this
case, the names for the  parameter url and the Workspace output are the
the ones expected by the git clone Task spec. See the git
clone documentation for more parameters and options.


Check the full code samples to see how all the pieces fit
together in the file.
Create the PipelineRun


Now that you have a Pipeline, to instantiate it and set the actual values,
create a PipelineRun, pipelinerun.yaml:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: clone-read-run-
spec:
  pipelineRef:
    name: clone-read
  podTemplate:
    securityContext:
      fsGroup: 65532
This PipelineRun instantiates clone-read, as specified by the
pipelineRef target in the spec section.


Instantiate the Workspace:
workspaces:
- name: shared-data
  volumeClaimTemplate:
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
This creates a Persistent Volume Claim to store the cloned
files. The name shared-data matches the name used in the Pipeline.


Set the value of the repository URL parameter:
params:
- name: repo-url
  value: https://github.com/tektoncd/website
For this example you are going to clone the Tekton documentation website
source code.


Check the full code samples to see full PipelineRun code.
Git authentication
For the sake of this example, we are using a public repository, which requires
no authentication. If you want to clone a private repository, you must create a
Kubernetes Secret with your credentials, then pass that secret to
your Task as a Workspace.
Before you proceed, you have to set up SSH authentication with your git
provider. The process may be slightly different in each case:

GitHub
GitLab
Bitbucket

The following steps show you how to authenticate using an SSH key. For other
forms of authentication check the git-clone Task documentation and
the Git authentication section in the Pipelines documentation.


Create a Kubernetes Secret with your credentials, for example:
apiVersion: v1
kind: Secret
metadata:
  name: git-credentials
data:
  id_rsa: AS0tLS...
  known_hosts: AG033S...
  config: GS0FFL...
The values for the fields under data are the corresponding base64-encoded
files in the .ssh directory. For example, for id_rsa copy-paste the
output of:
cat ~/.ssh/id_rsa | base64 -w0


Update pipeline.yaml, add a new Workspace to both the Pipeline and the
Task:
workspaces:
- name: shared-data
  description: | 
    This workspace contains the cloned repo files, so they can be read by the
    next task.
- name: git-credentials
  description: My ssh credentials
tasks:
- name: fetch-source
  taskRef:
    name: git-clone
  workspaces:
  - name: output
    workspace: shared-data
  - name: ssh-directory
    workspace: git-credentials
The Workspace git-credentials is defined at Pipeline level and then passed
down to the Task as ssh-directory, which is the name the Task expects.


Update pipelinerun.yaml to use the Secret as a Workspace and change the
git URL from https to SSH:
workspaces:
- name: shared-data
  volumeClaimTemplate:
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
- name: git-credentials
  secret:
    secretName: git-credentials
params:
- name: repo-url
  value: git@github.com:tektoncd/website.git
The new Workspace name, git-credentials, matches the Workspace added to
the Pipeline.


Use the source code in a second task
To learn how to share data between Tasks using a Workspace, you are going to
create a second Task that displays  the README file from the cloned git
repository. You can find more useful examples in the How-to section
and the examples folder in the Pipelines git repository.


Add a new entry to the tasks section of pipeline.yaml:
- name: show-readme
  runAfter: ["fetch-source"]
  taskRef:
    name: show-readme
  workspaces:
  - name: source
    workspace: shared-data
This is referencing the Task show-readme. Unlike git clone, this Task is
not from the Tekton community hub, you have to create it yourself.


Create a file called show-readme.yaml and add the following:
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: show-readme
spec:
  description: Read and display README file.
  workspaces:
  - name: source
  steps:
  - name: read
    image: alpine:latest
    script: | 
      #!/usr/bin/env sh
      cat $(workspaces.source.path)/README.md
This Task uses a Workspace that was already set. You donâ€™t have to update
pipelinerun.yaml.


Run the Pipeline
Install a Task from Tekton Hub
To use the git clone Task in your pipeline, you have to install it on your
cluster first. You can do this with the tkn command:
tkn hub install task git-clone
Or using kubectl:
kubectl apply -f \
https://raw.githubusercontent.com/tektoncd/catalog/main/task/git-clone/0.6/git-clone.yaml
Alternatively, you can bundle a Task or a Pipeline and let your
Pipeline fetch it directly from a registry.
Apply the files
Now you are ready to test the code.


Apply the show-readme Task:
kubectl apply -f show-readme.yaml


Apply the Pipeline:
kubectl apply -f pipeline.yaml


Create the PipelineRun:
kubectl create -f pipelinerun.yaml
This creates a PipelineRun with a unique name each time:
pipelinerun.tekton.dev/clone-read-run-4kgjr created


Use the PipelineRun name from the output of the previous step to monitor the
Pipeline execution:
tkn pipelinerun logs  clone-read-run-4kgjr -f
You may have to wait a few seconds. The output confirms that the respository
was cloned succesfully and displays the README file at the end:
[fetch-source : clone] + '[' false '=' true ]
[fetch-source : clone] + '[' false '=' true ]
[fetch-source : clone] + '[' false '=' true ]
[fetch-source : clone] + CHECKOUT_DIR=/workspace/output/
[fetch-source : clone] + '[' true '=' true ]
[fetch-source : clone] + cleandir
[fetch-source : clone] + '[' -d /workspace/output/ ]
[fetch-source : clone] + rm -rf '/workspace/output//*'
[fetch-source : clone] + rm -rf '/workspace/output//.[!.]*'
[fetch-source : clone] + rm -rf '/workspace/output//..?*'
[fetch-source : clone] + test -z 
[fetch-source : clone] + test -z 
[fetch-source : clone] + test -z 
[fetch-source : clone] + /ko-app/git-init '-url=https://github.com/tektoncd/website' '-revision=' '-refspec=' '-path=/workspace/output/' '-sslVerify=true' '-submodules=true' '-depth=1' '-sparseCheckoutDirectories='
[fetch-source : clone] {"level":"info","ts":1652300245.5099113,"caller":"git/git.go:170","msg":"Successfully cloned https://github.com/tektoncd/website @ 4930334b17edeaa737e2e6d0c7f7139b0afb1896 (grafted, HEAD) in path /workspace/output/"}
[fetch-source : clone] {"level":"info","ts":1652300245.5349698,"caller":"git/git.go:208","msg":"Successfully initialized and updated submodules in path /workspace/output/"}
[fetch-source : clone] + cd /workspace/output/
[fetch-source : clone] + git rev-parse HEAD
[fetch-source : clone] + RESULT_SHA=4930334b17edeaa737e2e6d0c7f7139b0afb1896
[fetch-source : clone] + EXIT_CODE=0
[fetch-source : clone] + '[' 0 '!=' 0 ]
[fetch-source : clone] + printf '%s' 4930334b17edeaa737e2e6d0c7f7139b0afb1896
[fetch-source : clone] + printf '%s' https://github.com/tektoncd/website

[show-readme : read] # TektonCD Website
[show-readme : read] 
[show-readme : read] This repo contains the code behind [the Tekton org's](https://github.com/tektoncd)
[show-readme : read] website at [tekton.dev](https://tekton.dev).
[show-readme : read] 
[show-readme : read] For more information on the Tekton Project, see
[show-readme : read] [the community repo](https://github.com/tektoncd/community).
[show-readme : read] 
[show-readme : read] For more information on contributing to the website see:
[show-readme : read] 
[show-readme : read] * [CONTRIBUTING.md](CONTRIBUTING.md)
[show-readme : read] * [DEVELOPMENT.md](DEVELOPMENT.md)


Full code samples
The Pipeline:
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: clone-read
spec:
  description: | 
    This pipeline clones a git repo, then echoes the README file to the stout.
  params:
  - name: repo-url
    type: string
    description: The git repo URL to clone from.
  workspaces:
  - name: shared-data
    description: | 
      This workspace contains the cloned repo files, so they can be read by the
      next task.
  - name: git-credentials
    description: My ssh credentials
  tasks:
  - name: fetch-source
    taskRef:
      name: git-clone
    workspaces:
    - name: output
      workspace: shared-data
    - name: ssh-directory
      workspace: git-credentials
    params:
    - name: url
      value: $(params.repo-url)
  - name: show-readme
    runAfter: ["fetch-source"]
    taskRef:
      name: show-readme
    workspaces:
    - name: source
      workspace: shared-data
The PipelineRun:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: clone-read-run-
spec:
  pipelineRef:
    name: clone-read
  podTemplate:
    securityContext:
      fsGroup: 65532
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
  - name: git-credentials
    secret:
      secretName: git-credentials
  params:
  - name: repo-url
    value: git@github.com:tektoncd/website.git
The show-readme Task:
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: show-readme
spec:
  description: Read and display README file.
  workspaces:
  - name: source
  steps:
  - name: read
    image: alpine:latest
    script: | 
      #!/usr/bin/env sh
      cat $(workspaces.source.path)/README.md
The Kubernetes Secret. These values are not real, check the section about git
authentication to figure out how to encode your
credentials.
apiVersion: v1
kind: Secret
metadata:
  name: git-credentials
data:
  id_rsa: AS0tLS...
  known_hosts: AG033S...
Further reading

Build and push and image with Kaniko and Tekton.
Full Tasks and Pipelines documentation.


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified March 14, 2023: Move line to spec.description (5251985)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Build and push an image with Tekton
	Create a Pipeline to fetch the source code, build, and push an image with Kaniko and Tekton.
	
		

  
    


  
    


  

		
	    
	


   
This page includes content about running Tekton with specific platforms and
cloud providers. The accuracy and freshness of this vendor documentation varies
by vendor.
If you want to contribute with platform-specific documentation, follow the
vendor contributions guidelines.



This guide shows you how to:

Create a Task to clone source code from a git repository.
Create a second Task to use the cloned code to build a Docker image and push
it to a registry.

If you are already familiar with Tekton and just want to see the example, you
can skip to the full code samples.
Prerequisites


To follow this How-to you must have a Kubernetes cluster up and running and
kubectl properly configured to issue commands to your cluster.


Install Tekton Pipelines:
kubectl apply --filename \
https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
See the Pipelines installation documentation for other
installation options and vendor specific instructions.


Install the Tekton CLI, tkn, on your machine.


If this is your first time using Tekton Pipelines, we recommend that you
complete the Getting Started tutorials before proceeding with
this guide.
Clone the repository
Create a new Pipeline, pipeline.yaml, that uses the git clone Task to clone
the source code from a git repository:
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: clone-build-push
spec:
  description: | 
    This pipeline clones a git repo, builds a Docker image with Kaniko and
    pushes it to a registry
  params:
  - name: repo-url
    type: string
  workspaces:
  - name: shared-data
  tasks:
  - name: fetch-source
    taskRef:
      name: git-clone
    workspaces:
    - name: output
      workspace: shared-data
    params:
    - name: url
      value: $(params.repo-url)
Then create the corresponding pipelinerun.yaml file:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: clone-build-push-run-
spec:
  pipelineRef:
    name: clone-build-push
  podTemplate:
    securityContext:
      fsGroup: 65532
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
  params:
  - name: repo-url
    value: https://github.com/google/docsy-example.git
For this how-to we are using a public repository as an example. You can also
use git clone with private repositories, using SSH
authentication.
Build the container image with Kaniko
To build the image use the Kaniko Task from the community
hub.


Add the image reference to the params section in pipeline.yaml:
params: 
- name: image-reference
  type: string
This parameter is used to add the tag corresponding the container registry
where you are going to push the image.


Create the new build-push Task in the same pipeline.yaml file:
tasks:
...
  - name: build-push
    runAfter: ["fetch-source"]
    taskRef:
      name: kaniko
    workspaces:
    - name: source
      workspace: shared-data
    params:
    - name: IMAGE
      value: $(params.image-reference)
This new Task refers to kaniko, which is going to be installed from the
community hub. A Task has its own set of workspaces and
params passed down from the parameters and Workspaces defined at Pipeline
level. In this case, the Workspace source and the value of IMAGE. Check
the kaniko Task documentation to see all the available options.


Instantiate the build-push Task. Add the value of image-reference to
the params section in pipelinerun.yaml:
params:
- name: image-reference
  value: container.registry.com/sublocation/my_app:version
Replace container.registry.com/sublocation/my_app:version with the actual
tag for your registry. You can set up a local registry for
testing purposes.


Check the full code samples to see how all the pieces fit
together.
Container registry authentication
In most cases, to push the image to a container registry you must provide
authentication credentials first.










      General Authentication
    

    
      
    
      Google Cloud
    

    
      
    





        

Set up authentication with the Docker credential helper and generate the
Docker configuration file, $HOME/.docker/config.json, for your registry.
This step is different depending on your registry.

Red Hat Quay
Docker Hub
Azure container registry
Amazon ECR
Jfrog Artifactory

Check your cloud provider documentation to complete this step.


Create a Kubernetes Secret, docker-credentials.yaml with your
credentials:
apiVersion: v1
kind: Secret
metadata:
  name: docker-credentials
data:
  config.json: efuJAmF1...
The value of config.json is the base64-encoded ~/.docker/config.json
file. You can get this data with the following command:
cat ~/.docker/config.json | base64 -w0


Update pipeline.yaml and add a Workspace to mount the credentials
directory:
At the Pipeline level:
workspaces:
- name: docker-credentials
And under the build-push Task:
workspaces:
- name: dockerconfig
  workspace: docker-credentials


Instantiate the new docker-credentials Workspace in your
pipelinerun.yaml file by adding a new entry under workspaces:
- name: docker-credentials
  secret:
    secretName: docker-credentials




    
      
    

  
          GKE Workload Identity
If you are running your Pipelines on Google Kubernetes Engine (GKE), create a
cluster with Workload Identity enabled or enable Workload Identity
on an existing cluster. This allows you to to run your pipeline and
push images to Artifact Registry without authentication
credentials. If you are using Workload Identity, skip step 2 when you run
your pipeline.
Set up an Artifact Registry repository:


Enable the Artifact Registry API:
gcloud services enable artifactregistry.googleapis.com


Create a Docker  repository to push the image to:
gcloud artifacts repositories create <repository-name> \
  --repository-format=docker \
  --location=us-central1 --description="Docker repository"
Replace:

<repository-name> with the name of your repository.
<location> with the name of your preferred location. For example,
us-central1.



Configure the GKE cluster to allow the Pipeline to push images to Artifact
Registry:


Create a Kubernetes Service Account:
kubectl create serviceaccount <sa-name>
Where <sa-name> is the name of the service account. For example, tekton-sa.


Create a Google Service Account with the same name:
gcloud iam service-accounts create <sa-name>


Grant the Google Service Account permissions to push to the Artifact
Registry container repository:
gcloud artifacts repositories add-iam-policy-binding <ar-repository> \
  --location <location> \
  --member=serviceAccount:build-robot@<project_id>.iam.gserviceaccount.com \
  --role=roles/artifactregistry.reader \
  --role=roles/artifactregistry.writer
Where

<ar-repository> is the name of the repository.
<location> is the repository repository location.
<project_id> is the project id.



Set up the Workload Identity mappings on the Kubernetes cluster:
kubectl annotate serviceaccount \
<sa-name> \
iam.gke.io/gcp-service-account=build-robot@<project_id>.iam.gserviceaccount.com


Set up Workload Identity mappings for the Google Service Account:
gcloud iam service-accounts add-iam-policy-binding \
  --role roles/iam.workloadIdentityUser \
  --member "serviceAccount:<project_id>.svc.id.goog[default/<sa-name>]" \
  build-robot@<project_id>.iam.gserviceaccount.com


This creates two service accounts, an IAM service account and a
Kubernetes service account, and â€œlinksâ€ them. Workload Identity
allows workloads in your GKE cluster to impersonate IAM service accounts to
access Google Cloud services.
Use Docker authentication
If you prefer to use Docker authentication to push your image to Artifact
Registry, there are two options:

Use the gcloud credential helper.
Use the Standalone Docker credential helper.

In both cases your credentials are saved to a Docker configuration file in your
user home directory: $HOME/.docker/config.json. Use this file to follow the
â€œGeneral Authenticationâ€ instructions.


    
      
    

  


See the complete files in the full code samples section.
Run your Pipeline
You are ready to install the Tasks and run the pipeline.


Install the git-clone and kaniko Tasks:
tkn hub install task git-clone
tkn hub install task kaniko


Apply the Secret with your Docker credentials.
kubectl apply -f docker-credentials.yaml


Apply the Pipeline:
kubectl apply -f pipeline.yaml


Create the PipelineRun:
kubectl create -f pipelinerun.yaml
This creates a PipelineRun with a unique name each time:
pipelinerun.tekton.dev/clone-build-push-run-4kgjr created


Use the PipelineRun name from the output of the previous step to monitor the
Pipeline execution:
tkn pipelinerun logs  clone-build-push-run-4kgjr -f
After a few seconds, the output confirms that the image was built and
pushed successfully:
[fetch-source : clone] + '[' false '=' true ]
[fetch-source : clone] + '[' false '=' true ]
[fetch-source : clone] + '[' false '=' true ]
[fetch-source : clone] + CHECKOUT_DIR=/workspace/output/
[fetch-source : clone] + '[' true '=' true ]
[fetch-source : clone] + cleandir
[fetch-source : clone] + '[' -d /workspace/output/ ]
[fetch-source : clone] + rm -rf '/workspace/output//*'
[fetch-source : clone] + rm -rf '/workspace/output//.[!.]*'
[fetch-source : clone] + rm -rf '/workspace/output//..?*'
[fetch-source : clone] + test -z 
[fetch-source : clone] + test -z 
[fetch-source : clone] + test -z 
[fetch-source : clone] + /ko-app/git-init '-url=https://github.com/google/docsy-example.git' '-revision=' '-refspec=' '-path=/workspace/output/' '-sslVerify=true' '-submodules=true' '-depth=1' '-sparseCheckoutDirectories='
[fetch-source : clone] {"level":"info","ts":1654637310.4419358,"caller":"git/git.go:170","msg":"Successfully cloned https://github.com/google/docsy-example.git @ 1c7f7e300c90cd690ca5be66b43fe58713bb21c9 (grafted, HEAD) in path /workspace/output/"}
[fetch-source : clone] {"level":"info","ts":1654637320.384655,"caller":"git/git.go:208","msg":"Successfully initialized and updated submodules in path /workspace/output/"}
[fetch-source : clone] + cd /workspace/output/
[fetch-source : clone] + git rev-parse HEAD
[fetch-source : clone] + RESULT_SHA=1c7f7e300c90cd690ca5be66b43fe58713bb21c9
[fetch-source : clone] + EXIT_CODE=0
[fetch-source : clone] + '[' 0 '!=' 0 ]
[fetch-source : clone] + printf '%s' 1c7f7e300c90cd690ca5be66b43fe58713bb21c9
[fetch-source : clone] + printf '%s' https://github.com/google/docsy-example.git

[build-push : build-and-push] WARN
[build-push : build-and-push] User provided docker configuration exists at /kaniko/.docker/config.json 
[build-push : build-and-push] INFO Retrieving image manifest klakegg/hugo:ext-alpine 
[build-push : build-and-push] INFO Retrieving image klakegg/hugo:ext-alpine from registry index.docker.io 
[build-push : build-and-push] INFO Built cross stage deps: map[]                
[build-push : build-and-push] INFO Retrieving image manifest klakegg/hugo:ext-alpine 
[build-push : build-and-push] INFO Returning cached image manifest              
[build-push : build-and-push] INFO Executing 0 build triggers                   
[build-push : build-and-push] INFO Unpacking rootfs as cmd RUN apk add git requires it. 
[build-push : build-and-push] INFO RUN apk add git                              
[build-push : build-and-push] INFO Taking snapshot of full filesystem...        
[build-push : build-and-push] INFO cmd: /bin/sh                                 
[build-push : build-and-push] INFO args: [-c apk add git]                       
[build-push : build-and-push] INFO Running: [/bin/sh -c apk add git]            
[build-push : build-and-push] fetch https://dl-cdn.alpinelinux.org/alpine/v3.14/main/x86_64/APKINDEX.tar.gz
[build-push : build-and-push] fetch https://dl-cdn.alpinelinux.org/alpine/v3.14/community/x86_64/APKINDEX.tar.gz
[build-push : build-and-push] OK: 76 MiB in 41 packages
[build-push : build-and-push] INFO[0012] Taking snapshot of full filesystem...        
[build-push : build-and-push] INFO[0013] Pushing image to us-east1-docker.pkg.dev/tekton-tests/tektonstuff/docsy:v1 
[build-push : build-and-push] INFO[0029] Pushed image to 1 destinations               

[build-push : write-url] us-east1-docker.pkg.dev/my-tekton-tests/tekton-samples/docsy:v1


Full code samples
The Pipeline:
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: clone-build-push
spec:
  description: |
    This pipeline clones a git repo, builds a Docker image with Kaniko and
    pushes it to a registry    
  params:
  - name: repo-url
    type: string
  - name: image-reference
    type: string
  workspaces:
  - name: shared-data
  - name: docker-credentials
  tasks:
  - name: fetch-source
    taskRef:
      name: git-clone
    workspaces:
    - name: output
      workspace: shared-data
    params:
    - name: url
      value: $(params.repo-url)
  - name: build-push
    runAfter: ["fetch-source"]
    taskRef:
      name: kaniko
    workspaces:
    - name: source
      workspace: shared-data
    - name: dockerconfig
      workspace: docker-credentials
    params:
    - name: IMAGE
      value: $(params.image-reference)
The PipelineRun:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: clone-build-push-run-
spec:
  pipelineRef:
    name: clone-build-push
  podTemplate:
    securityContext:
      fsGroup: 65532
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
  - name: docker-credentials
    secret:
      secretName: docker-credentials
  params:
  - name: repo-url
    value: https://github.com/google/docsy-example.git
  - name: image-reference
    value: container.registry.com/sublocation/my_app:version 
The Docker credentials Kubernetes Secret:
apiVersion: v1
kind: Secret
metadata:
  name: docker-credentials
data:
  config.json: efuJAmF1...
Use your credentials as the value of the data field. Check the registry
authentication section for more
information.
Further reading

Clone a git repository with Tekton.
More Tekton Pipelines examples (code).


	
		

Feedback
Was this page helpful?
Yes
No

  


  




		
	
	
	
  Last modified January 29, 2023: Rework left-hand nav order (dbe0a93)\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Install Tekton Pipelines
	Install Tekton Pipelines on your cluster
	
		

  
    


  
    


  

		
	    
	


   
This page includes content about running Tekton with specific platforms and
cloud providers. The accuracy and freshness of this vendor documentation varies
by vendor.
If you want to contribute with platform-specific documentation, follow the
vendor contributions guidelines.



This guide explains how to install Tekton Pipelines.
Prerequisites

A Kubernetes cluster running version 1.28 or later.
Kubectl.
Grant cluster-admin privileges to the current user. See the Kubernetes
role-based access control (RBAC) docs for more information.
(Optional) Install a Metrics Server if you need support for high
availability use cases.

See the local installation guide if you want to test Tekton on
your computer.
Installation










      Kubernetes
    

    
      
    
      Google Cloud
    

    
      
    
      OpenShift
    

    
      
    





        To install Tekton Pipelines on a Kubernetes cluster:


Run one of the following commands depending on which version of Tekton
Pipelines you want to install:


Latest official release:
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
Note: These instructions are ideal as a quick start installation guide with Tekton Pipelines and not meant for the production use. Please refer to the operator to install, upgrade and manage Tekton projects.


Nightly release:
kubectl apply --filename https://storage.googleapis.com/tekton-releases-nightly/pipeline/latest/release.yaml


Specific release:
 kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/previous/<version_number>/release.yaml
Replace <version_number> with the numbered version you want to install.
For example, v0.26.0.


Untagged release:
If your container runtime does not support image-reference:tag@digest:
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.notags.yaml




Multi-tenant installation is only partially supported today, read the guide
for reference.


Monitor the installation:
kubectl get pods --namespace tekton-pipelines --watch
When all components show 1/1 under the READY column, the installation is
complete. Hit Ctrl + C to stop monitoring.


Congratulations! You have successfully installed Tekton Pipelines on your
Kubernetes cluster.


    
      
    

  
          Before you proceed, create or select a project on Google Cloud and install the
gcloud CLI on your computer.
To install Tekton Pipelines:


Enable the Google Kubernetes Engine (GKE) API:
gcloud services enable container.googleapis.com


Create a cluster with Workload Identity enabled. For example:
gcloud container clusters create tekton-cluster \
  --num-nodes=<nodes> \
  --region=<location> \
  --workload-pool=<project-id>.svc.id.goog
Where:


<location> is the cluster location. For example, us-central1.
See the documentation about regional and zonal
clusters for more information.


<project-id> is the project ID.


<nodes> is the number of nodes.


Workload Identity allows your GKE cluster to access Google Cloud services
using an Identity Access Management (IAM) service account.
For example, the Tekton build and push guide explains how to
authenticate to Artifact Registry on a cluster with Workload Identity
enabled.
You can also enable Workload Idenitity on an existing cluster.


Follow the regular Kubernetes installation steps.


Private clusters
If you are running a private cluster and experience problems
with GKE DNS resolution, allow the port 8443 in your firewall
rules.
gcloud compute firewall-rules update <firewall_rule_name> --allow tcp:8443
See the documentation about firewall rules for private
clusters for more information.
Autopilot
If you are using Autopilot mode on your GKE cluster and
experience some problems, try the following:


Allow port 8443 in your firewall rules.
gcloud compute firewall-rules update <firewall_rule_name> --allow tcp:8443


Disable the affinity assistant.
kubectl patch cm feature-flags -n tekton-pipelines \
  -p '{"data":{"disable-affinity-assistant":"true"}}'


Increase the ephemeral storage.




    
      
    

  
          To install Tekton Pipelines on OpenShift, you must first apply the anyuid
security context constraint to the tekton-pipelines-controller service
account. This is required to run the webhook Pod.  See Security Context
Constraints for more information.


Log on as a user with cluster-admin privileges. The following example
uses the default system:admin user:
oc login -u system:admin


Set up the namespace (project) and configure the service account:
oc new-project tekton-pipelines
oc adm policy add-scc-to-user anyuid -z tekton-pipelines-controller
oc adm policy add-scc-to-user anyuid -z tekton-pipelines-webhook


Install Tekton Pipelines:
Because OpenShift uses random user id (and user id range per namespace) for pods, we need to remove the securityContext.runAsUser and securityContext.runAsGroup from any container from the release.yaml.
You will need to have yq installed for this to work. Another way would be to download the yaml, search and replace (here replace with nothing) in your favourite editor.
curl https://storage.googleapis.com/tekton-releases/pipeline/latest/release.notags.yaml | yq 'del(.spec.template.spec.containers[].securityContext.runAsUser, .spec.template.spec.containers[].securityContext.runAsGroup)' | oc apply -f -
See the OpenShift CLI documentation for more information on
the oc command.


Monitor the installation using the following command until all components
show a Running status:
oc get pods --namespace tekton-pipelines --watch
Note: Hit CTRL + C to stop monitoring.


Congratulations! You have successfully installed Tekton Pipelines on your
OpenShift environment.
To run OpenShift 4.x on your laptop (or desktop), take a look at Red Hat
CodeReady Containers.


    
      
    

  


Additional configuration options
You can enable additional alpha and beta features, customize execution
parameters, configure availability, and many more options. See the
addition configurations options for more information.
Next steps
To get started with Tekton check the Introductory tutorials,
the how-to guides, and the examples folder.


	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Resolver Reference
Writing a resolver is made easier with the
github.com/tektoncd/pipeline/pkg/resolution/resolver/framework package.
This package exposes a number of interfaces that let your code control
what kind of behaviour it should have when running.
To get started really quickly see the resolver
template, or for a howto guide see how to write
a resolver.
The Resolver Interface
Implementing this interface is required. It provides just enough
configuration for the framework to get a resolver running.










      Upgraded Framework
    

    
      
    
      Previous Framework (Deprecated)
    

    
      
    





        


Method  to Implement
Description




Initialize
Use this method to perform any setup required before the resolver starts receiving requests.


GetName
Use this method to return a name to refer to your Resolver by. e.g. "Git"


GetSelector
Use this method to specify the labels that a resolution request must have to be routed to your resolver.


Validate
Use this method to validate the resolution Spec given to your resolver.


Resolve
Use this method to perform get the resource based on the ResolutionRequestSpec as input and return it, along with any metadata about it in annotations





    
      
    

  
        


Method  to Implement
Description




Initialize
Use this method to perform any setup required before the resolver starts receiving requests.


GetName
Use this method to return a name to refer to your Resolver by. e.g. "Git"


GetSelector
Use this method to specify the labels that a resolution request must have to be routed to your resolver.


ValidateParams
Use this method to validate the params given to your resolver.


Resolve
Use this method to perform get the resource based on params as input and return it, along with any metadata about it in annotations





    
      
    

  


The ConfigWatcher Interface
Implement this optional interface if your Resolver requires some amount
of admin configuration. For example, if you want to allow admin users to
configure things like timeouts, namespaces, lists of allowed registries,
api endpoints or base urls, service account names to use, etcâ€¦



Method to Implement
Description




GetConfigName
Use this method to return the name of the configmap admins will use to configure this resolver. Once this interface is implemented your Validate and Resolve methods will be able to access your latest resolver configuration by calling framework.GetResolverConfigFromContext(ctx). Note that this configmap must exist when your resolver starts - put a default one in your resolverâ€™s config/ directory.



The TimedResolution Interface
Implement this optional interface if your Resolver needs to custimze the
timeout a resolution request can take. This may be based on knowledge of
the underlying storage (e.g. some git repositories are slower to clone
than others) or might be something an admin configures with a configmap.
The default timeout of a request is 1 minute if this interface is not
implemented. Note: There is currently a global maximum timeout of 1
minute for all resolution requests to prevent zombie requests
remaining in an incomplete state forever.



Method to Implement
Description




GetResolutionTimeout
Return a custom timeout duration from this method to control how long a resolution request to this resolver may take.




	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Migrating From Tekton v1alpha1 to Tekton v1beta1

Changes to fields
Changes to input parameters
Replacing PipelineResources with Tasks

Replacing a git resource
Replacing a pullrequest resource
Replacing a gcs resource
Replacing an image resource
Replacing a cluster resource


Changes to PipelineResources

This document describes the differences between v1alpha1 Tekton entities and their
v1beta1 counterparts. It also describes how to replace the supported types of
PipelineResources with Tasks from the Tekton Catalog of equivalent functionality.
Changes to fields
In Tekton v1beta1, the following fields have been changed:



Old field
New field




spec.inputs.params
spec.params


spec.inputs
Removed from Tasks


spec.outputs
Removed from Tasks


spec.inputs.resources
spec.resources.inputs


spec.outputs.resources
spec.resources.outputs



Changes to input parameters
In Tekton v1beta1, input parameters have been moved from spec.inputs.params to spec.params.
For example, consider the following v1alpha1 parameters:
# Task.yaml (v1alpha1)
spec:
  inputs:
    params:
      - name: ADDR
        description: Address to curl.
        type: string

# TaskRun.yaml (v1alpha1)
spec:
  inputs:
    params:
      - name: ADDR
        value: https://example.com/foo.json
The above parameters are now represented as follows in v1beta1:
# Task.yaml (v1beta1)
spec:
  params:
    - name: ADDR
      description: Address to curl.
      type: string

# TaskRun.yaml (v1beta1)
spec:
  params:
    - name: ADDR
      value: https://example.com/foo.json
Replacing PipelineResources with Tasks
See â€œReplacing PipelineResources with Tasksâ€ for information and examples on how to replace PipelineResources when migrating from v1alpha1 to v1beta1.
Changes to PipelineResources
In Tekton v1beta1, PipelineResources have been moved from spec.input.resources
and spec.output.resources to spec.resources.inputs and spec.resources.outputs,
respectively.
For example, consider the following v1alpha1 definition:
# Task.yaml (v1alpha1)
spec:
  inputs:
    resources:
      - name: skaffold
        type: git
  outputs:
    resources:
      - name: baked-image
        type: image

# TaskRun.yaml (v1alpha1)
spec:
  inputs:
    resources:
      - name: skaffold
        resourceSpec:
          type: git
          params:
            - name: revision
              value: v0.32.0
            - name: url
              value: https://github.com/GoogleContainerTools/skaffold
  outputs:
    resources:
      - name: baked-image
        resourceSpec:
          - type: image
            params:
              - name: url
                value: gcr.io/foo/bar
The above definition becomes the following in v1beta1:
# Task.yaml (v1beta1)
spec:
  resources:
    inputs:
      - name: src-repo
        type: git
    outputs:
      - name: baked-image
        type: image

# TaskRun.yaml (v1beta1)
spec:
  resources:
    inputs:
      - name: src-repo
        resourceSpec:
          type: git
          params:
            - name: revision
              value: main
            - name: url
              value: https://github.com/tektoncd/pipeline
    outputs:
      - name: baked-image
        resourceSpec:
          - type: image
            params:
              - name: url
                value: gcr.io/foo/bar

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Getting Started with Resolvers
Introduction
This guide will take you from an empty Kubernetes cluster to a
functioning Tekton Pipelines installation and a PipelineRun executing
with a Pipeline stored in a git repo.
Prerequisites

A computer with
kubectl.
A Kubernetes cluster running at least Kubernetes 1.28. A kind
cluster
should work fine for following the guide on your local machine.
An image registry that you can push images to. If youâ€™re using kind
make sure your KO_DOCKER_REPO environment variable is set to
kind.local.
A publicly available git repository where you can put a pipeline yaml
file.

Step 1: Install Tekton Pipelines and the Resolvers
See the installation instructions for Tekton Pipeline, and
the installation instructions for the built-in resolvers.
Step 2: Ensure Pipelines is configured to enable resolvers
Starting with v0.41.0, remote resolvers for Tekton Pipelines are enabled by default,
but can be disabled via feature flags in the resolvers-feature-flags configmap in
the tekton-pipelines-resolvers namespace. Check that configmap to verify that the
resolvers you wish to have enabled are set to "true".
The feature flags for the built-in resolvers are:

The bundles resolver: enable-bundles-resolver
The git resolver: enable-git-resolver
The hub resolver: enable-hub-resolver
The cluster resolver: enable-cluster-resolver

Step 3: Try it out!
In order to test out your install youâ€™ll need a Pipeline stored in a
public git repository. First cd into a clone of your repo and then
create a new branch:
# checkout a new branch in the public repo you're using
git checkout -b add-a-simple-pipeline
Then create a basic pipeline:
cat <<"EOF" > pipeline.yaml
kind: Pipeline
apiVersion: tekton.dev/v1beta1
metadata:
  name: a-simple-pipeline
spec:
  params:
  - name: username
  tasks:
  - name: task-1
    params:
    - name: username
      value: $(params.username)
    taskSpec:
      params:
      - name: username
      steps:
      - image: alpine:3.15
        script: |
          echo "hello $(params.username)"
EOF
Commit the pipeline and push it to your git repo:
git add ./pipeline.yaml
git commit -m "Add a basic pipeline to test Tekton Pipeline remote resolution"

# push to your publicly accessible repository, replacing origin with
# your git remote's name
git push origin add-a-simple-pipeline
And finally create a PipelineRun that uses your pipeline:
# first assign your public repo's url to an environment variable
REPO_URL=# insert your repo's url here

# create a pipelinerun yaml file
cat <<EOF > pipelinerun.yaml
kind: PipelineRun
apiVersion: tekton.dev/v1beta1
metadata:
  name: run-basic-pipeline-from-git
spec:
  pipelineRef:
    resolver: git
    params:
    - name: url
      value: ${REPO_URL}
    - name: revision
      value: add-a-simple-pipeline
    - name: pathInRepo
      value: pipeline.yaml
  params:
  - name: username
    value: liza
EOF

# execute the pipelinerun
kubectl apply -f ./pipelinerun.yaml
Step 6: Monitor the PipelineRun
First letâ€™s watch the PipelineRun to see if it succeeds:
kubectl get pipelineruns -w
Shortly the PipelineRun should move into a Succeeded state.
Now we can check the logs of the PipelineRunâ€™s only task:
kubectl logs run-basic-pipeline-from-git-task-1-pod
# This should print "hello liza"

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	How to write a Resolver
This how-to will outline the steps a developer needs to take when creating
a new (very basic) Resolver. Rather than focus on support for a particular version
control system or cloud platform this Resolver will simply respond with
some hard-coded YAML.
If you arenâ€™t yet familiar with the meaning of â€œresolutionâ€ when it
comes to Tekton, a short summary follows. You might also want to read a
little bit into Tekton Pipelines, particularly the docs on specifying a
target Pipeline to
run
and, if youâ€™re feeling particularly brave or bored, the really long
design doc describing Tekton
Resolution.
Whatâ€™s a Resolver?
A Resolver is a program that runs in a Kubernetes cluster alongside
Tekton Pipelines and â€œresolvesâ€
requests for Tasks and Pipelines from remote locations. More
concretely: if a user submitted a PipelineRun that needed a Pipeline
YAML stored in a git repo, then it would be a Resolver thatâ€™s
responsible for fetching the YAML file from git and returning it to
Tekton Pipelines.
This pattern extends beyond just git, allowing a developer to integrate
support for other version control systems, cloud buckets, or storage systems
without having to modify Tekton Pipelines itself.
Just want to see the working example?
If youâ€™d prefer to look at the end result of this howto you can take a
visit the
./resolver-template
in the Tekton Resolution repo. That template is built on the code from
this howto to get you up and running quickly.
Pre-requisites
Before getting started with this howto youâ€™ll need to be comfortable
developing in Go and have a general understanding of how Tekton
Resolution works.
Youâ€™ll also need the following:

A computer with
kubectl and
ko installed.
A Kubernetes cluster running at least Kubernetes 1.28. A kind
cluster
should work fine for following the guide on your local machine.
An image registry that you can push images to. If youâ€™re using kind
make sure your KO_DOCKER_REPO environment variable is set to
kind.local.
Tekton Pipelines and remote resolvers installed in your Kubernetes
cluster. See the installation
guide for
instructions on installing it.

First Steps
The first thing to do is create an initial directory structure for your
project. For this example weâ€™ll create a directory and initialize a new
go module with a few subdirectories for our code:
$ mkdir demoresolver

$ cd demoresolver

$ go mod init example.com/demoresolver

$ mkdir -p cmd/demoresolver

$ mkdir config
The cmd/demoresolver directory will contain code for the resolver and the
config directory will eventually contain a yaml file for deploying the
resolver to Kubernetes.
Initializing the resolverâ€™s binary
A Resolver is ultimately just a program running in your cluster, so the
first step is to fill out the initial code for starting that program.
Our resolver here is going to be extremely simple and doesnâ€™t need any
flags or special environment variables, so weâ€™ll just initialize it with
a little bit of boilerplate.
Create cmd/demoresolver/main.go with the following setup code:










      Latest Framework
    

    
      
    
      Previous Framework (Deprecated)
    

    
      
    





        package main

import (
  "context"
  "github.com/tektoncd/pipeline/pkg/remoteresolution/resolver/framework"
  "knative.dev/pkg/injection/sharedmain"
)

func main() {
  sharedmain.Main("controller",
    framework.NewController(context.Background(), &resolver{}),
  )
}

type resolver struct {}


    
      
    

  
        package main

import (
  "context"
  "github.com/tektoncd/pipeline/pkg/resolution/resolver/framework"
  "knative.dev/pkg/injection/sharedmain"
)

func main() {
  sharedmain.Main("controller",
    framework.NewController(context.Background(), &resolver{}),
  )
}

type resolver struct {}


    
      
    

  


This wonâ€™t compile yet but you can download the dependencies by running:
# Depending on your go version you might not need the -compat flag
$ go mod tidy -compat=1.17
Writing the Resolver
If you try to build the binary right now youâ€™ll receive the following
error:
$ go build -o /dev/null ./cmd/demoresolver

cmd/demoresolver/main.go:11:78: cannot use &resolver{} (type *resolver) as
type framework.Resolver in argument to framework.NewController:
        *resolver does not implement framework.Resolver (missing GetName method)
Weâ€™ve already defined our own resolver type but in order to get the
resolver running youâ€™ll need to add the methods defined in the
framework.Resolver interface
to your main.go file. Going through each method in turn:
The Initialize method
This method is used to start any libraries or otherwise setup any
prerequisites your resolver needs. For this example we wonâ€™t need
anything so this method can just return nil.
// Initialize sets up any dependencies needed by the resolver. None atm.
func (r *resolver) Initialize(context.Context) error {
  return nil
}
The GetName method
This method returns a string name that will be used to refer to this
resolver. Youâ€™d see this name show up in places like logs. For this
simple example weâ€™ll return "Demo":
// GetName returns a string name to refer to this resolver by.
func (r *resolver) GetName(context.Context) string {
  return "Demo"
}
The GetSelector method
This method should return a map of string labels and their values that
will be used to direct requests to this resolver. For this example the
only label weâ€™re interested in matching on is defined by
tektoncd/resolution:
// GetSelector returns a map of labels to match requests to this resolver.
func (r *resolver) GetSelector(context.Context) map[string]string {
  return map[string]string{
    common.LabelKeyResolverType: "demo",
  }
}
What this does is tell the resolver framework that any
ResolutionRequest object with a label of
"resolution.tekton.dev/type": "demo" should be routed to our
example resolver.
Weâ€™ll also need to add another import for this package at the top:










      Latest Framework
    

    
      
    
      Previous Framework (Deprecated)
    

    
      
    





        import (
  "context"
  
  // Add this one; it defines LabelKeyResolverType we use in GetSelector
  "github.com/tektoncd/pipeline/pkg/resolution/common"
  "github.com/tektoncd/pipeline/pkg/remoteresolution/resolver/framework"
  "knative.dev/pkg/injection/sharedmain"
  pipelinev1 "github.com/tektoncd/pipeline/pkg/apis/pipeline/v1"
)


    
      
    

  
        import (
  "context"
  
  // Add this one; it defines LabelKeyResolverType we use in GetSelector
  "github.com/tektoncd/pipeline/pkg/resolution/common"
  "github.com/tektoncd/pipeline/pkg/resolution/resolver/framework"
  "knative.dev/pkg/injection/sharedmain"
  pipelinev1 "github.com/tektoncd/pipeline/pkg/apis/pipeline/v1"
)


    
      
    

  


The Validate method
The Validate method checks that the resolution-spec submitted as part of
a resolution request are valid. Our example resolver doesnâ€™t expect
any params in the spec so weâ€™ll simply ensure that the there are no params.
Our example resolver also expects format for the url to be demoscheme://<path> so weâ€™ll validate this format.
In the previous version, this was instead called ValidateParams method. See below
for the differences.










      Latest Framework
    

    
      
    
      Previous Framework (Deprecated)
    

    
      
    





        // Validate ensures that the resolution spec from a request is as expected.
func (r *resolver) Validate(ctx context.Context, req *v1beta1.ResolutionRequestSpec) error {
  if len(req.Params) > 0 {
    return errors.New("no params allowed")
  }
  url := req.URL
  u, err := neturl.ParseRequestURI(url)
  if err != nil {
    return err
  }
  if u.Scheme != "demoscheme" {
    return fmt.Errorf("Invalid Scheme. Want %s, Got %s", "demoscheme", u.Scheme)
  }
  if u.Path == "" {
    return errors.New("Empty path.")
  }
  return nil
}
Youâ€™ll also need to add the net/url as neturl and "errors" package to your list of imports at
the top of the file.


    
      
    

  
        // ValidateParams ensures that the params from a request are as expected.
func (r *resolver) ValidateParams(ctx context.Context, params []pipelinev1.Param) error {
  if len(req.Params) > 0 {
    return errors.New("no params allowed")
  }
  return nil
}


    
      
    

  


Youâ€™ll also need to add the "errors" package to your list of imports at
the top of the file.
The Resolve method
We implement the Resolve method to do the heavy lifting of fetching
the contents of a file and returning them.  It takes in the resolution request spec as input.
For this example weâ€™re just going to return a hard-coded string of YAML. Since Tekton Pipelines
currently only supports fetching Pipeline resources via remote
resolution thatâ€™s what weâ€™ll return.
The method signature weâ€™re implementing here has a
framework.ResolvedResource interface as one of its return values. This
is another type we have to implement but it has a small footprint:










      Latest Framework
    

    
      
    
      Previous Framework (Deprecated)
    

    
      
    





        // Resolve uses the given resolution spec to resolve the requested file or resource.
func (r *resolver) Resolve(ctx context.Context, req *v1beta1.ResolutionRequestSpec) (framework.ResolvedResource, error) {
  return &myResolvedResource{}, nil
}

// our hard-coded resolved file to return
const pipeline = `
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: my-pipeline
spec:
  tasks:
  - name: hello-world
    taskSpec:
      steps:
      - image: alpine:3.15.1
        script: |
          echo "hello world"
`

// myResolvedResource wraps the data we want to return to Pipelines
type myResolvedResource struct {}

// Data returns the bytes of our hard-coded Pipeline
func (*myResolvedResource) Data() []byte {
  return []byte(pipeline)
}

// Annotations returns any metadata needed alongside the data. None atm.
func (*myResolvedResource) Annotations() map[string]string {
  return nil
}

// RefSource is the source reference of the remote data that records where the remote 
// file came from including the url, digest and the entrypoint. None atm.
func (*myResolvedResource) RefSource() *pipelinev1.RefSource {
	return nil
}


    
      
    

  
        // Resolve uses the given resolution spec to resolve the requested file or resource.
func (r *resolver) Resolve(ctx context.Context, params []pipelinev1.Param) (framework.ResolvedResource, error) {
  return &myResolvedResource{}, nil
}

// our hard-coded resolved file to return
const pipeline = `
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: my-pipeline
spec:
  tasks:
  - name: hello-world
    taskSpec:
      steps:
      - image: alpine:3.15.1
        script: |
          echo "hello world"
`

// myResolvedResource wraps the data we want to return to Pipelines
type myResolvedResource struct {}

// Data returns the bytes of our hard-coded Pipeline
func (*myResolvedResource) Data() []byte {
  return []byte(pipeline)
}

// Annotations returns any metadata needed alongside the data. None atm.
func (*myResolvedResource) Annotations() map[string]string {
  return nil
}

// RefSource is the source reference of the remote data that records where the remote 
// file came from including the url, digest and the entrypoint. None atm.
func (*myResolvedResource) RefSource() *pipelinev1.RefSource {
	return nil
}


    
      
    

  


// Resolve uses the given resolution spec to resolve the requested file or resource.
func (r *resolver) Resolve(ctx context.Context, req *v1beta1.ResolutionRequestSpec) (framework.ResolvedResource, error) {
  return &myResolvedResource{}, nil
}

// our hard-coded resolved file to return
const pipeline = `
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: my-pipeline
spec:
  tasks:
  - name: hello-world
    taskSpec:
      steps:
      - image: alpine:3.15.1
        script: |
          echo "hello world"
`

// myResolvedResource wraps the data we want to return to Pipelines
type myResolvedResource struct {}

// Data returns the bytes of our hard-coded Pipeline
func (*myResolvedResource) Data() []byte {
  return []byte(pipeline)
}

// Annotations returns any metadata needed alongside the data. None atm.
func (*myResolvedResource) Annotations() map[string]string {
  return nil
}

// RefSource is the source reference of the remote data that records where the remote 
// file came from including the url, digest and the entrypoint. None atm.
func (*myResolvedResource) RefSource() *pipelinev1.RefSource {
	return nil
}
Best practice: In order to enable Tekton Chains to record the source
information of the remote data in the SLSA provenance, the resolver should
implement the RefSource() method to return a correct RefSource value. See the
following example.
// RefSource is the source reference of the remote data that records where the remote 
// file came from including the url, digest and the entrypoint.
func (*myResolvedResource) RefSource() *pipelinev1.RefSource {
	return &v1.RefSource{
		URI: "https://github.com/user/example",
		Digest: map[string]string{
			"sha1": "example",
		},
		EntryPoint: "foo/bar/task.yaml",
	}
}
The deployment configuration
Finally, our resolver needs some deployment configuration so that it can
run in Kubernetes.
A full description of the config is beyond the scope of a short howto
but in summary weâ€™ll tell Kubernetes to run our resolver application
along with some environment variables and other configuration that the
underlying knative framework expects. The deployed application is put
in the tekton-pipelines namespace and uses ko to build its
container image. Finally the ServiceAccount our deployment uses is
tekton-pipelines-resolvers, which is the default ServiceAccount shared by all
resolvers in the tekton-pipelines-resolvers namespace.
The full configuration follows:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demoresolver
  namespace: tekton-pipelines-resolvers
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demoresolver
  template:
    metadata:
      labels:
        app: demoresolver
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: demoresolver
              topologyKey: kubernetes.io/hostname
            weight: 100
      serviceAccountName: tekton-pipelines-resolvers
      containers:
      - name: controller
        image: ko://example.com/demoresolver/cmd/demoresolver
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 1000m
            memory: 1000Mi
        ports:
        - name: metrics
          containerPort: 9090
        env:
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: config-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: config-observability
        - name: METRICS_DOMAIN
          value: tekton.dev/resolution
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
            - all
Phew, ok, put all that in a file at config/demo-resolver-deployment.yaml
and youâ€™ll be ready to deploy your application to Kubernetes and see it
work!
Trying it out
Now that all the code is written your new resolver should be ready to
deploy to a Kubernetes cluster. Weâ€™ll use ko to build and deploy the
application:
$ ko apply -f ./config/demo-resolver-deployment.yaml
Assuming the resolver deployed successfully you should be able to see it
in the output from the following command:
$ kubectl get deployments -n tekton-pipelines

# And here's approximately what you should see when you run this command:
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
controller    1/1     1            1           2d21h
demoresolver  1/1     1            1           91s
webhook       1/1     1            1           2d21
To exercise your new resolver, letâ€™s submit a request for its hard-coded
pipeline. Create a file called test-request.yaml with the following
content:
apiVersion: resolution.tekton.dev/v1beta1
kind: ResolutionRequest
metadata:
  name: test-request
  labels:
    resolution.tekton.dev/type: demo
And submit this request with the following command:
$ kubectl apply -f ./test-request.yaml && kubectl get --watch resolutionrequests
You should soon see your ResolutionRequest printed to screen with a True
value in its SUCCEEDED column:
resolutionrequest.resolution.tekton.dev/test-request created
NAME           SUCCEEDED   REASON
test-request   True
Press Ctrl-C to get back to the command line.
If you now take a look at the ResolutionRequestâ€™s YAML youâ€™ll see the
hard-coded pipeline yaml in its status.data field. It wonâ€™t be totally
recognizable, though, because itâ€™s encoded as base64. Have a look with the
following command:
$ kubectl get resolutionrequest test-request -o yaml
You can convert that base64 data back into yaml with the following
command:
$ kubectl get resolutionrequest test-request -o jsonpath="{$.status.data}" | base64 -d
Great work, youâ€™ve successfully written a Resolver from scratch!
Next Steps
At this point you could start to expand the Resolve() method in your
Resolver to fetch data from your storage backend of choice.
Or if you prefer to take a look at a more fully-realized example of a
Resolver, see the code for the gitresolver hosted in the Tekton
Pipeline repo.
Finally, another direction you could take this would be to try writing a
PipelineRun for Tekton Pipelines that speaks to your Resolver. Can
you get a PipelineRun to execute successfully that uses the hard-coded
Pipeline your Resolver returns?

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Tekton Controller Performance Configuration
Configure ThreadsPerController, QPS and Burst

Overview
Performance Configuration

Configure Thread, QPS and Burst



Overview

This document will show us how to configure tekton-pipeline-controllerâ€™s performance. In general, there are mainly have three parameters will impact the performance of tekton controller, they are ThreadsPerController, QPS and Burst.

ThreadsPerController: Threads (goroutines) to create per controller. Itâ€™s the number of threads to use when processing the controllerâ€™s work queue.




QPS: Queries per Second. Maximum QPS to the master from this client.


Burst: Maximum burst for throttle.


Performance Configuration

Configure Thread, QPS and Burst

Default, the value of ThreadsPerController, QPS and Burst is 2, 5.0 and 10 accordingly.
Sometimes, above default values canâ€™t meet performance requirements, then you need to overwrite these values. You can modify them in the tekton controller deployment. You can specify these customized values in the tekton-pipelines-controller container via threads-per-controller, kube-api-qps and kube-api-burst flags accordingly. For example:
spec:
  serviceAccountName: tekton-pipelines-controller
  containers:
    - name: tekton-pipelines-controller
      image: ko://github.com/tektoncd/pipeline/cmd/controller
      args: [
          "-kube-api-qps", "50",
          "-kube-api-burst", "50",
          "-threads-per-controller", "32",
          # other flags defined here...
        ]
Now, the ThreadsPerController, QPS and Burst have been changed to be 32, 50 and 50.
Note:

Although in above example, you set QPS and Burst to be 50 and 50. However, the actual values of them are multiplied by 2, so the actual QPS and Burst is 100 and 100.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Tekton Controllers flags
The different controllers tektoncd/pipeline ships comes with a set of flags
that can be changed (in the yaml payloads) for advanced use cases. This page
is documenting them.
Common set of flags
The following flags are available on all â€œcontrollersâ€, aka controller, webhook, events and resolvers.
  -add_dir_header
        If true, adds the file directory to the header of the log messages
  -alsologtostderr
        log to standard error as well as files (no effect when -logtostderr=true)
  -cluster string
        Defaults to the current cluster in kubeconfig.
  -disable-ha
        Whether to disable high-availability functionality for this component.  This flag will be deprecated and removed when we have promoted this feature to stable, so do not pass it without filing an issue upstream!
  -kube-api-burst int
        Maximum burst for throttle.
  -kube-api-qps float
        Maximum QPS to the server from the client.
  -kubeconfig string
        Path to a kubeconfig. Only required if out-of-cluster.
  -log_backtrace_at value
        when logging hits line file:N, emit a stack trace
  -log_dir string
        If non-empty, write log files in this directory (no effect when -logtostderr=true)
  -log_file string
        If non-empty, use this log file (no effect when -logtostderr=true)
  -log_file_max_size uint
        Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
  -logtostderr
        log to standard error instead of files (default true)
  -one_output
        If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)
  -server string
        The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.
  -skip_headers
        If true, avoid header prefixes in the log messages
  -skip_log_headers
        If true, avoid headers when opening log files (no effect when -logtostderr=true)
  -stderrthreshold value
        logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2)
  -v value
        number for the log level verbosity
  -vmodule value
        comma-separated list of pattern=N settings for file-filtered logging
controller
The main controller binary has additional flags to configure its behavior.
  -entrypoint-image string
        The container image containing our entrypoint binary.
  -namespace string
        Namespace to restrict informer to. Optional, defaults to all namespaces.
  -nop-image string
        The container image used to stop sidecars
  -resync-period duration
        The period between two resync run (going through all objects) (default 10h0m0s)
  -shell-image string
        The container image containing a shell
  -shell-image-win string
        The container image containing a windows shell
  -sidecarlogresults-image string
        The container image containing the binary for accessing results.
  -threads-per-controller int
        Threads (goroutines) to create per controller (default 2)
  -workingdirinit-image string
        The container image containing our working dir init binary.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	HA Support for Tekton Pipeline Controllers

Overview
Controller HA

Configuring Controller Replicas
Configuring Leader Election
Disabling Controller HA


Webhook HA

Configuring Webhook Replicas
Avoiding Disruptions



Overview
This document is aimed at helping Cluster Admins when configuring High Availability (HA) support for the Tekton Pipeline Controller and Webhook components. HA support allows components to remain operational when a disruption occurs, such as nodes being drained for upgrades.
Controller HA
For the Controller, HA is achieved by following an active/active model, where all replicas of the Controller can receive and process work items. In this HA approach the workqueue is distributed across buckets, where each replica owns a subset of those buckets and can process the load if the given replica is the leader of that bucket.
By default, only one Controller replica is configured, to reduce resource usage. This effectively disables HA for the Controller by default.
Configuring Controller Replicas
In order to achieve HA for the Controller, the number of replicas for the Controller should be greater than one. This allows other instances to take over in case of any disruption on the current active controller.
You can modify the replicas number in the Controller deployment under spec.replicas, or apply an update to a running deployment:
kubectl -n tekton-pipelines scale deployment tekton-pipelines-controller --replicas=3
Configuring Leader Election
Leader election can be configured in config-leader-election.yaml. The ConfigMap defines the following parameters:



Parameter
Default




data.buckets
1


data.leaseDuration
15s


data.renewDeadline
10s


data.retryPeriod
2s



Note: The maximum value of data.buckets at this time is 10.
Disabling Controller HA
If HA is not required, you can disable it by scaling the deployment back to one replica. You can also modify the controller deployment, by specifying in the tekton-pipelines-controller container the disable-ha flag. For example:
spec:
  serviceAccountName: tekton-pipelines-controller
  containers:
    - name: tekton-pipelines-controller
      # ...
      args: [
          # Other flags defined here...
          "-disable-ha=true",
        ]
Note: If you set -disable-ha=false and run multiple replicas of the Controller, each replica will process work items separately, which will lead to unwanted behavior when creating resources (e.g., TaskRuns, etc.).
In general, setting -disable-ha=false is not recommended. Instead, to disable HA, simply run one replica of the Controller deployment.
Webhook HA
The Webhook deployment is stateless, which means it can more easily be configured for HA, and even autoscale replicas in response to load.
By default, only one Webhook replica is configured, to reduce resource usage. This effectively disables HA for the Webhook by default.
Configuring Webhook Replicas
In order to achieve HA for the Webhook deployment, you can modify the replicas number in the Webhook deployment under spec.replicas, or apply an update to a running deployment:
kubectl -n tekton-pipelines scale deployment tekton-pipelines-webhook --replicas=3
You can also modify the HorizontalPodAutoscaler to set a minimum number of replicas:
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tekton-pipelines-webhook
# ...
spec:
  minReplicas: 1

By default, the Webhook deployment is not configured to block a Cluster Autoscaler from scaling down the node thatâ€™s running the only replica of the deployment using the cluster-autoscaler.kubernetes.io/safe-to-evict annotation.
This means that during node drains, the Webhook might be unavailable temporarily, during which time Tekton resources canâ€™t be created, updated or deleted.
To avoid this, you can add the safe-to-evict annotation set to false to block node drains during autoscaling, or, better yet, configure multiple replicas of the Webhook deployment.
Avoiding Disruptions
To avoid the Webhook Service becoming unavailable during node unavailability (e.g., during node upgrades), you can ensure that a minimum number of Webhook replicas are available at time by defining a PodDisruptionBudget which sets a minAvailable greater than zero:
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: tekton-pipelines-webhook
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/name: webhook
    app.kubernetes.io/component: webhook
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
    # ...
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: default
      app.kubernetes.io/part-of: tekton-pipelines
Webhook replicas are configured to avoid being scheduled onto the same node by default, so that a single node disruption doesnâ€™t make all Webhook replicas unavailable.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Deprecations

Introduction
Deprecation Table

Introduction
This doc provides a list of features in Tekton Pipelines that are
deprecated or recently removed.
Deprecation Table
The following features are deprecated but have not yet been removed.



Deprecated Features
Deprecation Announcement
API Compatibility Policy
Earliest Date or Release of Removal




Several fields of Task.Step are deprecated
v0.36.0
Beta
Feb 25, 2023


ClusterTask is deprecated
v0.41.0
Beta
July 13, 2023


The config-trusted-resources configMap is deprecated
v0.45.0
Alpha
v0.46.0


The default-cloud-events-sink setting in the config-defaults configMap is deprecated in favour of the new config-events configMap.
v0.50.0
N/A
v0.59.0


v1beta1 Tasks, TaskRuns, Pipelines, and PipelineRuns are deprecated in favor of v1
v0.50.0
Beta
v0.62.0


The disable-affinity-assistant feature flag is deprecated in favor of the coschedule feature flag
v0.51.0
N/A
April 27, 2024


The resolver framework is deprecated in favor of an upgraded framework
v0.60.0
N/A
v0.72.



v1beta1 deprecation
The v1beta1 versions of Task, TaskRun, Pipeline, and PipelineRun are deprecated in favor of the v1 versions of these APIs,
as of release v0.50.0. Following the beta CRD compatibility policy,
the earliest release the v1beta1 versions of these CRDs may be removed is 1 year later, or v0.62.0 (LTS).
The v1beta1 client libraries will be retained until v0.62.0 has reached its end of life, 1 year later.
Therefore, the earliest release the client libraries may be removed is v0.74.0, 12 months after v0.62.0.
Removed features
The features listed below have been removed but may still be supported in releases that have not reached their EOL.



Removed Feature
Removal Pull Request
Removal Date
Latest LTS Release with Support
EOL of Supported Release




The PipelineRun.Status.TaskRuns and PipelineRun.Status.Runs fields and the embedded-status feature flag along with their functionalities have been tombstoned since v0.45.
[TEP100] Remove Taskruns and Runs Fields for PipelineRunStatus
Jan 25, 2023
v0.44.0
Jan 24, 2024


PipelineResources are removed, along with the components of the API that rely on them as proposed in TEP-0074. See Removed PipelineResources related features for more info.
[TEP074] Remove Generic PipelineResources with Rest of Resources Types
Mar 8, 2023
v0.44.0
Jan 24, 2024


v1alpha1 Runs are removed, as proposed in TEP-0114, along with the feature flags enable-custom-task and custom-task-version.
TEP-0114: Remove support for v1alpha1.Run
April 7, 2023
v0.44.0
Jan 24, 2024



Removed PipelineResources related features:
The following features are removed as part of the deprecation of PipelineResources.
See TEP-0074 for more information.


the fieldstask.spec.resources, taskRun.spec.resources, pipeline.spec.resources, pipelineRun.spec.resources, and taskRun.status.cloudEvents


images built upon PipelineResources

the kubeconfigwriter image used with Cluster PipelineResource
the imagedigestexporter image of Image PipelineResource
the pullrequest-init image used with Pullrequest PipelineResource
the gsutil image used with Storage PipelineResource



The tekton_pipelines_controller_cloudevent_count metric


The artifacts bucket/pvc setup by the pkg/artifacts package related with Storage PipelineResources


The generic pipelineResources functions including inputs and outputs resources and the from type


TaskRun.Status.ResourcesResult is deprecated and tombstoned #6301



	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Debug

Overview
Debugging TaskRuns

Adding Breakpoints

Breakpoint on Failure

Failure of a Step
Halting a Step on failure
Exiting onfailure breakpoint


Breakpoint before step




Debug Environment

Mounts
Debug Scripts



Overview
Debug spec is used for troubleshooting and breakpointing runtime resources. This doc helps understand the inner
workings of debug in Tekton. Currently only the TaskRun resource is supported.
This is an alpha feature. The enable-api-fields feature flag must be set to "alpha"
to specify debug in a taskRun.
Debugging TaskRuns
The following provides explanation on how Debugging TaskRuns is possible through Tekton. To understand how to use
the debug spec for TaskRuns follow the TaskRun Debugging Documentation.
Breakpoint on Failure
Halting a TaskRun execution on Failure of a step.
Failure of a Step
The entrypoint binary is used to manage the lifecycle of a step. Steps are aligned beforehand by the TaskRun controller
allowing each step to run in a particular order. This is done using -wait_file and the -post_file flags. The former
letâ€™s the entrypoint binary know that it has to wait on creation of a particular file before starting execution of the step.
And the latter provides information on the step number and signal the next step on completion of the step.
On success of a step, the -post-file is written as is, signalling the next step which would have the same argument given
for -wait_file to resume the entrypoint process and move ahead with the step.
On failure of a step, the -post_file is written with appending .err to it denoting that the previous step has failed with
and error. The subsequent steps are skipped in this case as well, marking the TaskRun as a failure.
Halting a Step on failure
The failed step writes <step-no>.err to /tekton/run and stops running completely. To be able to debug a step we would
need it to continue running (not exit), not skip the next steps and signal health of the step. By disabling step skipping,
stopping write of the <step-no>.err file and waiting on a signal by the user to disable the halt, we would be simulating a
â€œbreakpointâ€.
In this breakpoint, which is essentially a limbo state the TaskRun finds itself in, the user can interact with the step
environment using a CLI or an IDE.
Exiting onfailure breakpoint
To exit a step which has been paused upon failure, the step would wait on a file similar to <step-no>.breakpointexit which
would unpause and exit the step container. eg: Step 0 fails and is paused. Writing 0.breakpointexit in /tekton/run
would unpause and exit the step container.
Breakpoint before step
TaskRun will be stuck waiting for user debugging before the step execution.
When beforeStep-Breakpoint takes effect, the user can see the following information
from the corresponding step container log:
debug before step breakpoint has taken effect, waiting for user's decision:
1) continue, use cmd: /tekton/debug/scripts/debug-beforestep-continue
2) fail-continue, use cmd: /tekton/debug/scripts/debug-beforestep-fail-continue

Executing /tekton/debug/scripts/debug-beforestep-continue will continue to execute the step program
Executing /tekton/debug/scripts/debug-beforestep-fail-continue will not continue to execute the task, and will mark the step as failed

Debug Environment
Additional environment augmentations made available to the TaskRun Pod to aid in troubleshooting and managing step lifecycle.
Mounts
/tekton/debug/scripts : Contains scripts which the user can run to mark the step as a success, failure or exit the breakpoint.
Shared between all the containers.
/tekton/debug/info/<n> : Contains information about the step. Single EmptyDir shared between all step containers, but renamed
to reflect step number. eg: Step 0 will have /tekton/debug/info/0, Step 1 will have /tekton/debug/info/1 etc.
Debug Scripts
/tekton/debug/scripts/debug-continue : Mark the step as completed with success by writing to /tekton/run. eg: User wants to exit
onfailure breakpoint for failed step 0. Running this script would create /tekton/run/0 and /tekton/run/0/out.breakpointexit.
/tekton/debug/scripts/debug-fail-continue : Mark the step as completed with failure by writing to /tekton/run. eg: User wants to exit
onfailure breakpoint for failed step 0. Running this script would create /tekton/run/0 and /tekton/run/0/out.breakpointexit.err.
/tekton/debug/scripts/debug-beforestep-continue : Mark the step continue to execute by writing to /tekton/run. eg: User wants to exit
before step breakpoint for before step 0. Running this script would create /tekton/run/0 and /tekton/run/0/out.beforestepexit.
/tekton/debug/scripts/debug-beforestep-fail-continue : Mark the step not continue to execute by writing to /tekton/run. eg: User wants to exit
before step breakpoint for before step 0. Running this script would create /tekton/run/0 and /tekton/run/0/out.beforestepexit.err.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Additional Configuration Options
	Additional configurations when installing Tekton Pipelines
	
		

  
    


  
    


  

		
	    
	This document describes additional options to configure your Tekton Pipelines
installation.
Table of Contents

Configuring built-in remote Task and Pipeline resolution
Configuring CloudEvents notifications
Configuring self-signed cert for private registry
Configuring environment variables
Customizing basic execution parameters

Customizing the Pipelines Controller behavior
Alpha Features
Beta Features


Enabling larger results using sidecar logs
Configuring High Availability
Configuring tekton pipeline controller performance
Platform Support
Creating a custom release of Tekton Pipelines
Verify Tekton Pipelines Release

Verify signatures using cosign
Verify the transparency logs using rekor-cli


Verify Tekton Resources
Pipelinerun with Affinity Assistant
TaskRuns with imagePullBackOff Timeout
Disabling Inline Spec in TaskRun and PipelineRun
Next steps

Configuring built-in remote Task and Pipeline resolution
Four remote resolvers are currently provided as part of the Tekton Pipelines installation.
By default, these remote resolvers are enabled. Each resolver can be disabled by setting
the appropriate feature flag in the resolvers-feature-flags ConfigMap in the tekton-pipelines-resolvers
namespace:

The bundles resolver, disabled by setting the enable-bundles-resolver
feature flag to false.
The git resolver, disabled by setting the enable-git-resolver
feature flag to false.
The hub resolver, disabled by setting the enable-hub-resolver
feature flag to false.
The cluster resolver, disabled by setting the enable-cluster-resolver
feature flag to false.

Configuring CloudEvents notifications
When configured so, Tekton can generate CloudEvents for TaskRun,
PipelineRun and CustomRunlifecycle events. The main configuration parameter is the
URL of the sink. When not set, no notification is generated.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-events
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  formats: tektonv1
  sink: https://my-sink-url
The sink used to be configured in the config-defaults config map.
This option is still available, but deprecated, and will be removed.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  default-cloud-events-sink: https://my-sink-url
Additionally, CloudEvents for CustomRuns require an extra configuration to be
enabled. This setting exists to avoid collisions with CloudEvents that might
be sent by custom task controllers:
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  send-cloudevents-for-runs: true
Configuring self-signed cert for private registry
The SSL_CERT_DIR is set to /etc/ssl/certs as the default cert directory. If you are using a self-signed cert for private registry and the cert file is not under the default cert directory, configure your registry cert in the config-registry-cert ConfigMap with the key cert.
Configuring environment variables
Environment variables can be configured in the following ways, mentioned in order of precedence from lowest to highest.

Implicit environment variables
Step/StepTemplate environment variables
Environment variables specified via a default PodTemplate.
Environment variables specified via a PodTemplate.

The environment variables specified by a PodTemplate supercedes all other ways of specifying environment variables. However, there exists a configuration i.e. default-forbidden-env, the environment variable specified in this list cannot be updated via a PodTemplate.
For example:
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
data:
  default-timeout-minutes: "50"
  default-service-account: "tekton"
  default-forbidden-env: "TEST_TEKTON"
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: mytask
  namespace: default
spec:
  steps:
    - name: echo-env
      image: ubuntu
      command: ["bash", "-c"]
      args: ["echo $TEST_TEKTON "]
      env:
          - name: "TEST_TEKTON"
            value: "true"
---
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: mytaskrun
  namespace: default
spec:
  taskRef:
    name: mytask
  podTemplate:
    env:
        - name: "TEST_TEKTON"
          value: "false"
In the above example the environment variable TEST_TEKTON will not be overriden by value specified in podTemplate, because the config-default option default-forbidden-env is configured with value TEST_TEKTON.
Configuring default resources requirements
Resource requirements of containers created by the controller can be assigned default values. This allows to fully control the resources requirement of TaskRun.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
data:
  default-container-resource-requirements: |
    place-scripts: # updates resource requirements of a 'place-scripts' container
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"    
  
    prepare: # updates resource requirements of a 'prepare' container
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "256Mi"
        cpu: "500m"
  
    working-dir-initializer: # updates resource requirements of a 'working-dir-initializer' container
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "512Mi"
        cpu: "500m"
  
    prefix-scripts: # updates resource requirements of containers which starts with 'scripts-'
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  
    prefix-sidecar-scripts: # updates resource requirements of containers which starts with 'sidecar-scripts-'
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  
    default: # updates resource requirements of init-containers and containers which has empty resource resource requirements
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "256Mi"
        cpu: "500m"
Any resource requirements set at the Task and TaskRun levels will overidde the default one specified in the config-defaults configmap.
Customizing basic execution parameters
You can specify your own values that replace the default service account (ServiceAccount), timeout (Timeout), resolver (Resolver), and Pod template (PodTemplate) values used by Tekton Pipelines in TaskRun and PipelineRun definitions. To do so, modify the ConfigMap config-defaults with your desired values.
The example below customizes the following:

the default service account from default to tekton.
the default timeout from 60 minutes to 20 minutes.
the default app.kubernetes.io/managed-by label is applied to all Pods created to execute TaskRuns.
the default Pod template to include a node selector to select the node where the Pod will be scheduled by default. A list of supported fields is available here.
For more information, see PodTemplate in TaskRuns or PodTemplate in PipelineRuns.
the default Workspace configuration can be set for any Workspaces that a Task declares but that a TaskRun does not explicitly provide.
the default maximum combinations of Parameters in a Matrix that can be used to fan out a PipelineTask. For
more information, see Matrix.
the default resolver type to git.

apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
data:
  default-service-account: "tekton"
  default-timeout-minutes: "20"
  default-pod-template: |
    nodeSelector:
      kops.k8s.io/instancegroup: build-instance-group    
  default-managed-by-label-value: "my-tekton-installation"
  default-task-run-workspace-binding: |
        emptyDir: {}
  default-max-matrix-combinations-count: "1024"
  default-resolver-type: "git"
Note: The _example key in the provided config-defaults.yaml
file lists the keys you can customize along with their default values.
Customizing the Pipelines Controller behavior
To customize the behavior of the Pipelines Controller, modify the ConfigMap feature-flags via
kubectl edit configmap feature-flags -n tekton-pipelines.
Note: Changing feature flags may result in undefined behavior for TaskRuns and PipelineRuns
that are running while the change occurs.
The flags in this ConfigMap are as follows:


disable-affinity-assistant - set this flag to true to disable the Affinity Assistant
that is used to provide Node Affinity for TaskRun pods that share workspace volume.
The Affinity Assistant is incompatible with other affinity rules
configured for TaskRun pods.
Note: This feature flag is deprecated and will be removed in release v0.60. Consider using coschedule feature flag to configure Affinity Assistant behavior.
Note: Affinity Assistant use Inter-pod affinity and anti-affinity
that require substantial amount of processing which can slow down scheduling in large clusters
significantly. We do not recommend using them in clusters larger than several hundred nodes
Note: Pod anti-affinity requires nodes to be consistently labelled, in other words every
node in the cluster must have an appropriate label matching topologyKey. If some or all nodes
are missing the specified topologyKey label, it can lead to unintended behavior.


coschedule: set this flag determines how PipelineRun Pods are scheduled with Affinity Assistant.
Acceptable values are â€œworkspacesâ€ (default), â€œpipelinerunsâ€, â€œisolate-pipelinerunâ€, or â€œdisabledâ€.
Setting it to â€œworkspacesâ€ will schedule all the taskruns sharing the same PVC-based workspace in a pipelinerun to the same node.
Setting it to â€œpipelinerunsâ€ will schedule all the taskruns in a pipelinerun to the same node.
Setting it to â€œisolate-pipelinerunâ€ will schedule all the taskruns in a pipelinerun to the same node,
and only allows one pipelinerun to run on a node at a time. Setting it to â€œdisabledâ€ will not apply any coschedule policy.


await-sidecar-readiness: set this flag to "false" to allow the Tekton controller to start a
TasksRunâ€™s first step immediately without waiting for sidecar containers to be running first. Using
this option should decrease the time it takes for a TaskRun to start running, and will allow TaskRun
pods to be scheduled in environments that donâ€™t support Downward API
volumes (e.g. some virtual kubelet implementations). However, this may lead to unexpected behaviour
with Tasks that use sidecars, or in clusters that use injected sidecars (e.g. Istio). Setting this flag
to "false" will mean the running-in-environment-with-injected-sidecars flag has no effect.


running-in-environment-with-injected-sidecars: set this flag to "false" to allow the
Tekton controller to start a TasksRunâ€™s first step immediately if it has no Sidecars specified.
Using this option should decrease the time it takes for a TaskRun to start running.
However, for clusters that use injected sidecars (e.g. Istio) this can lead to unexpected behavior.


require-git-ssh-secret-known-hosts: set this flag to "true" to require that
Git SSH Secrets include a known_hosts field. This ensures that a git remote serverâ€™s
key is validated before data is accepted from it when authenticating over SSH. Secrets
that donâ€™t include a known_hosts will result in the TaskRun failing validation and
not running.


enable-tekton-oci-bundles: set this flag to "true" to enable the
tekton OCI bundle usage (see the tekton bundle
contract). Enabling this option
allows the use of bundle field in taskRef and pipelineRef for
Pipeline, PipelineRun and TaskRun. By default, this option is
disabled ("false"), which means it is disallowed to use the
bundle field.


disable-creds-init - set this flag to "true" to disable Tektonâ€™s built-in credential initialization
and use Workspaces to mount credentials from Secrets instead.
The default is false. For more information, see the associated issue.


enable-api-fields: When using v1beta1 APIs, setting this field to â€œstableâ€ or â€œbetaâ€
enables beta features. When using v1 APIs, setting this field to â€œstableâ€
allows only stable features, and setting it to â€œbetaâ€ allows only beta features.
Set this field to â€œalphaâ€ to allow alpha features to be used.


enable-kubernetes-sidecar: Set this flag to "true" to enable native kubernetes sidecar support. This will allow Tekton sidecars to run as Kubernetes sidecars. Must be using Kubernetes v1.29 or greater.


For example:
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
data:
  enable-api-fields: "alpha" # Allow alpha fields to be used in Tasks and Pipelines.


trusted-resources-verification-no-match-policy: Setting this flag to fail will fail the taskrun/pipelinerun if no matching policies found. Setting to warn will skip verification and log a warning if no matching policies are found, but not fail the taskrun/pipelinerun. Setting to ignore will skip verification if no matching policies found.
Defaults to â€œignoreâ€.


results-from: set this flag to â€œtermination-messageâ€ to use the containerâ€™s termination message to fetch results from. This is the default method of extracting results. Set it to â€œsidecar-logsâ€ to enable use of a results sidecar logs to extract results instead of termination message.


enable-provenance-in-status: Set this flag to "true" to enable populating
the provenance field in TaskRun and PipelineRun status. The provenance
field contains metadata about resources used in the TaskRun/PipelineRun such as the
source from where a remote Task/Pipeline definition was fetched. By default, this is set to true.
To disable populating this field, set this flag to "false".


set-security-context: Set this flag to true to set a security context for containers injected by Tekton that will allow TaskRun pods
to run in namespaces with restricted pod security admission. By default, this is set to false.


Alpha Features
Alpha features in the following table are still in development and their syntax is subject to change.

To enable the features without an individual flag:
set the enable-api-fields feature flag to "alpha" in the feature-flags ConfigMap alongside your Tekton Pipelines deployment via kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"enable-api-fields":"alpha"}}'.
To enable the features with an individual flag:
set the individual flag accordingly in the feature-flag ConfigMap alongside your Tekton Pipelines deployment. Example: kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"<FLAG-NAME>":"<FLAG-VALUE>"}}'.

Features currently in â€œalphaâ€ are:



Feature
Proposal
Release
Individual Flag




Bundles 
TEP-0005
v0.18.0
enable-tekton-oci-bundles


Hermetic Execution Mode
TEP-0025
v0.25.0



Windows Scripts
TEP-0057
v0.28.0



Debug
TEP-0042
v0.26.0



StdoutConfig and StderrConfig
TEP-0011
v0.38.0



Trusted Resources
TEP-0091
v0.49.0
trusted-resources-verification-no-match-policy


Configure Default Resolver
TEP-0133
v0.46.0



Coschedule
TEP-0135
v0.51.0
coschedule


keep pod on cancel
N/A
v0.52.0
keep-pod-on-cancel


CEL in WhenExpression
TEP-0145
v0.53.0
enable-cel-in-whenexpression


Param Enum
TEP-0144
v0.54.0
enable-param-enum



Beta Features
Beta features are fields of stable CRDs that follow our â€œbetaâ€ compatibility policy.
To enable these features, set the enable-api-fields feature flag to "beta" in
the feature-flags ConfigMap alongside your Tekton Pipelines deployment via
kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"enable-api-fields":"beta"}}'.
Features currently in â€œbetaâ€ are:



Feature
Proposal
Alpha Release
Beta Release
Individual Flag




Remote Tasks and Remote Pipelines
TEP-0060

v0.41.0



Provenance field in Status
issue#5550
v0.41.0
v0.48.0
enable-provenance-in-status


Isolated Step & Sidecar Workspaces
TEP-0029
v0.24.0
v0.50.0



Matrix
TEP-0090
v0.38.0
v0.53.0



Task-level Resource Requirements
TEP-0104
v0.39.0
v0.53.0



Reusable Steps via StepActions
TEP-0142
v0.54.0
enable-step-actions



Larger Results via Sidecar Logs
TEP-0127
v0.43.0
v0.61.0
results-from


Step and Sidecar Overrides
TEP-0094
v0.34.0

v0.61.0


Ignore Task Failure
TEP-0050
v0.55.0
v0.62.0
N/A



Enabling larger results using sidecar logs
Note: The maximum size of a Taskâ€™s results is limited by the container termination message feature of Kubernetes,
as results are passed back to the controller via this mechanism. At present, the limit is per task is â€œ4096 bytesâ€. All
results produced by the task share this upper limit.
To exceed this limit of 4096 bytes, you can enable larger results using sidecar logs. By enabling this feature, you will
have a configurable limit (with a default of 4096 bytes) per result with no restriction on the number of results. The
results are still stored in the taskRun CRD, so they should not exceed the 1.5MB CRD size limit.
Note: to enable this feature, you need to grant get access to all pods/log to the tekton-pipelines-controller.
This means that the tekton pipeline controller has the ability to access the pod logs.

Create a cluster role and rolebinding by applying the following spec to provide log access to tekton-pipelines-controller.

kubectl apply -f optional_config/enable-log-access-to-controller/

Set the results-from feature flag to use sidecar logs by setting results-from: sidecar-logs in the
configMap.

kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"results-from":"sidecar-logs"}}'

If you want the size per result to be something other than 4096 bytes, you can set the max-result-size feature flag
in bytes by setting max-result-size: 8192(whatever you need here). Note: The value you can set here cannot exceed
the size of the CRD limit of 1.5 MB.

kubectl patch cm feature-flags -n tekton-pipelines -p '{"data":{"max-result-size":"<VALUE-IN-BYTES>"}}'
Configuring High Availability
If you want to run Tekton Pipelines in a way so that webhooks are resiliant against failures and support
high concurrency scenarios, you need to run a Metrics Server in
your Kubernetes cluster. This is required by the Horizontal Pod Autoscalers
to compute replica count.
See HA Support for Tekton Pipeline Controllers for instructions on configuring
High Availability in the Tekton Pipelines Controller.
The default configuration is defined in webhook-hpa.yaml which can be customized
to better fit specific usecases.
Configuring tekton pipeline controller performance
Out-of-the-box, Tekton Pipelines Controller is configured for relatively small-scale deployments but there have several options for configuring Pipelinesâ€™ performance are available. See the Performance Configuration document which describes how to change the default ThreadsPerController, QPS and Burst settings to meet your requirements.
Running TaskRuns and PipelineRuns with restricted pod security standards
To allow TaskRuns and PipelineRuns to run in namespaces with restricted pod security standards,
set the â€œset-security-contextâ€ feature flag to â€œtrueâ€ in the feature-flags configMap. This configuration option applies a SecurityContext
to any containers injected into TaskRuns by the Pipelines controller. This SecurityContext may not be supported in all Kubernetes implementations (for example, OpenShift).
Note: running TaskRuns and PipelineRuns in the â€œtekton-pipelinesâ€ namespace is discouraged.
Platform Support
The Tekton project provides support for running on x86 Linux Kubernetes nodes.
The project produces images capable of running on other architectures and operating systems, but may not be able to help debug issues specific to those platforms as readily as those that affect Linux on x86.
The controller and webhook components are currently built for:

linux/amd64
linux/arm64
linux/arm (Arm v7)
linux/ppc64le (PowerPC)
linux/s390x (IBM Z)

The entrypoint component is also built for Windows, which enables TaskRun workloads to execute on Windows nodes.
See Windows documentation for more information.
Creating a custom release of Tekton Pipelines
You can create a custom release of Tekton Pipelines by following and customizing the steps in Creating an official release. For example, you might want to customize the container images built and used by Tekton Pipelines.
Verify Tekton Pipelines Release

We will refine this process over time to be more streamlined. For now, please follow the steps listed in this section
to verify Tekton pipeline release.

Tekton Pipelineâ€™s images are being signed by Tekton Chains since 0.27.1. You can verify the images with
cosign using the Tektonâ€™s public key.
Verify signatures using cosign
With Go 1.16+, you can install cosign by running:
go install github.com/sigstore/cosign/cmd/cosign@latest
You can verify Tekton Pipelines official images using the Tekton public key:
cosign verify -key https://raw.githubusercontent.com/tektoncd/chains/main/tekton.pub gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1
which results in:
Verification for gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The signatures were verified against the specified public key
  - Any certificates were verified against the Fulcio roots.
{
  "Critical": {
    "Identity": {
      "docker-reference": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller"
    },
    "Image": {
      "Docker-manifest-digest": "sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8"
    },
    "Type": "Tekton container signature"
  },
  "Optional": {}
}
The verification shows a list of checks performed and returns the digest in Critical.Image.Docker-manifest-digest
which can be used to retrieve the provenance from the transparency logs for that image using rekor-cli.
Verify the transparency logs using rekor-cli
Install the rekor-cli by running:
go install -v github.com/sigstore/rekor/cmd/rekor-cli@latest
Now, use the digest collected from the previous section in
Critical.Image.Docker-manifest-digest, for example,
sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8.
Search the transparency log with the digest just collected:
rekor-cli search --sha sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8
which results in:
Found matching entries (listed by UUID):
68a53d0e75463d805dc9437dda5815171502475dd704459a5ce3078edba96226
Tekton Chains generates provenance based on the custom format
in which the subject holds the list of artifacts which were built as part of the release. For the Pipeline release,
subject includes a list of images including pipeline controller, pipeline webhook, etc. Use the UUID to get the provenance:
rekor-cli get --uuid 68a53d0e75463d805dc9437dda5815171502475dd704459a5ce3078edba96226 --format json | jq -r .Attestation | base64 --decode | jq
which results in:
{
  "_type": "https://in-toto.io/Statement/v0.1",
  "predicateType": "https://tekton.dev/chains/provenance",
  "subject": [
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller",
      "digest": {
        "sha256": "0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint",
      "digest": {
        "sha256": "2fa7f7c3408f52ff21b2d8c4271374dac4f5b113b1c4dbc7d5189131e71ce721"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init",
      "digest": {
        "sha256": "83d5ec6addece4aac79898c9631ee669f5fee5a710a2ed1f98a6d40c19fb88f7"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/imagedigestexporter",
      "digest": {
        "sha256": "e4d77b5b8902270f37812f85feb70d57d6d0e1fed2f3b46f86baf534f19cd9c0"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/nop",
      "digest": {
        "sha256": "59b5304bcfdd9834150a2701720cf66e3ebe6d6e4d361ae1612d9430089591f8"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/pullrequest-init",
      "digest": {
        "sha256": "4992491b2714a73c0a84553030e6056e6495b3d9d5cc6b20cf7bc8c51be779bb"
      }
    },
    {
      "name": "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook",
      "digest": {
        "sha256": "bf0ef565b301a1981cb2e0d11eb6961c694f6d2401928dccebe7d1e9d8c914de"
      }
    }
  ],
  ...
Now, verify the digest in the release.yaml by matching it with the provenance, for example, the digest for the release v0.28.1:
curl -s https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.28.1/release.yaml | grep github.com/tektoncd/pipeline/cmd/controller:v0.28.1 | awk -F"github.com/tektoncd/pipeline/cmd/controller:v0.28.1@" '{print $2}'
which results in:
sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8
Now, you can verify the deployment specifications in the release.yaml to match each of these images and their digest.
The tekton-pipelines-controller deployment specification has a container named tekton-pipeline-controller and a
list of image references with their digest as part of the args:
      containers:
        - name: tekton-pipelines-controller
          image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1@sha256:0c320bc09e91e22ce7f01e47c9f3cb3449749a5f72d5eaecb96e710d999c28e8
          args: [
            # These images are built on-demand by `ko resolve` and are replaced
            # by image references by digest.
              "-git-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init:v0.28.1@sha256:83d5ec6addece4aac79898c9631ee669f5fee5a710a2ed1f98a6d40c19fb88f7",
              "-entrypoint-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint:v0.28.1@sha256:2fa7f7c3408f52ff21b2d8c4271374dac4f5b113b1c4dbc7d5189131e71ce721",
              "-nop-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/nop:v0.28.1@sha256:59b5304bcfdd9834150a2701720cf66e3ebe6d6e4d361ae1612d9430089591f8",
              "-imagedigest-exporter-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/imagedigestexporter:v0.28.1@sha256:e4d77b5b8902270f37812f85feb70d57d6d0e1fed2f3b46f86baf534f19cd9c0",
              "-pr-image",
              "gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/pullrequest-init:v0.28.1@sha256:4992491b2714a73c0a84553030e6056e6495b3d9d5cc6b20cf7bc8c51be779bb",
Similarly, you can verify the rest of the images which were published as part of the Tekton Pipelines release:
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/nop
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/imagedigestexporter
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/pullrequest-init
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook
Verify Tekton Resources
Trusted Resources is a feature to verify Tekton Tasks and Pipelines. The current
version of feature supports v1beta1 Task and Pipeline. For more details
please take a look at Trusted Resources.
Pipelineruns with Affinity Assistant
The cluster operators can review the guidelines to cordon a node in the cluster
with the tekton controller and the affinity assistant is enabled.
TaskRuns with imagePullBackOff Timeout
Tekton pipelines has adopted a fail fast strategy with a taskRun failing with TaskRunImagePullFailed in case of an
imagePullBackOff. This can be limited in some cases, and it generally depends on the infrastructure. To allow the
cluster operators to decide whether to wait in case of an imagePullBackOff, a setting is available to configure
the wait time such that the controller will wait for the specified duration before declaring a failure.
For example, with the following config-defaults, the controller does not mark the taskRun as failure for 5 minutes since
the pod is scheduled in case the image pull fails with imagePullBackOff. The default-imagepullbackoff-timeout is
of type time.Duration and can be set to a duration such as â€œ1mâ€, â€œ5mâ€, â€œ10sâ€, â€œ1hâ€, etc.
See issue https://github.com/tektoncd/pipeline/issues/5987 for more details.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: tekton-pipelines
data:
  default-imagepullbackoff-timeout: "5m"
Disabling Inline Spec in Pipeline, TaskRun and PipelineRun
Tekton users may embed the specification of a Task (via taskSpec) or a Pipeline (via pipelineSpec) as an alternative to referring to an external resource via taskRef and pipelineRef respectively.  This behaviour can be selectively disabled for three Tekton resources: TaskRun, PipelineRun and Pipeline.
In certain clusters and scenarios, an admin might want to disable the customisation of Tasks and Pipelines and only allow users to run pre-defined resources. To achieve that the admin should disable embedded specification via the disable-inline-spec flag, and remote resolvers too.
To disable inline specification, set the disable-inline-spec flag to "pipeline,pipelinerun,taskrun"
in the feature-flags configmap.
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  disable-inline-spec: "pipeline,pipelinerun,taskrun"
Inline specifications can be disabled for specific resources only. To achieve that, set the disable-inline-spec flag to a comma-separated list of the desired resources. Valid values are pipeline, pipelinerun and taskrun.
The default value of disable-inline-spec is â€œâ€, which means inline specification is enabled in all cases.
Next steps
To get started with Tekton check the Introductory tutorials,
the how-to guides, and the examples folder.


	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Artifacts

Overview
Artifact Provenance Data

Passing Artifacts between Steps
Passing Artifacts between Tasks



Overview

ğŸŒ± Artifacts is an alpha feature.
The enable-artifacts feature flag must be set to "true" to read or write artifacts in a step.

Artifacts provide a way to track the origin of data produced and consumed within your Tekton Tasks.
Artifact Provenance Data
Artifacts fall into two categories:

Inputs: Artifacts downloaded and used by the Step/Task.
Outputs: Artifacts created and uploaded by the Step/Task.
Example Structure:

{
  "inputs":[
    {
      "name": "<input-category-name>", 
      "values": [
        {
          "uri": "pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c", 
          "digest": { "sha256": "b35caccc..." }
        }
      ]
    }
  ],
  "outputs": [
    {
      "name": "<output-category-name>",
      "values": [
        {
          "uri": "pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library",
          "digest": {
            "sha256": "df85b9e3...",
            "sha1": "95588b8f..."
          }
        }
      ]
    }
  ]
}
The content is written by the Step to a file $(step.artifacts.path):
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  generateName: step-artifacts-
spec:
  taskSpec:
    description: |
            A simple task that populates artifacts to TaskRun stepState
    steps:
      - name: artifacts-producer
        image: bash:latest
        script: |
          cat > $(step.artifacts.path) << EOF
          {
            "inputs":[
              {
                "name":"source",
                "values":[
                  {
                    "uri":"pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c",
                    "digest":{
                      "sha256":"b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
                    }
                  }
                ]
              }
            ],
            "outputs":[
              {
                "name":"image",
                "values":[
                  {
                    "uri":"pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library",
                    "digest":{
                      "sha256":"df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48",
                      "sha1":"95588b8f34c31eb7d62c92aaa4e6506639b06ef2"
                    }
                  }
                ]
              }
            ]
          }
          EOF          
The content is written by the Step to a file $(artifacts.path):
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  generateName: step-artifacts-
spec:
  taskSpec:
    description: |
            A simple task that populates artifacts to TaskRun stepState
    steps:
      - name: artifacts-producer
        image: bash:latest
        script: |
          cat > $(artifacts.path) << EOF
          {
            "inputs":[
              {
                "name":"source",
                "values":[
                  {
                    "uri":"pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c",
                    "digest":{
                      "sha256":"b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
                    }
                  }
                ]
              }
            ],
            "outputs":[
              {
                "name":"image",
                "values":[
                  {
                    "uri":"pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library",
                    "digest":{
                      "sha256":"df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48",
                      "sha1":"95588b8f34c31eb7d62c92aaa4e6506639b06ef2"
                    }
                  }
                ]
              }
            ]
          }
          EOF          
It is recommended to use purl format for artifacts uri as shown in the example.
Output Artifacts in SLSA Provenance
Artifacts are classified as either:

Build Outputs - packages, images, etc. that are being published by the build.
Build Byproducts - logs, caches, etc. that are incidental artifacts that are produced by the build.

By default, Tekton Chains will consider all output artifacts as byProducts when generating in the SLSA provenance. In order to treat an artifact as a subject of the build, you must set a boolean field "buildOutput": true for the output artifact.
e.g.
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  generateName: step-artifacts-
spec:
  taskSpec:
    description: |
            A simple task that populates artifacts to TaskRun stepState
    steps:
      - name: artifacts-producer
        image: bash:latest
        script: |
          cat > $(artifacts.path) << EOF
          {
            "outputs":[
              {
                "name":"image",
                "buildOutput": true,
                "values":[
                  {
                    "uri":"pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library",
                    "digest":{
                      "sha256":"df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48",
                      "sha1":"95588b8f34c31eb7d62c92aaa4e6506639b06ef2"
                    }
                  }
                ]
              }
            ]
          }
          EOF          
This informs Tekton Chains your desire to handle the artifact.

[!TIP]
When authoring a StepAction or a Task, you can parametrize this field to allow users to indicate their desire depending on what they are uploading - this can be useful for actions that may produce either a build output or a byproduct depending on the context!

Passing Artifacts between Steps
You can pass artifacts from one step to the next using:

Specific Artifact: $(steps.<step-name>.inputs.<artifact-category-name>) or $(steps.<step-name>.outputs.<artifact-category-name>)

The example below shows how to access the previousâ€™ step artifacts from another step in the same task
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  generateName: step-artifacts-
spec:
  taskSpec:
    description: |
            A simple task that populates artifacts to TaskRun stepState
    steps:
      - name: artifacts-producer
        image: bash:latest
        script: |
          # the script is for creating the output artifacts
          cat > $(step.artifacts.path) << EOF
          {
            "inputs":[
              {
                "name":"source",
                "values":[
                  {
                    "uri":"pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c",
                    "digest":{
                      "sha256":"b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
                    }
                  }
                ]
              }
            ],
            "outputs":[
              {
                "name":"image",
                "values":[
                  {
                    "uri":"pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library",
                    "digest":{
                      "sha256":"df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48",
                      "sha1":"95588b8f34c31eb7d62c92aaa4e6506639b06ef2"
                    }
                  }
                ]
              }
            ]
          }
          EOF          
      - name: artifacts-consumer
        image: bash:latest
        script: |
                    echo $(steps.artifacts-producer.outputs.image)
The resolved value of $(steps.<step-name>.outputs.<artifact-category-name>) is the values of an artifact. For this example,
$(steps.artifacts-producer.outputs.image) is resolved to
[
                  {
                    "uri":"pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library",
                    "digest":{
                      "sha256":"df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48",
                      "sha1":"95588b8f34c31eb7d62c92aaa4e6506639b06ef2"
                    }
                  }
]
Upon resolution and execution of the TaskRun, the Status will look something like:
{
  "artifacts": {
    "inputs": [
      {
        "name": "source",
        "values": [
          {
            "digest": {
              "sha256": "b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
            },
            "uri": "pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c"
          }
        ]
      }
    ],
    "outputs": [
      {
        "name": "image",
        "values": [
          {
            "digest": {
              "sha1": "95588b8f34c31eb7d62c92aaa4e6506639b06ef2",
              "sha256": "df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48"
            },
            "uri": "pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library"
          }
        ]
      }
    ]
  },
  "steps": [
    {
      "container": "step-artifacts-producer",
      "imageID": "docker.io/library/bash@sha256:5353512b79d2963e92a2b97d9cb52df72d32f94661aa825fcfa0aede73304743",
      "inputs": [
        {
          "name": "source",
          "values": [
            {
              "digest": {
                "sha256": "b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
              },
              "uri": "pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c"
            }
          ]
        }
      ],
      "name": "artifacts-producer",
      "outputs": [
        {
          "name": "image",
          "values": [
            {
              "digest": {
                "sha1": "95588b8f34c31eb7d62c92aaa4e6506639b06ef2",
                "sha256": "df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48"
              },
              "uri": "pkg:oci/nginx:stable-alpine3.17-slim?repository_url=docker.io/library"
            }
          ]
        }
      ],
      "terminated": {
        "containerID": "containerd://010f02d103d1db48531327a1fe09797c87c1d50b6a216892319b3af93e0f56e7",
        "exitCode": 0,
        "finishedAt": "2024-03-18T17:05:06Z",
        "message": "...",
        "reason": "Completed",
        "startedAt": "2024-03-18T17:05:06Z"
      },
      "terminationReason": "Completed"
    },
    {
      "container": "step-artifacts-consumer",
      "imageID": "docker.io/library/bash@sha256:5353512b79d2963e92a2b97d9cb52df72d32f94661aa825fcfa0aede73304743",
      "name": "artifacts-consumer",
      "terminated": {
        "containerID": "containerd://42428aa7e5a507eba924239f213d185dd4bc0882b6f217a79e6792f7fec3586e",
        "exitCode": 0,
        "finishedAt": "2024-03-18T17:05:06Z",
        "reason": "Completed",
        "startedAt": "2024-03-18T17:05:06Z"
      },
      "terminationReason": "Completed"
    }
  ]
}
Passing Artifacts between Tasks
You can pass artifacts from one task to the another using:

Specific Artifact: $(tasks.<task-name>.inputs.<artifact-category-name>) or $(tasks.<task-name>.outputs.<artifact-category-name>)

The example below shows how to access the previousâ€™ task artifacts from another task in a pipeline
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: pipelinerun-consume-tasks-artifacts
spec:
  pipelineSpec:
    tasks:
      - name: produce-artifacts-task
        taskSpec:
          description: |
                        A simple task that produces artifacts
          steps:
            - name: produce-artifacts
              image: bash:latest
              script: |
                #!/usr/bin/env bash
                cat > $(artifacts.path) << EOF
                {
                  "inputs":[
                    {
                      "name":"input-artifacts",
                      "values":[
                        {
                          "uri":"pkg:example.github.com/inputs",
                          "digest":{
                            "sha256":"b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
                          }
                        }
                      ]
                    }
                  ],
                  "outputs":[
                    {
                      "name":"image",
                      "values":[
                        {
                          "uri":"pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c",
                          "digest":{
                            "sha256":"df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48",
                            "sha1":"95588b8f34c31eb7d62c92aaa4e6506639b06ef2"
                          }
                        }
                      ]
                    }
                  ]
                }
                EOF                
      - name: consume-artifacts
        runAfter:
          - produce-artifacts-task
        taskSpec:
          steps:
            - name: artifacts-consumer-python
              image: python:latest
              script: |
                #!/usr/bin/env python3
                import json
                data = json.loads('$(tasks.produce-artifacts-task.outputs.image)')
                if data[0]['uri'] != "pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c":
                  exit(1)                
Similar to Step Artifacts. The resolved value of $(tasks.<task-name>.outputs.<artifact-category-name>) is the values of an artifact. For this example,
$(tasks.produce-artifacts-task.outputs.image) is resolved to
[
  {
    "uri":"pkg:example.github.com/inputs",
    "digest":{
      "sha256":"b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
    }
 }
]
Upon resolution and execution of the TaskRun, the Status will look something like:
{
 "artifacts": {
      "inputs": [
        {
          "name": "input-artifacts",
          "values": [
            {
              "digest": {
                "sha256": "b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0"
              },
              "uri": "pkg:example.github.com/inputs"
            }
          ]
        }
      ],
      "outputs": [
        {
          "name": "image",
          "values": [
            {
              "digest": {
                "sha1": "95588b8f34c31eb7d62c92aaa4e6506639b06ef2",
                "sha256": "df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48"
              },
              "uri": "pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c"
            }
          ]
        }
      ]
    },
    "completionTime": "2024-05-28T14:10:58Z",
    "conditions": [
      {
        "lastTransitionTime": "2024-05-28T14:10:58Z",
        "message": "All Steps have completed executing",
        "reason": "Succeeded",
        "status": "True",
        "type": "Succeeded"
      }
    ],
    "podName": "pipelinerun-consume-tasks-a41ee44e4f964e95adfd3aea417d52f90-pod",
    "provenance": {
      "featureFlags": {
        "AwaitSidecarReadiness": true,
        "Coschedule": "workspaces",
        "DisableAffinityAssistant": false,
        "DisableCredsInit": false,
        "DisableInlineSpec": "",
        "EnableAPIFields": "beta",
        "EnableArtifacts": true,
        "EnableCELInWhenExpression": false,
        "EnableConciseResolverSyntax": false,
        "EnableKeepPodOnCancel": false,
        "EnableParamEnum": false,
        "EnableProvenanceInStatus": true,
        "EnableStepActions": true,
        "EnableTektonOCIBundles": false,
        "EnforceNonfalsifiability": "none",
        "MaxResultSize": 4096,
        "RequireGitSSHSecretKnownHosts": false,
        "ResultExtractionMethod": "termination-message",
        "RunningInEnvWithInjectedSidecars": true,
        "ScopeWhenExpressionsToTask": false,
        "SendCloudEventsForRuns": false,
        "SetSecurityContext": false,
        "VerificationNoMatchPolicy": "ignore"
      }
    },
    "startTime": "2024-05-28T14:10:48Z",
    "steps": [
      {
        "container": "step-produce-artifacts",
        "imageID": "docker.io/library/bash@sha256:23f90212fd89e4c292d7b41386ef1a6ac2b8a02bbc6947680bfe184cbc1a2899",
        "name": "produce-artifacts",
        "terminated": {
          "containerID": "containerd://1291ce07b175a7897beee6ba62eaa1528427bacb1f76b31435eeba68828c445a",
          "exitCode": 0,
          "finishedAt": "2024-05-28T14:10:57Z",
          "message": "...",
          "reason": "Completed",
          "startedAt": "2024-05-28T14:10:57Z"
        },
        "terminationReason": "Completed"
      }
    ],
    "taskSpec": {
      "description": "A simple task that produces artifacts\n",
      "steps": [
        {
          "computeResources": {},
          "image": "bash:latest",
          "name": "produce-artifacts",
          "script": "#!/usr/bin/env bash\ncat > /tekton/artifacts/provenance.json << EOF\n{\n  \"inputs\":[\n    {\n      \"name\":\"input-artifacts\",\n      \"values\":[\n        {\n          \"uri\":\"pkg:example.github.com/inputs\",\n          \"digest\":{\n            \"sha256\":\"b35cacccfdb1e24dc497d15d553891345fd155713ffe647c281c583269eaaae0\"\n          }\n        }\n      ]\n    }\n  ],\n  \"outputs\":[\n    {\n      \"name\":\"image\",\n      \"values\":[\n        {\n          \"uri\":\"pkg:github/package-url/purl-spec@244fd47e07d1004f0aed9c\",\n          \"digest\":{\n            \"sha256\":\"df85b9e3983fe2ce20ef76ad675ecf435cc99fc9350adc54fa230bae8c32ce48\",\n            \"sha1\":\"95588b8f34c31eb7d62c92aaa4e6506639b06ef2\"\n          }\n        }\n      ]\n    }\n  ]\n}\nEOF\n"
        }
      ]
    }
}

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	StepActions

Overview
Configuring a StepAction

Declaring Parameters

Passing Params to StepAction


Emitting Results

Fetching Emitted Results from StepActions


Declaring WorkingDir
Declaring SecurityContext
Declaring VolumeMounts
Referencing a StepAction

Specifying Remote StepActions


Controlling Step Execution with when Expressions


Known Limitations

Cannot pass Step Results between Steps



Overview

ğŸŒ± StepActions is an beta feature.
The enable-step-actions feature flag must be set to "true" to specify a StepAction in a Step.

A StepAction is the reusable and scriptable unit of work that is performed by a Step.
A Step is not reusable, the work it performs is reusable and referenceable. Steps are in-lined in the Task definition and either perform work directly or perform a StepAction. A StepAction cannot be run stand-alone (unlike a TaskRun or a PipelineRun). It has to be referenced by a Step. Another way to think about this is that a Step is not composed of StepActions (unlike a Task being composed of Steps and Sidecars). Instead, a Step is an actionable component, meaning that it has the ability to refer to a StepAction. The author of the StepAction must be able to compose a Step using a StepAction and provide all the necessary context (or orchestration) to it.
Configuring a StepAction
A StepAction definition supports the following fields:

Required

apiVersion - Specifies the API version. For example,
tekton.dev/v1alpha1.
kind - Identifies this resource object as a StepAction object.
metadata - Specifies metadata that uniquely identifies the
StepAction resource object. For example, a name.
spec - Specifies the configuration information for this StepAction resource object.
image - Specifies the image to use for the Step.

The container image must abide by the container contract.




Optional

command

cannot be used at the same time as using script.


args
script

cannot be used at the same time as using command.


env
params
results
workingDir
securityContext
volumeMounts



The example below demonstrates the use of most of the above-mentioned fields:
apiVersion: tekton.dev/v1beta1
kind: StepAction
metadata:
  name: example-stepaction-name
spec:
  env:
    - name: HOME
      value: /home
  image: ubuntu
  command: ["ls"]
  args: ["-lh"]
Declaring Parameters
Like with Tasks, a StepAction must declare all the parameters that it uses. The same rules for Parameter name, type (including object, array and string) apply as when declaring them in Tasks. A StepAction can also provide default value to a Parameter.
Parameters are passed to the StepAction from its corresponding Step referencing it.
apiVersion: tekton.dev/v1beta1
kind: StepAction
metadata:
  name: stepaction-using-params
spec:
  params:
    - name: gitrepo
      type: object
      properties:
        url:
          type: string
        commit:
          type: string
    - name: flags
      type: array
    - name: outputPath
      type: string
      default: "/workspace"
  image: some-git-image
  args: [
    "-url=$(params.gitrepo.url)",
    "-revision=$(params.gitrepo.commit)",
    "-output=$(params.outputPath)",
    "$(params.flags[*])",
  ]

ğŸŒ± params cannot be directly used in a script in StepActions.
Directly substituting params in scripts makes the workload prone to shell attacks. Therefore, we do not allow direct usage of params in scripts in StepActions. Instead, rely on passing params to env variables and reference them in scripts. We cannot do the same for inlined-steps because it breaks v1 API compatibility for existing users.

Passing Params to StepAction
A StepAction may require params. In this case, a Task needs to ensure that the StepAction has access to all the required params.
When referencing a StepAction, a Step can also provide it with params, just like how a TaskRun provides params to the underlying Task.
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: step-action
spec:
  params:
    - name: param-for-step-action
      description: "this is a param that the step action needs."
  steps:
    - name: action-runner
      ref:
        name: step-action
      params:
        - name: step-action-param
          value: $(params.param-for-step-action)
Note: If a Step declares params for an inlined Step, it will also lead to a validation error. This is because an inlined Step gets its params from the TaskRun.
Emitting Results
A StepAction also declares the results that it will emit.
apiVersion: tekton.dev/v1alpha1
kind: StepAction
metadata:
  name: stepaction-declaring-results
spec:
  results:
    - name: current-date-unix-timestamp
      description: The current date in unix timestamp format
    - name: current-date-human-readable
      description: The current date in human readable format
  image: bash:latest
  script: |
    #!/usr/bin/env bash
    date +%s | tee $(results.current-date-unix-timestamp.path)
    date | tee $(results.current-date-human-readable.path)    
It is possible that a StepAction with Results is used multiple times in the same Task or multiple StepActions in the same Task produce Results with the same name. Resolving the Result names becomes critical otherwise there could be unexpected outcomes. The Task needs to be able to resolve these Result names clashes by mapping it to a different Result name. For this reason, we introduce the capability to store results on a Step level.
StepActions can also emit Results to $(step.results.<resultName>.path).
apiVersion: tekton.dev/v1alpha1
kind: StepAction
metadata:
  name: stepaction-declaring-results
spec:
  results:
    - name: current-date-unix-timestamp
      description: The current date in unix timestamp format
    - name: current-date-human-readable
      description: The current date in human readable format
  image: bash:latest
  script: |
    #!/usr/bin/env bash
    date +%s | tee $(step.results.current-date-unix-timestamp.path)
    date | tee $(step.results.current-date-human-readable.path)    
Results from the above StepAction can be fetched by the Task or in another Step/StepAction via $(steps.<stepName>.results.<resultName>).
Fetching Emitted Results from StepActions
A Task can fetch Results produced by the StepActions (i.e. only Results emitted to $(step.results.<resultName>.path), NOT $(results.<resultName>.path)) using variable replacement syntax. We introduce a field to Task Results called Value whose value can be set to the variable $(steps.<stepName>.results.<resultName>).
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: task-fetching-results
spec:
  results:
    - name: git-url
      description: "url of git repo"
      value: $(steps.git-clone.results.url)
    - name: registry-url
      description: "url of docker registry"
      value: $(steps.kaniko.results.url)
  steps:
    - name: git-clone
      ref:
        name: clone-step-action
    - name: kaniko
      ref:
        name: kaniko-step-action
Results emitted to $(step.results.<resultName>.path) are not automatically available as TaskRun Results. The Task must explicitly fetch it from the underlying Step referencing StepActions.
For example, lets assume that in the previous example, the â€œkanikoâ€ StepAction also produced a Result named â€œdigestâ€. In that case, the Task should also fetch the â€œdigestâ€ from â€œkanikoâ€ Step.
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: task-fetching-results
spec:
  results:
    - name: git-url
      description: "url of git repo"
      value: $(steps.git-clone.results.url)
    - name: registry-url
      description: "url of docker registry"
      value: $(steps.kaniko.results.url)
    - name: digest
      description: "digest of the image"
      value: $(steps.kaniko.results.digest)
  steps:
    - name: git-clone
      ref:
        name: clone-step-action
    - name: kaniko
      ref:
        name: kaniko-step-action
Passing Results between Steps
StepResults (i.e. results written to $(step.results.<result-name>.path), NOT $(results.<result-name>.path)) can be shared with following steps via replacement variable $(steps.<step-name>.results.<result-name>).
Pipeline supports two new types of results and parameters: array []string and object map[string]string.



Result Type
Parameter Type
Specification
enable-api-fields




string
string
$(steps.<step-name>.results.<result-name>)
stable


array
array
$(steps.<step-name>.results.<result-name>[*])
alpha or beta


array
string
$(steps.<step-name>.results.<result-name>[i])
alpha or beta


object
string
$(tasks.<task-name>.results.<result-name>.key)
alpha or beta



Note: Whole Array Results (using star notation) cannot be referred in script and env.
The example below shows how you could pass step results from a step into following steps, in this case, into a StepAction.
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: step-action-run
spec:
  TaskSpec:
    steps:
      - name: inline-step
        results:
          - name: result1
            type: array
          - name: result2
            type: string
          - name: result3
            type: object
            properties:
              IMAGE_URL:
                type: string
              IMAGE_DIGEST:
                type: string
        image: alpine
        script: |
          echo -n "[\"image1\", \"image2\", \"image3\"]" | tee $(step.results.result1.path)
          echo -n "foo" | tee $(step.results.result2.path)
          echo -n "{\"IMAGE_URL\":\"ar.com\", \"IMAGE_DIGEST\":\"sha234\"}" | tee $(step.results.result3.path)          
      - name: action-runner
        ref:
          name: step-action
        params:
          - name: param1
            value: $(steps.inline-step.results.result1[*])
          - name: param2
            value: $(steps.inline-step.results.result2)
          - name: param3
            value: $(steps.inline-step.results.result3[*])
Note: Step Results can only be referenced in a Step's/StepAction's env, command and args. Referencing in any other field will throw an error.
Declaring WorkingDir
You can declare workingDir in a StepAction:
apiVersion: tekton.dev/v1alpha1
kind: StepAction
metadata:
  name: example-stepaction-name
spec:
  image:  gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init:latest
  workingDir: /workspace
  script: |
    # clone the repo
    ...    
The Task using the StepAction has more context about how the Steps have been orchestrated. As such, the Task should be able to update the workingDir of the StepAction so that the StepAction is executed from the correct location.
The StepAction can parametrize the workingDir and work relative to it. This way, the Task does not really need control over the workingDir, it just needs to pass the path as a parameter.
apiVersion: tekton.dev/v1alpha1
kind: StepAction
metadata:
  name: example-stepaction-name
spec:
  image: ubuntu
  params:
    - name: source
      description: "The path to the source code."
  workingDir: $(params.source)
Declaring SecurityContext
You can declare securityContext in a StepAction:
apiVersion: tekton.dev/v1alpha1
kind: StepAction
metadata:
  name: example-stepaction-name
spec:
  image:  gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init:latest
  securityContext:
      runAsUser: 0
  script: |
    # clone the repo
    ...    
Note that the securityContext from StepAction will overwrite the securityContext from TaskRun.
Declaring VolumeMounts
You can define VolumeMounts in StepActions. The name of the VolumeMount MUST be a single reference to a string Parameter. For example, $(params.registryConfig) is valid while $(params.registryConfig)-foo and "unparametrized-name" are invalid. This is to ensure reusability of StepActions such that Task authors have control of which Volumes they bind to the VolumeMounts.
apiVersion: tekton.dev/v1alpha1
kind: StepAction
metadata:
  name: myStep
spec:
  params:
    - name: registryConfig
    - name: otherConfig
  volumeMounts:
    - name: $(params.registryConfig)
      mountPath: /registry-config
    - name: $(params.otherConfig)
      mountPath: /other-config
  image: ...
  script: ...
Referencing a StepAction
StepActions can be referenced from the Step using the ref field, as follows:
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: step-action-run
spec:
  taskSpec:
    steps:
      - name: action-runner
        ref:
          name: step-action
Upon resolution and execution of the TaskRun, the Status will look something like:
status:
  completionTime: "2023-10-24T20:28:42Z"
  conditions:
  - lastTransitionTime: "2023-10-24T20:28:42Z"
    message: All Steps have completed executing
    reason: Succeeded
    status: "True"
    type: Succeeded
  podName: step-action-run-pod
  provenance:
    featureFlags:
      EnableStepActions: true
      ...
  startTime: "2023-10-24T20:28:32Z"
  steps:
  - container: step-action-runner
    imageID: docker.io/library/alpine@sha256:eece025e432126ce23f223450a0326fbebde39cdf496a85d8c016293fc851978
    name: action-runner
    terminationReason: Completed
    terminated:
      containerID: containerd://46a836588967202c05b594696077b147a0eb0621976534765478925bb7ce57f6
      exitCode: 0
      finishedAt: "2023-10-24T20:28:42Z"
      reason: Completed
      startedAt: "2023-10-24T20:28:42Z"
  taskSpec:
    steps:
    - computeResources: {}
      image: alpine
      name: action-runner
If a Step is referencing a StepAction, it cannot contain the fields supported by StepActions. This includes:

image
command
args
script
env
volumeMounts

Using any of the above fields and referencing a StepAction in the same Step is not allowed and will cause a validation error.
# This is not allowed and will result in a validation error
# because the image is expected to be provided by the StepAction
# and not inlined.
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: step-action-run
spec:
  taskSpec:
    steps:
      - name: action-runner
        ref:
          name: step-action
        image: ubuntu
Executing the above TaskRun will result in an error that looks like:
Error from server (BadRequest): error when creating "STDIN": admission webhook "validation.webhook.pipeline.tekton.dev" denied the request: validation failed: image cannot be used with Ref: spec.taskSpec.steps[0].image
When a Step is referencing a StepAction, it can contain the following fields:

computeResources
workspaces (Isolated workspaces)
volumeDevices
imagePullPolicy
onError
stdoutConfig
stderrConfig
securityContext
envFrom
timeout
ref
params

Using any of the above fields and referencing a StepAction is allowed and will not cause an error. For example, the TaskRun below will execute without any errors:
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: step-action-run
spec:
  taskSpec:
    steps:
      - name: action-runner
        ref:
          name: step-action
        params:
          - name: step-action-param
            value: hello
        computeResources:
          requests:
            memory: 1Gi
            cpu: 500m
        timeout: 1h
        onError: continue
Specifying Remote StepActions
A ref field may specify a StepAction in a remote location such as git.
Support for specific types of remote will depend on the Resolvers your
clusterâ€™s operator has installed. For more information including a tutorial, please check resolution docs. The below example demonstrates referencing a StepAction in git:
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  generateName: step-action-run-
spec:
  taskSpec:
    steps:
      - name: action-runner
        ref:
          resolver: git
          params:
            - name: url
              value: https://github.com/repo/repo.git
            - name: revision
              value: main
            - name: pathInRepo
              value: remote_step.yaml
The default resolver type can be configured by the default-resolver-type field in the config-defaults ConfigMap (alpha feature). See additional-configs.md for details.
Controlling Step Execution with when Expressions
You can define when in a step to control its execution.
The components of when expressions are input, operator, values, cel:



Component
Description
Syntax




input
Input for the when expression, defaults to an empty string if not provided.
* Static values e.g. "ubuntu" * Variables (parameters or results) e.g. "$(params.image)" or "$(tasks.task1.results.image)" or "$(tasks.task1.results.array-results[1])"


operator
operator represents an inputâ€™s relationship to a set of values, a valid operator must be provided.
in or notin


values
An array of string values, the values array must be provided and has to be non-empty.
* An array param e.g. ["$(params.images[*])"] * An array result of a task ["$(tasks.task1.results.array-results[*])"] * An array result of a step["(steps.step1.results.array-results[*])"]* values can contain static values e.g. "ubuntu" * values can contain variables (parameters or results) or a Workspacesâ€™s bound state e.g. ["$(params.image)"] or ["$(steps.step1.results.image)"] or ["$(tasks.task1.results.array-results[1])"] or ["$(steps.step1.results.array-results[1])"]


cel
The Common Expression Language (CEL) implements common semantics for expression evaluation, enabling different applications to more easily interoperate. This is an alpha feature, enable-cel-in-whenexpression needs to be set to true to use this feature.
cel-syntax



The below example shows how to use when expressions to control step executions:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-2
spec:
  resources:
    requests:
      storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
---
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  generateName: step-when-example
spec:
  workspaces:
    - name: custom
      persistentVolumeClaim:
        claimName: my-pvc-2
  taskSpec:
    description: |
            A simple task that shows how to use when determine if a step should be executed
    steps:
      - name: should-execute
        image: bash:latest
        script: |
          #!/usr/bin/env bash
          echo "executed..."          
        when:
          - input: "$(workspaces.custom.bound)"
            operator: in
            values: [ "true" ]
      - name: should-skip
        image: bash:latest
        script: |
          #!/usr/bin/env bash
          echo skipskipskip          
        when:
          - input: "$(workspaces.custom2.bound)"
            operator: in
            values: [ "true" ]
      - name: should-continue
        image: bash:latest
        script: |
          #!/usr/bin/env bash
          echo blabalbaba          
      - name: produce-step
        image: alpine
        results:
          - name: result2
            type: string
        script: |
                    echo -n "foo" | tee $(step.results.result2.path)
      - name: run-based-on-step-results
        image: alpine
        script: |
                    echo "wooooooo"
        when:
          - input: "$(steps.produce-step.results.result2)"
            operator: in
            values: [ "bar" ]
    workspaces:
      - name: custom
The StepState for a skipped step looks like something similar to the below:
      {
        "container": "step-run-based-on-step-results",
        "imageID": "docker.io/library/alpine@sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b",
        "name": "run-based-on-step-results",
        "terminated": {
          "containerID": "containerd://bf81162e79cf66a2bbc03e3654942d3464db06ff368c0be263a8a70f363a899b",
          "exitCode": 0,
          "finishedAt": "2024-03-26T03:57:47Z",
          "reason": "Completed",
          "startedAt": "2024-03-26T03:57:47Z"
        },
        "terminationReason": "Skipped"
      }
Where terminated.exitCode is 0 and terminationReason is Skipped to indicate the Step exited successfully and was skipped.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Tasks

Overview
Configuring a Task

Task vs. ClusterTask
Defining Steps

Reserved directories
Running scripts within Steps

Windows scripts


Specifying a timeout
Specifying onError for a step
Accessing Stepâ€™s exitCode in subsequent Steps
Produce a task result with onError
Breakpoint on failure with onError
Redirecting step output streams with stdoutConfig and stderrConfig


Specifying Parameters
Specifying Workspaces
Emitting Results

Larger Results using sidecar logs


Specifying Volumes
Specifying a Step template
Specifying Sidecars
Specifying a DisplayName
Adding a description
Using variable substitution

Substituting parameters and resources
Substituting Array parameters
Substituting Workspace paths
Substituting Volume names and types
Substituting in Script blocks




Code examples

Building and pushing a Docker image

Mounting multiple Volumes
Mounting a ConfigMap as a Volume source
Using a Secret as an environment source
Using a Sidecar in a Task




Debugging

Inspecting the file structure
Inspecting the Pod
Running Step Containers as a Non Root User


Task Authoring Recommendations

Overview
A Task is a collection of Steps that you
define and arrange in a specific order of execution as part of your continuous integration flow.
A Task executes as a Pod on your Kubernetes cluster. A Task is available within a specific
namespace, while a ClusterTask is available across the entire cluster.
A Task declaration includes the following elements:

Parameters
Steps
Workspaces
Results

Configuring a Task
A Task definition supports the following fields:

Required:

apiVersion - Specifies the API version. For example,
tekton.dev/v1beta1.
kind - Identifies this resource object as a Task object.
metadata - Specifies metadata that uniquely identifies the
Task resource object. For example, a name.
spec - Specifies the configuration information for
this Task resource object.
steps - Specifies one or more container images to run in the Task.


Optional:

description - An informative description of the Task.
params - Specifies execution parameters for the Task.
workspaces - Specifies paths to volumes required by the Task.
results - Specifies the names under which Tasks write execution results.
volumes - Specifies one or more volumes that will be available to the Steps in the Task.
stepTemplate - Specifies a Container step definition to use as the basis for all Steps in the Task.
sidecars - Specifies Sidecar containers to run alongside the Steps in the Task.



The non-functional example below demonstrates the use of most of the above-mentioned fields:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: example-task-name
spec:
  params:
    - name: pathToDockerFile
      type: string
      description: The path to the dockerfile to build
      default: /workspace/workspace/Dockerfile
    - name: builtImageUrl
      type: string
      description: location to push the built image to
  steps:
    - name: ubuntu-example
      image: ubuntu
      args: ["ubuntu-build-example", "SECRETS-example.md"]
    - image: gcr.io/example-builders/build-example
      command: ["echo"]
      args: ["$(params.pathToDockerFile)"]
    - name: dockerfile-pushexample
      image: gcr.io/example-builders/push-example
      args: ["push", "$(params.builtImageUrl)"]
      volumeMounts:
        - name: docker-socket-example
          mountPath: /var/run/docker.sock
  volumes:
    - name: example-volume
      emptyDir: {}
Task vs. ClusterTask
Note: ClusterTasks are deprecated. Please use the cluster resolver instead.
A ClusterTask is a Task scoped to the entire cluster instead of a single namespace.
A ClusterTask behaves identically to a Task and therefore everything in this document
applies to both.
Note: When using a ClusterTask, you must explicitly set the kind sub-field in the taskRef field to ClusterTask.
If not specified, the kind sub-field defaults to Task.
Below is an example of a Pipeline declaration that uses a ClusterTask:
Note:

There is no v1 API specification for ClusterTask but a v1beta1 clustertask can still be referenced in a v1 pipeline.
The cluster resolver syntax below can be used to reference any task, not just a clustertask.













  










  
  








  










  
  






      v1 & v1beta1
    

    
      
    
      v1beta1
    

    
      
    





        apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: demo-pipeline
spec:
  tasks:
    - name: build-skaffold-web
      taskRef:
        resolver: cluster
        params:
        - name: kind
          value: task
        - name: name
          value: build-push
        - name: namespace
          value: default


    
      
    

  
        apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: demo-pipeline
  namespace: default
spec:
  tasks:
    - name: build-skaffold-web
      taskRef:
        name: build-push
        kind: ClusterTask
      params: ....


    
      
    

  


Defining Steps
A Step is a reference to a container image that executes a specific tool on a
specific input and produces a specific output. To add Steps to a Task you
define a steps field (required) containing a list of desired Steps. The order in
which the Steps appear in this list is the order in which they will execute.
The following requirements apply to each container image referenced in a steps field:

The container image must abide by the container contract.
Each container image runs to completion or until the first failure occurs.
The CPU, memory, and ephemeral storage resource requests set on Steps
will be adjusted to comply with any LimitRanges
present in the Namespace. In addition, Kubernetes determines a podâ€™s effective resource
requests and limits by summing the requests and limits for all its containers, even
though Tekton runs Steps sequentially.
For more detail, see Compute Resources in Tekton.

Note: If the image referenced in the step field is from a private registry, TaskRuns or PipelineRuns that consume the task
must provide the imagePullSecrets in a podTemplate.
Below is an example of setting the resource requests and limits for a step:












  










  
  








  










  
  






      v1
    

    
      
    
      v1beta1
    

    
      
    





        spec:
  steps:
    - name: step-with-limts
      computeResources:
        requests:
          memory: 1Gi
          cpu: 500m
        limits:
          memory: 2Gi
          cpu: 800m


    
      
    

  
        spec:
  steps:
    - name: step-with-limts
      resources:
        requests:
          memory: 1Gi
          cpu: 500m
        limits:
          memory: 2Gi
          cpu: 800m


    
      
    

  


Reserved directories
There are several directories that all Tasks run by Tekton will treat as special

/workspace - This directory is where resources and workspaces
are mounted. Paths to these are available to Task authors via variable substitution
/tekton - This directory is used for Tekton specific functionality:

/tekton/results is where results are written to.
The path is available to Task authors via $(results.name.path)
There are other subfolders which are implementation details of Tekton
and users should not rely on their specific behavior as it may change in the future



Running scripts within Steps
A step can specify a script field, which contains the body of a script. That script is
invoked as if it were stored inside the container image, and any args are passed directly
to it.
Note: If the script field is present, the step cannot also contain a command field.
Scripts that do not start with a shebang
line will have the following default preamble prepended:
#!/bin/sh
set -e
You can override this default preamble by prepending a shebang that specifies the desired parser.
This parser must be present within that Step's container image.
The example below executes a Bash script:
steps:
  - image: ubuntu # contains bash
    script: |
      #!/usr/bin/env bash
      echo "Hello from Bash!"      
The example below executes a Python script:
steps:
  - image: python # contains python
    script: |
      #!/usr/bin/env python3
      print("Hello from Python!")      
The example below executes a Node script:
steps:
  - image: node # contains node
    script: |
      #!/usr/bin/env node
      console.log("Hello from Node!")      
You can execute scripts directly in the workspace:
steps:
  - image: ubuntu
    script: |
      #!/usr/bin/env bash
      /workspace/my-script.sh  # provided by an input resource      
You can also execute scripts within the container image:
steps:
  - image: my-image # contains /bin/my-binary
    script: |
      #!/usr/bin/env bash
      /bin/my-binary      
Windows scripts
Scripts in tasks that will eventually run on windows nodes need a custom shebang line, so that Tekton knows how to run the script. The format of the shebang line is:
#!win <interpreter command> <args>
Unlike linux, we need to specify how to interpret the script file which is generated by Tekton. The example below shows how to execute a powershell script:
steps:
  - image: mcr.microsoft.com/windows/servercore:1809
    script: |
      #!win powershell.exe -File
      echo 'Hello from PowerShell'      
Microsoft provide powershell images, which contain Powershell Core (which is slightly different from powershell found in standard windows images). The example below shows how to use these images:
steps:
  - image: mcr.microsoft.com/powershell:nanoserver
    script: |
      #!win pwsh.exe -File
      echo 'Hello from PowerShell Core'      
As can be seen the command is different. The windows shebang can be used for any interpreter, as long as it exists in the image and can interpret commands from a file. The example below executes a Python script:
  steps:
  - image: python
    script: |
      #!win python
      print("Hello from Python!")      
Note that other than the #!win shebang the example is identical to the earlier linux example.
Finally, if no interpreter is specified on the #!win line then the script will be treated as a windows .cmd file which will be excecuted. The example below shows this:
  steps:
  - image: mcr.microsoft.com/powershell:lts-nanoserver-1809
    script: |
      #!win
      echo Hello from the default cmd file      
Specifying a timeout
A Step can specify a timeout field.
If the Step execution time exceeds the specified timeout, the Step kills
its running process and any subsequent Steps in the TaskRun will not be
executed. The TaskRun is placed into a Failed condition.  An accompanying log
describing which Step timed out is written as the Failed conditionâ€™s message.
The timeout specification follows the duration format as specified in the Go time package (e.g. 1s or 1ms).
The example Step below is supposed to sleep for 60 seconds but will be canceled by the specified 5 second timeout.
steps:
  - name: sleep-then-timeout
    image: ubuntu
    script: |
      #!/usr/bin/env bash
      echo "I am supposed to sleep for 60 seconds!"
      sleep 60      
    timeout: 5s
Specifying onError for a step
When a step in a task results in a failure, the rest of the steps in the task are skipped and the taskRun is
declared a failure. If you would like to ignore such step errors and continue executing the rest of the steps in
the task, you can specify onError for such a step.
onError can be set to either continue or stopAndFail as part of the step definition. If onError is
set to continue, the entrypoint sets the original failed exit code of the script
in the container terminated state. A step with onError set to continue does not fail the taskRun and continues
executing the rest of the steps in a task.
To ignore a step error, set onError to continue:
steps:
  - image: docker.io/library/golang:latest
    name: ignore-unit-test-failure
    onError: continue
    script: |
            go test .
The original failed exit code of the script is available in the terminated state of
the container.
kubectl get tr taskrun-unit-test-t6qcl -o json | jq .status
{
  "conditions": [
    {
      "message": "All Steps have completed executing",
      "reason": "Succeeded",
      "status": "True",
      "type": "Succeeded"
    }
  ],
  "steps": [
    {
      "container": "step-ignore-unit-test-failure",
      "imageID": "...",
      "name": "ignore-unit-test-failure",
      "terminated": {
        "containerID": "...",
        "exitCode": 1,
        "reason": "Completed",
      }
    },
  ],
For an end-to-end example, see the taskRun ignoring a step error
and the pipelineRun ignoring a step error.
Accessing Stepâ€™s exitCode in subsequent Steps
A step can access the exit code of any previous step by reading the file pointed to by the exitCode path variable:
cat $(steps.step-<step-name>.exitCode.path)
The exitCode of a step without any name can be referenced using:
cat $(steps.step-unnamed-<step-index>.exitCode.path)
Produce a task result with onError
When a step is set to ignore the step error and if that step is able to initialize a result file before failing,
that result is made available to its consumer task.
steps:
  - name: ignore-failure-and-produce-a-result
    onError: continue
    image: busybox
    script: |
      echo -n 123 | tee $(results.result1.path)
      exit 1      
The task consuming the result using the result reference $(tasks.task1.results.result1) in a pipeline will be able
to access the result and run with the resolved value.
Now, a step can fail before initializing a result and the pipeline can ignore such step failure. But, the  pipeline
will fail with InvalidTaskResultReference if it has a task consuming that task result. For example, any task
consuming $(tasks.task1.results.result2) will cause the pipeline to fail.
steps:
  - name: ignore-failure-and-produce-a-result
    onError: continue
    image: busybox
    script: |
      echo -n 123 | tee $(results.result1.path)
      exit 1
      echo -n 456 | tee $(results.result2.path)      
Breakpoint on failure with onError
Debugging a taskRun is supported to debug a container and comes with a set of
tools to declare the step as a failure or a success. Specifying
breakpoint at the taskRun level overrides ignoring a step error using onError.
Redirecting step output streams with stdoutConfig and stderrConfig
This is an alpha feature. The enable-api-fields feature flag must be set to "alpha"
for Redirecting Step Output Streams to function.
This feature defines optional Step fields stdoutConfig and stderrConfig which can be used to redirection the output streams stdout and stderr respectively:
- name: ...
  ...
  stdoutConfig:
    path: ...
  stderrConfig:
    path: ...
Once stdoutConfig.path or stderrConfig.path is specified, the corresponding output stream will be duplicated to both the given file and the standard output stream of the container, so users can still view the output through the Pod log API. If both stdoutConfig.path and stderrConfig.path are set to the same value, outputs from both streams will be interleaved in the same file, but there will be no ordering guarantee on the data. If multiple Stepâ€™s stdoutConfig.path fields are set to the same value, the file content will be overwritten by the last outputting step.
Variable substitution will be applied to the new fields, so one could specify $(results.<name>.path) to the stdoutConfig.path or stderrConfig.path field to extract the stdout of a step into a Task result.
Example Usage
Redirecting stdout of boskosctl to jq and publish the resulting project-id as a Task result:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: boskos-acquire
spec:
  results:
  - name: project-id
  steps:
  - name: boskosctl
    image: gcr.io/k8s-staging-boskos/boskosctl
    args:
    - acquire
    - --server-url=http://boskos.test-pods.svc.cluster.local
    - --owner-name=christie-test-boskos
    - --type=gke-project
    - --state=free
    - --target-state=busy
    stdoutConfig:
      path: /data/boskosctl-stdout
    volumeMounts:
    - name: data
      mountPath: /data
  - name: parse-project-id
    image: imega/jq
    args:
    - -r
    - .name
    - /data/boskosctl-stdout
    stdoutConfig:
      path: $(results.project-id.path)
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data

NOTE:

If the intent is to share output between Steps via a file, the user must ensure that the paths provided are shared between the Steps (e.g via volumes).
There is currently a limit on the overall size of the Task results. If the stdout/stderr of a step is set to the path of a Task result and the step prints too many data, the result manifest would become too large. Currently the entrypoint binary will fail if that happens.
If the stdout/stderr of a Step is set to the path of a Task result, e.g. $(results.empty.path), but that result is not defined for the Task, the Step will run but the output will be captured in a file named $(results.empty.path) in the current working directory. Similarly, any stubstition that is not valid, e.g. $(some.invalid.path)/out.txt, will be left as-is and will result in a file path $(some.invalid.path)/out.txt relative to the current working directory.


Specifying Parameters
You can specify parameters, such as compilation flags or artifact names, that you want to supply to the Task at execution time.
Parameters are passed to the Task from its corresponding TaskRun.
Parameter name
Parameter name format:

Must only contain alphanumeric characters, hyphens (-), underscores (_), and dots (.). However, object parameter name and its key names canâ€™t contain dots (.). See the reasons in the third item added in this PR.
Must begin with a letter or an underscore (_).

For example, foo.Is-Bar_ is a valid parameter name for string or array type, but is invalid for object parameter because it contains dots. On the other hand, barIsBa$ or 0banana are invalid for all types.

NOTE:

Parameter names are case insensitive. For example, APPLE and apple will be treated as equal. If they appear in the same TaskSpecâ€™s params, it will be rejected as invalid.
If a parameter name contains dots (.), it must be referenced by using the bracket notation with either single or double quotes i.e. $(params['foo.bar']), $(params["foo.bar"]). See the following example for more information.


Parameter type
Each declared parameter has a type field, which can be set to string, array or object.
object type
object type is useful in cases where users want to group related parameters. For example, an object parameter called gitrepo can contain both the url and the commmit to group related information:
spec:
  params:
    - name: gitrepo
      type: object
      properties:
        url:
          type: string
        commit:
          type: string
Refer to the TaskRun example and the PipelineRun example in which object parameters are demonstrated.

NOTE:

object param must specify the properties section to define the schema i.e. what keys are available for this object param. See how to define properties section in the following example and the TEP-0075.
When providing value for an object param, one may provide values for just a subset of keys in specâ€™s default, and provide values for the rest of keys at runtime (example).
When using object in variable replacement, users can only access its individual key (â€œchildâ€ member) of the object by its name i.e. $(params.gitrepo.url). Using an entire object as a value is only allowed when the value is also an object like this example. See more details about using object param from the TEP-0075.


array type
array type is useful in cases where the number of compilation flags being supplied to a task varies throughout the Task's execution.
array param can be defined by setting type to array.  Also, array params only supports string array i.e.
each array element has to be of type string.
spec:
  params:
    - name: flags
      type: array
string type
If not specified, the type field defaults to string. When the actual parameter value is supplied, its parsed type is validated against the type field.
The following example illustrates the use of Parameters in a Task. The Task declares 3 input parameters named gitrepo (of type object), flags
(of type array) and someURL (of type string). These parameters are used in the steps.args list

For object parameter, you can only use individual members (aka keys).
You can expand parameters of type array inside an existing array using the star operator. In this example, flags contains the star operator: $(params.flags[*]).

Note: Input parameter values can be used as variables throughout the Task by using variable substitution.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: task-with-parameters
spec:
  params:
    - name: gitrepo
      type: object
      properties:
        url:
          type: string
        commit:
          type: string
    - name: flags
      type: array
    - name: someURL
      type: string
    - name: foo.bar
      description: "the name contains dot character"
      default: "test"
  steps:
    - name: do-the-clone
      image: some-git-image
      args: [
        "-url=$(params.gitrepo.url)",
        "-revision=$(params.gitrepo.commit)"
      ]
    - name: build
      image: my-builder
      args: [
        "build",
        "$(params.flags[*])",
        # It would be equivalent to use $(params["someURL"]) here,
        # which is necessary when the parameter name contains '.'
        # characters (e.g. `$(params["some.other.URL"])`). See the example in step "echo-param"
        'url=$(params.someURL)',
      ]
    - name: echo-param
      image: bash
      args: [
        "echo",
        "$(params['foo.bar'])",
      ]
The following TaskRun supplies the value for the parameter gitrepo, flags and someURL:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: run-with-parameters
spec:
  taskRef:
    name: task-with-parameters
  params:
    - name: gitrepo
      value:
        url: "abc.com"
        commit: "c12b72"
    - name: flags
      value:
        - "--set"
        - "arg1=foo"
        - "--randomflag"
        - "--someotherflag"
    - name: someURL
      value: "http://google.com"
Default value
Parameter declarations (within Tasks and Pipelines) can include default values which will be used if the parameter is
not specified, for example to specify defaults for both string params and array params
(full example) :
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: task-with-array-default
spec:
  params:
    - name: flags
      type: array
      default:
        - "--set"
        - "arg1=foo"
        - "--randomflag"
        - "--someotherflag"
Param enum

ğŸŒ± enum is an alpha feature. The enable-param-enum feature flag must be set to "true" to enable this feature.

Parameter declarations can include enum which is a predefine set of valid values that can be accepted by the Param. If a Param has both enum and default value, the default value must be in the enum set. For example, the valid/allowed values for Param â€œmessageâ€ is bounded to v1, v2 and v3:
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: param-enum-demo
spec:
  params:
  - name: message
    type: string
    enum: ["v1", "v2", "v3"]
    default: "v1"
  steps:
  - name: build
    image: bash:latest
    script: |
            echo "$(params.message)"
If the Param value passed in by TaskRuns is NOT in the predefined enum list, the TaskRuns will fail with reason InvalidParamValue.
See usage in this example
Specifying Workspaces
Workspaces allow you to specify
one or more volumes that your Task requires during execution. It is recommended that Tasks uses at most
one writeable Workspace. For example:
spec:
  steps:
    - name: write-message
      image: ubuntu
      script: |
        #!/usr/bin/env bash
        set -xe
        echo hello! > $(workspaces.messages.path)/message        
  workspaces:
    - name: messages
      description: The folder where we write the message to
      mountPath: /custom/path/relative/to/root
For more information, see Using Workspaces in Tasks
and the Workspaces in a TaskRun example YAML file.
Propagated Workspaces
Workspaces can be propagated to embedded task specs, not referenced Tasks. For more information, see Propagated Workspaces.
Emitting Results
A Task is able to emit string results that can be viewed by users and passed to other Tasks in a Pipeline. These
results have a wide variety of potential uses. To highlight just a few examples from the Tekton Catalog: the
git-clone Task emits a
cloned commit SHA as a result, the generate-build-id Task
emits a randomized ID as a result, and the kaniko Task
emits a container image digest as a result. In each case these results convey information for users to see when
looking at their TaskRuns and can also be used in a Pipeline to pass data along from one Task to the next.
Task results are best suited for holding small amounts of data, such as commit SHAs, branch names,
ephemeral namespaces, and so on.
To define a Task's results, use the results field.
In the example below, the Task specifies two files in the results field:
current-date-unix-timestamp and current-date-human-readable.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: print-date
  annotations:
    description: |
            A simple task that prints the date
spec:
  results:
    - name: current-date-unix-timestamp
      description: The current date in unix timestamp format
    - name: current-date-human-readable
      description: The current date in human readable format
  steps:
    - name: print-date-unix-timestamp
      image: bash:latest
      script: |
        #!/usr/bin/env bash
        date +%s | tee $(results.current-date-unix-timestamp.path)        
    - name: print-date-human-readable
      image: bash:latest
      script: |
        #!/usr/bin/env bash
        date | tee $(results.current-date-human-readable.path)        
In this example, $(results.name.path)
is replaced with the path where Tekton will store the Taskâ€™s results.
When this Task is executed in a TaskRun, the results will appear in the TaskRunâ€™s status:












  










  
  








  










  
  






      v1
    

    
      
    
      v1beta1
    

    
      
    





        apiVersion: tekton.dev/v1
kind: TaskRun
# ...
status:
  # ...
  results:
    - name: current-date-human-readable
      value: |
                Wed Jan 22 19:47:26 UTC 2020
    - name: current-date-unix-timestamp
      value: |
                1579722445


    
      
    

  
        apiVersion: tekton.dev/v1beta1
kind: TaskRun
# ...
status:
  # ...
  taskResults:
    - name: current-date-human-readable
      value: |
                Wed Jan 22 19:47:26 UTC 2020
    - name: current-date-unix-timestamp
      value: |
                1579722445


    
      
    

  


Tekton does not perform any processing on the contents of results; they are emitted
verbatim from your Task including any leading or trailing whitespace characters. Make sure to write only the
precise string you want returned from your Task into the result files that your Task creates.
The stored results can be used at the Task level
or at the Pipeline level.

Note Tekton does not enforce Task results unless there is a consumer: when a Task declares a result,
it may complete successfully even if no result was actually produced. When a Task that declares results is
used in a Pipeline, and a component of the Pipeline attempts to consume the Taskâ€™s result, if the result
was not produced the pipeline will fail. TEP-0048
propopses introducing default values for results to help Pipeline authors manage this case.

Emitting Object Results
Emitting a task result of type object is implemented based on the
TEP-0075.
You can initialize object results from a task using JSON escaped string. For example, to assign the following data to an object result:
{"url":"abc.dev/sampler","digest":"19f02276bf8dbdd62f069b922f10c65262cc34b710eea26ff928129a736be791"}
You will need to use escaped JSON to write to pod termination message:
{\"url\":\"abc.dev/sampler\",\"digest\":\"19f02276bf8dbdd62f069b922f10c65262cc34b710eea26ff928129a736be791\"}
An example of a task definition producing an object result:
kind: Task
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
metadata:
  name: write-object
  annotations:
    description: |
            A simple task that writes object
spec:
  results:
    - name: object-results
      type: object
      description: The object results
      properties:
        url:
          type: string
        digest:
          type: string
  steps:
    - name: write-object
      image: bash:latest
      script: |
        #!/usr/bin/env bash
        echo -n "{\"url\":\"abc.dev/sampler\",\"digest\":\"19f02276bf8dbdd62f069b922f10c65262cc34b710eea26ff928129a736be791\"}" | tee $(results.object-results.path)        

Note:

that the opening and closing braces  are mandatory along with an escaped JSON.
object result must specify the properties section to define the schema i.e. what keys are available for this object result. Failing to emit keys from the defined object results will result in validation error at runtime.


Emitting Array Results
Tekton Task also supports defining a result of type array and object in addition to string.
Emitting a task result of type array is a beta feature implemented based on the
TEP-0076.
You can initialize array results from a task using JSON escaped string, for example, to assign the following
list of animals to an array result:
["cat", "dog", "squirrel"]
You will have to initialize the pod termination message as escaped JSON:
[\"cat\", \"dog\", \"squirrel\"]
An example of a task definition producing an array result with such greetings ["hello", "world"]:
kind: Task
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
metadata:
  name: write-array
  annotations:
    description: |
            A simple task that writes array
spec:
  results:
    - name: array-results
      type: array
      description: The array results
  steps:
    - name: write-array
      image: bash:latest
      script: |
        #!/usr/bin/env bash
        echo -n "[\"hello\",\"world\"]" | tee $(results.array-results.path)        
Note that the opening and closing square brackets are mandatory along with an escaped JSON.
Now, similar to the Go zero-valued slices, an array result is considered as uninitialized (i.e. nil) if itâ€™s set to an empty
array i.e. []. For example, echo -n "[]" |  tee $(results.result.path); is equivalent to result := []string{}.
The result initialized in this way will have zero length. And trying to access this array with a star notation i.e.
$(tasks.write-array-results.results.result[*]) or an element of such array i.e. $(tasks.write-array-results.results.result[0])
results in InvalidTaskResultReference with index out of range.
Depending on your use case, you might have to initialize a result array to the desired length just like using make() function in Go.
make() function is used to allocate an array and returns a slice of the specified length i.e.
result := make([]string, 5) results in ["", "", "", "", ""], similarly set the array result to following JSON escaped
expression to allocate an array of size 2:
echo -n "[\"\", \"\"]" | tee $(results.array-results.path) # an array of size 2 with empty string
echo -n "[\"first-array-element\", \"\"]" | tee $(results.array-results.path) # an array of size 2 with only first element initialized
echo -n "[\"\", \"second-array-element\"]" | tee $(results.array-results.path) # an array of size 2 with only second element initialized
echo -n "[\"first-array-element\", \"second-array-element\"]" | tee $(results.array-results.path) # an array of size 2 with both elements initialized
This is also important to maintain the order of the elements in an array. The order in which the task result was
initialized is the order in which the result is consumed by the dependent tasks. For example, a task is producing
two array results images and configmaps. The pipeline author can implement deployment by indexing into each array result:
    - name: deploy-stage-1
      taskRef:
        name: deploy
      params:
        - name: image
          value: $(tasks.setup.results.images[0])
        - name: configmap
          value: $(tasks.setup.results.configmap[0])
      ...
    - name: deploy-stage-2
      taskRef:
        name: deploy
      params:
        - name: image
          value: $(tasks.setup.results.images[1])
        - name: configmap
          value: $(tasks.setup.results.configmap[1])
As a task author, make sure the task array results are initialized accordingly or set to a zero value in case of no
image or configmap to maintain the order.
Note: Tekton uses termination
messages. As
written in
tektoncd/pipeline#4808,
the maximum size of a Task's results is limited by the container termination message feature of Kubernetes.
At present, the limit is â€œ4096 bytesâ€.
This also means that the number of Steps in a Task affects the maximum size of a Result,
as each Step is implemented as a container in the TaskRunâ€™s pod.
The more containers we have in our pod, the smaller the allowed size of each containerâ€™s
message, meaning that the more steps you have in a Task, the smaller the result for each step can be.
For example, if you have 10 steps, the size of each stepâ€™s Result will have a maximum of less than 1KB.
If your Task writes a large number of small results, you can work around this limitation
by writing each result from a separate Step so that each Step has its own termination message.
If a termination message is detected as being too large the TaskRun will be placed into a failed state
with the following message: Termination message is above max allowed size 4096, caused by large task result. Since Tekton also uses the termination message for some internal information, so the real
available size will less than 4096 bytes.
As a general rule-of-thumb, if a result needs to be larger than a kilobyte, you should likely use a
Workspace to store and pass it between Tasks within a Pipeline.
Larger Results using sidecar logs
This is a beta feature which is guarded behind its own feature flag.  The results-from feature flag must be set to
"sidecar-logs" to enable larger results using sidecar logs.
Instead of using termination messages to store results, the taskrun controller injects a sidecar container which monitors
the results of all the steps. The sidecar mounts the volume where results of all the steps are stored. As soon as it
finds a new result, it logs it to std out. The controller has access to the logs of the sidecar container.
CAUTION: we need you to enable access to kubernetes pod/logs.
This feature allows users to store up to 4 KB per result by default. Because we are not limited by the size of the
termination messages, users can have as many results as they require (or until the CRD reaches its limit). If the size
of a result exceeds this limit, then the TaskRun will be placed into a failed state with the following message: Result exceeded the maximum allowed limit.
Note: If you require even larger results, you can specify a different upper limit per result by setting
max-result-size feature flag to your desired size in bytes (see instructions).
CAUTION: the larger you make the size, more likely will the CRD reach its max limit enforced by the etcd server
leading to bad user experience.
Refer to the detailed instructions listed in additional config
to learn how to enable this feature.
Specifying Volumes
Specifies one or more Volumes that the Steps in your
Task require to execute in addition to volumes that are implicitly created for input and output resources.
For example, you can use Volumes to do the following:

Mount a Kubernetes Secret.
Create an emptyDir persistent Volume that caches data across multiple Steps.
Mount a Kubernetes ConfigMap
as Volume source.
Mount a hostâ€™s Docker socket to use a Dockerfile for building container images.
Note: Building a container image on-cluster using docker build is very
unsafe and is mentioned only for the sake of the example. Use kaniko instead.

Specifying a Step template
The stepTemplate field specifies a Container
configuration that will be used as the starting point for all of the Steps in your
Task. Individual configurations specified within Steps supersede the template wherever
overlap occurs.
In the example below, the Task specifies a stepTemplate field with the environment variable
FOO set to bar. The first Step in the Task uses that value for FOO, but the second Step
overrides the value set in the template with baz. Additional, the Task specifies a stepTemplate
field with the environment variable TOKEN set to public. The last one Step in the Task uses
private in the referenced secret to override the value set in the template.
stepTemplate:
  env:
    - name: "FOO"
      value: "bar"
    - name: "TOKEN"
      value: "public"
steps:
  - image: ubuntu
    command: [echo]
    args: ["FOO is ${FOO}"]
  - image: ubuntu
    command: [echo]
    args: ["FOO is ${FOO}"]
    env:
      - name: "FOO"
        value: "baz"
  - image: ubuntu
    command: [echo]
    args: ["TOKEN is ${TOKEN}"]
    env:
      - name: "TOKEN"
        valueFrom:
          secretKeyRef:
            key: "token"
            name: "test"
---
# The secret 'test' part data is as follows.
data:
  # The decoded value of 'cHJpdmF0ZQo=' is 'private'.
  token: "cHJpdmF0ZQo="
Specifying Sidecars
The sidecars field specifies a list of Containers
to run alongside the Steps in your Task. You can use Sidecars to provide auxiliary functionality, such as
Docker in Docker or running a mock API server that your app can hit during testing.
Sidecars spin up before your Task executes and are deleted after the Task execution completes.
For further information, see Sidecars in TaskRuns.
Note: Starting in v0.62 you can enable native Kubernetes sidecar support using the enable-kubernetes-sidecar feature flag (see instructions). If kubernetes does not wait for your sidecar application to be ready, use a startupProbe to help kubernetes identify when it is ready.
Refer to the detailed instructions listed in additional config
to learn how to enable this feature.
In the example below, a Step uses a Docker-in-Docker Sidecar to build a Docker image:
steps:
  - image: docker
    name: client
    script: |
      #!/usr/bin/env bash
      cat > Dockerfile << EOF
      FROM ubuntu
      RUN apt-get update
      ENTRYPOINT ["echo", "hello"]
      EOF
      docker build -t hello . && docker run hello
      docker images      
    volumeMounts:
      - mountPath: /var/run/
        name: dind-socket
sidecars:
  - image: docker:18.05-dind
    name: server
    securityContext:
      privileged: true
    volumeMounts:
      - mountPath: /var/lib/docker
        name: dind-storage
      - mountPath: /var/run/
        name: dind-socket
volumes:
  - name: dind-storage
    emptyDir: {}
  - name: dind-socket
    emptyDir: {}
Sidecars, just like Steps, can also run scripts:
sidecars:
  - image: busybox
    name: hello-sidecar
    script: |
            echo 'Hello from sidecar!'
Note: Tektonâ€™s current Sidecar implementation contains a bug.
Tekton uses a container image named nop to terminate Sidecars.
That image is configured by passing a flag to the Tekton controller.
If the configured nop image contains the exact command the Sidecar
was executing before receiving a â€œstopâ€ signal, the Sidecar keeps
running, eventually causing the TaskRun to time out with an error.
For more information, see issue 1347.
Specifying a display name
The displayName field is an optional field that allows you to add a user-facing name to the task that may be used to populate a UI.
Adding a description
The description field is an optional field that allows you to add an informative description to the Task.
Using variable substitution
Tekton provides variables to inject values into the contents of certain fields.
The values you can inject come from a range of sources including other fields
in the Task, context-sensitive information that Tekton provides, and runtime
information received from a TaskRun.
The mechanism of variable substitution is quite simple - string replacement is
performed by the Tekton Controller when a TaskRun is executed.
Tasks allow you to substitute variable names for the following entities:

Parameters and resources
Array parameters
Workspaces
Volume names and types

See the complete list of variable substitutions for Tasks
and the list of fields that accept substitutions.
Substituting parameters and resources
params and resources attributes can replace
variable values as follows:

To reference a parameter in a Task, use the following syntax, where <name> is the name of the parameter:
# dot notation
# Here, the name cannot contain dots (eg. foo.bar is not allowed). If the name contains `dots`, it can only be accessed via the bracket notation.
$(params.<name> )
# or bracket notation (wrapping <name> with either single or double quotes):
# Here, the name can contain dots (eg. foo.bar is allowed).
$(params['<name>'])
$(params["<name>"])

To access parameter values from resources, see variable substitution

Substituting Array parameters
You can expand referenced parameters of type array using the star operator. To do so, add the operator ([*])
to the named parameter to insert the array elements in the spot of the reference string.
For example, given a params field with the contents listed below, you can expand
command: ["first", "$(params.array-param[*])", "last"] to command: ["first", "some", "array", "elements", "last"]:
params:
  - name: array-param
    value:
      - "some"
      - "array"
      - "elements"
You must reference parameters of type array in a completely isolated string within a larger string array.
Referencing an array parameter in any other way will result in an error. For example, if build-args is a parameter of
type array, then the following example is an invalid Step because the string isnâ€™t isolated:
- name: build-step
  image: gcr.io/cloud-builders/some-image
  args: ["build", "additionalArg $(params.build-args[*])"]
Similarly, referencing build-args in a non-array field is also invalid:
- name: build-step
  image: "$(params.build-args[*])"
  args: ["build", "args"]
A valid reference to the build-args parameter is isolated and in an eligible field (args, in this case):
- name: build-step
  image: gcr.io/cloud-builders/some-image
  args: ["build", "$(params.build-args[*])", "additionalArg"]
array param when referenced in args section of the step can be utilized in the script as command line arguments:
- name: build-step
  image: gcr.io/cloud-builders/some-image
  args: ["$(params.flags[*])"]
  script: |
    #!/usr/bin/env bash
    echo "The script received $# flags."
    echo "The first command line argument is $1."    
Indexing into an array to reference an individual array element is supported as an alpha feature (enable-api-fields: alpha).
Referencing an individual array element in args:
- name: build-step
  image: gcr.io/cloud-builders/some-image
  args: ["$(params.flags[0])"]
Referencing an individual array element in script:
- name: build-step
  image: gcr.io/cloud-builders/some-image
  script: |
    #!/usr/bin/env bash
    echo "$(params.flags[0])"    
Substituting Workspace paths
You can substitute paths to Workspaces specified within a Task as follows:
$(workspaces.myworkspace.path)
Since the Volume name is randomized and only set when the Task executes, you can also
substitute the volume name as follows:
$(workspaces.myworkspace.volume)
Substituting Volume names and types
You can substitute Volume names and types
by parameterizing them. Tekton supports popular Volume types such as ConfigMap, Secret, and PersistentVolumeClaim.
See this example to find out how to perform this type of substitution
in your Task.
Substituting in Script blocks
Variables can contain any string, including snippets of script that can
be injected into a Taskâ€™s Script field. If you are using Tektonâ€™s variables
in your Taskâ€™s Script field be aware that the strings youâ€™re interpolating
could include executable instructions.
Preventing a substituted variable from executing as code depends on the container
image, language or shell that your Task uses. Hereâ€™s an example of interpolating
a Tekton variable into a bash Script block that prevents the variableâ€™s string
contents from being executed:
# Task.yaml
spec:
  steps:
    - image: an-image-that-runs-bash
      env:
        - name: SCRIPT_CONTENTS
          value: $(params.script)
      script: |
                printf '%s' "${SCRIPT_CONTENTS}" > input-script
This works by injecting Tektonâ€™s variable as an environment variable into the Stepâ€™s
container. The printf program is then used to write the environment variableâ€™s
content to a file.
Code examples
Study the following code examples to better understand how to configure your Tasks:

Building and pushing a Docker image
Mounting multiple Volumes
Mounting a ConfigMap as a Volume source
Using a Secret as an environment source
Using a Sidecar in a Task

_Tip: See the collection of Tasks in the
Tekton community catalog for
more examples.
Building and pushing a Docker image
The following example Task builds and pushes a Dockerfile-built image.
Note: Building a container image using docker build on-cluster is very
unsafe and is shown here only as a demonstration. Use kaniko instead.
spec:
  params:
    # This may be overridden, but is a sensible default.
    - name: dockerfileName
      type: string
      description: The name of the Dockerfile
      default: Dockerfile
    - name: image
      type: string
      description: The image to build and push
  workspaces:
  - name: source
  steps:
    - name: dockerfile-build
      image: gcr.io/cloud-builders/docker
      workingDir: "$(workspaces.source.path)"
      args:
        [
          "build",
          "--no-cache",
          "--tag",
          "$(params.image)",
          "--file",
          "$(params.dockerfileName)",
          ".",
        ]
      volumeMounts:
        - name: docker-socket
          mountPath: /var/run/docker.sock

    - name: dockerfile-push
      image: gcr.io/cloud-builders/docker
      args: ["push", "$(params.image)"]
      volumeMounts:
        - name: docker-socket
          mountPath: /var/run/docker.sock

  # As an implementation detail, this Task mounts the host's daemon socket.
  volumes:
    - name: docker-socket
      hostPath:
        path: /var/run/docker.sock
        type: Socket
Mounting multiple Volumes
The example below illustrates mounting multiple Volumes:
spec:
  steps:
    - image: ubuntu
      script: |
        #!/usr/bin/env bash
        curl https://foo.com > /var/my-volume        
      volumeMounts:
        - name: my-volume
          mountPath: /var/my-volume

    - image: ubuntu
      script: |
        #!/usr/bin/env bash
        cat /etc/my-volume        
      volumeMounts:
        - name: my-volume
          mountPath: /etc/my-volume

  volumes:
    - name: my-volume
      emptyDir: {}
Mounting a ConfigMap as a Volume source
The example below illustrates how to mount a ConfigMap to act as a Volume source:
spec:
  params:
    - name: CFGNAME
      type: string
      description: Name of config map
    - name: volumeName
      type: string
      description: Name of volume
  steps:
    - image: ubuntu
      script: |
        #!/usr/bin/env bash
        cat /var/configmap/test        
      volumeMounts:
        - name: "$(params.volumeName)"
          mountPath: /var/configmap

  volumes:
    - name: "$(params.volumeName)"
      configMap:
        name: "$(params.CFGNAME)"
Using a Secret as an environment source
The example below illustrates how to use a Secret as an environment source:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: goreleaser
spec:
  params:
    - name: package
      type: string
      description: base package to build in
    - name: github-token-secret
      type: string
      description: name of the secret holding the github-token
      default: github-token
  workspaces:
  - name: source
  steps:
    - name: release
      image: goreleaser/goreleaser
      workingDir: $(workspaces.source.path)/$(params.package)
      command:
        - goreleaser
      args:
        - release
      env:
        - name: GOPATH
          value: /workspace
        - name: GITHUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: $(params.github-token-secret)
              key: bot-token
Using a Sidecar in a Task
The example below illustrates how to use a Sidecar in your Task:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: with-sidecar-task
spec:
  params:
    - name: sidecar-image
      type: string
      description: Image name of the sidecar container
    - name: sidecar-env
      type: string
      description: Environment variable value
  sidecars:
    - name: sidecar
      image: $(params.sidecar-image)
      env:
        - name: SIDECAR_ENV
          value: $(params.sidecar-env)
  steps:
    - name: test
      image: hello-world
Debugging
This section describes techniques for debugging the most common issues in Tasks.
Inspecting the file structure
A common issue when configuring Tasks stems from not knowing the location of your data.
For the most part, files ingested and output by your Task live in the /workspace directory,
but the specifics can vary. To inspect the file structure of your Task, add a step that outputs
the name of every file stored in the /workspace directory to the build log. For example:
- name: build-and-push-1
  image: ubuntu
  command:
    - /bin/bash
  args:
    - -c
    - |
      set -ex
      find /workspace      
You can also choose to examine the contents of every file used by your Task:
- name: build-and-push-1
  image: ubuntu
  command:
    - /bin/bash
  args:
    - -c
    - |
      set -ex
      find /workspace | xargs cat      
Inspecting the Pod
To inspect the contents of the Pod used by your Task at a specific stage in the Task's execution,
log into the Pod and add a Step that pauses the Task at the desired stage. For example:
- name: pause
  image: docker
  args: ["sleep", "6000"]
Running Step Containers as a Non Root User
All steps that do not require to be run as a root user should make use of TaskRun features to
designate the container for a step runs as a user without root permissions. As a best practice,
running containers as non root should be built into the container image to avoid any possibility
of the container being run as root. However, as a further measure of enforcing this practice,
steps can make use of a securityContext to specify how the container should run.
An example of running Task steps as a non root user is shown below:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: show-non-root-steps
spec:
  steps:
    # no securityContext specified so will use
    # securityContext from TaskRun podTemplate
    - name: show-user-1001
      image: ubuntu
      command:
        - ps
      args:
        - "aux"
    # securityContext specified so will run as
    # user 2000 instead of 1001
    - name: show-user-2000
      image: ubuntu
      command:
        - ps
      args:
        - "aux"
      securityContext:
        runAsUser: 2000
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  generateName: show-non-root-steps-run-
spec:
  taskRef:
    name: show-non-root-steps
  podTemplate:
    securityContext:
      runAsNonRoot: true
      runAsUser: 1001
In the example above, the step show-user-2000 specifies via a securityContext that the container
for the step should run as user 2000. A securityContext must still be specified via a TaskRun podTemplate
for this TaskRun to run in a Kubernetes environment that enforces running containers as non root as a requirement.
The runAsNonRoot property specified via the podTemplate above validates that steps part of this TaskRun are
running as non root users and will fail to start any step container that attempts to run as root. Only specifying
runAsNonRoot: true will not actually run containers as non root as the property simply validates that steps are not
running as root. It is the runAsUser property that is actually used to set the non root user ID for the container.
If a step defines its own securityContext, it will be applied for the step container over the securityContext
specified at the pod level via the TaskRun podTemplate.
More information about Pod and Container Security Contexts can be found via the Kubernetes website.
The example Task/TaskRun above can be found as a TaskRun example.
Task Authoring Recommendations
Recommendations for authoring Tasks are available in the Tekton Catalog.

Except as otherwise noted, the contents of this page are licensed under the
Creative Commons Attribution 4.0 License.
Code samples are licensed under the Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	TaskRuns


Overview
Configuring a TaskRun

Specifying the target Task

Tekton Bundles
Remote Tasks


Specifying Parameters

Propagated Parameters
Propagated Object Parameters
Extra Parameters


Specifying Resource limits
Specifying Task-level ComputeResources
Specifying a Pod template
Specifying Workspaces

Propagated Workspaces


Specifying Sidecars
Configuring Task Steps and Sidecars in a TaskRun
Specifying LimitRange values
Specifying Retries
Configuring the failure timeout
Specifying ServiceAccount credentials


TaskRun status

The status field


Monitoring execution status

Monitoring Steps
Steps
Monitoring Results


Cancelling a TaskRun
Debugging a TaskRun

Breakpoint on Failure
Debug Environment


Events
Running a TaskRun Hermetically
Code examples

Example TaskRun with a referenced Task
Example TaskRun with an embedded Task
Example of reusing a Task
Example of Using custom ServiceAccount credentials
Example of Running Step Containers as a Non Root User




Overview
A TaskRun allows you to instantiate and execute a Task on-cluster. A Task specifies one or more
Steps that execute container images and each container image performs a specific piece of build work. A TaskRun executes the
Steps in the Task in the order they are specified until all Steps have executed successfully or a failure occurs.
Configuring a TaskRun
A TaskRun definition supports the following fields:

Required:

apiVersion - Specifies the API version, for example
tekton.dev/v1beta1.
kind - Identifies this resource object as a TaskRun object.
metadata - Specifies the metadata that uniquely identifies the
TaskRun, such as a name.
spec - Specifies the configuration for the TaskRun.

taskRef or taskSpec - Specifies the Tasks that the
TaskRun will execute.




Optional:

serviceAccountName - Specifies a ServiceAccount
object that provides custom credentials for executing the TaskRun.
params - Specifies the desired execution parameters for the Task.
timeout - Specifies the timeout before the TaskRun fails.
podTemplate - Specifies a Pod template to use as
the starting point for configuring the Pods for the Task.
workspaces - Specifies the physical volumes to use for the
Workspaces declared by a Task.
debug- Specifies any breakpoints and debugging configuration for the Task execution.
stepOverrides - Specifies configuration to use to override the Taskâ€™s Steps.
sidecarOverrides - Specifies configuration to use to override the Taskâ€™s Sidecars.



Specifying the target Task
To specify the Task you want to execute in your TaskRun, use the taskRef field as shown below:
spec:
  taskRef:
    name: read-task
You can also embed the desired Task definition directly in the TaskRun using the taskSpec field:
spec:
  taskSpec:
    workspaces:
    - name: source
    steps:
      - name: build-and-push
        image: gcr.io/kaniko-project/executor:v0.17.1
        # specifying DOCKER_CONFIG is required to allow kaniko to detect docker credential
        workingDir: $(workspaces.source.path)
        env:
          - name: "DOCKER_CONFIG"
            value: "/tekton/home/.docker/"
        command:
          - /kaniko/executor
        args:
          - --destination=gcr.io/my-project/gohelloworld
Tekton Bundles
A Tekton Bundle is an OCI artifact that contains Tekton resources like Tasks which can be referenced within a taskRef.
You can reference a Tekton bundle in a TaskRef in both v1 and v1beta1 using remote resolution. The example syntax shown below for v1 uses remote resolution and requires enabling beta features.
spec:
  taskRef:
    resolver: bundles
    params:
    - name: bundle
      value: docker.io/myrepo/mycatalog
    - name: name
      value: echo-task
    - name: kind
      value: Task
You may also specify a tag as you would with a Docker image which will give you a repeatable reference to a Task.
spec:
  taskRef:
    resolver: bundles
    params:
    - name: bundle
      value: docker.io/myrepo/mycatalog:v1.0.1
    - name: name
      value: echo-task
    - name: kind
      value: Task
You may also specify a fixed digest instead of a tag which ensures the referenced task is constant.
spec:
  taskRef:
    resolver: bundles
    params:
    - name: bundle
      value: docker.io/myrepo/mycatalog@sha256:abc123
    - name: name
      value: echo-task
    - name: kind
      value: Task
A working example can be found here.
Any of the above options will fetch the image using the ImagePullSecrets attached to the
ServiceAccount specified in the TaskRun. See the Service Account
section for details on how to configure a ServiceAccount on a TaskRun. The TaskRun
will then run that Task without registering it in the cluster allowing multiple versions
of the same named Task to be run at once.
Tekton Bundles may be constructed with any toolsets that produces valid OCI image artifacts so long as
the artifact adheres to the contract. Additionally, you may also use the tkn
cli (coming soon).
Remote Tasks
(beta feature)
A taskRef field may specify a Task in a remote location such as git.
Support for specific types of remote will depend on the Resolvers your
clusterâ€™s operator has installed. For more information including a tutorial, please check resolution docs. The below example demonstrates referencing a Task in git:
spec:
  taskRef:
    resolver: git
    params:
    - name: url
      value: https://github.com/tektoncd/catalog.git
    - name: revision
      value: abc123
    - name: pathInRepo
      value: /task/golang-build/0.3/golang-build.yaml
Specifying Parameters
If a Task has parameters, you can use the params field to specify their values:
spec:
  params:
    - name: flags
      value: -someflag
Note: If a parameter does not have an implicit default value, you must explicitly set its value.
Propagated Parameters
When using an inlined taskSpec, parameters from the parent TaskRun will be
available to the Task without needing to be explicitly defined.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  generateName: hello-
spec:
  params:
    - name: message
      value: "hello world!"
  taskSpec:
    # There are no explicit params defined here.
    # They are derived from the TaskRun params above.
    steps:
    - name: default
      image: ubuntu
      script: |
                echo $(params.message)
On executing the task run, the parameters will be interpolated during resolution.
The specifications are not mutated before storage and so it remains the same.
The status is updated.
kind: TaskRun
metadata:
  name: hello-dlqm9
  ...
spec:
  params:
  - name: message
    value: hello world!
  serviceAccountName: default
  taskSpec:
    steps:
    - image: ubuntu
      name: default
      script: |
                echo $(params.message)
status:
  conditions:
  - lastTransitionTime: "2022-05-20T15:24:41Z"
    message: All Steps have completed executing
    reason: Succeeded
    status: "True"
    type: Succeeded
  ...
  steps:
  - container: step-default
    ...
  taskSpec:
    steps:
    - image: ubuntu
      name: default
      script: |
                echo "hello world!"
Propagated Object Parameters
When using an inlined taskSpec, object parameters from the parent TaskRun will be
available to the Task without needing to be explicitly defined.
Note: If an object parameter is being defined explicitly then you must define the spec of the object in Properties.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  generateName: object-param-result-
spec:
  params:
  - name: gitrepo
    value:
      commit: sha123
      url: xyz.com
  taskSpec:
    steps:
    - name: echo-object-params
      image: bash
      args:
      - echo
      - --url=$(params.gitrepo.url)
      - --commit=$(params.gitrepo.commit)
On executing the task run, the object parameters will be interpolated during resolution.
The specifications are not mutated before storage and so it remains the same.
The status is updated.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: object-param-result-vlnmb
  ...
spec:
  params:
  - name: gitrepo
    value:
      commit: sha123
      url: xyz.com
  serviceAccountName: default
  taskSpec:
    steps:
    - args:
      - echo
      - --url=$(params.gitrepo.url)
      - --commit=$(params.gitrepo.commit)
      image: bash
      name: echo-object-params
status:
  completionTime: "2022-09-08T17:09:37Z"
  conditions:
  - lastTransitionTime: "2022-09-08T17:09:37Z"
    message: All Steps have completed executing
    reason: Succeeded
    status: "True"
    type: Succeeded
    ...
  steps:
  - container: step-echo-object-params
    ...
  taskSpec:
    steps:
    - args:
      - echo
      - --url=xyz.com
      - --commit=sha123
      image: bash
      name: echo-object-params
Extra Parameters
(alpha only)
You can pass in extra Parameters if needed depending on your use cases. An example use
case is when your CI system autogenerates TaskRuns and it has Parameters it wants to
provide to all TaskRuns. Because you can pass in extra Parameters, you donâ€™t have to
go through the complexity of checking each Task and providing only the required params.
Parameter Enums

ğŸŒ± enum is an alpha feature. The enable-param-enum feature flag must be set to "true" to enable this feature.

If a Parameter is guarded by Enum in the Task, you can only provide Parameter values in the TaskRun that are predefined in the Param.Enum in the Task. The TaskRun will fail with reason InvalidParamValue otherwise.
You can also specify Enum for TaskRun with an embedded Task. The same param validation will be executed in this scenario.
See more details in Param.Enum.
Specifying Resource limits
Each Step in a Task can specify its resource requirements. See
Defining Steps. Resource requirements defined in Steps and Sidecars
may be overridden by a TaskRunâ€™s StepSpecs and SidecarSpecs.
Specifying Task-level ComputeResources
(beta only)
Task-level compute resources can be configured in TaskRun.ComputeResources, or PipelineRun.TaskRunSpecs.ComputeResources.
e.g.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: task
spec:
  steps:
    - name: foo
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: taskrun
spec:
  taskRef:
    name: task
  computeResources:
    requests:
      cpu: 1
    limits:
      cpu: 2
Further details and examples could be found in Compute Resources in Tekton.
Specifying a Pod template
You can specify a Pod template configuration that will serve as the configuration starting
point for the Pod in which the container images specified in your Task will execute. This allows you to
customize the Pod configuration specifically for that TaskRun.
In the following example, the Task specifies a volumeMount (my-cache) object, also provided by the TaskRun,
using a PersistentVolumeClaim volume. A specific scheduler is also configured in the  SchedulerName field.
The Pod executes with regular (non-root) user permissions.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: mytask
  namespace: default
spec:
  steps:
    - name: writesomething
      image: ubuntu
      command: ["bash", "-c"]
      args: ["echo 'foo' > /my-cache/bar"]
      volumeMounts:
        - name: my-cache
          mountPath: /my-cache
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: mytaskrun
  namespace: default
spec:
  taskRef:
    name: mytask
  podTemplate:
    schedulerName: volcano
    securityContext:
      runAsNonRoot: true
      runAsUser: 1001
    volumes:
      - name: my-cache
        persistentVolumeClaim:
          claimName: my-volume-claim
Specifying Workspaces
If a Task specifies one or more Workspaces, you must map those Workspaces to
the corresponding physical volumes in your TaskRun definition. For example, you
can map a PersistentVolumeClaim volume to a Workspace as follows:
workspaces:
  - name: myworkspace # must match workspace name in the Task
    persistentVolumeClaim:
      claimName: mypvc # this PVC must already exist
    subPath: my-subdir
For more information, see the following topics:

For information on mapping Workspaces to Volumes, see Using Workspace variables in TaskRuns.
For a list of supported Volume types, see Specifying VolumeSources in Workspaces.
For an end-to-end example, see Workspaces in a TaskRun.

Propagated Workspaces
When using an embedded spec, workspaces from the parent TaskRun will be
propagated to any inlined specs without needing to be explicitly defined. This
allows authors to simplify specs by automatically propagating top-level
workspaces down to other inlined resources.
Workspace substutions will only be made for commands, args and script fields of steps, stepTemplates, and sidecars.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  generateName: propagating-workspaces-
spec:
  taskSpec:
    steps:
      - name: simple-step
        image: ubuntu
        command:
          - echo
        args:
          - $(workspaces.tr-workspace.path)
  workspaces:
  - emptyDir: {}
    name: tr-workspace
Upon execution, the workspaces will be interpolated during resolution through to the taskSpec.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: propagating-workspaces-ndxnc
  ...
spec:
  ...
status:
  ...
  taskSpec:
    steps:
      ...
    workspaces:
    - name: tr-workspace
Propagating Workspaces to Referenced Tasks
Workspaces can only be propagated to embedded task specs, not referenced Tasks.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: workspace-propagation
spec:
  steps:
    - name: simple-step
      image: ubuntu
      command:
        - echo
      args:
        - $(workspaces.tr-workspace.path)
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  generateName: propagating-workspaces-
spec:
  taskRef:
    name: workspace-propagation
  workspaces:
  - emptyDir: {}
    name: tr-workspace
Upon execution, the above TaskRun will fail because the Task is referenced and workspace is not propagated. It must be explicitly defined in the spec of the defined Task.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  ...
spec:
  taskRef:
    kind: Task
    name: workspace-propagation
  workspaces:
  - emptyDir: {}
    name: tr-workspace
status:
  conditions:
  - lastTransitionTime: "2022-09-13T15:12:35Z"
    message: workspace binding "tr-workspace" does not match any declared workspace
    reason: TaskRunValidationFailed
    status: "False"
    type: Succeeded
  ...
Specifying Sidecars
A Sidecar is a container that runs alongside the containers specified
in the Steps of a task to provide auxiliary support to the execution of
those Steps. For example, a Sidecar can run a logging daemon, a service
that updates files on a shared volume, or a network proxy.
Tekton supports the injection of Sidecars into a Pod belonging to
a TaskRun with the condition that each Sidecar running inside the
Pod are terminated as soon as all Steps in the Task complete execution.
This might result in the Pod including each affected Sidecar with a
retry count of 1 and a different container image than expected.
We are aware of the following issues affecting Tektonâ€™s implementation of Sidecars:


The configured nop image must not provide the command that the
Sidecar is expected to run, otherwise it will not exit, resulting in the Sidecar
running forever and the Task eventually timing out. For more information, see the
associated issue.


The kubectl get pods command returns the status of the Pod as â€œCompletedâ€ if a
Sidecar exits successfully and as â€œErrorâ€ if a Sidecar exits with an error,
disregarding the exit codes of the container images that actually executed the Steps
inside the Pod. Only the above command is affected. The Pod's description correctly
denotes a â€œFailedâ€ status and the container statuses correctly denote their exit codes
and reasons.


Configuring Task Steps and Sidecars in a TaskRun
(beta only)
A TaskRun can specify StepSpecs or SidecarSpecs to configure Step or Sidecar
specified in a Task. Only named Steps and Sidecars may be configured.
For example, given the following Task definition:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: image-build-task
spec:
  steps:
    - name: build
      image: gcr.io/kaniko-project/executor:latest
  sidecars:
    - name: logging
      image: my-logging-image
An example TaskRun definition could look like:












  










  
  








  










  
  






      v1
    

    
      
    
      v1beta1
    

    
      
    





        apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: image-build-taskrun
spec:
  taskRef:
    name: image-build-task
  stepSpecs:
    - name: build
      computeResources:
        requests:
          memory: 1Gi
  sidecarSpecs:
    - name: logging
      computeResources:
        requests:
          cpu: 100m
        limits:
          cpu: 500m


    
      
    

  
        apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: image-build-taskrun
spec:
  taskRef:
    name: image-build-task
  stepOverrides:
    - name: build
      resources:
        requests:
          memory: 1Gi
  sidecarOverrides:
    - name: logging
      resources:
        requests:
          cpu: 100m
        limits:
          cpu: 500m


    
      
    

  


StepSpecs and SidecarSpecs must include the name field and may include resources.
No other fields can be overridden.
If the overridden Task uses a StepTemplate, configuration on
Step will take precedence over configuration in StepTemplate, and configuration in StepSpec will
take precedence over both.
When merging resource requirements, different resource types are considered independently.
For example, if a Step configures both CPU and memory, and a StepSpec configures only memory,
the CPU values from the Step will be preserved. Requests and limits are also considered independently.
For example, if a Step configures a memory request and limit, and a StepSpec configures only a
memory request, the memory limit from the Step will be preserved.
Specifying LimitRange values
In order to only consume the bare minimum amount of resources needed to execute one Step at a
time from the invoked Task, Tekton will request the compute values for CPU, memory, and ephemeral
storage for each Step based on the LimitRange
object(s), if present. Any Request or Limit specified by the user (on Task for example) will be left unchanged.
For more information, see the LimitRange support in Pipeline.
Specifying Retries
You can use the retries field to set how many times you want to retry on a failed TaskRun.
All TaskRun failures are retriable except for Cancellation.
For a retriable TaskRun, when an error occurs:

The error status is archived in status.RetriesStatus
The Succeeded condition in status is updated:

Type: Succeeded
Status: Unknown
Reason: ToBeRetried

status.StartTime, status.PodName and status.Results are unset to trigger another retry attempt.

Configuring the failure timeout
You can use the timeout field to set the TaskRun's desired timeout value for each retry attempt. If you do
not specify this value, the global default timeout value applies (the same, to each retry attempt). If you set the timeout to 0,
the TaskRun will have no timeout and will run until it completes successfully or fails from an error.
The timeout value is a duration conforming to Goâ€™s
ParseDuration format. For example, valid
values are 1h30m, 1h, 1m, 60s, and 0.
If a TaskRun runs longer than its timeout value, the pod associated with the TaskRun will be deleted. This
means that the logs of the TaskRun are not preserved. The deletion of the TaskRun pod is necessary in order to
stop TaskRun step containers from running.
The global default timeout is set to 60 minutes when you first install Tekton. You can set
a different global default timeout value using the default-timeout-minutes field in
config/config-defaults.yaml. If you set the global timeout to 0,
all TaskRuns that do not have a timeout set will have no timeout and will run until it completes successfully
or fails from an error.

:note: An internal detail of the PipelineRun and TaskRun reconcilers in the Tekton controller is that it will requeue a PipelineRun or TaskRun for re-evaluation, versus waiting for the next update, under certain conditions.  The wait time for that re-queueing is the elapsed time subtracted from the timeout; however, if the timeout is set to â€˜0â€™, that calculation produces a negative number, and the new reconciliation event will fire immediately, which can impact overall performance, which is counter to the intent of wait time calculation.  So instead, the reconcilers will use the configured global timeout as the wait time when the associated timeout has been set to â€˜0â€™.

Specifying ServiceAccount credentials
You can execute the Task in your TaskRun with a specific set of credentials by
specifying a ServiceAccount object name in the serviceAccountName field in your TaskRun
definition. If you do not explicitly specify this, the TaskRun executes with the credentials
specified in the configmap-defaults ConfigMap. If this default is not specified, TaskRuns
will execute with the default service account
set for the target namespace.
For more information, see ServiceAccount.
TaskRun status
The status field defines the observed state of TaskRun
The status field


Required:

status - The most relevant information about the TaskRunâ€™s state. This field includes:



status.conditions, which contains the latest observations of the TaskRunâ€™s state. See here for information on typical status properties.
podName - Name of the pod containing the containers responsible for executing this taskâ€™s steps.
startTime - The time at which the TaskRun began executing, conforms to RFC3339 format.
completionTime - The time at which the TaskRun finished executing, conforms to RFC3339 format.
taskSpec - TaskSpec defines the desired state of the Task executed via the TaskRun.



Optional:


results - List of results written out by the taskâ€™s containers.


provenance - Provenance contains metadata about resources used in the TaskRun such as the source from where a remote task definition was fetched. It carries minimum amount of metadata in TaskRun status so that Tekton Chains can utilize it for provenance, its two subfields are:

refSource: the source from where a remote Task definition was fetched.
featureFlags: Identifies the feature flags used during the TaskRun.



steps - Contains the state of each step container.

steps[].terminationReason - When the step is terminated, it stores the stepâ€™s final state.



retriesStatus - Contains the history of TaskRunâ€™s status in case of a retry in order to keep record of failures. No status stored within retriesStatus will have any date within as it is redundant.


sidecars - This field is a list. The list has one entry per sidecar in the manifest. Each entry represents the imageid of the corresponding sidecar.


spanContext - Contains tracing span context fields.




Monitoring execution status
As your TaskRun executes, its status field accumulates information on the execution of each Step
as well as the TaskRun as a whole. This information includes start and stop times, exit codes, the
fully-qualified name of the container image, and the corresponding digest.
Note: If any Pods have been OOMKilled
by Kubernetes, the TaskRun is marked as failed even if its exit code is 0.
The following example shows the status field of a TaskRun that has executed successfully:
completionTime: "2019-08-12T18:22:57Z"
conditions:
  - lastTransitionTime: "2019-08-12T18:22:57Z"
    message: All Steps have completed executing
    reason: Succeeded
    status: "True"
    type: Succeeded
podName: status-taskrun-pod
startTime: "2019-08-12T18:22:51Z"
steps:
  - container: step-hello
    imageID: docker-pullable://busybox@sha256:895ab622e92e18d6b461d671081757af7dbaa3b00e3e28e12505af7817f73649
    name: hello
    terminationReason: Completed
    terminated:
      containerID: docker://d5a54f5bbb8e7a6fd3bc7761b78410403244cf4c9c5822087fb0209bf59e3621
      exitCode: 0
      finishedAt: "2019-08-12T18:22:56Z"
      reason: Completed
      startedAt: "2019-08-12T18:22:54Z"
The following tables shows how to read the overall status of a TaskRun:



status
reason
message
completionTime is set
Description




Unknown
Started
n/a
No
The TaskRun has just been picked up by the controller.


Unknown
Pending
n/a
No
The TaskRun is waiting on a Pod in status Pending.


Unknown
Running
n/a
No
The TaskRun has been validated and started to perform its work.


Unknown
TaskRunCancelled
n/a
No
The user requested the TaskRun to be cancelled. Cancellation has not been done yet.


True
Succeeded
n/a
Yes
The TaskRun completed successfully.


False
Failed
n/a
Yes
The TaskRun failed because one of the steps failed.


False
[Error message]
n/a
No
The TaskRun encountered a non-permanent error, and itâ€™s still running. It may ultimately succeed.


False
[Error message]
n/a
Yes
The TaskRun failed with a permanent error (usually validation).


False
TaskRunCancelled
n/a
Yes
The TaskRun was cancelled successfully.


False
TaskRunCancelled
TaskRun cancelled as the PipelineRun it belongs to has timed out.
Yes
The TaskRun was cancelled because the PipelineRun timed out.


False
TaskRunTimeout
n/a
Yes
The TaskRun timed out.


False
TaskRunImagePullFailed
n/a
Yes
The TaskRun failed due to one of its steps not being able to pull the image.


False
FailureIgnored
n/a
Yes
The TaskRun failed but the failure was ignored.



When a TaskRun changes status, events are triggered accordingly.
The name of the Pod owned by a TaskRun  is univocally associated to the owning resource.
If a TaskRun resource is deleted and created with the same name, the child Pod will be created with the same name
as before. The base format of the name is <taskrun-name>-pod. The name may vary according to the logic of
kmeta.ChildName. In case of retries of a TaskRun
triggered by the PipelineRun controller, the base format of the name is <taskrun-name>-pod-retry<N> starting from
the first retry.
Some examples:



TaskRun Name
Pod Name




task-run
task-run-pod


task-run-0123456789-0123456789-0123456789-0123456789-0123456789-0123456789
task-run-0123456789-01234560d38957287bb0283c59440df14069f59-pod



Monitoring Steps
If multiple Steps are defined in the Task invoked by the TaskRun, you can monitor their execution
status in the status.steps field using the following command, where <name> is the name of the target
TaskRun:
kubectl get taskrun <name> -o yaml
The exact Task Spec used to instantiate the TaskRun is also included in the Status for full auditability.
Steps
The corresponding statuses appear in the status.steps list in the order in which the Steps have been
specified in the Task definition.
Monitoring Results
If one or more results fields have been specified in the invoked Task, the TaskRun's execution
status will include a Task Results section, in which the Results appear verbatim, including original
line returns and whitespace. For example:
Status:
  # [â€¦]
  Steps:
  # [â€¦]
  Task Results:
    Name:   current-date-human-readable
    Value:  Thu Jan 23 16:29:06 UTC 2020

    Name:   current-date-unix-timestamp
    Value:  1579796946
Cancelling a TaskRun
To cancel a TaskRun thatâ€™s currently executing, update its status to mark it as cancelled.
When you cancel a TaskRun, the running pod associated with that TaskRun is deleted. This
means that the logs of the TaskRun are not preserved. The deletion of the TaskRun pod is necessary
in order to stop TaskRun step containers from running.
Note: if keep-pod-on-cancel is set to
"true" in the feature-flags,  the pod associated with that TaskRun will not be deleted
Example of cancelling a TaskRun:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: go-example-git
spec:
  # [â€¦]
  status: "TaskRunCancelled"
Debugging a TaskRun
Breakpoint on Failure
TaskRuns can be halted on failure for troubleshooting by providing the following spec patch as seen below.
spec:
  debug:
    breakpoints:
      onFailure: "enabled"
Breakpoint before step
If you want to set a breakpoint before the step is executed, you can add the step name to the beforeSteps field in the following way:
spec:
  debug:
    breakpoints:
      beforeSteps: 
        - {{ stepName }}
Upon failure of a step, the TaskRun Pod execution is halted. If this TaskRun Pod continues to run without any lifecycle
change done by the user (running the debug-continue or debug-fail-continue script) the TaskRun would be subject to
TaskRunTimeout.
During this time, the user/client can get remote shell access to the step container with a command such as the following.
kubectl exec -it print-date-d7tj5-pod -c step-print-date-human-readable sh
Debug Environment
After the user/client has access to the container environment, they can scour for any missing parts because of which
their step might have failed.
To control the lifecycle of the step to mark it as a success or a failure or close the breakpoint, there are scripts
provided in the /tekton/debug/scripts directory in the container. The following are the scripts and the tasks they
perform :-
debug-continue: Mark the step as a success and exit the breakpoint.
debug-fail-continue: Mark the step as a failure and exit the breakpoint.
debug-beforestep-continue: Mark the step continue to execute
debug-beforestep-fail-continue: Mark the step not continue to execute
More information on the inner workings of debug can be found in the Debug documentation
Code examples
To better understand TaskRuns, study the following code examples:

Example TaskRun with a referenced Task
Example TaskRun with an embedded Task
Example of reusing a Task
Example of Using custom ServiceAccount credentials
Example of Running Step Containers as a Non Root User

Example TaskRun with a referenced Task
In this example, a TaskRun named read-repo-run invokes and executes an existing
Task named read-task. This Task reads the repository from the
â€œinputâ€ workspace.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: read-task
spec:
  workspaces:
  - name: input
  steps:
    - name: readme
      image: ubuntu
      script: cat $(workspaces.input.path)/README.md
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: read-repo-run
spec:
  taskRef:
    name: read-task
  workspaces:
  - name: input
    persistentVolumeClaim:
      claimName: mypvc
    subPath: my-subdir
Example TaskRun with an embedded Task
In this example, a TaskRun named build-push-task-run-2 directly executes
a Task from its definition embedded in the TaskRun's taskSpec field:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: build-push-task-run-2
spec:
  workspaces:
  - name: source
    persistentVolumeClaim:
      claimName: my-pvc
  taskSpec:
    workspaces:
    - name: source
    steps:
      - name: build-and-push
        image: gcr.io/kaniko-project/executor:v0.17.1
        workingDir: $(workspaces.source.path)
        # specifying DOCKER_CONFIG is required to allow kaniko to detect docker credential
        env:
          - name: "DOCKER_CONFIG"
            value: "/tekton/home/.docker/"
        command:
          - /kaniko/executor
        args:
          - --destination=gcr.io/my-project/gohelloworld
Example of Using custom ServiceAccount credentials
The example below illustrates how to specify a ServiceAccount to access a private git repository:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: test-task-with-serviceaccount-git-ssh
spec:
  serviceAccountName: test-task-robot-git-ssh
  workspaces:
  - name: source
    persistentVolumeClaim:
      claimName: repo-pvc
  - name: ssh-creds
    secret:
      secretName: test-git-ssh
  params:
    - name: url
      value: https://github.com/tektoncd/pipeline.git
  taskRef:
    name: git-clone
In the above code snippet, serviceAccountName: test-build-robot-git-ssh references the following
ServiceAccount:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-task-robot-git-ssh
secrets:
  - name: test-git-ssh
And secretName: test-git-ssh references the following Secret:
apiVersion: v1
kind: Secret
metadata:
  name: test-git-ssh
  annotations:
    tekton.dev/git-0: github.com
type: kubernetes.io/ssh-auth
data:
  # Generated by:
  # cat id_rsa | base64 -w 0
  ssh-privatekey: LS0tLS1CRUdJTiBSU0EgUFJJVk.....[example]
  # Generated by:
  # ssh-keyscan github.com | base64 -w 0
  known_hosts: Z2l0aHViLmNvbSBzc2g.....[example]
Example of Running Step Containers as a Non Root User
All steps that do not require to be run as a root user should make use of TaskRun features to
designate the container for a step runs as a user without root permissions. As a best practice,
running containers as non root should be built into the container image to avoid any possibility
of the container being run as root. However, as a further measure of enforcing this practice,
TaskRun pod templates can be used to specify how containers should be run within a TaskRun pod.
An example of using a TaskRun pod template is shown below to specify that containers running via this
TaskRunâ€™s pod should run as non root and run as user 1001 if the container itself does not specify what
user to run as:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: TaskRun
metadata:
  generateName: show-non-root-steps-run-
spec:
  taskRef:
    name: show-non-root-steps
  podTemplate:
    securityContext:
      runAsNonRoot: true
      runAsUser: 1001
If a Task step specifies that it is to run as a different user than what is specified in the pod template,
the stepâ€™s securityContext will be applied instead of what is specified at the pod level. An example of
this is available as a TaskRun example.
More information about Pod and Container Security Contexts can be found via the Kubernetes website.

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Pipelines

Pipelines

Overview
Configuring a Pipeline
Specifying Workspaces
Specifying Parameters
Adding Tasks to the Pipeline

Specifying Display Name
Specifying Remote Tasks
Specifying Pipelines in PipelineTasks
Specifying Parameters in PipelineTasks
Specifying Matrix in PipelineTasks
Specifying Workspaces in PipelineTasks
Tekton Bundles
Using the runAfter field
Using the retries field
Using the onError field
Produce results with OnError
Guard Task execution using when expressions

Guarding a Task and its dependent Tasks

Cascade when expressions to the specific dependent Tasks
Compose using Pipelines in Pipelines


Guarding a Task only


Configuring the failure timeout


Using variable substitution

Using the retries and retry-count variable substitutions


Using Results

Passing one Taskâ€™s Results into the Parameters or when expressions of another
Emitting Results from a Pipeline


Configuring the Task execution order
Adding a description
Adding Finally to the Pipeline

Specifying Display Name
Specifying Workspaces in finally tasks
Specifying Parameters in finally tasks
Specifying matrix in finally tasks
Consuming Task execution results in finally
Consuming Pipeline result with finally
PipelineRun Status with finally
Using Execution Status of pipelineTask
Using Aggregate Execution Status of All Tasks
Guard finally Task execution using when expressions

when expressions using Parameters in finally Tasks
when expressions using Results in finally â€˜Tasks`
when expressions using Execution Status of PipelineTask in finally tasks
when expressions using Aggregate Execution Status of Tasks in finally tasks


Known Limitations

Cannot configure the finally task execution order




Using Custom Tasks

Specifying the target Custom Task
Specifying a Custom Task Spec in-line (or embedded)
Specifying parameters
Specifying matrix
Specifying workspaces
Using Results
Specifying Timeout
Specifying Retries
Known Custom Tasks


Code examples



Overview
A Pipeline is a collection of Tasks that you define and arrange in a specific order
of execution as part of your continuous integration flow. Each Task in a Pipeline
executes as a Pod on your Kubernetes cluster. You can configure various execution
conditions to fit your business needs.
Configuring a Pipeline
A Pipeline definition supports the following fields:

Required:

apiVersion - Specifies the API version, for example
tekton.dev/v1beta1.
kind - Identifies this resource object as a Pipeline object.
metadata - Specifies metadata that uniquely identifies the
Pipeline object. For example, a name.
spec - Specifies the configuration information for
this Pipeline object. This must include:

tasks - Specifies the Tasks that comprise the Pipeline
and the details of their execution.




Optional:

params - Specifies the Parameters that the Pipeline requires.
workspaces - Specifies a set of Workspaces that the Pipeline requires.
tasks:

name - the name of this Task within the context of this Pipeline.
displayName - a user-facing name of this Task within the context of this Pipeline.
description - a description of this Task within the context of this Pipeline.
taskRef - a reference to a Task definition.
taskSpec - a specification of a Task.
runAfter - Indicates that a Task should execute after one or more other
Tasks without output linking.
retries - Specifies the number of times to retry the execution of a Task after
a failure. Does not apply to execution cancellations.
when - Specifies when expressions that guard
the execution of a Task; allow execution only when all when expressions evaluate to true.
timeout - Specifies the timeout before a Task fails.
params - Specifies the Parameters that a Task requires.
workspaces - Specifies the Workspaces that a Task requires.
matrix - Specifies the Parameters used to fan out a Task into
multiple TaskRuns or Runs.


results - Specifies the location to which the Pipeline emits its execution
results.
displayName - is a user-facing name of the pipeline that may be used to populate a UI.
description - Holds an informative description of the Pipeline object.
finally - Specifies one or more Tasks to be executed in parallel after
all other tasks have completed.

name - the name of this Task within the context of this Pipeline.
displayName - a user-facing name of this Task within the context of this Pipeline.
description - a description of this Task within the context of this Pipeline.
taskRef - a reference to a Task definition.
taskSpec - a specification of a Task.
retries - Specifies the number of times to retry the execution of a Task after
a failure. Does not apply to execution cancellations.
when - Specifies when expressions that guard
the execution of a Task; allow execution only when all when expressions evaluate to true.
timeout - Specifies the timeout before a Task fails.
params - Specifies the Parameters that a Task requires.
workspaces - Specifies the Workspaces that a Task requires.
matrix - Specifies the Parameters used to fan out a Task into
multiple TaskRuns or Runs.





Specifying Workspaces
Workspaces allow you to specify one or more volumes that each Task in the Pipeline
requires during execution. You specify one or more Workspaces in the workspaces field.
For example:
spec:
  workspaces:
    - name: pipeline-ws1 # The name of the workspace in the Pipeline
  tasks:
    - name: use-ws-from-pipeline
      taskRef:
        name: gen-code # gen-code expects a workspace with name "output"
      workspaces:
        - name: output
          workspace: pipeline-ws1
    - name: use-ws-again
      taskRef:
        name: commit # commit expects a workspace with name "src"
      runAfter:
        - use-ws-from-pipeline # important: use-ws-from-pipeline writes to the workspace first
      workspaces:
        - name: src
          workspace: pipeline-ws1
For simplicity you can also map the name of the Workspace in PipelineTask to match with
the Workspace from the Pipeline.
For example:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: pipeline
spec:
  workspaces:
    - name: source
  tasks:
    - name: gen-code
      taskRef:
        name: gen-code # gen-code expects a Workspace named "source"
      workspaces:
        - name: source # <- mapping workspace name
    - name: commit
      taskRef:
        name: commit # commit expects a Workspace named "source"
      workspaces:
        - name: source # <- mapping workspace name
      runAfter:
        - gen-code
For more information, see:

Using Workspaces in Pipelines
The Workspaces in a PipelineRun code example
The variables available in a PipelineRun, including workspaces.<name>.bound.
Mapping Workspaces

Specifying Parameters
(See also Specifying Parameters in Tasks)
You can specify global parameters, such as compilation flags or artifact names, that you want to supply
to the Pipeline at execution time. Parameters are passed to the Pipeline from its corresponding
PipelineRun and can replace template values specified within each Task in the Pipeline.
Parameter names:

Must only contain alphanumeric characters, hyphens (-), and underscores (_).
Must begin with a letter or an underscore (_).

For example, fooIs-Bar_ is a valid parameter name, but barIsBa$ or 0banana are not.
Each declared parameter has a type field, which can be set to either array or string.
array is useful in cases where the number of compilation flags being supplied to the Pipeline
varies throughout its execution. If no value is specified, the type field defaults to string.
When the actual parameter value is supplied, its parsed type is validated against the type field.
The description and default fields for a Parameter are optional.
The following example illustrates the use of Parameters in a Pipeline.
The following Pipeline declares two input parameters :

context which passes its value (a string) to the Task to set the value of the pathToContext parameter within the Task.
flags which passes its value (an array) to the Task to set the value of
the flags parameter within the Task. The flags parameter within the
Task must also be an array.
If you specify a value for the default field and invoke this Pipeline in a PipelineRun
without specifying a value for context, that value will be used.

Note: Input parameter values can be used as variables throughout the Pipeline
by using variable substitution.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: pipeline-with-parameters
spec:
  params:
    - name: context
      type: string
      description: Path to context
      default: /some/where/or/other
    - name: flags
      type: array
      description: List of flags
  tasks:
    - name: build-skaffold-web
      taskRef:
        name: build-push
      params:
        - name: pathToDockerFile
          value: Dockerfile
        - name: pathToContext
          value: "$(params.context)"
        - name: flags
          value: ["$(params.flags[*])"]
The following PipelineRun supplies a value for context:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pipelinerun-with-parameters
spec:
  pipelineRef:
    name: pipeline-with-parameters
  params:
    - name: "context"
      value: "/workspace/examples/microservices/leeroy-web"
    - name: "flags"
      value:
        - "foo"
        - "bar"
Param enum

ğŸŒ± enum is an alpha feature. The enable-param-enum feature flag must be set to "true" to enable this feature.

Parameter declarations can include enum which is a predefine set of valid values that can be accepted by the Pipeline Param. If a Param has both enum and default value, the default value must be in the enum set. For example, the valid/allowed values for Param â€œmessageâ€ is bounded to v1 and v2:
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: pipeline-param-enum
spec:
  params:
  - name: message
    enum: ["v1", "v2"]
    default: "v1"
  tasks:
  - name: task1
    params:
      - name: message
        value: $(params.message)
    steps:
    - name: build
      image: bash:3.2
      script: |
                echo "$(params.message)"
If the Param value passed in by PipelineRun is NOT in the predefined enum list, the PipelineRun will fail with reason InvalidParamValue.
If a PipelineTask references a Task with enum, the enums specified in the Pipeline spec.params (pipeline-level enum) must be
a subset of the enums specified in the referenced Task (task-level enum). An empty pipeline-level enum is invalid
in this scenario since an empty enum set indicates a â€œuniversal setâ€ which allows all possible values. The same rules apply to Pipelines with embbeded Tasks.
In the below example, the referenced Task accepts v1 and v2 as valid values, the Pipeline further restricts the valid value to v1.
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: param-enum-demo
spec:
  params:
  - name: message
    type: string
    enum: ["v1", "v2"]
  steps:
  - name: build
    image: bash:latest
    script: |
            echo "$(params.message)"
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: pipeline-param-enum
spec:
  params:
  - name: message
    enum: ["v1"]  # note that an empty enum set is invalid
  tasks:
  - name: task1
    params:
      - name: message
        value: $(params.message)
    taskRef:
      name: param-enum-demo
Note that this subset restriction only applies to the task-level params with a direct single reference to pipeline-level params. If a task-level param references multiple pipeline-level params, the subset validation is not applied.
apiVersion: tekton.dev/v1
kind: Pipeline
...
spec:
  params:
  - name: message1
    enum: ["v1"]
  - name: message2
    enum: ["v2"]
  tasks:
  - name: task1
    params:
      - name: message
        value: "$(params.message1) and $(params.message2)"
    taskSpec:
      params: message
      enum: [...] # the message enum is not required to be a subset of message1 or message2
    ...
Tekton validates user-provided values in a PipelineRun against the enum specified in the PipelineSpec.params. Tekton also validates
any resolved param value against the enum specified in each PipelineTask before creating the TaskRun.
See usage in this example
Propagated Params
Like with embedded pipelineruns, you can propagate params declared in the pipeline down to the inlined pipelineTasks and its inlined Steps. Wherever a resource (e.g. a pipelineTask) or a StepAction is referenced, the parameters need to be passed explicitly.
For example, the following is a valid yaml.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: pipelien-propagated-params
spec:
  params:
    - name: HELLO
      default: "Hello World!"
    - name: BYE
      default: "Bye World!"
  tasks:
    - name: echo-hello
      taskSpec:
        steps:
          - name: echo
            image: ubuntu
            script: |
              #!/usr/bin/env bash
              echo "$(params.HELLO)"              
    - name: echo-bye
      taskSpec:
        steps:
          - name: echo-action
            ref:
              name: step-action-echo
            params:
              - name: msg
                value: "$(params.BYE)" 
The same rules defined in pipelineruns apply here.
Adding Tasks to the Pipeline
Your Pipeline definition must reference at least one Task.
Each Task within a Pipeline must have a valid
name and a taskRef or a taskSpec. For example:
tasks:
  - name: build-the-image
    taskRef:
      name: build-push
Note: Using both apiVersion and kind will create CustomRun, donâ€™t set apiVersion if only referring to Task.
or
tasks:
  - name: say-hello
    taskSpec:
      steps:
      - image: ubuntu
        script: echo 'hello there'
Note that any task specified in taskSpec will be the same version as the Pipeline.
Specifying displayName in PipelineTasks
The displayName field is an optional field that allows you to add a user-facing name of the PipelineTask that can be
used to populate and distinguish in the dashboard. For example:
spec:
  tasks:
    - name: scan
      displayName: "Code Scan"
      taskRef:
        name: sonar-scan
The displayName also allows you to parameterize the human-readable name of your choice based on the
params, the task results,
and the context variables. For example:
spec:
  params:
    - name: application
  tasks:
    - name: scan
      displayName: "Code Scan for $(params.application)"
      taskRef:
        name: sonar-scan
    - name: upload-scan-report
      displayName: "Upload Scan Report $(tasks.scan.results.report)"
      taskRef:
        name: upload
Specifying task results in the displayName does not introduce an inherent resource dependency among tasks. The
pipeline author is responsible for specifying dependency explicitly either using runAfter
or rely on whenExpressions or task results in params.
Fully resolved displayName is also available in the status as part of the pipelineRun.status.childReferences. The
clients such as the dashboard, CLI, etc. can retrieve the displayName from the childReferences. The displayName mainly
drives a better user experience and at the same time it is not validated for the content or length by the controller.
Specifying Remote Tasks
(beta feature)
A taskRef field may specify a Task in a remote location such as git.
Support for specific types of remote will depend on the Resolvers your
clusterâ€™s operator has installed. For more information including a tutorial, please check resolution docs. The below example demonstrates referencing a Task in git:
tasks:
- name: "go-build"
  taskRef:
    resolver: git
    params:
    - name: url
      value: https://github.com/tektoncd/catalog.git
    - name: revision
      # value can use params declared at the pipeline level or a static value like main
      value: $(params.gitRevision)
    - name: pathInRepo
      value: task/golang-build/0.3/golang-build.yaml
Specifying Pipelines in PipelineTasks

ğŸŒ± Specifying pipelines in PipelineTasks is an alpha feature.
The enable-api-fields feature flag must be set to "alpha" to specify PipelineRef or PipelineSpec in a PipelineTask.
This feature is in Preview Only mode and not yet supported/implemented.

Apart from taskRef and taskSpec, pipelineRef and pipelineSpec allows you to specify a pipeline  in pipelineTask.
This allows you to generate a child pipelineRun which is inherited by the parent pipelineRun.
kind: Pipeline
metadata:
  name: security-scans
spec:
  tasks:
    - name: scorecards
      taskSpec:
        steps:
          - image: alpine
            name: step-1
            script: |
              echo "Generating scorecard report ..."
    - name: codeql
      taskSpec:
        steps:
          - image: alpine
            name: step-1
            script: |
              echo "Generating codeql report ..."
---
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: clone-scan-notify
spec:
  tasks:
    - name: git-clone
      taskSpec:
        steps:
          - image: alpine
            name: step-1
            script: |
              echo "Cloning a repo to run security scans ..."
    - name: security-scans
      runAfter:
        - git-clone
      pipelineRef:
        name: security-scans
---
For further information read Pipelines in Pipelines
Specifying Parameters in PipelineTasks
You can also provide Parameters:
spec:
  tasks:
    - name: build-skaffold-web
      taskRef:
        name: build-push
      params:
        - name: pathToDockerFile
          value: Dockerfile
        - name: pathToContext
          value: /workspace/examples/microservices/leeroy-web
Specifying Matrix in PipelineTasks

ğŸŒ± Matrix is an beta feature.
The enable-api-fields feature flag can be set to "beta" to specify Matrix in a PipelineTask.

You can also provide Parameters through the matrix field:
spec:
  tasks:
    - name: browser-test
      taskRef:
        name: browser-test
      matrix:
        params:
        - name: browser
          value:
          - chrome
          - safari
          - firefox
        include:
          - name: build-1
            params:
              - name: browser
                value: chrome
              - name: url
                value: some-url
For further information, read Matrix.
Specifying Workspaces in PipelineTasks
You can also provide Workspaces:
spec:
  tasks:
    - name: use-workspace
      taskRef:
        name: gen-code # gen-code expects a workspace with name "output"
      workspaces:
        - name: output
          workspace: shared-ws
Tekton Bundles
A Tekton Bundle is an OCI artifact that contains Tekton resources like Tasks which can be referenced within a taskRef.
There is currently a hard limit of 20 objects in a bundle.
You can reference a Tekton bundle in a TaskRef in both v1 and v1beta1 using remote resolution. The example syntax shown below for v1 uses remote resolution and requires enabling beta features.
spec:
  tasks:
    - name: hello-world
      taskRef:
        resolver: bundles
        params:
        - name: bundle
          value: docker.io/myrepo/mycatalog
        - name: name
          value: echo-task
        - name: kind
          value: Task
You may also specify a tag as you would with a Docker image which will give you a fixed,
repeatable reference to a Task.
spec:
  taskRef:
    resolver: bundles
    params:
    - name: bundle
      value: docker.io/myrepo/mycatalog:v1.0.1
    - name: name
      value: echo-task
    - name: kind
      value: Task
You may also specify a fixed digest instead of a tag.
spec:
  taskRef:
    resolver: bundles
    params:
    - name: bundle
      value: docker.io/myrepo/mycatalog@sha256:abc123
    - name: name
      value: echo-task
    - name: kind
      value: Task
Any of the above options will fetch the image using the ImagePullSecrets attached to the
ServiceAccount specified in the PipelineRun.
See the Service Account section
for details on how to configure a ServiceAccount on a PipelineRun. The PipelineRun will then
run that Task without registering it in the cluster allowing multiple versions of the same named
Task to be run at once.
Tekton Bundles may be constructed with any toolsets that produce valid OCI image artifacts
so long as the artifact adheres to the contract.
Using the runAfter field
If you need your Tasks to execute in a specific order within the Pipeline,
use the runAfter field to indicate that a Task must execute after
one or more other Tasks.
In the example below, we want to test the code before we build it. Since there
is no output from the test-app Task, the build-app Task uses runAfter
to indicate that test-app must run before it, regardless of the order in which
they are referenced in the Pipeline definition.
workspaces:
- name: source
tasks:
- name: test-app
  taskRef:
    name: make-test
  workspaces:
  - name: source
    workspace: source
- name: build-app
  taskRef:
    name: kaniko-build
  runAfter:
    - test-app
  workspaces:
  - name: source
    workspace: source
Using the retries field
For each Task in the Pipeline, you can specify the number of times Tekton
should retry its execution when it fails. When a Task fails, the corresponding
TaskRun sets its Succeeded Condition to False. The retries field
instructs Tekton to retry executing the Task when this happens. retries are executed
even when other Tasks in the Pipeline have failed, unless the PipelineRun has
been cancelled or
gracefully cancelled.
If you expect a Task to encounter problems during execution (for example,
you know that there will be issues with network connectivity or missing
dependencies), set its retries field to a suitable value greater than 0.
If you donâ€™t explicitly specify a value, Tekton does not attempt to execute
the failed Task again.
In the example below, the execution of the build-the-image Task will be
retried once after a failure; if the retried execution fails, too, the Task
execution fails as a whole.
tasks:
  - name: build-the-image
    retries: 1
    taskRef:
      name: build-push
Using the onError field
When a PipelineTask fails, the rest of the PipelineTasks are skipped and the PipelineRun is declared a failure. If you would like to
ignore such PipelineTask failure and continue executing the rest of the PipelineTasks, you can specify onError for such a PipelineTask.
OnError can be set to stopAndFail (default) and continue. The failure of a PipelineTask with stopAndFail would stop and fail the whole PipelineRun.  A PipelineTask fails with continue does not fail the whole PipelineRun, and the rest of the PipelineTask will continue to execute.
To ignore a PipelineTask failure, set onError to continue:
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: demo
spec:
  tasks:
    - name: task1
      onError: continue
      taskSpec:
        steps:
          - name: step1
            image: alpine
            script: |
                            exit 1
At runtime, the failure is ignored to determine the PipelineRun status. The PipelineRun message contains the ignored failure info:
status:
  conditions:
  - lastTransitionTime: "2023-09-28T19:08:30Z"
    message: 'Tasks Completed: 1 (Failed: 1 (Ignored: 1), Cancelled 0), Skipped: 0'
    reason: Succeeded
    status: "True"
    type: Succeeded
  ...
Note that the TaskRun status remains as it is irrelevant to OnError. Failed but ignored TaskRuns result in a failed status with reason
FailureIgnored.
For example, the TaskRun created by the above PipelineRun has the following status:
$ kubectl get tr demo-run-task1
NAME                                SUCCEEDED   REASON           STARTTIME   COMPLETIONTIME
demo-run-task1                      False       FailureIgnored   12m         12m
To specify onError for a step, please see specifying onError for a step.
Note: Setting Retry and OnError:continue at the same time is NOT allowed.
Produce results with OnError
When a PipelineTask is set to ignore error and the PipelineTask is able to initialize a result before failing, the result is made available to the consumer PipelineTasks.
  tasks:
    - name: task1
      onError: continue
      taskSpec:
        results:
          - name: result1
        steps:
          - name: step1
            image: alpine
            script: |
              echo -n 123 | tee $(results.result1.path)
              exit 1              
The consumer PipelineTasks can access the result by referencing $(tasks.task1.results.result1).
If the result is NOT initialized before failing, and there is a PipelineTask consuming it:
  tasks:
    - name: task1
      onError: continue
      taskSpec:
        results:
          - name: result1
        steps:
          - name: step1
            image: alpine
            script: |
              exit 1
              echo -n 123 | tee $(results.result1.path)              

If the consuming PipelineTask has OnError:stopAndFail, the PipelineRun will fail with InvalidTaskResultReference.
If the consuming PipelineTask has OnError:continue, the consuming PipelineTask will be skipped with reason Results were missing,
and the PipelineRun will continue to execute.

Guard Task execution using when expressions
To run a Task only when certain conditions are met, it is possible to guard task execution using the when field. The when field allows you to list a series of references to when expressions.
The components of when expressions are input, operator and values:



Component
Description
Syntax




input
Input for the when expression, defaults to an empty string if not provided.
* Static values e.g. "ubuntu" * Variables (parameters or results) e.g. "$(params.image)" or "$(tasks.task1.results.image)" or "$(tasks.task1.results.array-results[1])"


operator
operator represents an inputâ€™s relationship to a set of values, a valid operator must be provided.
in or notin


values
An array of string values, the values array must be provided and has to be non-empty.
* An array param e.g. ["$(params.images[*])"] * An array result of a task ["$(tasks.task1.results.array-results[*])"] * values can contain static values e.g. "ubuntu" * values can contain variables (parameters or results) or a Workspacesâ€™s bound state e.g. ["$(params.image)"] or ["$(tasks.task1.results.image)"] or ["$(tasks.task1.results.array-results[1])"]



The Parameters are read from the Pipeline and Results are read directly from previous Tasks. Using Results in a when expression in a guarded Task introduces a resource dependency on the previous Task that produced the Result.
The declared when expressions are evaluated before the Task is run. If all the when expressions evaluate to True, the Task is run. If any of the when expressions evaluate to False, the Task is not run and the Task is listed in the Skipped Tasks section of the PipelineRunStatus.
In these examples, first-create-file task will only be executed if the path parameter is README.md, echo-file-exists task will only be executed if the exists result from check-file task is yes and run-lint task will only be executed if the lint-config optional workspace has been provided by a PipelineRun.
tasks:
  - name: first-create-file
    when:
      - input: "$(params.path)"
        operator: in
        values: ["README.md"]
    taskRef:
      name: first-create-file
---
tasks:
  - name: echo-file-exists
    when:
      - input: "$(tasks.check-file.results.exists)"
        operator: in
        values: ["yes"]
    taskRef:
      name: echo-file-exists
---
tasks:
  - name: run-lint
    when:
      - input: "$(workspaces.lint-config.bound)"
        operator: in
        values: ["true"]
    taskRef:
      name: lint-source
---
tasks:
  - name: deploy-in-blue
    when:
      - input: "blue"
        operator: in
        values: ["$(params.deployments[*])"]
    taskRef:
      name: deployment
For an end-to-end example, see PipelineRun with when expressions.
There are a lot of scenarios where when expressions can be really useful. Some of these are:

Checking if the name of a git branch matches
Checking if the Result of a previous Task is as expected
Checking if a git file has changed in the previous commits
Checking if an image exists in the registry
Checking if the name of a CI job matches
Checking if an optional Workspace has been provided

Use CEL expression in WhenExpression

ğŸŒ± CEL in WhenExpression is an alpha feature.
The enable-cel-in-whenexpression feature flag must be set to "true" to enable the use of CEL in WhenExpression.

CEL (Common Expression Language) is a declarative language designed for simplicity, speed, safety, and portability which can be used to express a wide variety of conditions and computations.
You can define a CEL expression in WhenExpression to guard the execution of a Task.  The CEL expression must evaluate to either true or false. You can use a single line of CEL string to replace current WhenExpressionsâ€™s input+operator+values. For example:
# current WhenExpressions
when:
  - input: "foo"
    operator: "in"
    values: ["foo", "bar"]
  - input: "duh"
    operator: "notin"
    values: ["foo", "bar"]

# with cel
when:
  - cel: "'foo' in ['foo', 'bar']"
  - cel: "!('duh' in ['foo', 'bar'])"
CEL can offer more conditional functions, such as numeric comparisons (e.g. >, <=, etc), logic operators (e.g. OR, AND), Regex Pattern Matching. For example:
  when:
    # test coverage result is larger than 90%
    - cel: "'$(tasks.unit-test.results.test-coverage)' > 0.9"
    # params is not empty, or params2 is 8.5 or 8.6
    - cel: "'$(params.param1)' != '' || '$(params.param2)' == '8.5' || '$(params.param2)' == '8.6'"
    # param branch matches pattern `release/.*`
    - cel: "'$(params.branch)'.matches('release/.*')"
Variable substitution in CEL
CEL supports string substitutions, you can reference string, array indexing or object value of a param/result. For example:
  when:
    # string result
    - cel: "$(tasks.unit-test.results.test-coverage) > 0.9"
    # array indexing result
    - cel: "$(tasks.unit-test.results.test-coverage[0]) > 0.9"
    # object result key
    - cel: "'$(tasks.objectTask.results.repo.url)'.matches('github.com/tektoncd/.*')"
    # string param
    - cel: "'$(params.foo)' == 'foo'"
    # array indexing
    - cel: "'$(params.branch[0])' == 'foo'"
    # object param key
    - cel: "'$(params.repo.url)'.matches('github.com/tektoncd/.*')"
Note: the reference needs to be wrapped with single quotes.
Whole Array and Object replacements are not supported yet. The following usage is not supported:
  when:
    - cel: "'foo' in '$(params.array_params[*])'"
    - cel: "'foo' in '$(params.object_params[*])'"

In addition to the cases listed above, you can craft any valid CEL expression as defined by the cel-spec language definition
CEL expression is validated at admission webhook and a validation error will be returned if the expression is invalid.
Note: To use Tektonâ€™s variable substitution, you need to wrap the reference with single quotes. This also means that if you pass another CEL expression via params or results, it wonâ€™t be executed. Therefore CEL injection is disallowed.
For example:
This is valid: '$(params.foo)' == 'foo'
This is invalid: $(params.foo) == 'foo'
CEL's variable substitution is not supported yet and thus invalid: params.foo == 'foo'
Guarding a Task and its dependent Tasks
To guard a Task and its dependent Tasks:

cascade the when expressions to the specific dependent Tasks to be guarded as well
compose the Task and its dependent Tasks as a unit to be guarded and executed together using Pipelines in Pipelines

Cascade when expressions to the specific dependent Tasks
Pick and choose which specific dependent Tasks to guard as well, and cascade the when expressions to those Tasks.
Taking the use case below, a user who wants to guard manual-approval and its dependent Tasks:
                                     tests
                                       |
                                       v
                                 manual-approval
                                 |            |
                                 v        (approver)
                            build-image       |
                                |             v
                                v          slack-msg
                            deploy-image
The user can design the Pipeline to solve their use case as such:
tasks:
#...
- name: manual-approval
  runAfter:
    - tests
  when:
    - input: $(params.git-action)
      operator: in
      values:
        - merge
  taskRef:
    name: manual-approval

- name: build-image
  when:
    - input: $(params.git-action)
      operator: in
      values:
        - merge
  runAfter:
    - manual-approval
  taskRef:
    name: build-image

- name: deploy-image
  when:
    - input: $(params.git-action)
      operator: in
      values:
        - merge
  runAfter:
    - build-image
  taskRef:
    name: deploy-image

- name: slack-msg
  params:
    - name: approver
      value: $(tasks.manual-approval.results.approver)
  taskRef:
    name: slack-msg
Compose using Pipelines in Pipelines
Compose a set of Tasks as a unit of execution using Pipelines in Pipelines, which allows for guarding a Task and
its dependent Tasks (as a sub-Pipeline) using when expressions.
Note: Pipelines in Pipelines is an experimental feature
Taking the use case below, a user who wants to guard manual-approval and its dependent Tasks:
                                     tests
                                       |
                                       v
                                 manual-approval
                                 |            |
                                 v        (approver)
                            build-image       |
                                |             v
                                v          slack-msg
                            deploy-image
The user can design the Pipelines to solve their use case as such:
## sub pipeline (approve-build-deploy-slack)
tasks:
  - name: manual-approval
    runAfter:
      - integration-tests
    taskRef:
      name: manual-approval

  - name: build-image
    runAfter:
      - manual-approval
    taskRef:
      name: build-image

  - name: deploy-image
    runAfter:
      - build-image
    taskRef:
      name: deploy-image

  - name: slack-msg
    params:
      - name: approver
        value: $(tasks.manual-approval.results.approver)
    taskRef:
      name: slack-msg

---
## main pipeline
tasks:
#...
- name: approve-build-deploy-slack
  runAfter:
    - tests
  when:
    - input: $(params.git-action)
      operator: in
      values:
        - merge
  taskRef:
    apiVersion: tekton.dev/v1beta1
    kind: Pipeline
    name: approve-build-deploy-slack
Guarding a Task only
When when expressions evaluate to False, the Task will be skipped and:

The ordering-dependent Tasks will be executed
The resource-dependent Tasks (and their dependencies) will be skipped because of missing Results from the skipped
parent Task. When we add support for default Results, then the
resource-dependent Tasks may be executed if the default Results from the skipped parent Task are specified. In
addition, if a resource-dependent Task needs a file from a guarded parent Task in a shared Workspace, make sure
to handle the execution of the child Task in case the expected file is missing from the Workspace because the
guarded parent Task is skipped.

On the other hand, the rest of the Pipeline will continue executing.
                                     tests
                                       |
                                       v
                                 manual-approval
                                 |            |
                                 v        (approver)
                            build-image       |
                                |             v
                                v          slack-msg
                            deploy-image
Taking the use case above, a user who wants to guard manual-approval only can design the Pipeline as such:
tasks:
#...
- name: manual-approval
  runAfter:
    - tests
  when:
    - input: $(params.git-action)
      operator: in
      values:
        - merge
  taskRef:
    name: manual-approval

- name: build-image
  runAfter:
    - manual-approval
  taskRef:
    name: build-image

- name: deploy-image
  runAfter:
    - build-image
  taskRef:
    name: deploy-image

- name: slack-msg
  params:
    - name: approver
      value: $(tasks.manual-approval.results.approver)
  taskRef:
    name: slack-msg
If manual-approval is skipped, execution of its dependent Tasks (slack-msg, build-image and deploy-image)
would be unblocked regardless:

build-image and deploy-image should be executed successfully
slack-msg will be skipped because it is missing the approver Result from manual-approval

dependents of slack-msg would have been skipped too if it had any of them
if manual-approval specifies a default approver Result, such as â€œNoneâ€, then slack-msg would be executed
(supporting default Results is in progress)



Configuring the failure timeout
You can use the Timeout field in the Task spec within the Pipeline to set the timeout
of the TaskRun that executes that Task within the PipelineRun that executes your Pipeline.
The Timeout value is a duration conforming to Goâ€™s ParseDuration
format. For example, valid values are 1h30m, 1h, 1m, and 60s.
Note: If you do not specify a Timeout value, Tekton instead honors the timeout for the PipelineRun.
In the example below, the build-the-image Task is configured to time out after 90 seconds:
spec:
  tasks:
    - name: build-the-image
      taskRef:
        name: build-push
      timeout: "0h1m30s"
Using variable substitution
Tekton provides variables to inject values into the contents of certain fields.
The values you can inject come from a range of sources including other fields
in the Pipeline, context-sensitive information that Tekton provides, and runtime
information received from a PipelineRun.
The mechanism of variable substitution is quite simple - string replacement is
performed by the Tekton Controller when a PipelineRun is executed.
See the complete list of variable substitutions for Pipelines
and the list of fields that accept substitutions.
For an end-to-end example, see using context variables.
Using the retries and retry-count variable substitutions
Tekton supports variable substitution for the retries
parameter of PipelineTask. Variables like context.pipelineTask.retries and
context.task.retry-count can be added to the parameters of a PipelineTask.
context.pipelineTask.retries will be replaced by retries of the PipelineTask, while
context.task.retry-count will be replaced by current retry number of the PipelineTask.
params:
- name: pipelineTask-retries
  value: "$(context.pipelineTask.retries)"
taskSpec:
  params:
  - name: pipelineTask-retries
  steps:
  - image: ubuntu
    name: print-if-retries-exhausted
    script: |
      if [ "$(context.task.retry-count)" == "$(params.pipelineTask-retries)" ]
      then
        echo "This is the last retry."
      fi
      exit 1      
Note: Every PipelineTask can only access its own retries and retry-count. These
values arenâ€™t accessible for other PipelineTasks.
Using Results
Tasks can emit Results when they execute. A Pipeline can use these
Results for two different purposes:

A Pipeline can pass the Result of a Task into the Parameters or when expressions of another.
A Pipeline can itself emit Results and include data from the Results of its Tasks.


Note Tekton does not enforce that results are produced at Task level. If a pipeline attempts to
consume a result that was declared by a Task, but not produced, it will fail. TEP-0048
propopses introducing default values for results to help Pipeline authors manage this case.

Passing one Taskâ€™s Results into the Parameters or when expressions of another
Sharing Results between Tasks in a Pipeline happens via
variable substitution - one Task emits
a Result and another receives it as a Parameter with a variable such as
$(tasks.<task-name>.results.<result-name>). Pipeline support two new types of
results and parameters: array []string and object map[string]string.
Array result is a beta feature and can be enabled by setting enable-api-fields to alpha or beta.



Result Type
Parameter Type
Specification
enable-api-fields




string
string
$(tasks.<task-name>.results.<result-name>)
stable


array
array
$(tasks.<task-name>.results.<result-name>[*])
alpha or beta


array
string
$(tasks.<task-name>.results.<result-name>[i])
alpha or beta


object
object
$(tasks.<task-name>.results.<result-name>[*])
alpha or beta


object
string
$(tasks.<task-name>.results.<result-name>.key)
alpha or beta



Note: Whole Array and Object Results (using star notation) cannot be referred in script.
When one Task receives the Results of another, there is a dependency created between those
two Tasks. In order for the receiving Task to get data from another Task's Result,
the Task producing the Result must run first. Tekton enforces this Task ordering
by ensuring that the Task emitting the Result executes before any Task that uses it.
In the snippet below, a param is provided its value from the commit Result emitted by the
checkout-source Task. Tekton will make sure that the checkout-source Task runs
before this one.
params:
  - name: foo
    value: "$(tasks.checkout-source.results.commit)"
  - name: array-params
    value: "$(tasks.checkout-source.results.array-results[*])"
  - name: array-indexing-params
    value: "$(tasks.checkout-source.results.array-results[1])"
  - name: object-params
    value: "$(tasks.checkout-source.results.object-results[*])"
  - name: object-element-params
    value: "$(tasks.checkout-source.results.object-results.objectkey)"
Note: If checkout-source exits successfully without initializing commit Result,
the receiving Task fails and causes the Pipeline to fail with InvalidTaskResultReference:
unable to find result referenced by param 'foo' in 'task';: Could not find result with name 'commit' for task run 'checkout-source'
In the snippet below, a when expression is provided its value from the exists Result emitted by the
check-file Task. Tekton will make sure that the check-file Task runs before this one.
when:
  - input: "$(tasks.check-file.results.exists)"
    operator: in
    values: ["yes"]
For an end-to-end example, see Task Results in a PipelineRun.
Note that when expressions are whitespace-sensitive.  In particular, when producing results intended for inputs to when
expressions that may include newlines at their close (e.g. cat, jq), you may wish to truncate them.
taskSpec:
  params:
  - name: jsonQuery-check
  steps:
  - image: ubuntu
    name: store-name-in-results
    script: |
            curl -s https://my-json-server.typicode.com/typicode/demo/profile | jq -r .name | tr -d '\n' | tee $(results.name.path)
Emitting Results from a Pipeline
A Pipeline can emit Results of its own for a variety of reasons - an external
system may need to read them when the Pipeline is complete, they might summarise
the most important Results from the Pipeline's Tasks, or they might simply
be used to expose non-critical messages generated during the execution of the Pipeline.
A Pipeline's Results can be composed of one or many Task Results emitted during
the course of the Pipeline's execution. A Pipeline Result can refer to its Tasks'
Results using a variable of the form $(tasks.<task-name>.results.<result-name>).
After a Pipeline has executed the PipelineRun will be populated with the Results
emitted by the Pipeline. These will be written to the PipelineRun's
status.pipelineResults field.
In the example below, the Pipeline specifies a results entry with the name sum that
references the outputValue Result emitted by the calculate-sum Task.
results:
  - name: sum
    description: the sum of all three operands
    value: $(tasks.calculate-sum.results.outputValue)
For an end-to-end example, see Results in a PipelineRun.
In the example below, the Pipeline collects array and object results from Tasks.
    results:
      - name: array-results
        type: array
        description: whole array
        value: $(tasks.task1.results.array-results[*])
      - name: array-indexing-results
        type: string
        description: array element
        value: $(tasks.task1.results.array-results[1])
      - name: object-results
        type: object
        description: whole object
        value: $(tasks.task2.results.object-results[*])
      - name: object-element
        type: string
        description: object element
        value: $(tasks.task2.results.object-results.foo)
For an end-to-end example see Array and Object Results in a PipelineRun.
A Pipeline Result is not emitted if any of the following are true:

A PipelineTask referenced by the Pipeline Result failed. The PipelineRun will also
have failed.
A PipelineTask referenced by the Pipeline Result was skipped.
A PipelineTask referenced by the Pipeline Result didnâ€™t emit the referenced Task Result. This
should be considered a bug in the Task and may fail a PipelineTask in future.
The Pipeline Result uses a variable that doesnâ€™t point to an actual PipelineTask. This will
result in an InvalidTaskResultReference validation error during PipelineRun execution.
The Pipeline Result uses a variable that doesnâ€™t point to an actual result in a PipelineTask.
This will cause an InvalidTaskResultReference validation error during PipelineRun execution.

Note: Since a Pipeline Result can contain references to multiple Task Results, if any of those
Task Result references are invalid the entire Pipeline Result is not emitted.
Note: If a PipelineTask referenced by the Pipeline Result was skipped, the Pipeline Result will not be emitted and the PipelineRun will not fail due to a missing result.
Configuring the Task execution order
You can connect Tasks in a Pipeline so that they execute in a Directed Acyclic Graph (DAG).
Each Task in the Pipeline becomes a node on the graph that can be connected with an edge
so that one will run before another and the execution of the Pipeline progresses to completion
without getting stuck in an infinite loop.
This is done using:


resource dependencies:

results of one Task being passed into params or when expressions of
another



ordering dependencies:

runAfter clauses on the corresponding Tasks



For example, the Pipeline defined as follows
tasks:
- name: lint-repo
  taskRef:
    name: pylint
- name: test-app
  taskRef:
    name: make-test
- name: build-app
  taskRef:
    name: kaniko-build-app
  runAfter:
    - test-app
- name: build-frontend
  taskRef:
    name: kaniko-build-frontend
  runAfter:
    - test-app
- name: deploy-all
  taskRef:
    name: deploy-kubectl
  runAfter:
    - build-app
    - build-frontend
executes according to the following graph:
        |            |
        v            v
     test-app    lint-repo
    /        \
   v          v
build-app  build-frontend
   \          /
    v        v
    deploy-all
In particular:

The lint-repo and test-app Tasks have no runAfter clauses
and start executing simultaneously.
Once test-app completes, both build-app and build-frontend start
executing simultaneously since they both runAfter the test-app Task.
The deploy-all Task executes once both build-app and build-frontend
complete, since it is supposed to runAfter them both.
The entire Pipeline completes execution once both lint-repo and deploy-all
complete execution.

Specifying a display name
The displayName field is an optional field that allows you to add a user-facing name of the Pipeline that can be used to populate a UI. For example:
spec:
  displayName: "Code Scan"
  tasks:
    - name: scan
      taskRef:
        name: sonar-scan
Adding a description
The description field is an optional field and can be used to provide description of the Pipeline.
Adding Finally to the Pipeline
You can specify a list of one or more final tasks under finally section. finally tasks are guaranteed to be executed
in parallel after all PipelineTasks under tasks have completed regardless of success or error. finally tasks are very
similar to PipelineTasks under tasks section and follow the same syntax. Each finally task must have a
valid name and a taskRef or
taskSpec. For example:
spec:
  tasks:
    - name: tests
      taskRef:
        name: integration-test
  finally:
    - name: cleanup-test
      taskRef:
        name: cleanup
Specifying displayName in finally tasks
Similar to specifying displayName in pipelineTasks, finally tasks also
allows to add a user-facing name of the finally task that can be used to populate and distinguish in the dashboard.
For example:
spec:
  finally:
    - name: notification
      displayName: "Notify"
      taskRef:
        name: notification
    - name: notification-using-context-variable
      displayName: "Notification from $(context.pipeline.name)"
      taskRef:
        name: notification
The displayName also allows you to parameterize the human-readable name of your choice based on the
params, the task results,
and the context variables.
Fully resolved displayName is also available in the status as part of the pipelineRun.status.childReferences. The
clients such as the dashboard, CLI, etc. can retrieve the displayName from the childReferences. The displayName mainly
drives a better user experience and at the same time it is not validated for the content or length by the controller.
Specifying Workspaces in finally tasks
finally tasks can specify workspaces which PipelineTasks might have utilized
e.g. a mount point for credentials held in Secrets. To support that requirement, you can specify one or more
Workspaces in the workspaces field for the finally tasks similar to tasks.
spec:
  workspaces:
    - name: shared-workspace
  tasks:
    - name: clone-app-source
      taskRef:
        name: clone-app-repo-to-workspace
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
  finally:
    - name: cleanup-workspace
      taskRef:
        name: cleanup-workspace
      workspaces:
        - name: shared-workspace
          workspace: shared-workspace
Specifying Parameters in finally tasks
Similar to tasks, you can specify Parameters in finally tasks:
spec:
  tasks:
    - name: tests
      taskRef:
        name: integration-test
  finally:
    - name: report-results
      taskRef:
        name: report-results
      params:
        - name: url
          value: "someURL"
Specifying matrix in finally tasks

ğŸŒ± Matrix is an beta feature.
The enable-api-fields feature flag can be set to "beta" to specify Matrix in a PipelineTask.

Similar to tasks, you can also provide Parameters through matrix
in finally tasks:
spec:
  tasks:
    - name: tests
      taskRef:
        name: integration-test
  finally:
    - name: report-results
      taskRef:
        name: report-results
      params:
        - name: url
          value: "someURL"
      matrix:
        params:
        - name: slack-channel
          value:
          - "foo"
          - "bar"
        include:
          - name: build-1
            params:
              - name: slack-channel
                value: "foo"
              - name: flags
                value: "-v"
For further information, read Matrix.
Consuming Task execution results in finally
finally tasks can be configured to consume Results of PipelineTask from the tasks section:
spec:
  tasks:
    - name: clone-app-repo
      taskRef:
        name: git-clone
  finally:
    - name: discover-git-commit
      params:
        - name: commit
          value: $(tasks.clone-app-repo.results.commit)
Note: The scheduling of such finally task does not change, it will still be executed in parallel with other
finally tasks after all non-finally tasks are done.
The controller resolves task results before executing the finally task discover-git-commit. If the task
clone-app-repo failed before initializing commit or skipped with when expression
resulting in uninitialized task result commit, the finally Task discover-git-commit will be included in the list of
skippedTasks and continues executing rest of the finally tasks. The pipeline exits with completion instead of
success if a finally task is added to the list of skippedTasks.
Consuming Pipeline result with finally
finally tasks can emit Results and these results emitted from the finally tasks can be configured in the
Pipeline Results. References of Results from finally will follow the same naming conventions as referencing Results from tasks: $(finally.<finally-pipelinetask-name>.result.<result-name>).
results:
  - name: comment-count-validate
    value: $(finally.check-count.results.comment-count-validate)
finally:
  - name: check-count
    taskRef:
      name: example-task-name
In this example, pipelineResults in status will show the name-value pair for the result comment-count-validate which is produced in the Task example-task-name.
PipelineRun Status with finally
With finally, PipelineRun status is calculated based on PipelineTasks under tasks section and finally tasks.
Without finally:



PipelineTasks under tasks
PipelineRun status
Reason




all PipelineTasks successful
true
Succeeded


one or more PipelineTasks skipped and rest successful
true
Completed


single failure of PipelineTask
false
failed



With finally:



PipelineTasks under tasks
finally tasks
PipelineRun status
Reason




all PipelineTask successful
all finally tasks successful
true
Succeeded


all PipelineTask successful
one or more failure of finally tasks
false
Failed


one or more PipelineTask skipped and rest successful
all finally tasks successful
true
Completed


one or more PipelineTask skipped and rest successful
one or more failure of finally tasks
false
Failed


single failure of PipelineTask
all finally tasks successful
false
failed


single failure of PipelineTask
one or more failure of finally tasks
false
failed



Overall, PipelineRun state transitioning is explained below for respective scenarios:

All PipelineTask and finally tasks are successful: Started -> Running -> Succeeded
At least one PipelineTask skipped and rest successful:  Started -> Running -> Completed
One PipelineTask failed / one or more finally tasks failed: Started -> Running -> Failed

Please refer to the table under Monitoring Execution Status to learn about
what kind of events are triggered based on the Pipelinerun status.
Using Execution Status of pipelineTask
A pipeline can check the status of a specific pipelineTask from the tasks section in finally through the task
parameters:
finally:
  - name: finaltask
    params:
      - name: task1Status
        value: "$(tasks.task1.status)"
    taskSpec:
      params:
        - name: task1Status
      steps:
        - image: ubuntu
          name: print-task-status
          script: |
            if [ $(params.task1Status) == "Failed" ]
            then
              echo "Task1 has failed, continue processing the failure"
            fi            
This kind of variable can have any one of the values from the following table:



Status
Description




Succeeded
taskRun for the pipelineTask completed successfully


Failed
taskRun for the pipelineTask completed with a failure or cancelled by the user


None
the pipelineTask has been skipped or no execution information available for the pipelineTask



For an end-to-end example, see status in a PipelineRun.
Using Aggregate Execution Status of All Tasks
A pipeline can check an aggregate status of all the tasks section in finally through the task parameters:
finally:
  - name: finaltask
    params:
      - name: aggregateTasksStatus
        value: "$(tasks.status)"
    taskSpec:
      params:
        - name: aggregateTasksStatus
      steps:
        - image: ubuntu
          name: check-task-status
          script: |
            if [ $(params.aggregateTasksStatus) == "Failed" ]
            then
              echo "Looks like one or more tasks returned failure, continue processing the failure"
            fi            
This kind of variable can have any one of the values from the following table:



Status
Description




Succeeded
all tasks have succeeded


Failed
one ore more tasks failed


Completed
all tasks completed successfully including one or more skipped tasks


None
no aggregate execution status available (i.e. none of the above), one or more tasks could be pending/running/cancelled/timedout



For an end-to-end example, see $(tasks.status) usage in a Pipeline.
Guard finally Task execution using when expressions
Similar to Tasks, finally Tasks can be guarded using when expressions
that operate on static inputs or variables. Like in Tasks, when expressions in finally Tasks can operate on
Parameters and Results. Unlike in Tasks, when expressions in finally tasks can also operate on the Execution Status of Tasks.
when expressions using Parameters in finally Tasks
when expressions in finally Tasks can utilize Parameters as demonstrated using golang-build
and send-to-channel-slack Catalog
Tasks:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pipelinerun-
spec:
  pipelineSpec:
    params:
      - name: enable-notifications
        type: string
        description: a boolean indicating whether the notifications should be sent
    tasks:
      - name: golang-build
        taskRef:
          name: golang-build
      # [â€¦]
    finally:
      - name: notify-build-failure # executed only when build task fails and notifications are enabled
        when:
          - input: $(tasks.golang-build.status)
            operator: in
            values: ["Failed"]
          - input: $(params.enable-notifications)
            operator: in
            values: ["true"]
        taskRef:
          name: send-to-slack-channel
      # [â€¦]
  params:
    - name: enable-notifications
      value: true
when expressions using Results in finally â€˜Tasks`
when expressions in finally tasks can utilize Results, as demonstrated using git-clone
and github-add-comment Catalog Tasks:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pipelinerun-
spec:
  pipelineSpec:
    tasks:
      - name: git-clone
        taskRef:
          name: git-clone
      - name: go-build
      # [â€¦]
    finally:
      - name: notify-commit-sha # executed only when commit sha is not the expected sha
        when:
          - input: $(tasks.git-clone.results.commit)
            operator: notin
            values: [$(params.expected-sha)]
        taskRef:
          name: github-add-comment
      # [â€¦]
  params:
    - name: expected-sha
      value: 54dd3984affab47f3018852e61a1a6f9946ecfa
If the when expressions in a finally task use Results from a skipped or failed non-finally Tasks, then the
finally task would also be skipped and be included in the list of Skipped Tasks in the Status, similarly to when using
Results in other parts of the finally task.
when expressions using Execution Status of PipelineTask in finally tasks
when expressions in finally tasks can utilize Execution Status of PipelineTasks,
as demonstrated using golang-build and
send-to-channel-slack Catalog Tasks:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pipelinerun-
spec:
  pipelineSpec:
    tasks:
      - name: golang-build
        taskRef:
          name: golang-build
      # [â€¦]
    finally:
      - name: notify-build-failure # executed only when build task fails
        when:
          - input: $(tasks.golang-build.status)
            operator: in
            values: ["Failed"]
        taskRef:
          name: send-to-slack-channel
      # [â€¦]
For an end-to-end example, see PipelineRun with when expressions.
when expressions using Aggregate Execution Status of Tasks in finally tasks
when expressions in finally tasks can utilize
Aggregate Execution Status of Tasks as demonstrated:
finally:
  - name: notify-any-failure # executed only when one or more tasks fail
    when:
      - input: $(tasks.status)
        operator: in
        values: ["Failed"]
    taskRef:
      name: notify-failure
For an end-to-end example, see PipelineRun with when expressions.
Known Limitations
Cannot configure the finally task execution order
Itâ€™s not possible to configure or modify the execution order of the finally tasks. Unlike Tasks in a Pipeline,
all finally tasks run simultaneously and start executing once all PipelineTasks under tasks have settled which means
no runAfter can be specified in finally tasks.
Using Custom Tasks
Custom Tasks have been promoted from v1alpha1 to v1beta1. Starting from v0.43.0 to v0.46.0, Pipeline Controller is able to create either v1alpha1 or v1beta1 Custom Task gated by a feature flag custom-task-version, defaulting to v1beta1. You can set custom-task-version to v1alpha1 or v1beta1 to control which version to create.
Starting from v0.47.0, feature flag custom-task-version is removed and only v1beta1 Custom Task will be supported. See the migration doc for details.
Custom Tasks
can implement behavior that doesnâ€™t correspond directly to running a workload in a Pod on the cluster.
For example, a custom task might execute some operation outside of the cluster and wait for its execution to complete.
A PipelineRun starts a custom task by creating a CustomRun instead of a TaskRun.
In order for a custom task to execute, there must be a custom task controller running on the cluster
that is responsible for watching and updating CustomRuns which reference their type.
Specifying the target Custom Task
To specify the custom task type you want to execute, the taskRef field
must include the custom taskâ€™s apiVersion and kind as shown below.
Using apiVersion will always create a CustomRun. If apiVersion is set, kind is required as well.
spec:
  tasks:
    - name: run-custom-task
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
This creates a Run/CustomRun of a custom task of type Example in the example.dev API group with the version v1alpha1.
Validation error will be returned if apiVersion or kind is missing.
You can also specify the name of a custom task resource object previously defined in the cluster.
spec:
  tasks:
    - name: run-custom-task
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
        name: myexample
If the taskRef specifies a name, the custom task controller should look up the
Example resource with that name and use that object to configure the execution.
If the taskRef does not specify a name, the custom task controller might support
some default behavior for executing unnamed tasks.
Specifying a Custom Task Spec in-line (or embedded)
For v1alpha1.Run
spec:
  tasks:
    - name: run-custom-task
      taskSpec:
        apiVersion: example.dev/v1alpha1
        kind: Example
        spec:
          field1: value1
          field2: value2
For v1beta1.CustomRun
spec:
  tasks:
    - name: run-custom-task
      taskSpec:
        apiVersion: example.dev/v1alpha1
        kind: Example
        customSpec:
          field1: value1
          field2: value2
If the custom task controller supports the in-line or embedded task spec, this will create a Run/CustomRun of a custom task of
type Example in the example.dev API group with the version v1alpha1.
If the taskSpec is not supported, the custom task controller should produce proper validation errors.
Please take a look at the
developer guide for custom controllers supporting taskSpec:

guidance for Run
guidance for CustomRun

taskSpec support for pipelineRun was designed and discussed in
TEP-0061
Specifying parameters
If a custom task supports parameters, you can use the
params field to specify their values:
spec:
  tasks:
    - name: run-custom-task
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
        name: myexample
      params:
        - name: foo
          value: bah
Context Variables
The Parameters in the Params field will accept
context variables that will be substituted, including:

PipelineRun name, namespace and uid
Pipeline name
PipelineTask retries

spec:
  tasks:
    - name: run-custom-task
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
        name: myexample
      params:
        - name: foo
          value: $(context.pipeline.name)
Specifying matrix

ğŸŒ± Matrix is an alpha feature.
The enable-api-fields feature flag must be set to "alpha" to specify Matrix in a PipelineTask.

If a custom task supports parameters, you can use the
matrix field to specify their values, if you want to fan:
spec:
  tasks:
    - name: run-custom-task
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
        name: myexample
      params:
        - name: foo
          value: bah
      matrix:
        params:
        - name: bar
          value:
            - qux
            - thud
        include:
          - name: build-1
            params:
            - name: common-package
              value: path-to-common-pkg
For further information, read Matrix.
Specifying workspaces
If the custom task supports it, you can provide Workspaces to share data with the custom task.
spec:
  tasks:
    - name: run-custom-task
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
        name: myexample
      workspaces:
        - name: my-workspace
Consult the documentation of the custom task that you are using to determine whether it supports workspaces and how to name them.
Using Results
If the custom task produces results, you can reference them in a Pipeline using the normal syntax,
$(tasks.<task-name>.results.<result-name>).
Specifying Timeout
v1alpha1.Run
If the custom task supports it as we recommended, you can provide timeout to specify the maximum running time of a CustomRun (including all retry attempts or other operations).
v1beta1.CustomRun
If the custom task supports it as we recommended, you can provide timeout to specify the maximum running time of one CustomRun execution.
spec:
  tasks:
    - name: run-custom-task
      timeout: 2s
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
        name: myexample
Consult the documentation of the custom task that you are using to determine whether it supports Timeout.
Specifying Retries
If the custom task supports it, you can provide retries to specify how many times you want to retry the custom task.
spec:
  tasks:
    - name: run-custom-task
      retries: 2
      taskRef:
        apiVersion: example.dev/v1alpha1
        kind: Example
        name: myexample
Consult the documentation of the custom task that you are using to determine whether it supports Retries.
Known Custom Tasks
We try to list as many known Custom Tasks as possible here so that users can easily find what they want. Please feel free to share the Custom Task you implemented in this table.
v1beta1.CustomRun



Custom Task
Description




Wait Task Beta
Waits a given amount of time before succeeding, specified by an input parameter named duration. Support timeout and retries.


Approvals
Pauses the execution of PipelineRuns and waits for manual approvals. Version 0.6.0 and up.



v1alpha1.Run



Custom Task
Description




Pipeline Loops
Runs a Pipeline in a loop with varying Parameter values.


Common Expression Language
Provides Common Expression Language support in Tekton Pipelines.


Wait
Waits a given amount of time, specified by a Parameter named â€œdurationâ€, before succeeding.


Approvals
Pauses the execution of PipelineRuns and waits for manual approvals. Version up to (and including) 0.5.0


Pipelines in Pipelines
Defines and executes a Pipeline in a Pipeline.


Task Group
Groups Tasks together as a Task.


Pipeline in a Pod
Runs Pipeline in a Pod.



Code examples
For a better understanding of Pipelines, study our code examples.

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	PipelineRuns


PipelineRuns

Overview
Configuring a PipelineRun

Specifying the target Pipeline

Tekton Bundles
Remote Pipelines


Specifying Task-level ComputeResources
Specifying Parameters

Propagated Parameters

Scope and Precedence
Default Values
Object Parameters




Specifying custom ServiceAccount credentials
Mapping ServiceAccount credentials to Tasks
Specifying a Pod template
Specifying taskRunSpecs
Specifying Workspaces

Propagated Workspaces

Referenced TaskRuns within Embedded PipelineRuns




Specifying LimitRange values
Configuring a failure timeout


PipelineRun status

The status field
Monitoring execution status
Marking off user errors


Cancelling a PipelineRun
Gracefully cancelling a PipelineRun
Gracefully stopping a PipelineRun
Pending PipelineRuns




Overview
A PipelineRun allows you to instantiate and execute a Pipeline on-cluster.
A Pipeline specifies one or more Tasks in the desired order of execution. A PipelineRun
executes the Tasks in the Pipeline in the order they are specified until all Tasks have
executed successfully or a failure occurs.
Note: A PipelineRun automatically creates corresponding TaskRuns for every
Task in your Pipeline.
The Status field tracks the current state of a PipelineRun, and can be used to monitor
progress.
This field contains the status of every TaskRun, as well as the full PipelineSpec used
to instantiate this PipelineRun, for full auditability.
Configuring a PipelineRun
A PipelineRun definition supports the following fields:

Required:

apiVersion - Specifies the API version. For example
tekton.dev/v1beta1.
kind - Indicates that this resource object is a PipelineRun object.
metadata - Specifies the metadata that uniquely identifies the
PipelineRun object. For example, a name.
spec - Specifies the configuration information for
this PipelineRun object.

pipelineRef or pipelineSpec - Specifies the target Pipeline.




Optional:

params - Specifies the desired execution parameters for the Pipeline.
serviceAccountName - Specifies a ServiceAccount
object that supplies specific execution credentials for the Pipeline.
status - Specifies options for cancelling a PipelineRun.
taskRunSpecs - Specifies a list of PipelineRunTaskSpec which allows for setting ServiceAccountName, Pod template, and Metadata for each task. This overrides the Pod template set for the entire Pipeline.
timeout - Specifies the timeout before the PipelineRun fails. timeout is deprecated and will eventually be removed, so consider using timeouts instead.
timeouts - Specifies the timeout before the PipelineRun fails. timeouts allows more granular timeout configuration, at the pipeline, tasks, and finally levels
podTemplate - Specifies a Pod template to use as the basis for the configuration of the Pod that executes each Task.
workspaces - Specifies a set of workspace bindings which must match the names of workspaces declared in the pipeline being used.



Specifying the target Pipeline
You must specify the target Pipeline that you want the PipelineRun to execute, either by referencing
an existing Pipeline definition, or embedding a Pipeline definition directly in the PipelineRun.
To specify the target Pipeline by reference, use the pipelineRef field:
spec:
  pipelineRef:
    name: mypipeline
To embed a Pipeline definition in the PipelineRun, use the pipelineSpec field:
spec:
  pipelineSpec:
    tasks:
      - name: task1
        taskRef:
          name: mytask
The Pipeline in the pipelineSpec example
example displays morning and evening greetings. Once you create and execute it, you can check the logs for its Pods:
kubectl logs $(kubectl get pods -o name | grep pipelinerun-echo-greetings-echo-good-morning)
Good Morning, Bob!

kubectl logs $(kubectl get pods -o name | grep pipelinerun-echo-greetings-echo-good-night)
Good Night, Bob!
You can also embed a Task definition the embedded Pipeline definition:
spec:
  pipelineSpec:
    tasks:
      - name: task1
        taskSpec:
          steps: ...
In the taskSpec in pipelineSpec example
itâ€™s Tasks all the way down!
You can also specify labels and annotations with taskSpec which are propagated to each taskRun and then to the
respective pods. These labels can be used to identify and filter pods for further actions (such as collecting pod metrics,
and cleaning up completed pod with certain labels, etc) even being part of one single Pipeline.
spec:
  pipelineSpec:
    tasks:
      - name: task1
        taskSpec:
          metadata:
            labels:
              pipeline-sdk-type: kfp
        # ...
      - name: task2
        taskSpec:
          metadata:
            labels:
              pipeline-sdk-type: tfx
        # ...
Tekton Bundles
A Tekton Bundle is an OCI artifact that contains Tekton resources like Tasks which can be referenced within a taskRef.
You can reference a Tekton bundle in a TaskRef in both v1 and v1beta1 using remote resolution. The example syntax shown below for v1 uses remote resolution and requires enabling beta features.
spec:
  pipelineRef:
    resolver: bundles
    params:
    - name: bundle
      value: docker.io/myrepo/mycatalog:v1.0
    - name: name
      value: mypipeline
    - name: kind
      value: Pipeline
The syntax and caveats are similar to using Tekton Bundles for  Task references
in Pipelines or TaskRuns.
Tekton Bundles may be constructed with any toolsets that produce valid OCI image artifacts
so long as the artifact adheres to the contract.
Remote Pipelines
(beta feature)
A pipelineRef field may specify a Pipeline in a remote location such as git.
Support for specific types of remote will depend on the Resolvers your
clusterâ€™s operator has installed. For more information including a tutorial, please check resolution docs. The below example demonstrates
referencing a Pipeline in git:
spec:
  pipelineRef:
    resolver: git
    params:
    - name: url
      value: https://github.com/tektoncd/catalog.git
    - name: revision
      value: abc123
    - name: pathInRepo
      value: /pipeline/buildpacks/0.1/buildpacks.yaml
Specifying Task-level ComputeResources
(alpha only)
Task-level compute resources can be configured in PipelineRun.TaskRunSpecs.ComputeResources or TaskRun.ComputeResources.
e.g.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: pipeline
spec:
  tasks:
    - name: task
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pipelinerun
spec:
  pipelineRef:
    name: pipeline
  taskRunSpecs:
    - pipelineTaskName: task
      computeResources:
        requests:
          cpu: 2
Further details and examples could be found in Compute Resources in Tekton.
Specifying Parameters
(See also Specifying Parameters in Tasks)
You can specify Parameters that you want to pass to the Pipeline during execution,
including different values of the same parameter for different Tasks in the Pipeline.
Note: You must specify all the Parameters that the Pipeline expects. Parameters
that have default values specified in Pipeline are not required to be provided by PipelineRun.
For example:
spec:
  params:
    - name: pl-param-x
      value: "100"
    - name: pl-param-y
      value: "500"
You can pass in extra Parameters if needed depending on your use cases. An example use
case is when your CI system autogenerates PipelineRuns and it has Parameters it wants to
provide to all PipelineRuns. Because you can pass in extra Parameters, you donâ€™t have to
go through the complexity of checking each Pipeline and providing only the required params.
Parameter Enums

ğŸŒ± enum is an alpha feature. The enable-param-enum feature flag must be set to "true" to enable this feature.

If a Parameter is guarded by Enum in the Pipeline, you can only provide Parameter values in the PipelineRun that are predefined in the Param.Enum in the Pipeline. The PipelineRun will fail with reason InvalidParamValue otherwise.
Tekton will also the validate the param values passed to any referenced Tasks (via taskRef) if Enum is specified for the Task. The PipelineRun will fail with reason InvalidParamValue if Enum validation is failed for any of the PipelineTask.
You can also specify Enum in an embedded Pipeline in a PipelineRun. The same Param validation will be executed in this scenario.
See more details in Param.Enum.
Propagated Parameters
When using an inlined spec, parameters from the parent PipelineRun will be
propagated to any inlined specs without needing to be explicitly defined. This
allows authors to simplify specs by automatically propagating top-level
parameters down to other inlined resources.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pr-echo-
spec:
  params:
    - name: HELLO
      value: "Hello World!"
    - name: BYE
      value: "Bye World!"
  pipelineSpec:
    tasks:
      - name: echo-hello
        taskSpec:
          steps:
            - name: echo
              image: ubuntu
              script: |
                #!/usr/bin/env bash
                echo "$(params.HELLO)"                
      - name: echo-bye
        taskSpec:
          steps:
            - name: echo
              image: ubuntu
              script: |
                #!/usr/bin/env bash
                echo "$(params.BYE)"                
On executing the pipeline run, the parameters will be interpolated during resolution.
The specifications are not mutated before storage and so it remains the same.
The status is updated.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pr-echo-szzs9
  ...
spec:
  params:
  - name: HELLO
    value: Hello World!
  - name: BYE
    value: Bye World!
  pipelineSpec:
    tasks:
    - name: echo-hello
      taskSpec:
        steps:
        - image: ubuntu
          name: echo
          script: |
            #!/usr/bin/env bash
            echo "$(params.HELLO)"            
    - name: echo-bye
      taskSpec:
        steps:
        - image: ubuntu
          name: echo
          script: |
            #!/usr/bin/env bash
            echo "$(params.BYE)"            
status:
  conditions:
  - lastTransitionTime: "2022-04-07T12:34:58Z"
    message: 'Tasks Completed: 2 (Failed: 0, Canceled 0), Skipped: 0'
    reason: Succeeded
    status: "True"
    type: Succeeded
  pipelineSpec:
    ...
  childReferences:
  - name: pr-echo-szzs9-echo-hello
    pipelineTaskName: echo-hello
    kind: TaskRun
  - name: pr-echo-szzs9-echo-bye
    pipelineTaskName: echo-bye
    kind: TaskRun
Scope and Precedence
When Parameters names conflict, the inner scope would take precedence as shown in this example:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pr-echo-
spec:
  params:
  - name: HELLO
    value: "Hello World!"
  - name: BYE
    value: "Bye World!"
  pipelineSpec:
    tasks:
      - name: echo-hello
        params:
        - name: HELLO
          value: "Sasa World!"
        taskSpec:
          params:
            - name: HELLO
              type: string
          steps:
            - name: echo
              image: ubuntu
              script: |
                #!/usr/bin/env bash
                echo "$(params.HELLO)"                
    ...
resolves to
# Successful execution of the above PipelineRun
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pr-echo-szzs9
  ...
spec:
  ...
status:
  conditions:
    - lastTransitionTime: "2022-04-07T12:34:58Z"
      message: 'Tasks Completed: 2 (Failed: 0, Canceled 0), Skipped: 0'
      reason: Succeeded
      status: "True"
      type: Succeeded
  ...
  childReferences:
  - name: pr-echo-szzs9-echo-hello
    pipelineTaskName: echo-hello
    kind: TaskRun
  ...
Default Values
When Parameter specifications have default values, the Parameter value provided at runtime would take precedence to give users control, as shown in this example:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pr-echo-
spec:
  params:
  - name: HELLO
    value: "Hello World!"
  - name: BYE
    value: "Bye World!"
  pipelineSpec:
    tasks:
      - name: echo-hello
        taskSpec:
          params:
          - name: HELLO
            type: string
            default: "Sasa World!"
          steps:
            - name: echo
              image: ubuntu
              script: |
                #!/usr/bin/env bash
                echo "$(params.HELLO)"                
    ...
resolves to
# Successful execution of the above PipelineRun
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pr-echo-szzs9
  ...
spec:
  ...
status:
  conditions:
    - lastTransitionTime: "2022-04-07T12:34:58Z"
      message: 'Tasks Completed: 2 (Failed: 0, Canceled 0), Skipped: 0'
      reason: Succeeded
      status: "True"
      type: Succeeded
  ...
  childReferences:
  - name: pr-echo-szzs9-echo-hello
    pipelineTaskName: echo-hello
    kind: TaskRun
  ...
Referenced Resources
When a PipelineRun definition has referenced specifications but does not explicitly pass Parameters, the PipelineRun will be created but the execution will fail because of missing Parameters.
# Invalid PipelineRun attempting to propagate Parameters to referenced Tasks
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pr-echo-
spec:
  params:
  - name: HELLO
    value: "Hello World!"
  - name: BYE
    value: "Bye World!"
  pipelineSpec:
    tasks:
      - name: echo-hello
        taskRef:
          name: echo-hello
      - name: echo-bye
        taskRef:
          name: echo-bye
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: echo-hello
spec:
  steps:
    - name: echo
      image: ubuntu
      script: |
        #!/usr/bin/env bash
        echo "$(params.HELLO)"        
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: echo-bye
spec:
  steps:
    - name: echo
      image: ubuntu
      script: |
        #!/usr/bin/env bash
        echo "$(params.BYE)"        
Fails as follows:
# Failed execution of the above PipelineRun
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pr-echo-24lmf
  ...
spec:
  params:
  - name: HELLO
    value: Hello World!
  - name: BYE
    value: Bye World!
  pipelineSpec:
    tasks:
    - name: echo-hello
      taskRef:
        kind: Task
        name: echo-hello
    - name: echo-bye
      taskRef:
        kind: Task
        name: echo-bye
status:
  conditions:
  - lastTransitionTime: "2022-04-07T20:24:51Z"
    message: 'invalid input params for task echo-hello: missing values for
              these params which have no default values: [HELLO]'
    reason: PipelineValidationFailed
    status: "False"
    type: Succeeded
  ...
Object Parameters
When using an inlined spec, object parameters from the parent PipelineRun will also be
propagated to any inlined specs without needing to be explicitly defined. This
allows authors to simplify specs by automatically propagating top-level
parameters down to other inlined resources.
When propagating object parameters, scope and precedence also holds as shown below.
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: pipelinerun-object-param-result
spec:
  params:
    - name: gitrepo
      value:
        url: abc.com
        commit: sha123
  pipelineSpec:
    tasks:
      - name: task1
        params:
          - name: gitrepo
            value:
              branch: main
              url: xyz.com
        taskSpec:
          steps:
            - name: write-result
              image: bash
              args: [
                "echo",
                "--url=$(params.gitrepo.url)",
                "--commit=$(params.gitrepo.commit)",
                "--branch=$(params.gitrepo.branch)",
              ]
resolves to
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pipelinerun-object-param-resultpxp59
  ...
spec:
  params:
  - name: gitrepo
    value:
      commit: sha123
      url: abc.com
  pipelineSpec:
    tasks:
    - name: task1
      params:
      - name: gitrepo
        value:
          branch: main
          url: xyz.com
      taskSpec:
        metadata: {}
        spec: null
        steps:
        - args:
          - echo
          - --url=$(params.gitrepo.url)
          - --commit=$(params.gitrepo.commit)
          - --branch=$(params.gitrepo.branch)
          image: bash
          name: write-result
status:
  completionTime: "2022-09-08T17:22:01Z"
  conditions:
  - lastTransitionTime: "2022-09-08T17:22:01Z"
    message: 'Tasks Completed: 1 (Failed: 0, Cancelled 0), Skipped: 0'
    reason: Succeeded
    status: "True"
    type: Succeeded
  pipelineSpec:
    tasks:
    - name: task1
      params:
      - name: gitrepo
        value:
          branch: main
          url: xyz.com
      taskSpec:
        metadata: {}
        spec: null
        steps:
        - args:
          - echo
          - --url=xyz.com
          - --commit=sha123
          - --branch=main
          image: bash
          name: write-result
  startTime: "2022-09-08T17:21:57Z"
  childReferences:
  - name: pipelinerun-object-param-resultpxp59-task1
    pipelineTaskName: task1
    kind: TaskRun
          ...
	taskSpec:
          steps:
          - args:
            - echo
            - --url=xyz.com
            - --commit=sha123
            - --branch=main
            image: bash
            name: write-result
Specifying custom ServiceAccount credentials
You can execute the Pipeline in your PipelineRun with a specific set of credentials by
specifying a ServiceAccount object name in the serviceAccountName field in your PipelineRun
definition. If you do not explicitly specify this, the TaskRuns created by your PipelineRun
will execute with the credentials specified in the configmap-defaults ConfigMap. If this
default is not specified, the TaskRuns will execute with the default service account
set for the target namespace.
For more information, see ServiceAccount.
Custom tasks may or may not use a service account name.
Consult the documentation of the custom task that you are using to determine whether it supports a service account name.
Mapping ServiceAccount credentials to Tasks
If you require more granularity in specifying execution credentials, use the taskRunSpecs[].taskServiceAccountName field to
map a specific serviceAccountName value to a specific Task in the Pipeline. This overrides the global
serviceAccountName you may have set for the Pipeline as described in the previous section.
For example, if you specify these mappings:












  










  
  








  










  
  






      v1
    

    
      
    
      v1beta1
    

    
      
    





        spec:
  taskRunTemplate:
    serviceAccountName: sa-1
  taskRunSpecs:
    - pipelineTaskName: build-task
      serviceAccountName: sa-for-build


    
      
    

  
        spec:
  serviceAccountName: sa-1
  taskRunSpecs:
    - pipelineTaskName: build-task
      taskServiceAccountName: sa-for-build


    
      
    

  


for this Pipeline:
kind: Pipeline
spec:
  tasks:
    - name: build-task
      taskRef:
        name: build-push
    - name: test-task
      taskRef:
        name: test
then test-task will execute using the sa-1 account while build-task will execute with sa-for-build.
Propagated Results
When using an embedded spec, Results from the parent PipelineRun will be
propagated to any inlined specs without needing to be explicitly defined. This
allows authors to simplify specs by automatically propagating top-level
results down to other inlined resources.
Result substitutions will only be made for name, commands, args, env and script fields of steps, sidecars.
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: uid-pipeline-run
spec:
  pipelineSpec:
    tasks:
    - name: add-uid
      taskSpec:
        results:
          - name: uid
            type: string
        steps:
          - name: add-uid
            image: busybox
            command: ["/bin/sh", "-c"]
            args:
              - echo "1001" | tee $(results.uid.path)
    - name: show-uid
      # params:
      #   - name: uid
      #     value: $(tasks.add-uid.results.uid)
      taskSpec:
        steps:
          - name: show-uid
            image: busybox
            command: ["/bin/sh", "-c"]
            args:
              - echo $(tasks.add-uid.results.uid)
              # - echo $(params.uid)
On executing the PipelineRun, the Results will be interpolated during resolution.
name:         uid-pipeline-run-show-uid
apiVersion:  tekton.dev/v1
kind:         TaskRun
metadata:
  ...
spec:
  taskSpec:
    steps:
      args:
        echo 1001
      command:
        - /bin/sh
        - -c
      image:  busybox
      name:   show-uid
status:
  completionTime:  2023-09-11T07:34:28Z
  conditions:
    lastTransitionTime:  2023-09-11T07:34:28Z
    message:               All Steps have completed executing
    reason:                Succeeded
    status:                True
    type:                  Succeeded
  podName:                uid-pipeline-run-show-uid-pod
  steps:
    container:  step-show-uid
    name:       show-uid
  taskSpec:
    steps:
      args:
        echo 1001
      command:
        /bin/sh
        -c
      computeResources:
      image:  busybox
      name:   show-uid
Specifying a Pod template
You can specify a Pod template configuration that will serve as the configuration starting
point for the Pod in which the container images specified in your Tasks will execute. This allows you to
customize the Pod configuration specifically for each TaskRun.
In the following example, the Task defines a volumeMount object named my-cache. The PipelineRun
provisions this object for the Task using a persistentVolumeClaim and executes it as user 1001.












  










  
  








  










  
  






      v1
    

    
      
    
      v1beta1
    

    
      
    





        apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: mytask
spec:
  steps:
    - name: writesomething
      image: ubuntu
      command: ["bash", "-c"]
      args: ["echo 'foo' > /my-cache/bar"]
      volumeMounts:
        - name: my-cache
          mountPath: /my-cache
---
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: mypipeline
spec:
  tasks:
    - name: task1
      taskRef:
        name: mytask
---
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: mypipelinerun
spec:
  pipelineRef:
    name: mypipeline
  taskRunTemplate:
    podTemplate:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
      volumes:
        - name: my-cache
          persistentVolumeClaim:
            claimName: my-volume-claim


    
      
    

  
        apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: mytask
spec:
  steps:
    - name: writesomething
      image: ubuntu
      command: ["bash", "-c"]
      args: ["echo 'foo' > /my-cache/bar"]
      volumeMounts:
        - name: my-cache
          mountPath: /my-cache
---
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: mypipeline
spec:
  tasks:
    - name: task1
      taskRef:
        name: mytask
---
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: mypipelinerun
spec:
  pipelineRef:
    name: mypipeline
  podTemplate:
    securityContext:
      runAsNonRoot: true
      runAsUser: 1001
    volumes:
      - name: my-cache
        persistentVolumeClaim:
          claimName: my-volume-claim


    
      
    

  


Custom tasks may or may not use a pod template.
Consult the documentation of the custom task that you are using to determine whether it supports a pod template.
Specifying taskRunSpecs
Specifies a list of PipelineTaskRunSpec which contains TaskServiceAccountName, TaskPodTemplate
and PipelineTaskName. Mapping the specs to the corresponding Task based upon the TaskName a PipelineTask
will run with the configured  TaskServiceAccountName and TaskPodTemplate overwriting the pipeline
wide ServiceAccountName  and podTemplate configuration,
for example:












  










  
  








  










  
  






      v1
    

    
      
    
      v1beta1
    

    
      
    





        spec:
  podTemplate:
    securityContext:
      runAsUser: 1000
      runAsGroup: 2000
      fsGroup: 3000
  taskRunSpecs:
    - pipelineTaskName: build-task
      serviceAccountName: sa-for-build
      podTemplate:
        nodeSelector:
          disktype: ssd


    
      
    

  
        spec:
  podTemplate:
    securityContext:
      runAsUser: 1000
      runAsGroup: 2000
      fsGroup: 3000
  taskRunSpecs:
    - pipelineTaskName: build-task
      taskServiceAccountName: sa-for-build
      taskPodTemplate:
        nodeSelector:
          disktype: ssd


    
      
    

  


If used with this Pipeline,  build-task will use the task specific PodTemplate (where nodeSelector has disktype equal to ssd)
along with securityContext from the pipelineRun.spec.podTemplate.
PipelineTaskRunSpec may also contain StepSpecs and SidecarSpecs; see
Overriding Task Steps and Sidecars for more information.
The optional annotations and labels can be added under a Metadata field as for a specific running context.
e.g.
Rendering needed secrets with Vault:
spec:
  pipelineRef:
    name: pipeline-name
  taskRunSpecs:
    - pipelineTaskName: task-name
      metadata:
        annotations:
          vault.hashicorp.com/agent-inject-secret-foo: "/path/to/foo"
          vault.hashicorp.com/role: role-name
Updating labels applied in a runtime context:
spec:
  pipelineRef:
    name: pipeline-name
  taskRunSpecs:
    - pipelineTaskName: task-name
      metadata:
        labels:
          app: cloudevent
If a metadata key is present in different levels, the value that will be used in the PipelineRun is determined using this precedence order: PipelineRun.spec.taskRunSpec.metadata > PipelineRun.metadata > Pipeline.spec.tasks.taskSpec.metadata.
Specifying Workspaces
If your Pipeline specifies one or more Workspaces, you must map those Workspaces to
the corresponding physical volumes in your PipelineRun definition. For example, you
can map a PersistentVolumeClaim volume to a Workspace as follows:
workspaces:
  - name: myworkspace # must match workspace name in Task
    persistentVolumeClaim:
      claimName: mypvc # this PVC must already exist
    subPath: my-subdir
workspaces[].subPath can be an absolute value or can reference pipelineRun context variables, such as,
$(context.pipelineRun.name) or $(context.pipelineRun.uid).
You can pass in extra Workspaces if needed depending on your use cases. An example use
case is when your CI system autogenerates PipelineRuns and it has Workspaces it wants to
provide to all PipelineRuns. Because you can pass in extra Workspaces, you donâ€™t have to
go through the complexity of checking each Pipeline and providing only the required Workspaces:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: pipeline
spec:
  tasks:
    - name: task
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: pipelinerun
spec:
  pipelineRef:
    name: pipeline
  workspaces:
    - name: unusedworkspace
      persistentVolumeClaim:
        claimName: mypvc
For more information, see the following topics:

For information on mapping Workspaces to Volumes, see Specifying Workspaces in PipelineRuns.
For a list of supported Volume types, see Specifying VolumeSources in Workspaces.
For an end-to-end example, see Workspaces in a PipelineRun.

Custom tasks may or may not use workspaces.
Consult the documentation of the custom task that you are using to determine whether it supports workspaces.
Propagated Workspaces
When using an embedded spec, workspaces from the parent PipelineRun will be
propagated to any inlined specs without needing to be explicitly defined. This
allows authors to simplify specs by automatically propagating top-level
workspaces down to other inlined resources.
Workspace substutions will only be made for commands, args and script fields of steps, stepTemplates, and sidecars.
# Inline specifications of a PipelineRun
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: recipe-time-
spec:
  workspaces:
    - name: shared-data
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 16Mi
          volumeMode: Filesystem
  pipelineSpec:
    #workspaces:
    #  - name: shared-data
    tasks:
    - name: fetch-secure-data
      # workspaces:
      #   - name: shared-data
      taskSpec:
        # workspaces:
        #   - name: shared-data
        steps:
        - name: fetch-and-write-secure
          image: ubuntu
          script: |
                        echo hi >> $(workspaces.shared-data.path)/recipe.txt
    - name: print-the-recipe
      # workspaces:
      #   - name: shared-data
      runAfter:
        - fetch-secure-data
      taskSpec:
        # workspaces:
        #   - name: shared-data
        steps:
        - name: print-secrets
          image: ubuntu
          script: cat $(workspaces.shared-data.path)/recipe.txt
On executing the pipeline run, the workspaces will be interpolated during resolution.
# Successful execution of the above PipelineRun
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: recipe-time-
  ...
spec:
  pipelineSpec:
  ...
status:
  completionTime: "2022-06-02T18:17:02Z"
  conditions:
  - lastTransitionTime: "2022-06-02T18:17:02Z"
    message: 'Tasks Completed: 2 (Failed: 0, Canceled 0), Skipped: 0'
    reason: Succeeded
    status: "True"
    type: Succeeded
  pipelineSpec:
    ...
  childReferences:
  - name: recipe-time-lslt9-fetch-secure-data
    pipelineTaskName: fetch-secure-data
    kind: TaskRun
  - name: recipe-time-lslt9-print-the-recipe
    pipelineTaskName: print-the-recipe
    kind: TaskRun
Workspace Referenced Resources
Workspaces cannot be propagated to referenced specifications. For example, the following Pipeline will fail when executed because the workspaces defined in the PipelineRun cannot be propagated to the referenced Pipeline.
# PipelineRun attempting to propagate Workspaces to referenced Tasks
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-task-storage
spec:
  resources:
    requests:
      storage: 16Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: fetch-and-print-recipe
spec:
  tasks:
  - name: fetch-the-recipe
    taskRef:
      name: fetch-secure-data
  - name: print-the-recipe
    taskRef:
      name: print-data
    runAfter:
      - fetch-the-recipe
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: recipe-time-
spec:
  pipelineRef:
    name: fetch-and-print-recipe
  workspaces:
  - name: shared-data
    persistentVolumeClaim:
      claimName: shared-task-storage
Upon execution, this will cause failures:
# Failed execution of the above PipelineRun

apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: recipe-time-
  ...
spec:
  pipelineRef:
    name: fetch-and-print-recipe
  workspaces:
  - name: shared-data
    persistentVolumeClaim:
      claimName: shared-task-storage
status:
  completionTime: "2022-06-02T19:02:58Z"
  conditions:
  - lastTransitionTime: "2022-06-02T19:02:58Z"
    message: 'Tasks Completed: 1 (Failed: 1, Canceled 0), Skipped: 1'
    reason: Failed
    status: "False"
    type: Succeeded
  pipelineSpec:
    ...
  childReferences:
  - name: recipe-time-v5scg-fetch-the-recipe
    pipelineTaskName: fetch-the-recipe
    kind: TaskRun
Referenced TaskRuns within Embedded PipelineRuns
As mentioned in the Workspace Referenced Resources, workspaces can only be propagated from PipelineRuns to embedded Pipeline specs, not Pipeline references. Similarly, workspaces can only be propagated from a Pipeline to embedded Task specs, not referenced Tasks. For example:
# PipelineRun attempting to propagate Workspaces to referenced Tasks
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: Task
metadata:
  name: fetch-secure-data
spec:
  workspaces: # If Referenced, Workspaces need to be explicitly declared
  - name: shared-data
  steps:
  - name: fetch-and-write
    image: ubuntu
    script: |
            echo $(workspaces.shared-data.path)
---
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: recipe-time-
spec:
  workspaces:
  - name: shared-data
    persistentVolumeClaim:
      claimName: shared-task-storage
  pipelineSpec:
    # workspaces: # Since this is embedded specs, Workspaces donâ€™t need to be declared
    #    ...
    tasks:
    - name: fetch-the-recipe
      workspaces: # If referencing resources, Workspaces need to be explicitly declared
      - name: shared-data
      taskRef: # Referencing a resource
        name: fetch-secure-data
    - name: print-the-recipe
      # workspaces: # Since this is embedded specs, Workspaces donâ€™t need to be declared
      #    ...
      taskSpec:
        # workspaces: # Since this is embedded specs, Workspaces donâ€™t need to be declared
        #    ...
        steps:
        - name: print-secrets
          image: ubuntu
          script: cat $(workspaces.shared-data.path)/recipe.txt
      runAfter:
        - fetch-the-recipe
The above pipelinerun successfully resolves to:
# Successful execution of the above PipelineRun
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: recipe-time-
  ...
spec:
  pipelineSpec:
    ...
  workspaces:
  - name: shared-data
    persistentVolumeClaim:
      claimName: shared-task-storage
status:
  completionTime: "2022-06-09T18:42:14Z"
  conditions:
  - lastTransitionTime: "2022-06-09T18:42:14Z"
    message: 'Tasks Completed: 2 (Failed: 0, Cancelled 0), Skipped: 0'
    reason: Succeeded
    status: "True"
    type: Succeeded
  pipelineSpec:
    ...
  childReferences:
  - name: recipe-time-pj6l7-fetch-the-recipe
    pipelineTaskName: fetch-the-recipe
    kind: TaskRun
  - name: recipe-time-pj6l7-print-the-recipe
    pipelineTaskName: print-the-recipe
    kind: TaskRun
Specifying LimitRange values
In order to only consume the bare minimum amount of resources needed to execute one Step at a
time from the invoked Task, Tekton will request the compute values for CPU, memory, and ephemeral
storage for each Step based on the LimitRange
object(s), if present. Any Request or Limit specified by the user (on Task for example) will be left unchanged.
For more information, see the LimitRange support in Pipeline.
Configuring a failure timeout
You can use the timeouts field to set the PipelineRun's desired timeout value in minutes.
There are three sub-fields:

pipeline: specifies the timeout for the entire PipelineRun. Defaults to to the global configurable default timeout of 60 minutes.
When timeouts.pipeline has elapsed, any running child TaskRuns will be canceled, regardless of whether they are normal Tasks
or finally Tasks, and the PipelineRun will fail.
tasks: specifies the timeout for the cumulative time taken by non-finally Tasks specified in pipeline.spec.tasks.
To specify a timeout for an individual Task, use pipeline.spec.tasks[].timeout.
When timeouts.tasks has elapsed, any running child TaskRuns will be canceled, finally Tasks will run if timeouts.finally is specified,
and the PipelineRun will fail.
finally: the timeout for the cumulative time taken by finally Tasks specified in pipeline.spec.finally.
(Since all finally Tasks run in parallel, this is functionally equivalent to the timeout for any finally Task.)
When timeouts.finally has elapsed, any running finally TaskRuns will be canceled,
and the PipelineRun will fail.

For example:
timeouts:
  pipeline: "0h0m60s"
  tasks: "0h0m40s"
  finally: "0h0m20s"
All three sub-fields are optional, and will be automatically processed according to the following constraint:

timeouts.pipeline >= timeouts.tasks + timeouts.finally

Each timeout field is a duration conforming to Goâ€™s
ParseDuration format. For example, valid
values are 1h30m, 1h, 1m, and 60s.
If any of the sub-fields are set to â€œ0â€, there is no timeout for that section of the PipelineRun,
meaning that it will run until it completes successfully or encounters an error.
To set timeouts.tasks or timeouts.finally to â€œ0â€, you must also set timeouts.pipeline to â€œ0â€.
The global default timeout is set to 60 minutes when you first install Tekton. You can set
a different global default timeout value using the default-timeout-minutes field in
config/config-defaults.yaml.
Example timeouts usages are as follows:
Combination 1: Set the timeout for the entire pipeline and reserve a portion of it for tasks.
kind: PipelineRun
spec:
  timeouts:
    pipeline: "0h4m0s"
    tasks: "0h1m0s"
Combination 2: Set the timeout for the entire pipeline and reserve a portion of it for finally.
kind: PipelineRun
spec:
  timeouts:
    pipeline: "0h4m0s"
    finally: "0h3m0s"
Combination 3: Set only a tasks timeout, with no timeout for the entire pipeline.
kind: PipelineRun
spec:
  timeouts:
    pipeline: "0"  # No timeout
    tasks: "0h3m0s"
Combination : Set only a finally timeout, with no timeout for the entire pipeline.
kind: PipelineRun
spec:
  timeouts:
    pipeline: "0"  # No timeout
    finally: "0h3m0s"
You can also use the Deprecated timeout field to set the PipelineRun's desired timeout value in minutes.
If you do not specify this value in the PipelineRun, the global default timeout value applies.
If you set the timeout to 0, the PipelineRun fails immediately upon encountering an error.

âš ï¸ ** timeout is deprecated and will be removed in future versions. Consider using timeouts instead.


:note: An internal detail of the PipelineRun and TaskRun reconcilers in the Tekton controller is that it will requeue a PipelineRun or TaskRun for re-evaluation, versus waiting for the next update, under certain conditions.  The wait time for that re-queueing is the elapsed time subtracted from the timeout; however, if the timeout is set to â€˜0â€™, that calculation produces a negative number, and the new reconciliation event will fire immediately, which can impact overall performance, which is counter to the intent of wait time calculation.  So instead, the reconcilers will use the configured global timeout as the wait time when the associated timeout has been set to â€˜0â€™.

PipelineRun status
The status field
Your PipelineRunâ€™s status field can contain the following fields:

Required:


status - Most relevant, status.conditions, which contains the latest observations of the PipelineRunâ€™s state. See here for information on typical status properties.
startTime - The time at which the PipelineRun began executing, in RFC3339 format.
completionTime - The time at which the PipelineRun finished executing, in RFC3339 format.
pipelineSpec - The exact PipelineSpec used when starting the PipelineRun.


Optional:

pipelineResults - Results emitted by this PipelineRun.
skippedTasks - A list of Tasks which were skipped when running this PipelineRun due to when expressions, including the when expressions applying to the skipped task.
childReferences - A list of references to each TaskRun or Run in this PipelineRun, which can be used to look up the status of the underlying TaskRun or Run. Each entry contains the following:

kind - Generally either TaskRun or Run.
apiVersion - The API version for the underlying TaskRun or Run.
whenExpressions - The list of when expressions guarding the execution of this task.


provenance - Metadata about the runtime configuration and the resources used in the PipelineRun. The data in the provenance field will be recorded into the build provenance by the provenance generator i.e. (Tekton Chains). Currently, there are 2 subfields:

refSource: the source from where a remote pipeline definition was fetched.
featureFlags: the configuration data of the feature-flags configmap.


finallyStartTime- The time at which the PipelineRunâ€™s finally Tasks, if any, began
executing, in RFC3339 format.



Monitoring execution status
As your PipelineRun executes, its status field accumulates information on the execution of each TaskRun
as well as the PipelineRun as a whole. This information includes the name of the pipeline Task associated
to a TaskRun, the complete status of the TaskRun and details
about whenExpressions that may be associated to a TaskRun.
The following example shows an extract from the status field of a PipelineRun that has executed successfully:
completionTime: "2020-05-04T02:19:14Z"
conditions:
  - lastTransitionTime: "2020-05-04T02:19:14Z"
    message: "Tasks Completed: 4, Skipped: 0"
    reason: Succeeded
    status: "True"
    type: Succeeded
startTime: "2020-05-04T02:00:11Z"
childReferences:
- name: triggers-release-nightly-frwmw-build
  pipelineTaskName: build
  kind: TaskRun
The following tables shows how to read the overall status of a PipelineRun.
Completion time is set once a PipelineRun reaches status True or False:



status
reason
completionTime is set
Description




Unknown
Started
No
The PipelineRun has just been picked up by the controller.


Unknown
Running
No
The PipelineRun has been validate and started to perform its work.


Unknown
Cancelled
No
The user requested the PipelineRun to be cancelled. Cancellation has not be done yet.


True
Succeeded
Yes
The PipelineRun completed successfully.


True
Completed
Yes
The PipelineRun completed successfully, one or more Tasks were skipped.


False
Failed
Yes
The PipelineRun failed because one of the TaskRuns failed.


False
[Error message]
Yes
The PipelineRun failed with a permanent error (usually validation).


False
Cancelled
Yes
The PipelineRun was cancelled successfully.


False
PipelineRunTimeout
Yes
The PipelineRun timed out.


False
CreateRunFailed
Yes
The PipelineRun create run resources failed.



When a PipelineRun changes status, events are triggered accordingly.
When a PipelineRun has Tasks that were skipped, the reason for skipping the task will be listed in the Skipped Tasks section of the status of the PipelineRun.
When a PipelineRun has Tasks with when expressions:

If the when expressions evaluate to true, the Task is executed then the TaskRun and its resolved when expressions will be listed in the Task Runs section of the status of the PipelineRun.
If the when expressions evaluate to false, the Task is skipped then its name and its resolved when expressions will be listed in the Skipped Tasks section of the status of the PipelineRun.

Conditions:
  Last Transition Time:  2020-08-27T15:07:34Z
  Message:               Tasks Completed: 1 (Failed: 0, Cancelled 0), Skipped: 1
  Reason:                Completed
  Status:                True
  Type:                  Succeeded
Skipped Tasks:
  Name:       skip-this-task
  Reason:     When Expressions evaluated to false
  When Expressions:
    Input:     foo
    Operator:  in
    Values:
      bar
    Input:     foo
    Operator:  notin
    Values:
      foo
ChildReferences:
- Name: pipelinerun-to-skip-task-run-this-task
  Pipeline Task Name:  run-this-task
  Kind: TaskRun
The name of the TaskRuns and Runs owned by a PipelineRun  are univocally associated to the owning resource.
If a PipelineRun resource is deleted and created with the same name, the child TaskRuns will be created with the
same name as before. The base format of the name is <pipelinerun-name>-<pipelinetask-name>. If the PipelineTask
has a Matrix, the name will have an int suffix with format <pipelinerun-name>-<pipelinetask-name>-<combination-id>.
The name may vary according the logic of kmeta.ChildName.
Some examples:



PipelineRun Name
PipelineTask Name
TaskRun Names




pipeline-run
task1
pipeline-run-task1


pipeline-run
task2-0123456789-0123456789-0123456789-0123456789-0123456789
pipeline-runee4a397d6eab67777d4e6f9991cd19e6-task2-0123456789-0


pipeline-run-0123456789-0123456789-0123456789-0123456789
task3
pipeline-run-0123456789-0123456789-0123456789-0123456789-task3


pipeline-run-0123456789-0123456789-0123456789-0123456789
task2-0123456789-0123456789-0123456789-0123456789-0123456789
pipeline-run-0123456789-012345607ad8c7aac5873cdfabe472a68996b5c


pipeline-run
task4 (with 2x2 Matrix)
pipeline-run-task1-0, pipeline-run-task1-2, pipeline-run-task1-3, pipeline-run-task1-4



Marking off user errors
A user error in Tekton is any mistake made by user, such as a syntax error when specifying pipelines, tasks. User errors can occur in various stages of the Tekton pipeline, from authoring the pipeline configuration to executing the pipelines. They are currently explicitly labeled in the Runâ€™s conditions message, for example:
# Failed PipelineRun with message labeled "[User error]"
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  ...
spec:
  ...
status:
  ...
  conditions:
  - lastTransitionTime: "2022-06-02T19:02:58Z"
    message: '[User error] PipelineRun default parameters is missing some parameters required by
      Pipeline pipelinerun-with-params''s parameters: pipelineRun missing parameters:
      [pl-param-x]'
    reason: 'ParameterMissing'
    status: "False"
    type: Succeeded
~/pipeline$ tkn pr list
NAME                      STARTED         DURATION   STATUS
pipelinerun-with-params   5 seconds ago   0s         Failed(ParameterMissing)
Cancelling a PipelineRun
To cancel a PipelineRun thatâ€™s currently executing, update its definition
to mark it as â€œCancelledâ€. When you do so, the spawned TaskRuns are also marked
as cancelled, all associated Pods are deleted, and their Retries are not executed.
Pending finally tasks are not scheduled.
For example:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: go-example-git
spec:
  # [â€¦]
  status: "Cancelled"
Gracefully cancelling a PipelineRun
To gracefully cancel a PipelineRun thatâ€™s currently executing, update its definition
to mark it as â€œCancelledRunFinallyâ€. When you do so, the spawned TaskRuns are also marked
as cancelled, all associated Pods are deleted, and their Retries are not executed.
finally tasks are scheduled normally.
For example:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: go-example-git
spec:
  # [â€¦]
  status: "CancelledRunFinally"
Gracefully stopping a PipelineRun
To gracefully stop a PipelineRun thatâ€™s currently executing, update its definition
to mark it as â€œStoppedRunFinallyâ€. When you do so, the spawned TaskRuns are completed normally,
including executing their retries, but no new non-finally task is scheduled. finally tasks are executed afterwards.
For example:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: go-example-git
spec:
  # [â€¦]
  status: "StoppedRunFinally"
Pending PipelineRuns
A PipelineRun can be created as a â€œpendingâ€ PipelineRun meaning that it will not actually be started until the pending status is cleared.
Note that a PipelineRun can only be marked â€œpendingâ€ before it has started, this setting is invalid after the PipelineRun has been started.
To mark a PipelineRun as pending, set .spec.status to PipelineRunPending when creating it:
apiVersion: tekton.dev/v1 # or tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: go-example-git
spec:
  # [â€¦]
  status: "PipelineRunPending"
To start the PipelineRun, clear the .spec.status field. Alternatively, update the value to Cancelled to cancel it.

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	CustomRuns

Overview
Configuring a CustomRun

Specifying the target Custom Task
Cancellation
Specifying Timeout
Specifying Retries
Specifying Parameters
Specifying Workspaces
Specifying Service Account


Monitoring execution status

Status Reporting
Monitoring Results


Code examples

Example CustomRun with a referenced custom task
Example CustomRun with an unnamed custom task
Example of specifying parameters



Overview
v1beta1.CustomRun has replaced v1alpha1.Run for executing Custom Tasks. Please refer to the migration doc for details
on updating v1alpha1.Run to v1beta1.CustomRun before upgrading to a release that does not support v1alpha1.Run.
A CustomRun allows you to instantiate and execute a Custom
Task,
which can be implemented by a custom task controller running on-cluster. Custom
Tasks can implement behavior thatâ€™s independent of how Tekton TaskRun is implemented.
In order for a CustomRun to actually execute, there must be a custom task
controller running on the cluster that is responsible for watching and updating
CustomRuns which reference their type. If no such controller is running, CustomRuns
will have no .status value and no further action will be taken.
Configuring a CustomRun
A CustomRun definition supports the following fields:

Required:

apiVersion - Specifies the API version. Currently,only
tekton.dev/v1beta1 is supported.
kind - Identifies this resource object as a CustomRun object.
metadata - Specifies the metadata that uniquely identifies the
CustomRun, such as a name.
spec - Specifies the configuration for the CustomRun.

customRef - Specifies the type and
(optionally) name of the custom task type to execute.
customSpec - Embed the custom task resource spec
directly in a CustomRun.




Optional:

timeout - specifies the maximum duration of a single execution
of a CustomRun.
retries - specifies the number of retries to execute upon CustomRunfailure.
params - Specifies the desired execution
parameters for the custom task.
serviceAccountName - Specifies a ServiceAccount
object for executing the CustomRun.
workspaces - Specifies the physical volumes to use for the
Workspaces required by a custom task.



Specifying the target Custom Task
A custom task resourceâ€™s CustomSpec may be directly embedded in the CustomRun or it may
be referred to by a CustomRef. But, not both at the same time.


Specifying the target Custom Task with customRef
Referring a custom task (i.e. CustomRef ) promotes reuse of custom task definitions.


Specifying the target Custom Task by embedding its spec
Embedding a custom task (i.e. CustomSpec ) helps in avoiding name collisions with other users within the same namespace.
Additionally, in a pipeline with multiple embedded custom tasks, the details of entire pipeline can be fetched in a
single API request.


Specifying the target Custom Task with customRef
To specify the custom task type you want to execute in your CustomRun, use the
customRef field as shown below:
apiVersion: tekton.dev/v1beta1
kind: CustomRun
metadata:
  name: my-example-run
spec:
  customRef:
    apiVersion: example.dev/v1beta1
    kind: MyCustomKind
  params:
  - name: duration
    value: 10s
When this CustomRun is created, the Custom Task controller responsible for
reconciling objects of kind â€œMyCustomKindâ€ in the â€œexample.dev/v1beta1â€ api group
will execute it based on the input params.
You can also specify the name and optional namespace (default is default)
of a custom task resource object previously defined in the cluster.
apiVersion: tekton.dev/v1beta1
kind: CustomRun
metadata:
  name: my-example-run
spec:
  customRef:
    apiVersion: example.dev/v1beta1
    kind: Example
    name: an-existing-example-task
If the customRef specifies a name, the custom task controller should look up the
Example resource with that name, and use that object to configure the
execution.
If the customRef does not specify a name, the custom task controller might support
some default behavior for executing unnamed tasks.
In either case, if the named resource cannot be found, or if unnamed tasks are
not supported, the custom task controller should update the CustomRunâ€™s status to
indicate the error.
Specifying the target Custom Task by embedding its customSpec
To specify the custom task spec, it can be embedded directly into a
CustomRunâ€™s spec as shown below:
apiVersion: tekton.dev/v1beta1
kind: CustomRun
metadata:
  name: embedded-run
spec:
  customSpec:
    apiVersion: example.dev/v1beta1
    kind: Example
    spec:
      field1: value1
      field2: value2
When this CustomRun is created, the custom task controller responsible for
reconciling objects of kind Example in the example.dev api group will
execute it.
Developer guide for custom controllers supporting customSpec.


A custom controller may or may not support a Spec. In cases where it is
not supported the custom controller should respond with proper validation
error.


Validation of the fields of the custom task is delegated to the custom
task controller. It is recommended to implement validations as asynchronous
(i.e. at reconcile time), rather than part of the webhook. Using a webhook
for validation is problematic because, it is not possible to filter custom
task resource objects before validation step, as a result each custom task
resource has to undergo validation by all the installed custom task
controllers.


A custom task may have an empty spec, but cannot have an empty
ApiVersion and Kind. Custom task controllers should handle
an empty spec, either with a default behaviour, in a case no default
behaviour is supported then, appropriate validation error should be
updated to the CustomRunâ€™s status.


Cancellation
The custom task is responsible for implementing cancellation to support pipelineRun level timeouts and cancellation. If the Custom Task implementor does not support cancellation via .spec.status, Pipeline can not timeout within the specified interval/duration and can not be cancelled as expected upon request.
Pipeline Controller sets the spec.Status and spec.StatusMessage to signal CustomRuns about the Cancellation, while CustomRun controller updates its status.conditions as following once noticed the change on spec.Status.
status
  conditions:
  - type: Succeeded
    status: False
    reason: CustomRunCancelled
Specifying Timeout
A custom task specification can be created with Timeout as follows:
apiVersion: tekton.dev/v1beta1
kind: CustomRun
metadata:
  generateName: simpleexample
spec:
  timeout: 10s # set timeouts here.
  params:
    - name: searching
      value: the purpose of my existence
  customRef:
    apiVersion: custom.tekton.dev/v1alpha1
    kind: Example
    name: exampleName
Supporting timeouts is optional but recommended.
Developer guide for custom controllers supporting Timeout

Tekton controllers will never directly update the status of the CustomRun,
it is the responsibility of the custom task controller to support timeout.
If timeout is not supported, itâ€™s the responsibility of the custom task
controller to reject CustomRuns that specify a timeout value.
When CustomRun.Spec.Status is updated to RunCancelled, the custom task controller
MUST cancel the CustomRun. Otherwise, pipeline-level Timeout and
Cancellation wonâ€™t work for the Custom Task.
A Custom Task controller can watch for this status update
(i.e. CustomRun.Spec.Status == RunCancelled) and or CustomRun.HasTimedOut()
and take any corresponding actions (i.e. a clean up e.g., cancel a cloud build,
stop the waiting timer, tear down the approval listener).
Once resources or timers are cleaned up, while it is REQUIRED to set a
conditions on the CustomRunâ€™s status of Succeeded/False with an optional
Reason of CustomRunTimedOut.
Timeout is specified for each retry attempt instead of all retries.

Specifying Retries
A custom task specification can be created with Retries as follows:
apiVersion: tekton.dev/v1beta1
kind: CustomRun
metadata:
  generateName: simpleexample
spec:
  retries: 3 # set retries
  params:
    - name: searching
      value: the purpose of my existence
  customRef:
    apiVersion: custom.tekton.dev/v1alpha1
    kind: Example
    name: exampleName
Supporting retries is optional but recommended.
Developer guide for custom controllers supporting retries

Tekton controller only depends on ConditionSucceeded to determine the
termination status of a CustomRun, therefore Custom task implementors
MUST NOT set ConditionSucceeded to False until all retries are exhausted.
Those custom tasks who do not wish to support retry, can simply ignore it.
It is recommended, that custom task should update the field RetriesStatus
of a CustomRun on each retry performed by the custom task.
Tekton controller does not validate that number of entries in RetriesStatus
is same as specified value of retries count.

Specifying Parameters
If a custom task supports parameters, you can use the
params field in the CustomRun to specify their values:
spec:
  params:
    - name: my-param
      value: chicken
If the custom task controller knows how to interpret the parameter value, it
will do so. It might enforce that some parameter values must be specified, or
reject unknown parameter values.
Specifying workspaces
If the custom task supports it, you can provide Workspaces to share data with the custom task .
spec:
  workspaces:
    - name: my-workspace
      emptyDir: {}
Consult the documentation of the custom task that you are using to determine whether it supports workspaces and how to name them.
Specifying a ServiceAccount
If the custom task supports it, you can execute the CustomRun with a specific set of credentials by
specifying a ServiceAccount object name in the serviceAccountName field in your CustomRun
definition. If you do not explicitly specify this, the CustomRun executes with the service account
specified in the configmap-defaults ConfigMap. If this default is not specified, CustomRuns
will execute with the default service account
set for the target namespace.
spec:
  serviceAccountName: my-account
Consult the documentation of the custom task that you are using to determine whether it supports a service account name.
Monitoring execution status
As your CustomRun executes, its status field accumulates information on the
execution of the CustomRun. This information includes the current state of the
CustomRun, start and completion times, and any output results reported by
the custom task controller.
Status Reporting
When the CustomRun<Example> is validated and created, the Custom Task controller will be notified and is expected to begin doing some operation. When the operation begins, the controller MUST update the CustomRunâ€™s .status.conditions to report that itâ€™s ongoing:
status
  conditions:
  - type: Succeeded
    status: Unknown
When the operation completes, if it was successful, the condition MUST report status: True, and optionally a brief reason and human-readable message:
status
  conditions:
  - type: Succeeded
    status: True
    reason: ExampleComplete # optional
    message: Yay, good times # optional
If the operation was unsuccessful, the condition MUST report status: False, and optionally a reason and human-readable message:
status
  conditions:
  - type: Succeeded
    status: False
    reason: ExampleFailed # optional
    message: Oh no bad times # optional
If the CustomRun was cancelled, the condition MUST report status: False, reason: CustomRunCancelled, and optionally a human-readable message:
status
  conditions:
  - type: Succeeded
    status: False
    reason: CustomRunCancelled
    message: Oh it's cancelled # optional
The following tables shows the overall status of a CustomRun:



status
Description





The custom task controller has not taken any action on the CustomRun.


Unknown
The custom task controller has started execution and the CustomRun is ongoing.


True
The CustomRun completed successfully.


False
The CustomRun completed unsuccessfully, and all retries were exhausted.



The CustomRun typeâ€™s .status will also allow controllers to report other fields, such as startTime, completionTime, results (see below), and arbitrary context-dependent fields the Custom Task author wants to report. A fully-specified CustomRun status might look like:
status
  conditions:
  - type: Succeeded
    status: True
    reason: ExampleComplete
    message: Yay, good times
  completionTime: "2020-06-18T11:55:01Z"
  startTime: "2020-06-18T11:55:01Z"
  results:
  - name: first-name
    value: Bob
  - name: last-name
    value: Smith
  arbitraryField: hello world
  arbitraryStructuredField:
    listOfThings: ["a", "b", "c"]
Monitoring Results
After the CustomRun completes, the custom task controller can report output
values in the results field:
results:
- name: my-result
  value: chicken
Code examples
To better understand CustomRuns, study the following code examples:

Example CustomRun with a referenced custom task
Example CustomRun with an unnamed custom task
Example of specifying parameters

Example CustomRun with a referenced custom task
In this example, a CustomRun named my-example-run invokes a custom task of the v1alpha1
version of the Example kind in the example.dev API group, with the name
my-example-task.
In this case the custom task controller is expected to look up the Example
resource named my-example-task and to use that configuration to configure the
execution of the CustomRun.
apiVersion: tekton.dev/v1beta1
kind: CustomRun
metadata:
  name: my-example-run
spec:
  customRef:
    apiVersion: example.dev/v1alpha1
    kind: Example
    name: my-example-task
Example CustomRun with an unnamed custom task
In this example, a CustomRun named my-example-run invokes a custom task of the v1alpha1
version of the Example kind in the example.dev API group, without a specified name.
In this case the custom task controller is expected to provide some default
behavior when the referenced task is unnamed.
apiVersion: tekton.dev/v1beta1
kind: CustomRun
metadata:
  name: my-example-run
spec:
  customRef:
    apiVersion: example.dev/v1alpha1
    kind: Example
Example of specifying parameters
In this example, a CustomRun named my-example-run invokes a custom task, and
specifies some parameter values to further configure the executionâ€™s behavior.
In this case the custom task controller is expected to validate and interpret
these parameter values and use them to configure the CustomRunâ€™s execution.
apiVersion: tekton.dev/v1alpha1
kind: CustomRun
metadata:
  name: my-example-run
spec:
  customRef:
    apiVersion: example.dev/v1alpha1
    kind: Example
    name: my-example-task
  params:
    - name: my-first-param
      value: i'm number one
    - name: my-second-param
      value: close second

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Replacing PipelineResources with Tasks
PipelineResources remained in alpha while the other resource kinds were promoted to beta.
Since then, PipelineResources have been removed.
Read more about the deprecation in TEP-0074.
More on the reasoning and whatâ€™s left to do in
Why arenâ€™t PipelineResources in Beta?.
To ease migration away from PipelineResources
some types have an equivalent Task in the Catalog.
To use these replacement Tasks you will need to combine them with your existing Tasks via a Pipeline.
For example, if you were using this Task which was fetching from git and building with
Kaniko:
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: build-push-kaniko
spec:
  inputs:
    resources:
      - name: workspace
        type: git
    params:
      - name: pathToDockerFile
        description: The path to the dockerfile to build
        default: /workspace/workspace/Dockerfile
      - name: pathToContext
        description: The build context used by Kaniko
        default: /workspace/workspace
  outputs:
    resources:
      - name: builtImage
        type: image
  steps:
    - name: build-and-push
      image: gcr.io/kaniko-project/executor:v0.17.1
      env:
        - name: "DOCKER_CONFIG"
          value: "/tekton/home/.docker/"
      args:
        - --dockerfile=$(inputs.params.pathToDockerFile)
        - --destination=$(outputs.resources.builtImage.url)
        - --context=$(inputs.params.pathToContext)
        - --oci-layout-path=$(inputs.resources.builtImage.path)
      securityContext:
        runAsUser: 0
To do the same thing with the git catalog Task and the kaniko Task you will need to combine them in a
Pipeline.
For example this Pipeline uses the Kaniko and git catalog Tasks:
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: kaniko-pipeline
spec:
  params:
    - name: git-url
    - name: git-revision
    - name: image-name
    - name: path-to-image-context
    - name: path-to-dockerfile
  workspaces:
    - name: git-source
  tasks:
    - name: fetch-from-git
      taskRef:
        name: git-clone
      params:
        - name: url
          value: $(params.git-url)
        - name: revision
          value: $(params.git-revision)
      workspaces:
        - name: output
          workspace: git-source
    - name: build-image
      taskRef:
        name: kaniko
      params:
        - name: IMAGE
          value: $(params.image-name)
        - name: CONTEXT
          value: $(params.path-to-image-context)
        - name: DOCKERFILE
          value: $(params.path-to-dockerfile)
      workspaces:
        - name: source
          workspace: git-source
  # If you want you can add a Task that uses the IMAGE_DIGEST from the kaniko task
  # via $(tasks.build-image.results.IMAGE_DIGEST) - this was a feature we hadn't been
  # able to fully deliver with the Image PipelineResource!
Note that the image PipelineResource is gone in this example (replaced with
a result), and also that now the Task doesnâ€™t need to know anything
about where the files come from that it builds from.
Replacing a git resource
You can replace a git resource with the git-clone Catalog Task.
Replacing a pullrequest resource
You can replace a pullrequest resource with the pullrequest Catalog Task.
Replacing a gcs resource
You can replace a gcs resource with the gcs Catalog Task.
Replacing an image resource
Since the image resource is simply a way to share the digest of a built image with subsequent
Tasks in your Pipeline, you can use Task results to
achieve equivalent functionality.
For examples of replacing an image resource, see the following Catalog Tasks:

The Kaniko Catalog Task
illustrates how to write the digest of an image to a result.
The Buildah Catalog Task
illustrates how to accept an image digest as a parameter.

Replacing a cluster resource
You can replace a cluster resource with the kubeconfig-creator Catalog Task.
Replacing a cloudEvent resource
You can replace a cloudEvent resource with the CloudEvent Catalog Task.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	PipelineResources

âš ï¸ PipelineResources are deprecated.
Consider using replacement features instead. Read more in documentation
and TEP-0074.


Note: PipelineResources have not been promoted to Beta in tandem with Pipelineâ€™s other CRDs.
This means that the level of support for PipelineResources remains Alpha.
PipelineResources are now deprecated.**
For Beta-supported alternatives to PipelineResources see
the v1alpha1 to v1beta1 migration guide
which lists each PipelineResource type and a suggested option for replacing it.
For more information on why PipelineResources are remaining alpha see the description
of their problems, along with next steps, below.



Why Arenâ€™t PipelineResources in Beta?

Why Arenâ€™t PipelineResources in Beta?
The short answer is that theyâ€™re not ready to be given a Beta level of support by Tektonâ€™s developers. The long answer is, well, longer:


Their behaviour can be opaque.
Theyâ€™re implemented as a mixture of injected Task Steps, volume configuration and type-specific code in Tekton
Pipelineâ€™s controller. This means errors from PipelineResources can manifest in quite a few different ways
and itâ€™s not always obvious whether an error directly relates to PipelineResource behaviour. This problem
is compounded by the fact that, while our docs explain each Resource typeâ€™s â€œhappy pathâ€, there never seems to
be enough info available to explain error cases sufficiently.


When they fail theyâ€™re difficult to debug.
Several PipelineResources inject their own Steps before a Task's Steps. Itâ€™s extremely difficult to manually
insert Steps before them to inspect the state of a container before they run.


There arenâ€™t enough of them.
The six types of existing PipelineResources only cover a tiny subset of the possible systems and side-effects we
want to support with Tekton Pipelines.


Adding extensibility to them makes them really similar to Tasks:

User-definable Steps? This is what Tasks provide.
User-definable params? Tasks already have these.
User-definable â€œresource resultsâ€? Tasks have Task Results.
Sharing data between Tasks using PVCs? workspaces provide this for Tasks.



They make Tasks less reusable.

A Task has to choose the type of PipelineResource it will accept.
If a Task accepts a git PipelineResource then itâ€™s not able to accept a gcs PipelineResource from a
TaskRun or PipelineRun even though both the git and gcs PipelineResources fetch files. They should
technically be interchangeable: all they do is write files from somewhere remote onto disk. Yet with the existing
PipelineResources implementation they arenâ€™t interchangeable.



They also present challenges from a documentation perspective:

Their purpose is ambiguous and itâ€™s difficult to articulate what the CRD is precisely for.
Four of the types interact with external systems (git, pull-request, gcs, gcs-build).
Five of them write files to a Taskâ€™s disk (git, pull-request, gcs, gcs-build, cluster).
One tells the Pipelines controller to emit CloudEvents to a specific endpoint (cloudEvent).
One writes config to disk for a Task to use (cluster).
One writes a digest in one Task and then reads it back in another Task (image).
Perhaps the one thing you can say about the PipelineResource CRD is that it can create
side-effects for your Tasks.

Whatâ€™s still missing
So what are PipelineResources still good for?  We think weâ€™ve identified some of the most important things:

You can augment Task-only workflows with PipelineResources that, without them, can only be done with Pipelines.

For example, letâ€™s say you want to checkout a git repo for your Task to test. You have two options. First, you could use a git PipelineResource and add it directly to your test Task. Second, you could write a Pipeline that has a git-clone Task which checks out the code onto a PersistentVolumeClaim workspace and then passes that PVC workspace to your test Task. For a lot of users the second workflow is totally acceptable but for others it isnâ€™t. Some of the most notable reasons weâ€™ve heard are:

Some users simply cannot allocate storage on their platform, meaning PersistentVolumeClaims are out of the question.
Expanding a single Task workflow into a Pipeline is labor-intensive and feels unnecessary.




Despite being difficult to explain the whole CRD clearly each individual type is relatively easy to explain.

For example, users can build a pretty good â€œhunchâ€ for what a git PipelineResource is without really reading any docs.


Configuring CloudEvents to be emitted by the Tekton Pipelines controller.

Work is ongoing to get notifications support into the Pipelines controller which should hopefully be able to replace the cloudEvents PipelineResource.



For each of these there is some amount of ongoing work or discussion. We have deprecated PipelineResources and are
actively working to cover the missing replacement features. Read more about the deprecation in TEP-0074.
For Beta-supported alternatives to PipelineResources see
the v1alpha1 to v1beta1 migration guide
which lists each PipelineResource type and a suggested option for replacing it.
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Authentication at Run Time
This document describes how Tekton handles authentication when executing
TaskRuns and PipelineRuns. Since authentication concepts and processes
apply to both of those entities in the same manner, this document collectively
refers to TaskRuns and PipelineRuns as Runs for the sake of brevity.

Overview
Understanding credential selection
Using Secrets as a non-root user
Limiting Secret access to specific Steps
Configuring authentication for Git

Configuring basic-auth authentication for Git
Configuring ssh-auth authentication for Git
Using a custom port for SSH authentication
Using SSH authentication in git type Tasks


Configuring authentication for Docker

Configuring basic-auth authentication for Docker
Configuring docker* authentication for Docker


Technical reference

basic-auth for Git
ssh-auth for Git
basic-auth for Docker
Errors and their meaning

â€œunsuccessful cred copyâ€ Warning

Multiple Steps with varying UIDs
A Workspace or Volume is also Mounted for the same credentials
A Task employes a read-only-Workspace or Volume for $HOME
The Step is named image-digest-exporter






Disabling Tektonâ€™s Built-In Auth

Why would an organization want to do this?
What are the effects of making this change?
How to disable the built-in auth



Overview
Tekton supports authentication via the Kubernetes first-class Secret types listed below.

	
		Git
		Docker
	
	
		
			kubernetes.io/basic-auth
				kubernetes.io/ssh-auth
			
			kubernetes.io/basic-auth
				kubernetes.io/dockercfg
				kubernetes.io/dockerconfigjson
			
	

A Run gains access to these Secrets through its associated ServiceAccount. Tekton requires that each
supported Secret includes a Tekton-specific annotation.
Tekton converts properly annotated Secrets of the supported types and stores them in a Step's container as follows:

Git: Tekton produces a ~/.gitconfig file or a ~/.ssh directory.
Docker: Tekton produces a ~/.docker/config.json file.

Each Secret type supports multiple credentials covering multiple domains and establishes specific rules governing
credential formatting and merging. Tekton follows those rules when merging credentials of each supported type.
To consume these Secrets, Tekton performs credential initialization within every Pod it instantiates, before executing
any Steps in the Run. During credential initialization, Tekton accesses each Secret associated with the Run and
aggregates them into a /tekton/creds directory. Tekton then copies or symlinks files from this directory into the userâ€™s
$HOME directory.
TODO(#5357): Update docs to explain recommended methods of passing secrets in via workspaces
Understanding credential selection
A Run might require multiple types of authentication. For example, a Run might require access to
multiple private Git and Docker repositories. You must properly annotate each Secret to specify the
domains for which Tekton can use the credentials that the Secret contains. Tekton ignores all
Secrets that are not properly annotated.
A credential annotation key must begin with tekton.dev/git- or tekton.dev/docker- and its value is the
URL of the host for which you want Tekton to use that credential. In the following example, Tekton uses a
basic-auth (username/password pair) Secret to access Git repositories at github.com and gitlab.com
as well as Docker repositories at gcr.io:
apiVersion: v1
kind: Secret
metadata:
  annotations:
    tekton.dev/git-0: https://github.com
    tekton.dev/git-1: https://gitlab.com
    tekton.dev/docker-0: https://gcr.io
type: kubernetes.io/basic-auth
stringData:
  username: <cleartext username>
  password: <cleartext password>
And in this example, Tekton uses an ssh-auth Secret to access Git repositories
at github.com only:
apiVersion: v1
kind: Secret
metadata:
  annotations:
    tekton.dev/git-0: github.com
type: kubernetes.io/ssh-auth
stringData:
  ssh-privatekey: <private-key>
  # This is non-standard, but its use is encouraged to make this more secure.
  # Omitting this results in the server's public key being blindly accepted.
Using Secrets as a non-root user
In certain scenarios you might need to use Secrets as a non-root user. For example:

Your platform randomizes the user and/or groups that your containers use to execute.
The Steps in your Task define a non-root securityContext.
Your Task specifies a global non-root securityContext that applies to all Steps in the Task.

The following are considerations for executing Runs as a non-root user:

ssh-auth for Git requires the user to have a valid home directory configured in /etc/passwd.
Specifying a UID that has no valid home directory results in authentication failure.
Since SSH authentication ignores the $HOME environment variable, you must either move or symlink
the appropriate Secret files from the $HOME directory defined by Tekton (/tekton/home) to
the non-root userâ€™s valid home directory to use SSH authentication for either Git or Docker.

For an example of configuring SSH authentication in a non-root securityContext,
see authenticating-git-commands.
Limiting Secret access to specific Steps
As described earlier in this document, Tekton stores supported Secrets in
$HOME/tekton/home and makes them available to all Steps within a Task.
If you want to limit a Secret to only be accessible to specific Steps but not
others, you must explicitly specify a Volume using the Secret definition and
manually VolumeMount it into the desired Steps instead of using the procedures
described later in this document.
Configuring authentication for Git
This section describes how to configure the following authentication schemes for use with Git:

Configuring basic-auth authentication for Git
Configuring ssh-auth authentication for Git
Using a custom port for SSH authentication
Using SSH authentication in git type Tasks

Configuring basic-auth authentication for Git
This section describes how to configure a basic-auth type Secret for use with Git. In the example below,
before executing any Steps in the Run, Tekton creates a ~/.gitconfig file containing the credentials
specified in the Secret.
Note: Github deprecated basic authentication with username and password. You can still use basic authentication, but you wil need to use a personal access token instead of the cleartext password in the following example. You can find out how to create such a token on the Github documentation site.


In secret.yaml, define a Secret that specifies the username and password that you want Tekton
to use to access the target Git repository:
apiVersion: v1
kind: Secret
metadata:
  name: basic-user-pass
  annotations:
    tekton.dev/git-0: https://github.com # Described below
type: kubernetes.io/basic-auth
stringData:
  username: <cleartext username>
  password: <cleartext password>
In the above example, the value for tekton.dev/git-0 specifies the URL for which Tekton will use this Secret,
as described in Understanding credential selection.


In serviceaccount.yaml, associate the Secret with the desired ServiceAccount:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-bot
secrets:
  - name: basic-user-pass


In run.yaml, associate the ServiceAccount with your Run by doing one of the following:


Associate the ServiceAccount with your TaskRun:
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: build-push-task-run-2
spec:
  serviceAccountName: build-bot
  taskRef:
    name: build-push


Associate the ServiceAccount with your PipelineRun:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: demo-pipeline
  namespace: default
spec:
  serviceAccountName: build-bot
  pipelineRef:
    name: demo-pipeline




Execute the Run:
kubectl apply --filename secret.yaml serviceaccount.yaml run.yaml


Configuring ssh-auth authentication for Git
This section describes how to configure an ssh-auth type Secret for use with Git. In the example below,
before executing any Steps in the Run, Tekton creates a ~/.ssh/config file containing the SSH key
specified in the Secret.


In secret.yaml, define a Secret that specifies your SSH private key:
apiVersion: v1
kind: Secret
metadata:
  name: ssh-key
  annotations:
    tekton.dev/git-0: github.com # Described below
type: kubernetes.io/ssh-auth
stringData:
  ssh-privatekey: <private-key>
  # This is non-standard, but its use is encouraged to make this more secure.
  # If it is not provided then the git server's public key will be requested
  # when the repo is first fetched.
  known_hosts: <known-hosts>
In the above example, the value for tekton.dev/git-0 specifies the URL for which Tekton will use this Secret,
as described in Understanding credential selection.


Generate the ssh-privatekey value. For example:
cat ~/.ssh/id_rsa


Set the value of the known_hosts field to the generated ssh-privatekey value from the previous step.


In serviceaccount.yaml, associate the Secret with the desired ServiceAccount:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-bot
secrets:
  - name: ssh-key


In run.yaml, associate the ServiceAccount with your Run by doing one of the following:


Associate the ServiceAccount with your TaskRun:
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: build-push-task-run-2
spec:
  serviceAccountName: build-bot
  taskRef:
    name: build-push


Associate the ServiceAccount with your PipelineRun:


apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: demo-pipeline
  namespace: default
spec:
  serviceAccountName: build-bot
  pipelineRef:
    name: demo-pipeline


Execute the Run:
kubectl apply --filename secret.yaml,serviceaccount.yaml,run.yaml


Using a custom port for SSH authentication
You can specify a custom SSH port in your Secret.
apiVersion: v1
kind: Secret
metadata:
  name: ssh-key-custom-port
  annotations:
    tekton.dev/git-0: example.com:2222
type: kubernetes.io/ssh-auth
stringData:
  ssh-privatekey: <private-key>
  known_hosts: <known-hosts>
Using SSH authentication in git type Tasks
You can use SSH authentication as described earlier in this document when invoking git commands
directly in the Steps of a Task. Since ssh ignores the $HOME variable and only uses the
userâ€™s home directory specified in /etc/passwd, each Step must symlink /tekton/home/.ssh
to the home directory of its associated user.
Note: This explicit symlinking is not necessary when using the
git-clone Task from Tekton Catalog.
For example usage, see authenticating-git-commands.
Configuring authentication for Docker
This section describes how to configure the following authentication schemes for use with Docker:

Configuring basic-auth authentication for Docker
Configuring docker* authentication for Docker

Configuring basic-auth authentication for Docker
This section describes how to configure the basic-auth (username/password pair) type Secret for use with Docker.
In the example below, before executing any Steps in the Run, Tekton creates a ~/.docker/config.json file containing
the credentials specified in the Secret.


In secret.yaml, define a Secret that specifies the username and password that you want Tekton
to use to access the target Docker registry:
apiVersion: v1
kind: Secret
metadata:
  name: basic-user-pass
  annotations:
    tekton.dev/docker-0: https://gcr.io # Described below
type: kubernetes.io/basic-auth
stringData:
  username: <cleartext username>
  password: <cleartext password>
In the above example, the value for tekton.dev/docker-0 specifies the URL for which Tekton will use this Secret,
as described in Understanding credential selection.


In serviceaccount.yaml, associate the Secret with the desired ServiceAccount:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-bot
secrets:
  - name: basic-user-pass


In run.yaml, associate the ServiceAccount with your Run by doing one of the following:


Associate the ServiceAccount with your TaskRun:
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: build-push-task-run-2
spec:
  serviceAccountName: build-bot
  taskRef:
    name: build-push


Associate the ServiceAccount with your PipelineRun:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: demo-pipeline
  namespace: default
spec:
  serviceAccountName: build-bot
  pipelineRef:
    name: demo-pipeline




Execute the Run:
kubectl apply --filename secret.yaml serviceaccount.yaml run.yaml


Configuring docker* authentication for Docker
This section describes how to configure authentication using the dockercfg and dockerconfigjson type
Secrets for use with Docker. In the example below, before executing any Steps in the Run, Tekton creates
a ~/.docker/config.json file containing the credentials specified in the Secret. When the Steps execute,
Tekton uses those credentials to access the target Docker registry.
f
Note: If you specify both the Tekton basic-auth and the above Kubernetes Secrets, Tekton merges all
credentials from all specified Secrets but Tektonâ€™s basic-auth Secret overrides either of the
Kubernetes Secrets.


Define a Secret based on your Docker client configuration file.
kubectl create secret generic regcred \
 --from-file=.dockerconfigjson=<path/to/.docker/config.json> \
 --type=kubernetes.io/dockerconfigjson
For more information, see Pull an Image from a Private Registry
in the Kubernetes documentation.


In serviceaccount.yaml, associate the Secret with the desired ServiceAccount:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-bot
secrets:
  - name: regcred


In run.yaml, associate the ServiceAccount with your Run by doing one of the following:


Associate the ServiceAccount with your TaskRun:
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: build-with-basic-auth
spec:
  serviceAccountName: build-bot
  steps:
  # ...


Associate the ServiceAccount with your PipelineRun:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: demo-pipeline
  namespace: default
spec:
  serviceAccountName: build-bot
  pipelineRef:
    name: demo-pipeline




Execute the build:
kubectl apply --filename secret.yaml --filename serviceaccount.yaml --filename taskrun.yaml


Technical reference
This section provides a technical reference for the implementation of the authentication mechanisms
described earlier in this document.
basic-auth for Git
Given URLs, usernames, and passwords of the form: https://url{n}.com,
user{n}, and pass{n}, Tekton generates the following:
=== ~/.gitconfig ===
[credential]
    helper = store
[credential "https://url1.com"]
    username = "user1"
[credential "https://url2.com"]
    username = "user2"
...
=== ~/.git-credentials ===
https://user1:pass1@url1.com
https://user2:pass2@url2.com
...
ssh-auth for Git
Given hostnames, private keys, and known_hosts of the form: url{n}.com,
key{n}, and known_hosts{n}, Tekton generates the following.
By default, if no value is specified for known_hosts, Tekton configures SSH to accept
any public key returned by the server on first query. Tekton does this
by setting Gitâ€™s core.sshCommand variable to ssh -o StrictHostKeyChecking=accept-new.
This behaviour can be prevented
using a feature-flag: require-git-ssh-secret-known-hosts.
Set this flag to true and all Git SSH Secrets must include a known_hosts.
=== ~/.ssh/id_key1 ===
{contents of key1}
=== ~/.ssh/id_key2 ===
{contents of key2}
...
=== ~/.ssh/config ===
Host url1.com
    HostName url1.com
    IdentityFile ~/.ssh/id_key1
Host url2.com
    HostName url2.com
    IdentityFile ~/.ssh/id_key2
...
=== ~/.ssh/known_hosts ===
{contents of known_hosts1}
{contents of known_hosts2}
...
basic-auth for Docker
Given URLs, usernames, and passwords of the form: https://url{n}.com,
user{n}, and pass{n}, Tekton generates the following. Since Docker doesnâ€™t
support the kubernetes.io/ssh-auth type Secret, Tekton ignores annotations
on Secrets of that type.
=== ~/.docker/config.json ===
{
  "auths": {
    "https://url1.com": {
      "auth": "$(echo -n user1:pass1 | base64)",
      "email": "not@val.id",
    },
    "https://url2.com": {
      "auth": "$(echo -n user2:pass2 | base64)",
      "email": "not@val.id",
    },
    ...
  }
}
Errors and their meaning
â€œunsuccessful cred copyâ€ Warning
This message has the following format:

warning: unsuccessful cred copy: ".docker" from "/tekton/creds" to "/tekton/home": unable to open destination: open /tekton/home/.docker/config.json: permission denied

The precise credential and paths mentioned can vary. This message is only a
warning but can be indicative of the following problems:
Multiple Steps with varying UIDs
Multiple Steps with different users / UIDs are trying to initialize docker
or git credentials in the same Task. If those Steps need access to the
credentials then they may fail as they might not have permission to access them.
This happens because, by default, /tekton/home is set to be a Step userâ€™s home
directory and Tekton makes this directory a shared volume that all Steps in a
Task have access to. Any credentials initialized by one Step are overwritten
by subsequent Steps also initializing credentials.
If the Steps reporting this warning do not use the credentials mentioned
in the message then you can safely ignore it.
This can most easily be resolved by ensuring that each Step executing in your
Task and TaskRun runs with the same UID. A blanket UID can be set with a
TaskRunâ€™s Pod template field.
If you require Steps to run with different UIDs then you should disable
Tektonâ€™s built-in credential initialization and use Workspaces to mount
credentials from Secrets instead. See the section on disabling Tektonâ€™s
credential initialization.
A Workspace or Volume is also Mounted for the same credentials
A Task has mounted both a Workspace (or Volume) for credentials and the TaskRun
has attached a service account with git or docker credentials that Tekton will
try to initialize.
The simplest solution to this problem is to not mix credentials mounted via
Workspace with those initialized using the process described in this document.
See the section on disabling Tektonâ€™s credential initialization.
A Task employs a read-only Workspace or Volume for $HOME
A Task has mounted a read-only Workspace (or Volume) for the userâ€™s HOME
directory and the TaskRun attaches a service account with git or docker
credentials that Tekton will try to initialize.
The simplest solution to this problem is to not mix credentials mounted via
Workspace with those initialized using the process described in this document.
See the section on disabling Tektonâ€™s credential initialization.
The contents of $HOME are chowned to a different user
A Task Step that modifies the ownership of files in the user home directory
may prevent subsequent Steps from initializing credentials in that same home
directory. The simplest solution to this problem is to avoid running chown
on files and directories under /tekton. Another option is to run all Steps
with the same UID.
The Step is named image-digest-exporter
If you see this warning reported specifically by an image-digest-exporter Step
you can safely ignore this message. The reason it appears is that this Step is
injected by Tekton and it runs with a non-root UID
that can differ from those of the Steps in the Task. The Step does not use
these credentials.

Disabling Tektonâ€™s Built-In Auth
Why would an organization want to do this?
There are a number of reasons that an organization may want to disable
Tektonâ€™s built-in credential handling:

The mechanism can be quite difficult to debug.
There are an extremely limited set of supported credential types.
Tasks with Steps that have different UIDs can break if multiple Steps
are trying to share access to the same credentials.
Tasks with Steps that have different UIDs can log more warning messages,
creating more noise in TaskRun logs. Again this is because multiple Steps
with differing UIDs cannot share access to the same credential files.

What are the effects of making this change?

Credentials must now be passed explicitly to Tasks either with Workspaces,
environment variables (using envFrom in your Steps and a Task param to
specify a Secret), or a custom volume and volumeMount definition.

How to disable the built-in auth
To disable Tektonâ€™s built-in auth, edit the feature-flag ConfigMap in the
tekton-pipelines namespace and update the value of disable-creds-init
from "false" to "true".
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Events in Tekton
Tektonâ€™s controllers emits Kubernetes events
when TaskRuns and PipelineRuns execute. This allows you to monitor and react to whatâ€™s happening during execution by
retrieving those events using the kubectl describe command. Tekton can also emit CloudEvents.
Note: Conditions do not emit events
but the underlying TaskRun do.
Events in TaskRuns
TaskRuns emit events for the following Reasons:

Started: emitted the first time the TaskRun is picked by the
reconciler from its work queue, so it only happens if webhook validation was
successful. This event in itself does not indicate that a Step is executing;
the Step executes once the following conditions are satisfied:

Validation of the Task and  its associated resources must succeed, and
Checks for associated Conditions must succeed, and
Scheduling of the associated Pod must succeed.


Succeeded: emitted once all steps in the TaskRun have executed successfully,
including post-steps injected by Tekton.
Failed: emitted if the TaskRun finishes running unsuccessfully because a Step failed,
or the TaskRun timed out or was cancelled. A TaskRun also emits Failed events
if it cannot execute at all due to failing validation.

Events in PipelineRuns
PipelineRuns emit events for the following Reasons:

Started: emitted the first time the PipelineRun is picked by the
reconciler from its work queue, so it only happens if webhook validation was
successful. This event in itself does not indicate that a Step is executing;
the Step executes once validation for the Pipeline as well as all associated Tasks
and Resources is successful.
Running: emitted when the PipelineRun passes validation and
actually begins execution.
Succeeded: emitted once all Tasks reachable via the DAG have
executed successfully.
Failed: emitted if the PipelineRun finishes running unsuccessfully because a Task failed or the
PipelineRun timed out or was cancelled. A PipelineRun also emits Failed events if it cannot
execute at all due to failing validation.

Events via CloudEvents
When you configure a sink, Tekton emits
events as described in the table below.
Tekton sends cloud events in a parallel routine to allow for retries without blocking the
reconciler. A routine is started every time the Succeeded condition changes - either state,
reason or message. Retries are sent using an exponential back-off strategy.
Because of retries, events are not guaranteed to be sent to the target sink in the order they happened.



Resource
Event
Event Type




TaskRun
Started
dev.tekton.event.taskrun.started.v1


TaskRun
Running
dev.tekton.event.taskrun.running.v1


TaskRun
Condition Change while Running
dev.tekton.event.taskrun.unknown.v1


TaskRun
Succeed
dev.tekton.event.taskrun.successful.v1


TaskRun
Failed
dev.tekton.event.taskrun.failed.v1


PipelineRun
Started
dev.tekton.event.pipelinerun.started.v1


PipelineRun
Running
dev.tekton.event.pipelinerun.running.v1


PipelineRun
Condition Change while Running
dev.tekton.event.pipelinerun.unknown.v1


PipelineRun
Succeed
dev.tekton.event.pipelinerun.successful.v1


PipelineRun
Failed
dev.tekton.event.pipelinerun.failed.v1


Run
Started
dev.tekton.event.run.started.v1


Run
Running
dev.tekton.event.run.running.v1


Run
Succeed
dev.tekton.event.run.successful.v1


Run
Failed
dev.tekton.event.run.failed.v1



CloudEvents for Runs are only sent when enabled in the configuration.
Note: CloudEvents for Runs rely on an ephemeral cache to avoid duplicate
events. In case of controller restart, the cache is reset and duplicate events
may be sent.
Format of CloudEvents
According to the CloudEvents spec, HTTP headers are included to match the context fields. For example:
"Ce-Id": "77f78ae7-ff6d-4e39-9d05-b9a0b7850527",
"Ce-Source": "/apis/tekton.dev/v1beta1/namespaces/default/taskruns/curl-run-6gplk",
"Ce-Specversion": "1.0",
"Ce-Subject": "curl-run-6gplk",
"Ce-Time": "2021-01-29T14:47:58.157819Z",
"Ce-Type": "dev.tekton.event.taskrun.unknown.v1",
Other HTTP headers are:
"Accept-Encoding": "gzip",
"Connection": "close",
"Content-Length": "3519",
"Content-Type": "application/json",
"User-Agent": "Go-http-client/1.1"
The payload is JSON, a map with a single root key taskRun or pipelineRun, depending on the source
of the event. Inside the root key, the whole spec and status of the resource is included. For example:
{
  "taskRun": {
    "metadata": {
      "annotations": {
        "pipeline.tekton.dev/release": "v0.20.1",
        "tekton.dev/pipelines.minVersion": "0.12.1",
        "tekton.dev/tags": "search"
      },
      "creationTimestamp": "2021-01-29T14:47:57Z",
      "generateName": "curl-run-",
      "generation": 1,
      "labels": {
        "app.kubernetes.io/managed-by": "tekton-pipelines",
        "app.kubernetes.io/version": "0.1",
        "tekton.dev/task": "curl"
      },
      "managedFields": "(...)",
      "name": "curl-run-6gplk",
      "namespace": "default",
      "resourceVersion": "156770",
      "selfLink": "/apis/tekton.dev/v1beta1/namespaces/default/taskruns/curl-run-6gplk",
      "uid": "4ccb4f01-3ecc-4eb4-87e1-76f04efeee5c"
    },
    "spec": {
      "params": [
        {
          "name": "url",
          "value": "https://api.hub.tekton.dev/resource/96"
        }
      ],
      "resources": {},
      "serviceAccountName": "default",
      "taskRef": {
        "kind": "Task",
        "name": "curl"
      },
      "timeout": "1h0m0s"
    },
    "status": {
      "conditions": [
        {
          "lastTransitionTime": "2021-01-29T14:47:58Z",
          "message": "pod status \"Initialized\":\"False\"; message: \"containers with incomplete status: [place-tools]\"",
          "reason": "Pending",
          "status": "Unknown",
          "type": "Succeeded"
        }
      ],
      "podName": "curl-run-6gplk-pod",
      "startTime": "2021-01-29T14:47:57Z",
      "steps": [
        {
          "container": "step-curl",
          "name": "curl",
          "waiting": {
            "reason": "PodInitializing"
          }
        }
      ],
      "taskSpec": {
        "description": "This task performs curl operation to transfer data from internet.",
        "params": [
          {
            "description": "URL to curl'ed",
            "name": "url",
            "type": "string"
          },
          {
            "default": [],
            "description": "options of url",
            "name": "options",
            "type": "array"
          },
          {
            "default": "docker.io/curlimages/curl:7.72.0@sha256:3c3ff0c379abb1150bb586c7d55848ed4dcde4a6486b6f37d6815aed569332fe",
            "description": "option of curl image",
            "name": "curl-image",
            "type": "string"
          }
        ],
        "steps": [
          {
            "args": [
              "$(params.options[*])",
              "$(params.url)"
            ],
            "command": [
              "curl"
            ],
            "image": "$(params.curl-image)",
            "name": "curl",
            "resources": {}
          }
        ]
      }
    }
  }
}

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Execution Logs
Tekton stores execution logs for TaskRuns and PipelineRuns within
the Pod holding the containers that run the Steps for your TaskRun or PipelineRun.
You can get execution logs using one of the following methods:


Get the logs directly from the Pod.
For example, you can use the kubectl as follows:
# Get the Pod name from the TaskRun instance.
kubectl get taskruns -o yaml | grep podName

# Or, get the Pod name from the PipelineRun.
kubectl get pipelineruns -o yaml | grep podName

# Get the logs for all containers in the Pod.
kubectl logs $POD_NAME --all-containers

# Or, get the logs for a specific container in the Pod.
kubectl logs $POD_NAME -c $CONTAINER_NAME
kubectl logs $POD_NAME -c step-run-kubectl


Get the logs using Tektonâ€™s tkn CLI.


Get the logs using Tekton Dashboard.


Configure an external service to consume and display the logs. For example, ElasticSearch, Beats, and Kibana.



	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Pipeline Controller Metrics
The following pipeline metrics are available at controller-service on port 9090.
We expose several kinds of exporters, including Prometheus, Google Stackdriver, and many others. You can set them up using observability configuration.



Name
Type
Labels/Tags
Status




tekton_pipelines_controller_pipelinerun_duration_seconds_[bucket, sum, count]
Histogram/LastValue(Gauge)
*pipeline=<pipeline_name>  *pipelinerun=<pipelinerun_name>  status=<status>  namespace=<pipelinerun-namespace>
experimental


tekton_pipelines_controller_pipelinerun_taskrun_duration_seconds_[bucket, sum, count]
Histogram/LastValue(Gauge)
*pipeline=<pipeline_name>  *pipelinerun=<pipelinerun_name>  status=<status>  *task=<task_name>  *taskrun=<taskrun_name> namespace=<pipelineruns-taskruns-namespace>   *reason=<reason>
experimental


tekton_pipelines_controller_pipelinerun_count
Counter
status=<status>   *reason=<reason>
deprecate


tekton_pipelines_controller_pipelinerun_total
Counter
status=<status>
experimental


tekton_pipelines_controller_running_pipelineruns_count
Gauge

deprecate


tekton_pipelines_controller_running_pipelineruns
Gauge

experimental


tekton_pipelines_controller_taskrun_duration_seconds_[bucket, sum, count]
Histogram/LastValue(Gauge)
status=<status>  *task=<task_name>  *taskrun=<taskrun_name> namespace=<pipelineruns-taskruns-namespace>  *reason=<reason>
experimental


tekton_pipelines_controller_taskrun_count
Counter
status=<status>  *reason=<reason>
deprecate


tekton_pipelines_controller_taskrun_total
Counter
status=<status>
experimental


tekton_pipelines_controller_running_taskruns_count
Gauge

deprecate


tekton_pipelines_controller_running_taskruns
Gauge

experimental


tekton_pipelines_controller_running_taskruns_throttled_by_quota_count
Gauge
 namespace=<pipelinerun-namespace>
deprecate


tekton_pipelines_controller_running_taskruns_throttled_by_node_count
Gauge
 namespace=<pipelinerun-namespace>
deprecate


tekton_pipelines_controller_running_taskruns_throttled_by_quota
Gauge
 namespace=<pipelinerun-namespace>
experimental


tekton_pipelines_controller_running_taskruns_throttled_by_node
Gauge
 namespace=<pipelinerun-namespace>
experimental


tekton_pipelines_controller_client_latency_[bucket, sum, count]
Histogram

experimental



The Labels/Tag marked as â€œ*â€ are optional. And thereâ€™s a choice between Histogram and LastValue(Gauge) for pipelinerun and taskrun duration metrics.
Configuring Metrics using config-observability configmap
A sample config-map has been provided as config-observability. By default, taskrun and pipelinerun metrics have these values:
    metrics.taskrun.level: "task"
    metrics.taskrun.duration-type: "histogram"
    metrics.pipelinerun.level: "pipeline"
    metrics.pipelinerun.duration-type: "histogram"
    metrics.count.enable-reason: "false"
Following values are available in the configmap:



configmap data
value
description




metrics.taskrun.level
taskrun
Level of metrics is taskrun


metrics.taskrun.level
task
Level of metrics is task and taskrun label isnâ€™t present in the metrics


metrics.taskrun.level
namespace
Level of metrics is namespace, and task and taskrun label isnâ€™t present in the metrics


metrics.pipelinerun.level
pipelinerun
Level of metrics is pipelinerun


metrics.pipelinerun.level
pipeline
Level of metrics is pipeline and pipelinerun label isnâ€™t present in the metrics


metrics.pipelinerun.level
namespace
Level of metrics is namespace, pipeline and pipelinerun label isnâ€™t present in the metrics


metrics.taskrun.duration-type
histogram
tekton_pipelines_controller_pipelinerun_taskrun_duration_seconds and tekton_pipelines_controller_taskrun_duration_seconds is of type histogram


metrics.taskrun.duration-type
lastvalue
tekton_pipelines_controller_pipelinerun_taskrun_duration_seconds and  tekton_pipelines_controller_taskrun_duration_seconds is of type gauge or lastvalue


metrics.pipelinerun.duration-type
histogram
tekton_pipelines_controller_pipelinerun_duration_seconds is of type histogram


metrics.pipelinerun.duration-type
lastvalue
tekton_pipelines_controller_pipelinerun_duration_seconds is of type gauge or lastvalue


metrics.count.enable-reason
false
Sets if the reason label should be included on count metrics


metrics.taskrun.throttle.enable-namespace
false
Sets if the namespace label should be included on the tekton_pipelines_controller_running_taskruns_throttled_by_quota metric



Histogram value isnâ€™t available when pipelinerun or taskrun labels are selected. The Lastvalue or Gauge will be provided. Histogram would serve no purpose because it would generate a single bar. TaskRun and PipelineRun level metrics arenâ€™t recommended because they lead to an unbounded cardinality which degrades the observability database.
To check that appropriate values have been applied in response to configmap changes, use the following commands:
kubectl port-forward -n tekton-pipelines service/tekton-pipelines-controller 9090
And then check that changes have been applied to metrics coming from http://127.0.0.1:9090/metrics

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Labels and Annotations
Tekton allows you to use custom Kubernetes Labels
to easily mark Tekton entities belonging to the same conceptual execution chain. Tekton also automatically adds select labels
to more easily identify resource relationships. This document describes the label propagation scheme, automatic labeling, and
provides usage examples.


Label propagation
Automatic labeling
Usage examples


Label propagation
Labels propagate among Tekton entities as follows:


For Pipelines instantiated using a PipelineRun, labels propagate
automatically from Pipelines to PipelineRuns to TaskRuns, and then to
the associated Pods. If a label is present in both Pipeline and
PipelineRun, the label in PipelineRun takes precedence.


Labels from Tasks referenced by TaskRuns within a PipelineRun propagate to the corresponding TaskRuns,
and then to the associated Pods. As for Pipeline and PipelineRun, if a label is present in both Task and
TaskRun, the label in TaskRun takes precedence.


For standalone TaskRuns (that is, ones not executing as part of a Pipeline), labels
propagate from the referenced Task, if one exists, to
the corresponding TaskRun, and then to the associated Pod. The same as above applies.


Automatic labeling
Tekton automatically adds labels to Tekton entities as described in the following table.
Note: *.tekton.dev labels are reserved for Tektonâ€™s internal use only. Do not add or remove them manually.

	
		
			Label
			Added To
			Propagates To
			Contains
		
		
			tekton.dev/pipeline
			PipelineRuns
			TaskRuns, Pods
			Name of the Pipeline that the PipelineRun references.
		
		
			tekton.dev/pipelineRun
			TaskRuns that are created automatically during the execution of a PipelineRun.
			TaskRuns, Pods
			Name of the PipelineRun that triggered the creation of the TaskRun.
		
		
			tekton.dev/task
			TaskRuns that reference an existing Task.
			Pods
			Name of the Task that the TaskRun references.
		
		
			tekton.dev/clusterTask
			TaskRuns that reference an existing ClusterTask.
			Pods
			Name of the ClusterTask that the TaskRun references.
		
		
			tekton.dev/taskRun
			Pods
			No propagation.
			Name of the TaskRun that created the Pod.
		
		
			tekton.dev/memberOf
			TaskRuns that are created automatically during the execution of a PipelineRun.
			TaskRuns, Pods
			tasks or finally depending on the PipelineTask's membership in the Pipeline.
		
		
			app.kubernetes.io/instance, app.kubernetes.io/component
			Pods, StatefulSets (Affinity Assistant)
			No propagation.
			Pod affinity values for TaskRuns.
		
	

Usage examples
Below are some examples of using labels:
The following command finds all Pods created by a PipelineRun named test-pipelinerun:
kubectl get pods --all-namespaces -l tekton.dev/pipelineRun=test-pipelinerun
The following command finds all TaskRuns that reference a Task named test-task:
kubectl get taskruns --all-namespaces -l tekton.dev/task=test-task
The following command finds all TaskRuns that reference a ClusterTask named test-clustertask:
kubectl get taskruns --all-namespaces -l tekton.dev/clusterTask=test-clustertask
Annotations propagation
Annotation propagate among Tekton entities as follows (similar to Labels):


For Pipelines instantiated using a PipelineRun, annotations propagate
automatically from Pipelines to PipelineRuns to TaskRuns, and then to
the associated Pods. If a annotation is present in both Pipeline and
PipelineRun, the annotation in PipelineRun takes precedence.


Annotations from Tasks referenced by TaskRuns within a PipelineRun propagate to the corresponding TaskRuns,
and then to the associated Pods. As for Pipeline and PipelineRun, if a annotation is present in both Task and
TaskRun, the annotation in TaskRun takes precedence.


For standalone TaskRuns (that is, ones not executing as part of a Pipeline), annotations
propagate from the referenced Task, if one exists, to
the corresponding TaskRun, and then to the associated Pod. The same as above applies.



	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Windows

Overview
Scheduling Tasks on Windows Nodes

Node Selectors
Node Affinity



Overview
If you need a Windows environment as part of a Tekton Task or Pipeline, you can include Windows container images in your Task steps. Because Windows containers can only run on a Windows host, you will need to have Windows nodes available in your Kubernetes cluster. You should read Windows support in Kubernetes to understand the functionality and limitations of Kubernetes on Windows.
Some important things to note about Windows containers and Kubernetes:

Windows containers cannot run on a Linux host.
Kubernetes does not support Windows only clusters. The Kubernetes control plane components can only run on Linux.
Kubernetes currently only supports process isolated containers, which means a containerâ€™s base image OS version must match that of the host OS. See Windows container version compatibility for more information.
A Kubernetes Pod cannot contain both Windows and Linux containers.

Some important things to note about Windows support in Tekton:

A Task can only have Windows or Linux containers, but not both.
A Pipeline can contain both Windows and Linux Tasks.
In a mixed-OS cluster, TaskRuns and PipelineRuns will need to be scheduled to the correct node using one of the methods described below.
Tektonâ€™s controller components can only run on Linux nodes.

Scheduling Tasks on Windows Nodes
In order to ensure that Tasks are scheduled to a node with the correct host OS, you will need to update the TaskRun or PipelineRun spec with rules to define this behaviour. This can be done in a couple of different ways, but the simplest option is to specify a node selector.
Node Selectors
Node selectors are the simplest way to schedule pods to a Windows or Linux node. By default, Kubernetes nodes include a label kubernetes.io/os to identify the host OS. The Kubelet populates this with runtime.GOOS as defined by Go. Use spec.podTemplate.nodeSelector (or spec.taskRunSpecs[i].podTemplate.nodeSelector in a PipelineRun) to schedule Tasks to a node with a specific label and value.
For example:
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: windows-taskrun
spec:
  taskRef:
    name: windows-task
  podTemplate:
    nodeSelector:
      kubernetes.io/os: windows
---
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: linux-taskrun
spec:
  taskRef:
    name: linux-task
  podTemplate:
    nodeSelector:
      kubernetes.io/os: linux
Node Affinity
Node affinity can be used as an alternative method of defining the OS requirement of a Task. These rules can be set under spec.podTemplate.affinity.nodeAffinity in a TaskRun definition. The example below produces the same result as the previous example which used node selectors.
For example:
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: windows-taskrun
spec:
  taskRef:
    name: windows-task
  podTemplate:
    affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - windows
---
apiVersion: tekton.dev/v1
kind: TaskRun
metadata:
  name: linux-taskrun
spec:
  taskRef:
    name: linux-task
  podTemplate:
    affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Tekton Pipeline Remote Resolution
Remote Resolution is a Tekton beta feature that allows users to fetch tasks and pipelines from remote sources outside the cluster. Tekton provides a few built-in resolvers that can fetch from git repositories, OCI registries etc as well as a framework for writing custom resolvers.
Remote Resolution was initially created as a separate project in TEP-060 and migrated into the core pipelines project in #4710.
Getting Started Tutorial
For new users getting started with Tekton Pipeline remote resolution, check out the
resolution-getting-started.md tutorial.
Configuring Built-in Resolvers
These resolvers are enabled by setting the appropriate feature flag in the resolvers-feature-flags
ConfigMap in the tekton-pipelines-resolvers namespace. See the section in install.md for details.
The default resolver type can be configured by the default-resolver-type field in the config-defaults ConfigMap (alpha feature). See additional-configs.md for details.
Developer Howto: Writing a Resolver From Scratch
For a developer getting started with writing a new Resolver, see
how-to-write-a-resolver.md and the
accompanying resolver-template.
Resolver Reference: The interfaces and methods to implement
For a table of the interfaces and methods a resolver must implement
along with those that are optional, see resolver-reference.md.

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Bundles Resolver
Resolver Type
This Resolver responds to type bundles.
Parameters



Param Name
Description
Example Value




secret
The name of the secret to use when constructing registry credentials
default


bundle
The bundle url pointing at the image to fetch
gcr.io/tekton-releases/catalog/upstream/golang-build:0.1


name
The name of the resource to pull out of the bundle
golang-build


kind
The resource kind to pull out of the bundle
task



Requirements

A cluster running Tekton Pipeline v0.41.0 or later.
The built-in remote resolvers installed.
The enable-bundles-resolver feature flag in the resolvers-feature-flags ConfigMap
in the tekton-pipelines-resolvers namespace set to true.
Beta features enabled.

Configuration
This resolver uses a ConfigMap for its settings. See
../config/resolvers/bundleresolver-config.yaml
for the name, namespace and defaults that the resolver ships with.
Options



Option Name
Description
Example Values




default-kind
The default layer kind in the bundle image.
task, pipeline



Usage
Task Resolution
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: remote-task-reference
spec:
  taskRef:
    resolver: bundles
    params:
    - name: bundle
      value: docker.io/ptasci67/example-oci@sha256:053a6cb9f3711d4527dd0d37ac610e8727ec0288a898d5dfbd79b25bcaa29828
    - name: name
      value: hello-world
    - name: kind
      value: task
Pipeline Resolution
Unfortunately the Tekton Catalog does not publish pipelines at the
moment. Hereâ€™s an example PipelineRun that talks to a private registry
but wonâ€™t work unless you tweak the bundle field to point to a
registry with a pipeline in it:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: bundle-demo
spec:
  pipelineRef:
    resolver: bundles
    params:
    - name: bundle
      value: 10.96.190.208:5000/simple/pipeline:latest
    - name: name
      value: hello-pipeline
    - name: kind
      value: pipeline
  params:
  - name: username
    value: "tekton pipelines"
ResolutionRequest Status
ResolutionRequest.Status.RefSource field captures the source where the remote resource came from. It includes the 3 subfields: url, digest and entrypoint.

uri: The image repository URI
digest: The map of the algorithm portion -> the hex encoded portion of the image digest.
entrypoint: The resource name in the OCI bundle image.

Example:

TaskRun Resolution

apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: remote-task-reference
spec:
  taskRef:
    resolver: bundles
    params:
    - name: bundle
      value: gcr.io/tekton-releases/catalog/upstream/git-clone:0.7
    - name: name
      value: git-clone
    - name: kind
      value: task
  params:
    - name: url
      value: https://github.com/octocat/Hello-World
  workspaces:
    - name: output
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 500Mi

ResolutionRequest

apiVersion: resolution.tekton.dev/v1beta1
kind: ResolutionRequest
metadata:
  ...
  labels:
    resolution.tekton.dev/type: bundles
  name: bundles-21ad80ec13f3e8b73fed5880a64d4611
  ...
spec:
  params:
  - name: bundle
    value: gcr.io/tekton-releases/catalog/upstream/git-clone:0.7
  - name: name
    value: git-clone
  - name: kind
    value: task
status:
  annotations: ...
  ...
  data: xxx
  observedGeneration: 1
  refSource:
    digest:
      sha256: f51ca50f1c065acba8290ef14adec8461915ecc5f70a8eb26190c6e8e0ededaf
    entryPoint: git-clone
    uri: gcr.io/tekton-releases/catalog/upstream/git-clone

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Simple Git Resolver
Resolver Type
This Resolver responds to type git.
Parameters



Param Name
Description
Example Value




url
URL of the repo to fetch and clone anonymously. Either url, or repo (with org) must be specified, but not both.
https://github.com/tektoncd/catalog.git


repo
The repository to find the resource in. Either url, or repo (with org) must be specified, but not both.
pipeline, test-infra


org
The organization to find the repository in. Default can be set in configuration.
tektoncd, kubernetes


token
An optional secret name in the PipelineRun namespace to fetch the token from. Defaults to empty, meaning it will try to use the configuration from the global configmap.
secret-name, (empty)


tokenKey
An optional key in the token secret name in the PipelineRun namespace to fetch the token from. Defaults to token.
token


revision
Git revision to checkout a file from. This can be commit SHA, branch or tag.
aeb957601cf41c012be462827053a21a420befca main v0.38.2


pathInRepo
Where to find the file in the repo.
task/golang-build/0.3/golang-build.yaml


serverURL
An optional server URL (that includes the https:// prefix) to connect for API operations
https:/github.mycompany.com


scmType
An optional SCM type to use for API operations
github, gitlab, gitea



Requirements

A cluster running Tekton Pipeline v0.41.0 or later.
The built-in remote resolvers installed.
The enable-git-resolver feature flag in the resolvers-feature-flags ConfigMap in the
tekton-pipelines-resolvers namespace set to true.
Beta features enabled.

Configuration
This resolver uses a ConfigMap for its settings. See
../config/resolvers/git-resolver-config.yaml
for the name, namespace and defaults that the resolver ships with.
Options



Option Name
Description
Example Values




default-revision
The default git revision to use if none is specified
main


fetch-timeout
The maximum time any single git clone resolution may take. Note: a global maximum timeout of 1 minute is currently enforced on all resolution requests.
1m, 2s, 700ms


default-url
The default git repository URL to use for anonymous cloning if none is specified.
https://github.com/tektoncd/catalog.git


scm-type
The SCM provider type. Required if using the authenticated API with org and repo.
github, gitlab, gitea, bitbucketcloud, bitbucketserver


server-url
The SCM providerâ€™s base URL for use with the authenticated API. Not needed if using github.com, gitlab.com, or BitBucket Cloud
api.internal-github.com


api-token-secret-name
The Kubernetes secret containing the SCM provider API token. Required if using the authenticated API with org and repo.
bot-token-secret


api-token-secret-key
The key within the token secret containing the actual secret. Required if using the authenticated API with org and repo.
oauth, token


api-token-secret-namespace
The namespace containing the token secret, if not default.
other-namespace


default-org
The default organization to look for repositories under when using the authenticated API, if not specified in the resolver parameters. Optional.
tektoncd, kubernetes



Usage
The git resolver has two modes: cloning a repository anonymously, or fetching individual files via an SCM providerâ€™s API using an API token.
Anonymous Cloning
Anonymous cloning is supported only for public repositories. This mode clones the full git repo.
Task Resolution
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: git-clone-demo-tr
spec:
  taskRef:
    resolver: git
    params:
    - name: url
      value: https://github.com/tektoncd/catalog.git
    - name: revision
      value: main
    - name: pathInRepo
      value: task/git-clone/0.6/git-clone.yaml
Pipeline resolution
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: git-clone-demo-pr
spec:
  pipelineRef:
    resolver: git
    params:
    - name: url
      value: https://github.com/tektoncd/catalog.git
    - name: revision
      value: main
    - name: pathInRepo
      value: pipeline/simple/0.1/simple.yaml
  params:
  - name: name
    value: Ranni
Authenticated API
The authenticated API supports private repositories, and fetches only the file at the specified path rather than doing a full clone.
When using the authenticated API, providers with implementations in go-scm can be used.
Note that not all go-scm implementations have been tested with the git resolver, but it is known to work with:

github.com and GitHub Enterprise
gitlab.com and self-hosted Gitlab
Gitea
BitBucket Server
BitBucket Cloud

Fetching from multiple Git providers with different configuration is not
supported. You can use the http resolver to fetch URL
from another provider with different credentials.
Task Resolution
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: git-api-demo-tr
spec:
  taskRef:
    resolver: git
    params:
    - name: org
      value: tektoncd
    - name: repo
      value: catalog
    - name: revision
      value: main
    - name: pathInRepo
      value: task/git-clone/0.6/git-clone.yaml
Task Resolution with a custom token to a custom SCM provider
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: git-api-demo-tr
spec:
  taskRef:
    resolver: git
    params:
    - name: org
      value: tektoncd
    - name: repo
      value: catalog
    - name: revision
      value: main
    - name: pathInRepo
      value: task/git-clone/0.6/git-clone.yaml
    # my-secret-token should be created in the namespace where the
    # pipelinerun is created and contain a GitHub personal access
    # token in the token key of the secret.
    - name: token
      value: my-secret-token
    - name: tokenKey
      value: token
    - name: scmType
      value: github
    - name: serverURL
      value: https://ghe.mycompany.com
Pipeline resolution
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: git-api-demo-pr
spec:
  pipelineRef:
    resolver: git
    params:
    - name: org
      value: tektoncd
    - name: repo
      value: catalog
    - name: revision
      value: main
    - name: pathInRepo
      value: pipeline/simple/0.1/simple.yaml
  params:
  - name: name
    value: Ranni
ResolutionRequest Status
ResolutionRequest.Status.RefSource field captures the source where the remote resource came from. It includes the 3 subfields: url, digest and entrypoint.

url

If users choose to use anonymous cloning, the url is just user-provided value for the url param in the SPDX download format.
If scm api is used, it would be the clone URL of the repo fetched from scm repository service in the SPDX download format.


digest

The algorithm name is fixed â€œsha1â€, but subject to be changed to â€œsha256â€ once Git eventually uses SHA256 at some point later. See https://git-scm.com/docs/hash-function-transition for more details.
The value is the actual commit sha at the moment of resolving the resource even if a user provides a tag/branch name for the param revision.


entrypoint: the user-provided value for the path param.

Example:

Pipeline Resolution

apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: git-demo
spec:
  pipelineRef:
    resolver: git
    params:
    - name: url
      value: https://github.com/<username>/<reponame>.git
    - name: revision
      value: main
    - name: pathInRepo
      value: pipeline.yaml

ResolutionRequest

apiVersion: resolution.tekton.dev/v1alpha1
kind: ResolutionRequest
metadata:
  labels:
    resolution.tekton.dev/type: git
  ...
spec:
  params:
    pathInRepo: pipeline.yaml
    revision: main
    url: https://github.com/<username>/<reponame>.git
status:
  refSource:
    uri: git+https://github.com/<username>/<reponame>.git
    digest:
      sha1: <The latest commit sha on main at the moment of resolving>
    entrypoint: pipeline.yaml
  data: a2luZDogUGxxxx...

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Cluster Resolver
Resolver Type
This Resolver responds to type cluster.
Parameters



Param Name
Description
Example Value




kind
The kind of resource to fetch.
task, pipeline


name
The name of the resource to fetch.
some-pipeline, some-task


namespace
The namespace in the cluster containing the resource.
default, other-namespace



Requirements

A cluster running Tekton Pipeline v0.41.0 or later.
The built-in remote resolvers installed.
The enable-cluster-resolver feature flag in the resolvers-feature-flags ConfigMap
in the tekton-pipelines-resolvers namespace set to true.
Beta features enabled.

Configuration
This resolver uses a ConfigMap for its settings. See
../config/resolvers/cluster-resolver-config.yaml
for the name, namespace and defaults that the resolver ships with.
Options



Option Name
Description
Example Values




default-kind
The default resource kind to fetch if not specified in parameters.
task, pipeline


default-namespace
The default namespace to fetch resources from if not specified in parameters.
default, some-namespace


allowed-namespaces
An optional comma-separated list of namespaces which the resolver is allowed to access. Defaults to empty, meaning all namespaces are allowed.
default,some-namespace, (empty)


blocked-namespaces
An optional comma-separated list of namespaces which the resolver is blocked from accessing. If the value is a * all namespaces will be disallowed and allowed namespace will need to be explicitely listed in allowed-namespaces. Defaults to empty, meaning all namespaces are allowed.
default,other-namespace, *, (empty)



Usage
Task Resolution
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: remote-task-reference
spec:
  taskRef:
    resolver: cluster
    params:
    - name: kind
      value: task
    - name: name
      value: some-task
    - name: namespace
      value: namespace-containing-task
Pipeline resolution
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: remote-pipeline-reference
spec:
  pipelineRef:
    resolver: cluster
    params:
    - name: kind
      value: pipeline
    - name: name
      value: some-pipeline
    - name: namespace
      value: namespace-containing-pipeline
ResolutionRequest Status
ResolutionRequest.Status.RefSource field captures the source where the remote resource came from. It includes the 3 subfields: url, digest and entrypoint.

url: url is the unique full identifier for the resource in the cluster. It is in the format of <resource uri>@<uid>. Resource URI part is the namespace-scoped uri i.e. /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME. See K8s Resource URIs for more details.
digest: hex-encoded sha256 checksum of the content in the in-cluster resourceâ€™s spec field. The reason why itâ€™s the checksum of the spec content rather than the whole object is because the metadata of in-cluster resources might be modified i.e. annotations. Therefore, the checksum of the spec content should be sufficient for source verifiers to verify if things have been changed maliciously even though the metadata is modified with good intentions.
entrypoint: empty because the path information is already available in the url field.

Example:

TaskRun Resolution

apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: cluster-demo
spec:
  taskRef:
    resolver: cluster
    params:
    - name: kind
      value: task
    - name: name
      value: a-simple-task
    - name: namespace
      value: default

ResolutionRequest

apiVersion: resolution.tekton.dev/v1beta1
kind: ResolutionRequest
metadata:
  labels:
    resolution.tekton.dev/type: cluster
  name: cluster-7a04be6baa3eeedd232542036b7f3b2d
  namespace: default
  ownerReferences: ...
spec:
  params:
  - name: kind
    value: task
  - name: name
    value: a-simple-task
  - name: namespace
    value: default
status:
  annotations: ...
  conditions: ...
  data: xxx
  refSource:
    digest:
      sha256: 245b1aa918434cc8195b4d4d026f2e43df09199e2ed31d4dfd9c2cbea1c7ce54
    uri: /apis/tekton.dev/v1beta1/namespaces/default/task/a-simple-task@3b82d8c4-f89e-47ea-a49d-3be0dca4c038

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	HTTP Resolver
This resolver responds to type http.
Parameters



Param Name
Description
Example Value





url
The URL to fetch from
https://raw.githubusercontent.com/tektoncd-catalog/git-clone/main/task/git-clone/git-clone.yaml



http-username
An optional username when fetching a task with credentials (need to be used in conjunction with http-password-secret)
git



http-password-secret
An optional secret in the PipelineRun namespace with a reference to a password when fetching a task with credentials (need to be used in conjunction with http-username)
http-password



http-password-secret-key
An optional key in the http-password-secret to be used when fetching a task with credentials
Default: password




A valid URL must be provided. Only HTTP or HTTPS URLs are supported.
Requirements

A cluster running Tekton Pipeline v0.41.0 or later.
The built-in remote resolvers installed.
The enable-http-resolver feature flag in the resolvers-feature-flags ConfigMap in the
tekton-pipelines-resolvers namespace set to true.
Beta features enabled.

Configuration
This resolver uses a ConfigMap for its settings. See
../config/resolvers/http-resolver-config.yaml
for the name, namespace and defaults that the resolver ships with.
Options



Option Name
Description
Example Values




fetch-timeout
The maximum time any fetching of URL resolution may take. Note: a global maximum timeout of 1 minute is currently enforced on all resolution requests.
1m, 2s, 700ms



Usage
Task Resolution
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: remote-task-reference
spec:
  taskRef:
    resolver: http
    params:
    - name: url
      value: https://raw.githubusercontent.com/tektoncd-catalog/git-clone/main/task/git-clone/git-clone.yaml
Task Resolution with Basic Auth
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: remote-task-reference
spec:
  taskRef:
    resolver: http
    params:
    - name: url
      value: https://raw.githubusercontent.com/owner/private-repo/main/task/task.yaml
    - name: http-username
      value: git
    - name: http-password-secret
      value: git-secret
    - name: http-password-secret-key
      value: git-token
Pipeline Resolution
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: http-demo
spec:
  pipelineRef:
    resolver: http
    params:
    - name: url
      value: https://raw.githubusercontent.com/tektoncd/catalog/main/pipeline/build-push-gke-deploy/0.1/build-push-gke-deploy.yaml

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Hub Resolver
Use resolver type hub.
Parameters



Param Name
Description
Example Value




catalog
The catalog from where to pull the resource (Optional)
Default:  tekton-catalog-tasks (for task kind);  tekton-catalog-pipelines (for pipeline kind)


type
The type of Hub from where to pull the resource (Optional). Either artifact or tekton
Default:  artifact


kind
Either task or pipeline (Optional)
Default: task


name
The name of the task or pipeline to fetch from the hub
golang-build


version
Version or a Constraint (see below of a task or a pipeline to pull in from. Wrap the number in quotes!
"0.5.0", ">= 0.5.0"



The Catalogs in the Artifact Hub follows the semVer (i.e. <major-version>.<minor-version>.0) and the Catalogs in the Tekton Hub follows the simplified semVer (i.e. <major-version>.<minor-version>). Both full and simplified semantic versioning will be accepted by the version parameter. The Hub Resolver will map the version to the format expected by the target Hub type.
Requirements

A cluster running Tekton Pipeline v0.41.0 or later.
The built-in remote resolvers installed.
The enable-hub-resolver feature flag in the resolvers-feature-flags ConfigMap in the
tekton-pipelines-resolvers namespace set to true.
Beta features enabled.

Configuration
This resolver uses a ConfigMap for its settings. See
../config/resolvers/hubresolver-config.yaml
for the name, namespace and defaults that the resolver ships with.
Options



Option Name
Description
Example Values




default-tekton-hub-catalog
The default tekton hub catalog from where to pull the resource.
Tekton


default-artifact-hub-task-catalog
The default artifact hub catalog from where to pull the resource for task kind.
tekton-catalog-tasks


default-artifact-hub-pipeline-catalog
The default artifact hub catalog from where to pull the resource for pipeline kind.
tekton-catalog-pipelines


default-kind
The default object kind for references.
task, pipeline


default-type
The default hub from where to pull the resource.
artifact, tekton



Configuring the Hub API endpoint
The Hub Resolver supports to resolve resources from the Artifact Hub and the Tekton Hub,
which can be configured by setting the type field of the resolver.
(Please note that the Tekton Hub will be deprecated after migration to the Artifact Hub is done.)
When setting the type field to artifact, the resolver will hit the public hub api at https://artifacthub.io/ by default
but you can configure your own (for example to use a private hub
instance) by setting the ARTIFACT_HUB_API environment variable in
../config/resolvers/resolvers-deployment.yaml. Example:
env
- name: ARTIFACT_HUB_API
  value: "https://artifacthub.io/"
When setting the type field to tekton, the resolver will hit the public
tekton catalog api at https://api.hub.tekton.dev by default but you can configure
your own instance of the Tekton Hub by setting the TEKTON_HUB_API environment
variable in
../config/resolvers/resolvers-deployment.yaml. Example:
env
- name: TEKTON_HUB_API
  value: "https://api.private.hub.instance.dev"
The Tekton Hub deployment guide can be found here.
Usage
Task Resolution
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: remote-task-reference
spec:
  taskRef:
    resolver: hub
    params:
    - name: catalog # optional
      value: tekton-catalog-tasks
    - name: type # optional
      value: artifact 
    - name: kind
      value: task
    - name: name
      value: git-clone
    - name: version
      value: "0.6"
Pipeline Resolution
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: hub-demo
spec:
  pipelineRef:
    resolver: hub
    params:
    - name: catalog # optional
      value: tekton-catalog-pipelines 
    - name: type # optional
      value: artifact
    - name: kind
      value: pipeline
    - name: name
      value: buildpacks
    - name: version
      value: "0.1"
  # Note: the buildpacks pipeline requires parameters.
  # Resolution of the pipeline will succeed but the PipelineRun
  # overall will not succeed without those parameters.
Version constraint
Instead of a version you can specify a constraint to choose from. The constraint is a string as documented in the go-version library.
Some examples:
params:
  - name: name
    value: git-clone
  - name: version
    value: ">=0.7.0"
Will only choose the git-clone task that is greater than version 0.7.0
params:
  - name: name
    value: git-clone
  - name: version
    value: ">=0.7.0, < 2.0.0"
Will select the latest git-clone task that is greater than version 0.7.0 and
less than version 2.0.0, so if the latest task is the version 0.9.0 it will
be selected.
Other operators for selection are available for comparisons, see the
go-version
source code.

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Trusted Resources

Overview
Instructions
Sign Resources
Enable Trusted Resources

Overview
Trusted Resources is a feature which can be used to sign Tekton Resources and verify them. Details of design can be found at TEPâ€“0091. This is an alpha feature and supports v1beta1 and v1 version of  Task and Pipeline.
Note: Currently, trusted resources only support verifying Tekton resources that come from remote places i.e. git, OCI registry and Artifact Hub. To use cluster resolver for in-cluster resources, make sure to set all default values for the resources before applied to cluster, because the mutating webhook will update the default fields if not given and fail the verification.
Verification failure will mark corresponding taskrun/pipelinerun as Failed status and stop the execution.
Instructions
Sign Resources
We have added sign and verify into Tekton Cli as a subcommand in release v0.28.0 and later. Please refer to cli docs to sign and Tekton resources.
A signed task example:
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    tekton.dev/signature: MEYCIQDM8WHQAn/yKJ6psTsa0BMjbI9IdguR+Zi6sPTVynxv6wIhAMy8JSETHP7A2Ncw7MyA7qp9eLsu/1cCKOjRL1mFXIKV
  creationTimestamp: null
  name: example-task
  namespace: tekton-trusted-resources
spec:
  steps:
  - image: ubuntu
    name: echo
Enable Trusted Resources
Enable feature flag
Update the config map:
apiVersion: v1
kind: ConfigMap
metadata:
  name: feature-flags
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  trusted-resources-verification-no-match-policy: "fail"
trusted-resources-verification-no-match-policy configurations:

ignore: if no matching policies are found, skip the verification, donâ€™t log, and donâ€™t fail the taskrun/pipelinerun
warn:  if no matching policies are found, skip the verification, log a warning, and donâ€™t fail the taskrun/pipelinerun
fail: Fail the taskrun/pipelinerun if no matching policies are found.

Notes:

To skip the verification: make sure no policies exist and trusted-resources-verification-no-match-policy is set to warn or ignore.
To enable the verification: install VerificationPolicy to match the resources.

Or patch the new values:
kubectl patch configmap feature-flags -n tekton-pipelines -p='{"data":{"trusted-resources-verification-no-match-policy":"fail"}}
TaskRun and PipelineRun status update
 
Trusted resources will update the taskrunâ€™s condition to indicate if it passes verification or not.
The following tables illustrate how the conditions are impacted by feature flag and verification result. Note that if not true or false means this case doesnâ€™t update the corresponding condition.
No Matching Policies:




Conditions.TrustedResourcesVerified
Conditions.Succeeded




no-match-policy: â€œignoreâ€




no-match-policy: â€œwarnâ€
False



no-match-policy: â€œfailâ€
False
False



Matching Policies(no matter what trusted-resources-verification-no-match-policy value is):




Conditions.TrustedResourcesVerified
Conditions.Succeeded




all policies pass
True



any enforce policy fails
False
False


only warn policies fail
False




A successful sample TrustedResourcesVerified condition is:
status:
  conditions:
  - lastTransitionTime: "2023-03-01T18:17:05Z"
    message: Trusted resource verification passed
    status: "True"
    type: TrustedResourcesVerified
Failed sample TrustedResourcesVerified and Succeeded conditions are:
status:
  conditions:
  - lastTransitionTime: "2023-03-01T18:17:05Z"
    message: resource verification failed # This will be filled with detailed error message.
    status: "False"
    type: TrustedResourcesVerified
  - lastTransitionTime: "2023-03-01T18:17:10Z"
    message: resource verification failed
    status: "False"
    type: Succeeded
Config key at VerificationPolicy
VerificationPolicy supports SecretRef or encoded public key data.
How does VerificationPolicy work?
You can create multiple VerificationPolicy and apply them to the cluster.


Trusted resources will look up policies from the resource namespace (usually this is the same as taskrun/pipelinerun namespace).


If multiple policies are found. For each policy we will check if the resource url is matching any of the patterns in the resources list. If matched then this policy will be used for verification.


If multiple policies are matched, the resource must pass all the â€œenforceâ€ mode policies. If the resource only matches policies in â€œwarnâ€ mode and fails to pass the â€œwarnâ€ policy, it will not fail the taskrun or pipelinerun, but log a warning instead.


To pass one policy, the resource can pass any public keys in the policy.


Take the following VerificationPolicies for example, a resource from â€œhttps://github.com/tektoncd/catalog.git", needs to pass both verification-policy-a and verification-policy-b, to pass verification-policy-a the resource needs to pass either key1 or key2.
Example:
apiVersion: tekton.dev/v1alpha1
kind: VerificationPolicy
metadata:
  name: verification-policy-a
  namespace: resource-namespace
spec:
  # resources defines a list of patterns
  resources:
    - pattern: "https://github.com/tektoncd/catalog.git"  #git resource pattern
    - pattern: "gcr.io/tekton-releases/catalog/upstream/git-clone"  # bundle resource pattern
    - pattern: " https://artifacthub.io/"  # hub resource pattern
  # authorities defines a list of public keys
  authorities:
    - name: key1
      key:
        # secretRef refers to a secret in the cluster, this secret should contain public keys data
        secretRef:
          name: secret-name-a
          namespace: secret-namespace
        hashAlgorithm: sha256
    - name: key2
      key:
        # data stores the inline public key data
        data: "STRING_ENCODED_PUBLIC_KEY"
  # mode can be set to "enforce" (default) or "warn".
  mode: enforce
apiVersion: tekton.dev/v1alpha1
kind: VerificationPolicy
metadata:
  name: verification-policy-b
  namespace: resource-namespace
spec:
  resources:
    - pattern: "https://github.com/tektoncd/catalog.git"
  authorities:
    - name: key3
      key:
        # data stores the inline public key data
        data: "STRING_ENCODED_PUBLIC_KEY"
namespace should be the same of corresponding resourcesâ€™ namespace.
pattern is used to filter out remote resources by their sources URL. e.g. git resources pattern can be set to https://github.com/tektoncd/catalog.git. The pattern should follow regex schema, we use go regex libraryâ€™s Match to match the pattern from VerificationPolicy to the ConfigSource URL resolved by remote resolution. Note that .* will match all resources.
To learn more about regex syntax please refer to syntax.
To learn more about ConfigSource please refer to resolvers doc for more context. e.g. gitresolver
key is used to store the public key, key can be configured with secretRef, data, kms note that only 1 of these 3 fields can be configured.

secretRef: refers to secret in cluster to store the public key.
data: contains the inline data of the pubic key in â€œPEM-encoded byte sliceâ€ format.
kms: refers to the uri of the public key, it should follow the format defined in sigstore.

hashAlgorithm is the algorithm for the public key, by default is sha256. It also supports SHA224, SHA384, SHA512.
mode controls whether a failing policy will fail the taskrun/pipelinerun, or only log the a warning

enforce (default) - fail the taskrun/pipelinerun if verification fails
warn - donâ€™t fail the taskrun/pipelinerun if verification fails but log a warning

Migrate Config key at configmap to VerificationPolicy
Note: key configuration in configmap is deprecated,
The following usage of public keys in configmap can be migrated to VerificationPolicy/
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-trusted-resources
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: tekton-pipelines
data:
  publickeys: "/etc/verification-secrets/cosign.pub, /etc/verification-secrets/cosign2.pub"
To migrate to VerificationPolicy: Stores the public key files in a secret, and configure the secret ref in VerificationPolicy
apiVersion: tekton.dev/v1alpha1
kind: VerificationPolicy
metadata:
  name: verification-policy-name
  namespace: resource-namespace
spec:
  authorities:
    - name: key1
      key:
        # secretRef refers to a secret in the cluster, this secret should contain public keys data
        secretRef:
          name: secret-name-cosign
          namespace: secret-namespace
        hashAlgorithm: sha256
    - name: key2
      key:
        secretRef:
          name: secret-name-cosign2
          namespace: secret-namespace
        hashAlgorithm: sha256

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Container Contract
Each container image that executes a Step in a Task must
comply with the container contract described in this document.
When a Task instantiates and runs containers that execute the Steps in the Task,
each containerâ€™s entrypoint is overwritten with a custom binary that ensures the
containers within the Task's Pod are executed in the order specified in the Task
definition. Because of this, we highly recommend that you always specify a command value.
However, if youâ€™re using the script field to
embed a script within a Step, do not specify a command value. For example:
        - name: setup-comment
          image: python:3-alpine
          script: |
            #!/usr/bin/env python
            import json
            (...)
If you do not specify a command value, the Pipelines controller performs a lookup for
the entrypoint value in the associated remote container registry. If the image is in
a private registry, you must include an ImagePullSecret
value in the service account definition used by the Task.
The Pipelines controller uses this value unless the service account is not
defined, at which point it assumes the value of default.
The final fallback occurs to the Docker config specified in the $HOME/.docker/config.json file.
If no credentials are specified in any of the locations described above, the Pipelines
controller performs an anonymous lookup of the image.
For example, consider the following Task, which uses two images named
gcr.io/cloud-builders/gcloud and gcr.io/cloud-builders/docker. In this example, the
Pipelines controller retrieves the entrypoint value from the registry, which allows
the Task to execute the gcloud and docker commands, respectively.
spec:
  steps:
    - image: gcr.io/cloud-builders/gcloud
      command: [gcloud]
    - image: gcr.io/cloud-builders/docker
      command: [docker]
However, if you specify a custom command value, the controller uses that value instead:
spec:
  steps:
    - image: gcr.io/cloud-builders/gcloud
      command:
        - bash
        - -c
        - echo "Hello!"
You also have the option to specify args to go with your command value:
steps:
  - image: ubuntu
    command: ["/bin/bash"]
    args: ["-c", "echo hello $FOO"]
    env:
      - name: "FOO"
        value: "world"

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Tekton Bundle Contract v0.1
When using a Tekton Bundle in a task or pipeline reference, the OCI artifact backing the
bundle must adhere to the following contract.
Contract
Only Tekton CRDs (eg, Task or Pipeline) may reside in a Tekton Bundle used as a Tekton
bundle reference.
Each layer of the image must map 1:1 with a single Tekton resource (eg Task).
No more than 20 individual layers (Pipelines and/or Tasks) maybe placed in a single image.
Each layer must contain all of the following annotations:

dev.tekton.image.name => ObjectMeta.Name of the resource
dev.tekton.image.kind => TypeMeta.Kind of the resource, all lower-cased and singular (eg, task)
dev.tekton.image.apiVersion => TypeMeta.APIVersion of the resource (eg
â€œtekton.dev/v1beta1â€)

The union of the { dev.tekton.image.apiVersion, dev.tekton.image.kind, dev.tekton.image.name }
annotations on a given layer must be unique among all layers of that image. In practical terms, this means no two
â€œtasksâ€ can have the same name for example.
Each layer must be compressed and stored with a supported OCI MIME type except for +zstd types. For list of the
supported types see
 
the official spec.
Furthermore, each layer must contain a YAML or JSON representation of the underlying resource. If the resource is
missing any identifying fields (missing an apiVersion for instance) then it will be considered invalid.
Any tool creating a Tekton bundle must enforce this format and ensure that the annotations and contents all match and
conform to this spec. Additionally, the Tekton controller will reject non-conforming Tekton Bundles.
Examples
Say you wanted to create a Tekton Bundle out of the following resources:
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: foo
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: bar
---
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: foobar
If we imagine what the contents of the resulting bundle look like, it would look something like this (YAML is just for
illustrative purposes):
# my-bundle
layers:
  - annotations:
    - name: "dev.tekton.image.name"
      value: "foo"
    - name: "dev.tekton.image.kind"
      value: "Task"
    - name: "dev.tekton.image.apiVersion"
      value: "tekton.dev/v1beta1"
    contents: <compressed bytes of Task object>
  - annotations:
    - name: "dev.tekton.image.name"
      value: "bar"
    - name: "dev.tekton.image.kind"
      value: "Task"
    - name: "dev.tekton.image.apiVersion"
      value: "tekton.dev/v1beta1"
    contents: <compressed bytes of Task object>
  - annotations:
    - name: "dev.tekton.image.name"
      value: "foobar"
    - name: "dev.tekton.image.kind"
      value: "Pipeline"
    - name: "dev.tekton.image.apiVersion"
      value: "tekton.dev/v1beta1"
    contents: <compressed bytes of Pipeline object>

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	Pipeline API
	
	
		

  
    


  
    


  

		
	    
	Packages:


resolution.tekton.dev/v1alpha1


resolution.tekton.dev/v1beta1


tekton.dev/v1


tekton.dev/v1alpha1


tekton.dev/v1beta1


resolution.tekton.dev/v1alpha1


Resource Types:

ResolutionRequest


ResolutionRequest is an object for requesting the content of
a Tekton resource like a pipeline.yaml.




Field
Description





metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


ResolutionRequestSpec




(Optional)
Spec holds the information for the request part of the resource request.





params

map[string]string



(Optional)
Parameters are the runtime attributes passed to
the resolver to help it figure out how to resolve the
resource being requested. For example: repo URL, commit SHA,
path to file, the kind of authentication to leverage, etc.







status


ResolutionRequestStatus




(Optional)
Status communicates the state of the request and, ultimately,
the content of the resolved resource.




ResolutionRequestSpec


(Appears on:ResolutionRequest)


ResolutionRequestSpec are all the fields in the spec of the
ResolutionRequest CRD.




Field
Description





params

map[string]string



(Optional)
Parameters are the runtime attributes passed to
the resolver to help it figure out how to resolve the
resource being requested. For example: repo URL, commit SHA,
path to file, the kind of authentication to leverage, etc.




ResolutionRequestStatus


(Appears on:ResolutionRequest)


ResolutionRequestStatus are all the fields in a ResolutionRequestâ€™s
status subresource.




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





ResolutionRequestStatusFields


ResolutionRequestStatusFields





(Members of ResolutionRequestStatusFields are embedded into this type.)





ResolutionRequestStatusFields


(Appears on:ResolutionRequestStatus)


ResolutionRequestStatusFields are the ResolutionRequest-specific fields
for the status subresource.




Field
Description





data

string



Data is a string representation of the resolved content
of the requested resource in-lined into the ResolutionRequest
object.




refSource


RefSource




RefSource is the source reference of the remote data that records where the remote
file came from including the url, digest and the entrypoint.





resolution.tekton.dev/v1beta1


Resource Types:

ResolutionRequest


ResolutionRequest is an object for requesting the content of
a Tekton resource like a pipeline.yaml.




Field
Description





metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


ResolutionRequestSpec




(Optional)
Spec holds the information for the request part of the resource request.





params


[]Param




(Optional)
Parameters are the runtime attributes passed to
the resolver to help it figure out how to resolve the
resource being requested. For example: repo URL, commit SHA,
path to file, the kind of authentication to leverage, etc.




url

string



(Optional)
URL is the runtime url passed to the resolver
to help it figure out how to resolver the resource being
requested.
This is currently at an ALPHA stability level and subject to
alpha API compatibility policies.







status


ResolutionRequestStatus




(Optional)
Status communicates the state of the request and, ultimately,
the content of the resolved resource.




ResolutionRequestSpec


(Appears on:ResolutionRequest)


ResolutionRequestSpec are all the fields in the spec of the
ResolutionRequest CRD.




Field
Description





params


[]Param




(Optional)
Parameters are the runtime attributes passed to
the resolver to help it figure out how to resolve the
resource being requested. For example: repo URL, commit SHA,
path to file, the kind of authentication to leverage, etc.




url

string



(Optional)
URL is the runtime url passed to the resolver
to help it figure out how to resolver the resource being
requested.
This is currently at an ALPHA stability level and subject to
alpha API compatibility policies.




ResolutionRequestStatus


(Appears on:ResolutionRequest)


ResolutionRequestStatus are all the fields in a ResolutionRequestâ€™s
status subresource.




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





ResolutionRequestStatusFields


ResolutionRequestStatusFields





(Members of ResolutionRequestStatusFields are embedded into this type.)





ResolutionRequestStatusFields


(Appears on:ResolutionRequestStatus)


ResolutionRequestStatusFields are the ResolutionRequest-specific fields
for the status subresource.




Field
Description





data

string



Data is a string representation of the resolved content
of the requested resource in-lined into the ResolutionRequest
object.




source


RefSource




Deprecated: Use RefSource instead




refSource


RefSource




RefSource is the source reference of the remote data that records the url, digest
and the entrypoint.





tekton.dev/v1

Package v1 contains API Schema definitions for the pipeline v1 API group

Resource Types:

Pipeline

PipelineRun

Task

TaskRun

Pipeline


Pipeline describes a list of Tasks to execute. It expresses how outputs
of tasks feed into inputs of subsequent tasks.




Field
Description





apiVersion
string


tekton.dev/v1





kind
string

Pipeline



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


PipelineSpec




(Optional)
Spec holds the desired state of the Pipeline from the client





displayName

string



(Optional)
DisplayName is a user-facing name of the pipeline that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the pipeline that may be
used to populate a UI.




tasks


[]PipelineTask




Tasks declares the graph of Tasks that execute when this Pipeline is run.




params


ParamSpecs




Params declares a list of input parameters that must be supplied when
this Pipeline is run.




workspaces


[]PipelineWorkspaceDeclaration




(Optional)
Workspaces declares a set of named workspaces that are expected to be
provided by a PipelineRun.




results


[]PipelineResult




(Optional)
Results are values that this pipeline can output once run




finally


[]PipelineTask




Finally declares the list of Tasks that execute just before leaving the Pipeline
i.e. either after all Tasks are finished executing successfully
or after a failure which would result in ending the Pipeline







PipelineRun


PipelineRun represents a single execution of a Pipeline. PipelineRuns are how
the graph of Tasks declared in a Pipeline are executed; they specify inputs
to Pipelines such as parameter values and capture operational aspects of the
Tasks execution such as service account and tolerations. Creating a
PipelineRun creates TaskRuns for Tasks in the referenced Pipeline.




Field
Description





apiVersion
string


tekton.dev/v1





kind
string

PipelineRun



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


PipelineRunSpec




(Optional)





pipelineRef


PipelineRef




(Optional)




pipelineSpec


PipelineSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




params


Params




Params is a list of parameter names and values.




status


PipelineRunSpecStatus




(Optional)
Used for cancelling a pipelinerun (and maybe more later on)




timeouts


TimeoutFields




(Optional)
Time after which the Pipeline times out.
Currently three keys are accepted in the map
pipeline, tasks and finally
with Timeouts.pipeline >= Timeouts.tasks + Timeouts.finally




taskRunTemplate


PipelineTaskRunTemplate




(Optional)
TaskRunTemplate represent template of taskrun




workspaces


[]WorkspaceBinding




(Optional)
Workspaces holds a set of workspace bindings that must match names
with those declared in the pipeline.




taskRunSpecs


[]PipelineTaskRunSpec




(Optional)
TaskRunSpecs holds a set of runtime specs







status


PipelineRunStatus




(Optional)




Task


Task represents a collection of sequential steps that are run as part of a
Pipeline using a set of inputs and producing a set of outputs. Tasks execute
when TaskRuns are created that provide the input parameters and resources and
output resources the Task requires.




Field
Description





apiVersion
string


tekton.dev/v1





kind
string

Task



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


TaskSpec




(Optional)
Spec holds the desired state of the Task from the client





params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the task. Params
must be supplied as inputs in TaskRuns unless they declare a default
value.




displayName

string



(Optional)
DisplayName is a user-facing name of the task that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the task that may be
used to populate a UI.




steps


[]Step




Steps are the steps of the build; each step is run sequentially with the
source mounted into /workspace.




volumes


[]Kubernetes core/v1.Volume




Volumes is a collection of volumes that are available to mount into the
steps of the build.




stepTemplate


StepTemplate




StepTemplate can be used as the basis for all step containers within the
Task, so that the steps inherit settings on the base container.




sidecars


[]Sidecar




Sidecars are run alongside the Taskâ€™s step containers. They begin before
the steps start and end after the steps complete.




workspaces


[]WorkspaceDeclaration




Workspaces are the volumes that this Task requires.




results


[]TaskResult




Results are values that this Task can output







TaskRun


TaskRun represents a single execution of a Task. TaskRuns are how the steps
specified in a Task are executed; they specify the parameters and resources
used to run the steps in a Task.




Field
Description





apiVersion
string


tekton.dev/v1





kind
string

TaskRun



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


TaskRunSpec




(Optional)





debug


TaskRunDebug




(Optional)




params


Params




(Optional)




serviceAccountName

string



(Optional)




taskRef


TaskRef




(Optional)
no more than one of the TaskRef and TaskSpec may be specified.




taskSpec


TaskSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




status


TaskRunSpecStatus




(Optional)
Used for cancelling a TaskRun (and maybe more later on)




statusMessage


TaskRunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Retries represents how many times this TaskRun should be retried in the event of task failure.




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which one retry attempt times out. Defaults to 1 hour.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




podTemplate


Template




PodTemplate holds pod specific configuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.




stepSpecs


[]TaskRunStepSpec




(Optional)
Specs to apply to Steps in this TaskRun.
If a field is specified in both a Step and a StepSpec,
the value from the StepSpec will be used.
This field is only supported when the alpha feature gate is enabled.




sidecarSpecs


[]TaskRunSidecarSpec




(Optional)
Specs to apply to Sidecars in this TaskRun.
If a field is specified in both a Sidecar and a SidecarSpec,
the value from the SidecarSpec will be used.
This field is only supported when the alpha feature gate is enabled.




computeResources


Kubernetes core/v1.ResourceRequirements




Compute resources to use for this TaskRun







status


TaskRunStatus




(Optional)




Algorithm
(string alias)

Algorithm Standard cryptographic hash algorithm

Artifact


(Appears on:Artifacts, StepState)


TaskRunStepArtifact represents an artifact produced or used by a step within a task run.
It directly uses the Artifact type for its structure.




Field
Description





name

string



The artifactâ€™s identifying category name




values


[]ArtifactValue




A collection of values related to the artifact




buildOutput

bool



Indicate if the artifact is a build output or a by-product




ArtifactValue


(Appears on:Artifact)


ArtifactValue represents a specific value or data element within an Artifact.




Field
Description





digest

map[github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.Algorithm]string







uri

string



Algorithm-specific digests for verifying the content (e.g., SHA256)




Artifacts


(Appears on:TaskRunStatusFields)


Artifacts represents the collection of input and output artifacts associated with
a task run or a similar process. Artifacts in this context are units of data or resources
that the process either consumes as input or produces as output.




Field
Description





inputs


[]Artifact








outputs


[]Artifact








ChildStatusReference


(Appears on:PipelineRunStatusFields)


ChildStatusReference is used to point to the statuses of individual TaskRuns and Runs within this PipelineRun.




Field
Description





name

string



Name is the name of the TaskRun or Run this is referencing.




displayName

string



DisplayName is a user-facing name of the pipelineTask that may be
used to populate a UI.




pipelineTaskName

string



PipelineTaskName is the name of the PipelineTask this is referencing.




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




Combination
(map[string]string alias)

Combination is a map, mainly defined to hold a single combination from a Matrix with key as param.Name and value as param.Value

Combinations
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.Combination alias)

Combinations is a Combination list

EmbeddedTask


(Appears on:PipelineTask)


EmbeddedTask is used to define a Task inline within a Pipelineâ€™s PipelineTasks.




Field
Description





spec

k8s.io/apimachinery/pkg/runtime.RawExtension



(Optional)
Spec is a specification of a custom task





-

[]byte



Raw is the underlying serialization of this object.
TODO: Determine how to detect ContentType and ContentEncoding of â€˜Rawâ€™ data.




-

k8s.io/apimachinery/pkg/runtime.Object



Object can hold a representation of this extension - useful for working with versioned
structs.







metadata


PipelineTaskMetadata




(Optional)




TaskSpec


TaskSpec





(Members of TaskSpec are embedded into this type.)

(Optional)
TaskSpec is a specification of a task




IncludeParams


IncludeParams allows passing in a specific combinations of Parameters into the Matrix.




Field
Description





name

string



Name the specified combination




params


Params




Params takes only Parameters of type "string"
The names of the params must match the names of the params in the underlying Task




Matrix


(Appears on:PipelineTask)


Matrix is used to fan out Tasks in a Pipeline




Field
Description





params


Params




Params is a list of parameters used to fan out the pipelineTask
Params takes only Parameters of type "array"
Each array element is supplied to the PipelineTask by substituting params of type "string" in the underlying Task.
The names of the params in the Matrix must match the names of the params in the underlying Task that they will be substituting.




include


IncludeParamsList




(Optional)
Include is a list of IncludeParams which allows passing in specific combinations of Parameters into the Matrix.




OnErrorType
(string alias)

(Appears on:Step)


OnErrorType defines a list of supported exiting behavior of a container on error




Value
Description


"continue"
Continue indicates continue executing the rest of the steps irrespective of the container exit code

"stopAndFail"
StopAndFail indicates exit the taskRun if the container exits with non-zero exit code



Param


(Appears on:ResolutionRequestSpec)


Param declares an ParamValues to use for the parameter called name.




Field
Description





name

string







value


ParamValue








ParamSpec


ParamSpec defines arbitrary parameters needed beyond typed inputs (such as
resources). Parameter values are provided by users as inputs on a TaskRun
or PipelineRun.




Field
Description





name

string



Name declares the name by which a parameter is referenced.




type


ParamType




(Optional)
Type is the user-specified type of the parameter. The possible types
are currently â€œstringâ€, â€œarrayâ€ and â€œobjectâ€, and â€œstringâ€ is the default.




description

string



(Optional)
Description is a user-facing description of the parameter that may be
used to populate a UI.




properties


map[string]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.PropertySpec




(Optional)
Properties is the JSON Schema properties to support key-value pairs parameter.




default


ParamValue




(Optional)
Default is the value a parameter takes if no input value is supplied. If
default is set, a Task may be executed without a supplied value for the
parameter.




enum

[]string



(Optional)
Enum declares a set of allowed param input values for tasks/pipelines that can be validated.
If Enum is not set, no input validation is performed for the param.




ParamSpecs
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.ParamSpec alias)

(Appears on:PipelineSpec, TaskSpec, StepActionSpec, StepActionSpec)


ParamSpecs is a list of ParamSpec

ParamType
(string alias)

(Appears on:ParamSpec, ParamValue, PropertySpec)


ParamType indicates the type of an input parameter;
Used to distinguish between a single string and an array of strings.




Value
Description


"array"

"object"

"string"



ParamValue


(Appears on:Param, ParamSpec, PipelineResult, PipelineRunResult, TaskResult, TaskRunResult)


ResultValue is a type alias of ParamValue




Field
Description





Type


ParamType








StringVal

string



Represents the stored type of ParamValues.




ArrayVal

[]string







ObjectVal

map[string]string







Params
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.Param alias)

(Appears on:IncludeParams, Matrix, PipelineRunSpec, PipelineTask, ResolverRef, Step, TaskRunInputs, TaskRunSpec)


Params is a list of Param

PipelineRef


(Appears on:PipelineRunSpec, PipelineTask)


PipelineRef can be used to refer to a specific instance of a Pipeline.




Field
Description





name

string



Name of the referent; More info: http://kubernetes.io/docs/user-guide/identifiers#names




apiVersion

string



(Optional)
API version of the referent




ResolverRef


ResolverRef




(Optional)
ResolverRef allows referencing a Pipeline in a remote location
like a git repo. This field is only supported when the alpha
feature gate is enabled.




PipelineResult


(Appears on:PipelineSpec)


PipelineResult used to describe the results of a pipeline




Field
Description





name

string



Name the given name




type


ResultsType




Type is the user-specified type of the result.
The possible types are â€˜stringâ€™, â€˜arrayâ€™, and â€˜objectâ€™, with â€˜stringâ€™ as the default.
â€˜arrayâ€™ and â€˜objectâ€™ types are alpha features.




description

string



(Optional)
Description is a human-readable description of the result




value


ParamValue




Value the expression used to retrieve the value




PipelineRunReason
(string alias)

PipelineRunReason represents a reason for the pipeline run â€œSucceededâ€ condition




Value
Description


"CELEvaluationFailed"
ReasonCELEvaluationFailed indicates the pipeline fails the CEL evaluation

"Cancelled"
PipelineRunReasonCancelled is the reason set when the PipelineRun cancelled by the user
This reason may be found with a corev1.ConditionFalse status, if the cancellation was processed successfully
This reason may be found with a corev1.ConditionUnknown status, if the cancellation is being processed or failed

"CancelledRunningFinally"
PipelineRunReasonCancelledRunningFinally indicates that pipeline has been gracefully cancelled
and no new Tasks will be scheduled by the controller, but final tasks are now running

"Completed"
PipelineRunReasonCompleted is the reason set when the PipelineRun completed successfully with one or more skipped Tasks

"PipelineRunCouldntCancel"
ReasonCouldntCancel indicates that a PipelineRun was cancelled but attempting to update
all of the running TaskRuns as cancelled failed.

"CouldntGetPipeline"
ReasonCouldntGetPipeline indicates that the reason for the failure status is that the
associated Pipeline couldnâ€™t be retrieved

"CouldntGetPipelineResult"
PipelineRunReasonCouldntGetPipelineResult indicates that the pipeline fails to retrieve the
referenced result. This could be due to failed TaskRuns or Runs that were supposed to produce
the results

"CouldntGetTask"
ReasonCouldntGetTask indicates that the reason for the failure status is that the
associated Pipelineâ€™s Tasks couldnâ€™t all be retrieved

"PipelineRunCouldntTimeOut"
ReasonCouldntTimeOut indicates that a PipelineRun was timed out but attempting to update
all of the running TaskRuns as timed out failed.

"CreateRunFailed"
ReasonCreateRunFailed indicates that the pipeline fails to create the taskrun or other run resources

"Failed"
PipelineRunReasonFailed is the reason set when the PipelineRun completed with a failure

"PipelineValidationFailed"
ReasonFailedValidation indicates that the reason for failure status is
that pipelinerun failed runtime validation

"InvalidPipelineResourceBindings"
ReasonInvalidBindings indicates that the reason for the failure status is that the
PipelineResources bound in the PipelineRun didnâ€™t match those declared in the Pipeline

"PipelineInvalidGraph"
ReasonInvalidGraph indicates that the reason for the failure status is that the
associated Pipeline is an invalid graph (a.k.a wrong order, cycle, â€¦)

"InvalidMatrixParameterTypes"
ReasonInvalidMatrixParameterTypes indicates a matrix contains invalid parameter types

"InvalidParamValue"
PipelineRunReasonInvalidParamValue indicates that the PipelineRun Param input value is not allowed.

"InvalidPipelineResultReference"
PipelineRunReasonInvalidPipelineResultReference indicates a pipeline result was declared
by the pipeline but not initialized in the pipelineTask

"InvalidTaskResultReference"
ReasonInvalidTaskResultReference indicates a task result was declared
but was not initialized by that task

"InvalidTaskRunSpecs"
ReasonInvalidTaskRunSpec indicates that PipelineRun.Spec.TaskRunSpecs[].PipelineTaskName is defined with
a not exist taskName in pipelineSpec.

"InvalidWorkspaceBindings"
ReasonInvalidWorkspaceBinding indicates that a Pipeline expects a workspace but a
PipelineRun has provided an invalid binding.

"ObjectParameterMissKeys"
ReasonObjectParameterMissKeys indicates that the object param value provided from PipelineRun spec
misses some keys required for the object param declared in Pipeline spec.

"ParamArrayIndexingInvalid"
ReasonParamArrayIndexingInvalid indicates that the use of param array indexing is out of bound.

"ParameterMissing"
ReasonParameterMissing indicates that the reason for the failure status is that the
associated PipelineRun didnâ€™t provide all the required parameters

"ParameterTypeMismatch"
ReasonParameterTypeMismatch indicates that the reason for the failure status is that
parameter(s) declared in the PipelineRun do not have the some declared type as the
parameters(s) declared in the Pipeline that they are supposed to override.

"PipelineRunPending"
PipelineRunReasonPending is the reason set when the PipelineRun is in the pending state

"RequiredWorkspaceMarkedOptional"
ReasonRequiredWorkspaceMarkedOptional indicates an optional workspace
has been passed to a Task that is expecting a non-optional workspace

"ResolvingPipelineRef"
ReasonResolvingPipelineRef indicates that the PipelineRun is waiting for
its pipelineRef to be asynchronously resolved.

"ResourceVerificationFailed"
ReasonResourceVerificationFailed indicates that the pipeline fails the trusted resource verification,
it could be the content has changed, signature is invalid or public key is invalid

"Running"
PipelineRunReasonRunning is the reason set when the PipelineRun is running

"Started"
PipelineRunReasonStarted is the reason set when the PipelineRun has just started

"StoppedRunningFinally"
PipelineRunReasonStoppedRunningFinally indicates that pipeline has been gracefully stopped
and no new Tasks will be scheduled by the controller, but final tasks are now running

"PipelineRunStopping"
PipelineRunReasonStopping indicates that no new Tasks will be scheduled by the controller, and the
pipeline will stop once all running tasks complete their work

"Succeeded"
PipelineRunReasonSuccessful is the reason set when the PipelineRun completed successfully

"PipelineRunTimeout"
PipelineRunReasonTimedOut is the reason set when the PipelineRun has timed out



PipelineRunResult


(Appears on:PipelineRunStatusFields)


PipelineRunResult used to describe the results of a pipeline




Field
Description





name

string



Name is the resultâ€™s name as declared by the Pipeline




value


ParamValue




Value is the result returned from the execution of this PipelineRun




PipelineRunRunStatus


PipelineRunRunStatus contains the name of the PipelineTask for this Run and the Runâ€™s Status




Field
Description





pipelineTaskName

string



PipelineTaskName is the name of the PipelineTask.




status


CustomRunStatus




(Optional)
Status is the RunStatus for the corresponding Run




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




PipelineRunSpec


(Appears on:PipelineRun)


PipelineRunSpec defines the desired state of PipelineRun




Field
Description





pipelineRef


PipelineRef




(Optional)




pipelineSpec


PipelineSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




params


Params




Params is a list of parameter names and values.




status


PipelineRunSpecStatus




(Optional)
Used for cancelling a pipelinerun (and maybe more later on)




timeouts


TimeoutFields




(Optional)
Time after which the Pipeline times out.
Currently three keys are accepted in the map
pipeline, tasks and finally
with Timeouts.pipeline >= Timeouts.tasks + Timeouts.finally




taskRunTemplate


PipelineTaskRunTemplate




(Optional)
TaskRunTemplate represent template of taskrun




workspaces


[]WorkspaceBinding




(Optional)
Workspaces holds a set of workspace bindings that must match names
with those declared in the pipeline.




taskRunSpecs


[]PipelineTaskRunSpec




(Optional)
TaskRunSpecs holds a set of runtime specs




PipelineRunSpecStatus
(string alias)

(Appears on:PipelineRunSpec)


PipelineRunSpecStatus defines the pipelinerun spec status the user can provide

PipelineRunStatus


(Appears on:PipelineRun)


PipelineRunStatus defines the observed state of PipelineRun




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





PipelineRunStatusFields


PipelineRunStatusFields





(Members of PipelineRunStatusFields are embedded into this type.)

PipelineRunStatusFields inlines the status fields.




PipelineRunStatusFields


(Appears on:PipelineRunStatus)


PipelineRunStatusFields holds the fields of PipelineRunStatusâ€™ status.
This is defined separately and inlined so that other types can readily
consume these fields via duck typing.




Field
Description





startTime


Kubernetes meta/v1.Time




StartTime is the time the PipelineRun is actually started.




completionTime


Kubernetes meta/v1.Time




CompletionTime is the time the PipelineRun completed.




results


[]PipelineRunResult




(Optional)
Results are the list of results written out by the pipeline taskâ€™s containers




pipelineSpec


PipelineSpec




PipelineRunSpec contains the exact spec used to instantiate the run




skippedTasks


[]SkippedTask




(Optional)
list of tasks that were skipped due to when expressions evaluating to false




childReferences


[]ChildStatusReference




(Optional)
list of TaskRun and Run names, PipelineTask names, and API versions/kinds for children of this PipelineRun.




finallyStartTime


Kubernetes meta/v1.Time




(Optional)
FinallyStartTime is when all non-finally tasks have been completed and only finally tasks are being executed.




provenance


Provenance




(Optional)
Provenance contains some key authenticated metadata about how a software artifact was built (what sources, what inputs/outputs, etc.).




spanContext

map[string]string



SpanContext contains tracing span context fields




PipelineRunTaskRunStatus


PipelineRunTaskRunStatus contains the name of the PipelineTask for this TaskRun and the TaskRunâ€™s Status




Field
Description





pipelineTaskName

string



PipelineTaskName is the name of the PipelineTask.




status


TaskRunStatus




(Optional)
Status is the TaskRunStatus for the corresponding TaskRun




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




PipelineSpec


(Appears on:Pipeline, PipelineRunSpec, PipelineRunStatusFields, PipelineTask)


PipelineSpec defines the desired state of Pipeline.




Field
Description





displayName

string



(Optional)
DisplayName is a user-facing name of the pipeline that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the pipeline that may be
used to populate a UI.




tasks


[]PipelineTask




Tasks declares the graph of Tasks that execute when this Pipeline is run.




params


ParamSpecs




Params declares a list of input parameters that must be supplied when
this Pipeline is run.




workspaces


[]PipelineWorkspaceDeclaration




(Optional)
Workspaces declares a set of named workspaces that are expected to be
provided by a PipelineRun.




results


[]PipelineResult




(Optional)
Results are values that this pipeline can output once run




finally


[]PipelineTask




Finally declares the list of Tasks that execute just before leaving the Pipeline
i.e. either after all Tasks are finished executing successfully
or after a failure which would result in ending the Pipeline




PipelineTask


(Appears on:PipelineSpec)


PipelineTask defines a task in a Pipeline, passing inputs from both
Params and from the output of previous tasks.




Field
Description





name

string



Name is the name of this task within the context of a Pipeline. Name is
used as a coordinate with the from and runAfter fields to establish
the execution order of tasks relative to one another.




displayName

string



(Optional)
DisplayName is the display name of this task within the context of a Pipeline.
This display name may be used to populate a UI.




description

string



(Optional)
Description is the description of this task within the context of a Pipeline.
This description may be used to populate a UI.




taskRef


TaskRef




(Optional)
TaskRef is a reference to a task definition.




taskSpec


EmbeddedTask




(Optional)
TaskSpec is a specification of a task
Specifying TaskSpec can be disabled by setting
disable-inline-spec feature flag..




when


WhenExpressions




(Optional)
When is a list of when expressions that need to be true for the task to run




retries

int



(Optional)
Retries represents how many times this task should be retried in case of task failure: ConditionSucceeded set to False




runAfter

[]string



(Optional)
RunAfter is the list of PipelineTask names that should be executed before
this Task executes. (Used to force a specific ordering in graph execution.)




params


Params




(Optional)
Parameters declares parameters passed to this task.




matrix


Matrix




(Optional)
Matrix declares parameters used to fan out this task.




workspaces


[]WorkspacePipelineTaskBinding




(Optional)
Workspaces maps workspaces from the pipeline spec to the workspaces
declared in the Task.




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which the TaskRun times out. Defaults to 1 hour.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




pipelineRef


PipelineRef




(Optional)
PipelineRef is a reference to a pipeline definition
Note: PipelineRef is in preview mode and not yet supported




pipelineSpec


PipelineSpec




(Optional)
PipelineSpec is a specification of a pipeline
Note: PipelineSpec is in preview mode and not yet supported
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




onError


PipelineTaskOnErrorType




(Optional)
OnError defines the exiting behavior of a PipelineRun on error
can be set to [ continue | stopAndFail ]




PipelineTaskMetadata


(Appears on:EmbeddedTask, PipelineTaskRunSpec)


PipelineTaskMetadata contains the labels or annotations for an EmbeddedTask




Field
Description





labels

map[string]string



(Optional)




annotations

map[string]string



(Optional)




PipelineTaskOnErrorType
(string alias)

(Appears on:PipelineTask)


PipelineTaskOnErrorType defines a list of supported failure handling behaviors of a PipelineTask on error




Value
Description


"continue"
PipelineTaskContinue indicates to continue executing the rest of the DAG when the PipelineTask fails

"stopAndFail"
PipelineTaskStopAndFail indicates to stop and fail the PipelineRun if the PipelineTask fails



PipelineTaskParam


PipelineTaskParam is used to provide arbitrary string parameters to a Task.




Field
Description





name

string







value

string







PipelineTaskRun


PipelineTaskRun reports the results of running a step in the Task. Each
task has the potential to succeed or fail (based on the exit code)
and produces logs.




Field
Description





name

string







PipelineTaskRunSpec


(Appears on:PipelineRunSpec)


PipelineTaskRunSpec  can be used to configure specific
specs for a concrete Task




Field
Description





pipelineTaskName

string







serviceAccountName

string







podTemplate


Template








stepSpecs


[]TaskRunStepSpec








sidecarSpecs


[]TaskRunSidecarSpec








metadata


PipelineTaskMetadata




(Optional)




computeResources


Kubernetes core/v1.ResourceRequirements




Compute resources to use for this TaskRun




PipelineTaskRunTemplate


(Appears on:PipelineRunSpec)


PipelineTaskRunTemplate is used to specify run specifications for all Task in pipelinerun.




Field
Description





podTemplate


Template




(Optional)




serviceAccountName

string



(Optional)




PipelineWorkspaceDeclaration


(Appears on:PipelineSpec)


WorkspacePipelineDeclaration creates a named slot in a Pipeline that a PipelineRun
is expected to populate with a workspace binding.
Deprecated: use PipelineWorkspaceDeclaration type instead




Field
Description





name

string



Name is the name of a workspace to be provided by a PipelineRun.




description

string



(Optional)
Description is a human readable string describing how the workspace will be
used in the Pipeline. It can be useful to include a bit of detail about which
tasks are intended to have access to the data on the workspace.




optional

bool



Optional marks a Workspace as not being required in PipelineRuns. By default
this field is false and so declared workspaces are required.




PropertySpec


(Appears on:ParamSpec, StepResult, TaskResult)


PropertySpec defines the struct for object keys




Field
Description





type


ParamType








Provenance


(Appears on:PipelineRunStatusFields, StepState, TaskRunStatusFields)


Provenance contains metadata about resources used in the TaskRun/PipelineRun
such as the source from where a remote build definition was fetched.
This field aims to carry minimum amoumt of metadata in *Run status so that
Tekton Chains can capture them in the provenance.




Field
Description





refSource


RefSource




RefSource identifies the source where a remote task/pipeline came from.




featureFlags

github.com/tektoncd/pipeline/pkg/apis/config.FeatureFlags



FeatureFlags identifies the feature flags that were used during the task/pipeline run




Ref


(Appears on:Step)


Ref can be used to refer to a specific instance of a StepAction.




Field
Description





name

string



Name of the referenced step




ResolverRef


ResolverRef




(Optional)
ResolverRef allows referencing a StepAction in a remote location
like a git repo.




RefSource


(Appears on:Provenance, ResolutionRequestStatusFields, ResolutionRequestStatusFields)


RefSource contains the information that can uniquely identify where a remote
built definition came from i.e. Git repositories, Tekton Bundles in OCI registry
and hub.




Field
Description





uri

string



URI indicates the identity of the source of the build definition.
Example: â€œhttps://github.com/tektoncd/catalogâ€




digest

map[string]string



Digest is a collection of cryptographic digests for the contents of the artifact specified by URI.
Example: {â€œsha1â€: â€œf99d13e554ffcb696dee719fa85b695cb5b0f428â€}




entryPoint

string



EntryPoint identifies the entry point into the build. This is often a path to a
build definition file and/or a target label within that file.
Example: â€œtask/git-clone/0.8/git-clone.yamlâ€




ResolverName
(string alias)

(Appears on:ResolverRef)


ResolverName is the name of a resolver from which a resource can be
requested.

ResolverRef


(Appears on:PipelineRef, Ref, TaskRef)


ResolverRef can be used to refer to a Pipeline or Task in a remote
location like a git repo. This feature is in beta and these fields
are only available when the beta feature gate is enabled.




Field
Description





resolver


ResolverName




(Optional)
Resolver is the name of the resolver that should perform
resolution of the referenced Tekton resource, such as â€œgitâ€.




params


Params




(Optional)
Params contains the parameters used to identify the
referenced Tekton resource. Example entries might include
â€œrepoâ€ or â€œpathâ€ but the set of params ultimately depends on
the chosen resolver.




ResultRef


ResultRef is a type that represents a reference to a task run result




Field
Description





pipelineTask

string







result

string







resultsIndex

int







property

string







ResultsType
(string alias)

(Appears on:PipelineResult, StepResult, TaskResult, TaskRunResult)


ResultsType indicates the type of a result;
Used to distinguish between a single string and an array of strings.
Note that there is ResultType used to find out whether a
RunResult is from a task result or not, which is different from
this ResultsType.




Value
Description


"array"

"object"

"string"



Sidecar


(Appears on:TaskSpec)


Sidecar has nearly the same data structure as Step but does not have the ability to timeout.




Field
Description





name

string



Name of the Sidecar specified as a DNS_LABEL.
Each Sidecar in a Task must have a unique name (DNS_LABEL).
Cannot be updated.




image

string



(Optional)
Image reference name.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the Sidecarâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the Sidecarâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




workingDir

string



(Optional)
Sidecarâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




ports


[]Kubernetes core/v1.ContainerPort




(Optional)
List of ports to expose from the Sidecar. Exposing a port here gives
the system additional information about the network connections a
container uses, but is primarily informational. Not specifying a port here
DOES NOT prevent that port from being exposed. Any port which is
listening on the default â€œ0.0.0.0â€ address inside a container will be
accessible from the network.
Cannot be updated.




envFrom


[]Kubernetes core/v1.EnvFromSource




(Optional)
List of sources to populate environment variables in the Sidecar.
The keys defined within a source must be a C_IDENTIFIER. All invalid keys
will be reported as an event when the container is starting. When a key exists in multiple
sources, the value associated with the last source will take precedence.
Values defined by an Env with a duplicate key will take precedence.
Cannot be updated.




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the Sidecar.
Cannot be updated.




computeResources


Kubernetes core/v1.ResourceRequirements




(Optional)
ComputeResources required by this Sidecar.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Sidecarâ€™s filesystem.
Cannot be updated.




volumeDevices


[]Kubernetes core/v1.VolumeDevice




(Optional)
volumeDevices is the list of block devices to be used by the Sidecar.




livenessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of Sidecar liveness.
Container will be restarted if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes




readinessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of Sidecar service readiness.
Container will be removed from service endpoints if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes




startupProbe


Kubernetes core/v1.Probe




(Optional)
StartupProbe indicates that the Pod the Sidecar is running in has successfully initialized.
If specified, no other probes are executed until this completes successfully.
If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
This can be used to provide different probe parameters at the beginning of a Podâ€™s lifecycle,
when it might take a long time to load data or warm a cache, than during steady-state operation.
This cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes




lifecycle


Kubernetes core/v1.Lifecycle




(Optional)
Actions that the management system should take in response to Sidecar lifecycle events.
Cannot be updated.




terminationMessagePath

string



(Optional)
Optional: Path at which the file to which the Sidecarâ€™s termination message
will be written is mounted into the Sidecarâ€™s filesystem.
Message written is intended to be brief final status, such as an assertion failure message.
Will be truncated by the node if greater than 4096 bytes. The total message length across
all containers will be limited to 12kb.
Defaults to /dev/termination-log.
Cannot be updated.




terminationMessagePolicy


Kubernetes core/v1.TerminationMessagePolicy




(Optional)
Indicate how the termination message should be populated. File will use the contents of
terminationMessagePath to populate the Sidecar status message on both success and failure.
FallbackToLogsOnError will use the last chunk of Sidecar log output if the termination
message file is empty and the Sidecar exited with an error.
The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
Defaults to File.
Cannot be updated.




imagePullPolicy


Kubernetes core/v1.PullPolicy




(Optional)
Image pull policy.
One of Always, Never, IfNotPresent.
Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/containers/images#updating-images




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Sidecar should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/




stdin

bool



(Optional)
Whether this Sidecar should allocate a buffer for stdin in the container runtime. If this
is not set, reads from stdin in the Sidecar will always result in EOF.
Default is false.




stdinOnce

bool



(Optional)
Whether the container runtime should close the stdin channel after it has been opened by
a single attach. When stdin is true the stdin stream will remain open across multiple attach
sessions. If stdinOnce is set to true, stdin is opened on Sidecar start, is empty until the
first client attaches to stdin, and then remains open and accepts data until the client disconnects,
at which time stdin is closed and remains closed until the Sidecar is restarted. If this
flag is false, a container processes that reads from stdin will never receive an EOF.
Default is false




tty

bool



(Optional)
Whether this Sidecar should allocate a TTY for itself, also requires â€˜stdinâ€™ to be true.
Default is false.




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command or Args.




workspaces


[]WorkspaceUsage




(Optional)
This is an alpha field. You must set the â€œenable-api-fieldsâ€ feature flag to â€œalphaâ€
for this field to be supported.
Workspaces is a list of workspaces from the Task that this Sidecar wants
exclusive access to. Adding a workspace to this list means that any
other Step or Sidecar that does not also request this Workspace will
not have access to it.




restartPolicy


Kubernetes core/v1.ContainerRestartPolicy




(Optional)
RestartPolicy refers to kubernetes RestartPolicy. It can only be set for an
initContainer and must have itâ€™s policy set to â€œAlwaysâ€. It is currently
left optional to help support Kubernetes versions prior to 1.29 when this feature
was introduced.




SidecarState


(Appears on:TaskRunStatusFields)


SidecarState reports the results of running a sidecar in a Task.




Field
Description





ContainerState


Kubernetes core/v1.ContainerState





(Members of ContainerState are embedded into this type.)





name

string







container

string







imageID

string







SkippedTask


(Appears on:PipelineRunStatusFields)


SkippedTask is used to describe the Tasks that were skipped due to their When Expressions
evaluating to False. This is a struct because we are looking into including more details
about the When Expressions that caused this Task to be skipped.




Field
Description





name

string



Name is the Pipeline Task name




reason


SkippingReason




Reason is the cause of the PipelineTask being skipped.




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




SkippingReason
(string alias)

(Appears on:SkippedTask)


SkippingReason explains why a PipelineTask was skipped.




Value
Description


"Matrix Parameters have an empty array"
EmptyArrayInMatrixParams means the task was skipped because Matrix parameters contain empty array.

"PipelineRun Finally timeout has been reached"
FinallyTimedOutSkip means the task was skipped because the PipelineRun has passed its Timeouts.Finally.

"PipelineRun was gracefully cancelled"
GracefullyCancelledSkip means the task was skipped because the pipeline run has been gracefully cancelled

"PipelineRun was gracefully stopped"
GracefullyStoppedSkip means the task was skipped because the pipeline run has been gracefully stopped

"Results were missing"
MissingResultsSkip means the task was skipped because itâ€™s missing necessary results

"None"
None means the task was not skipped

"Parent Tasks were skipped"
ParentTasksSkip means the task was skipped because its parent was skipped

"PipelineRun timeout has been reached"
PipelineTimedOutSkip means the task was skipped because the PipelineRun has passed its overall timeout.

"PipelineRun was stopping"
StoppingSkip means the task was skipped because the pipeline run is stopping

"PipelineRun Tasks timeout has been reached"
TasksTimedOutSkip means the task was skipped because the PipelineRun has passed its Timeouts.Tasks.

"When Expressions evaluated to false"
WhenExpressionsSkip means the task was skipped due to at least one of its when expressions evaluating to false



Step


(Appears on:TaskSpec)


Step runs a subcomponent of a Task




Field
Description





name

string



Name of the Step specified as a DNS_LABEL.
Each Step in a Task must have a unique name.




image

string



(Optional)
Docker image name.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




envFrom


[]Kubernetes core/v1.EnvFromSource




(Optional)
List of sources to populate environment variables in the Step.
The keys defined within a source must be a C_IDENTIFIER. All invalid keys
will be reported as an event when the Step is starting. When a key exists in multiple
sources, the value associated with the last source will take precedence.
Values defined by an Env with a duplicate key will take precedence.
Cannot be updated.




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the Step.
Cannot be updated.




computeResources


Kubernetes core/v1.ResourceRequirements




(Optional)
ComputeResources required by this Step.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.




volumeDevices


[]Kubernetes core/v1.VolumeDevice




(Optional)
volumeDevices is the list of block devices to be used by the Step.




imagePullPolicy


Kubernetes core/v1.PullPolicy




(Optional)
Image pull policy.
One of Always, Never, IfNotPresent.
Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/containers/images#updating-images




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command and the Args will be passed to the Script.




timeout


Kubernetes meta/v1.Duration




(Optional)
Timeout is the time after which the step times out. Defaults to never.
Refer to Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




workspaces


[]WorkspaceUsage




(Optional)
This is an alpha field. You must set the â€œenable-api-fieldsâ€ feature flag to â€œalphaâ€
for this field to be supported.
Workspaces is a list of workspaces from the Task that this Step wants
exclusive access to. Adding a workspace to this list means that any
other Step or Sidecar that does not also request this Workspace will
not have access to it.




onError


OnErrorType




OnError defines the exiting behavior of a container on error
can be set to [ continue | stopAndFail ]




stdoutConfig


StepOutputConfig




(Optional)
Stores configuration for the stdout stream of the step.




stderrConfig


StepOutputConfig




(Optional)
Stores configuration for the stderr stream of the step.




ref


Ref




(Optional)
Contains the reference to an existing StepAction.




params


Params




(Optional)
Params declares parameters passed to this step action.




results


[]StepResult




(Optional)
Results declares StepResults produced by the Step.
This is field is at an ALPHA stability level and gated by â€œenable-step-actionsâ€ feature flag.
It can be used in an inlined Step when used to store Results to $(step.results.resultName.path).
It cannot be used when referencing StepActions using [v1.Step.Ref].
The Results declared by the StepActions will be stored here instead.




when


WhenExpressions




(Optional)
When is a list of when expressions that need to be true for the task to run




StepOutputConfig


(Appears on:Step)


StepOutputConfig stores configuration for a step output stream.




Field
Description





path

string



(Optional)
Path to duplicate stdout stream to on containerâ€™s local filesystem.




StepResult


(Appears on:Step, StepActionSpec, Step, StepActionSpec)


StepResult used to describe the Results of a Step.
This is field is at an BETA stability level and gated by â€œenable-step-actionsâ€ feature flag.




Field
Description





name

string



Name the given name




type


ResultsType




(Optional)
The possible types are â€˜stringâ€™, â€˜arrayâ€™, and â€˜objectâ€™, with â€˜stringâ€™ as the default.




properties


map[string]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.PropertySpec




(Optional)
Properties is the JSON Schema properties to support key-value pairs results.




description

string



(Optional)
Description is a human-readable description of the result




StepState


(Appears on:TaskRunStatusFields)


StepState reports the results of running a step in a Task.




Field
Description





ContainerState


Kubernetes core/v1.ContainerState





(Members of ContainerState are embedded into this type.)





name

string







container

string







imageID

string







results


[]TaskRunResult








provenance


Provenance








terminationReason

string







inputs


[]Artifact








outputs


[]Artifact








StepTemplate


(Appears on:TaskSpec)


StepTemplate is a template for a Step




Field
Description





image

string



(Optional)
Image reference name.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the Stepâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the Stepâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




envFrom


[]Kubernetes core/v1.EnvFromSource




(Optional)
List of sources to populate environment variables in the Step.
The keys defined within a source must be a C_IDENTIFIER. All invalid keys
will be reported as an event when the Step is starting. When a key exists in multiple
sources, the value associated with the last source will take precedence.
Values defined by an Env with a duplicate key will take precedence.
Cannot be updated.




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the Step.
Cannot be updated.




computeResources


Kubernetes core/v1.ResourceRequirements




(Optional)
ComputeResources required by this Step.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.




volumeDevices


[]Kubernetes core/v1.VolumeDevice




(Optional)
volumeDevices is the list of block devices to be used by the Step.




imagePullPolicy


Kubernetes core/v1.PullPolicy




(Optional)
Image pull policy.
One of Always, Never, IfNotPresent.
Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/containers/images#updating-images




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/




TaskBreakpoints


(Appears on:TaskRunDebug)


TaskBreakpoints defines the breakpoint config for a particular Task




Field
Description





onFailure

string



(Optional)
if enabled, pause TaskRun on failure of a step
failed step will not exit




beforeSteps

[]string



(Optional)




TaskKind
(string alias)

(Appears on:TaskRef)


TaskKind defines the type of Task used by the pipeline.




Value
Description


"ClusterTask"
ClusterTaskRefKind is the task type for a reference to a task with cluster scope.
ClusterTasks are not supported in v1, but v1 types may reference ClusterTasks.

"Task"
NamespacedTaskKind indicates that the task type has a namespaced scope.



TaskRef


(Appears on:PipelineTask, TaskRunSpec)


TaskRef can be used to refer to a specific instance of a task.




Field
Description





name

string



Name of the referent; More info: http://kubernetes.io/docs/user-guide/identifiers#names




kind


TaskKind




TaskKind indicates the Kind of the Task:
1. Namespaced Task when Kind is set to â€œTaskâ€. If Kind is â€œâ€, it defaults to â€œTaskâ€.
2. Custom Task when Kind is non-empty and APIVersion is non-empty




apiVersion

string



(Optional)
API version of the referent
Note: A Task with non-empty APIVersion and Kind is considered a Custom Task




ResolverRef


ResolverRef




(Optional)
ResolverRef allows referencing a Task in a remote location
like a git repo. This field is only supported when the alpha
feature gate is enabled.




TaskResult


(Appears on:TaskSpec)


TaskResult used to describe the results of a task




Field
Description





name

string



Name the given name




type


ResultsType




(Optional)
Type is the user-specified type of the result. The possible type
is currently â€œstringâ€ and will support â€œarrayâ€ in following work.




properties


map[string]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.PropertySpec




(Optional)
Properties is the JSON Schema properties to support key-value pairs results.




description

string



(Optional)
Description is a human-readable description of the result




value


ParamValue




(Optional)
Value the expression used to retrieve the value of the result from an underlying Step.




TaskRunDebug


(Appears on:TaskRunSpec)


TaskRunDebug defines the breakpoint config for a particular TaskRun




Field
Description





breakpoints


TaskBreakpoints




(Optional)




TaskRunInputs


TaskRunInputs holds the input values that this task was invoked with.




Field
Description





params


Params




(Optional)




TaskRunReason
(string alias)

TaskRunReason is an enum used to store all TaskRun reason for
the Succeeded condition that are controlled by the TaskRun itself. Failure
reasons that emerge from underlying resources are not included here




Value
Description


"TaskRunCancelled"
TaskRunReasonCancelled is the reason set when the TaskRun is cancelled by the user

"Failed"
TaskRunReasonFailed is the reason set when the TaskRun completed with a failure

"TaskRunResolutionFailed"
TaskRunReasonFailedResolution indicated that the reason for failure status is
that references within the TaskRun could not be resolved

"TaskRunValidationFailed"
TaskRunReasonFailedValidation indicated that the reason for failure status is
that taskrun failed runtime validation

"FailureIgnored"
TaskRunReasonFailureIgnored is the reason set when the Taskrun has failed due to pod execution error and the failure is ignored for the owning PipelineRun.
TaskRuns failed due to reconciler/validation error should not use this reason.

"TaskRunImagePullFailed"
TaskRunReasonImagePullFailed is the reason set when the step of a task fails due to image not being pulled

"InvalidParamValue"
TaskRunReasonInvalidParamValue indicates that the TaskRun Param input value is not allowed.

"ResourceVerificationFailed"
TaskRunReasonResourceVerificationFailed indicates that the task fails the trusted resource verification,
it could be the content has changed, signature is invalid or public key is invalid

"TaskRunResultLargerThanAllowedLimit"
TaskRunReasonResultLargerThanAllowedLimit is the reason set when one of the results exceeds its maximum allowed limit of 1 KB

"Running"
TaskRunReasonRunning is the reason set when the TaskRun is running

"Started"
TaskRunReasonStarted is the reason set when the TaskRun has just started

"TaskRunStopSidecarFailed"
TaskRunReasonStopSidecarFailed indicates that the sidecar is not properly stopped.

"Succeeded"
TaskRunReasonSuccessful is the reason set when the TaskRun completed successfully

"TaskValidationFailed"
TaskRunReasonTaskFailedValidation indicated that the reason for failure status is
that task failed runtime validation

"TaskRunTimeout"
TaskRunReasonTimedOut is the reason set when one TaskRun execution has timed out

"ToBeRetried"
TaskRunReasonToBeRetried is the reason set when the last TaskRun execution failed, and will be retried



TaskRunResult


(Appears on:StepState, TaskRunStatusFields)


TaskRunStepResult is a type alias of TaskRunResult




Field
Description





name

string



Name the given name




type


ResultsType




(Optional)
Type is the user-specified type of the result. The possible type
is currently â€œstringâ€ and will support â€œarrayâ€ in following work.




value


ParamValue




Value the given value of the result




TaskRunSidecarSpec


(Appears on:PipelineTaskRunSpec, TaskRunSpec)


TaskRunSidecarSpec is used to override the values of a Sidecar in the corresponding Task.




Field
Description





name

string



The name of the Sidecar to override.




computeResources


Kubernetes core/v1.ResourceRequirements




The resource requirements to apply to the Sidecar.




TaskRunSpec


(Appears on:TaskRun)


TaskRunSpec defines the desired state of TaskRun




Field
Description





debug


TaskRunDebug




(Optional)




params


Params




(Optional)




serviceAccountName

string



(Optional)




taskRef


TaskRef




(Optional)
no more than one of the TaskRef and TaskSpec may be specified.




taskSpec


TaskSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




status


TaskRunSpecStatus




(Optional)
Used for cancelling a TaskRun (and maybe more later on)




statusMessage


TaskRunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Retries represents how many times this TaskRun should be retried in the event of task failure.




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which one retry attempt times out. Defaults to 1 hour.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




podTemplate


Template




PodTemplate holds pod specific configuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.




stepSpecs


[]TaskRunStepSpec




(Optional)
Specs to apply to Steps in this TaskRun.
If a field is specified in both a Step and a StepSpec,
the value from the StepSpec will be used.
This field is only supported when the alpha feature gate is enabled.




sidecarSpecs


[]TaskRunSidecarSpec




(Optional)
Specs to apply to Sidecars in this TaskRun.
If a field is specified in both a Sidecar and a SidecarSpec,
the value from the SidecarSpec will be used.
This field is only supported when the alpha feature gate is enabled.




computeResources


Kubernetes core/v1.ResourceRequirements




Compute resources to use for this TaskRun




TaskRunSpecStatus
(string alias)

(Appears on:TaskRunSpec)


TaskRunSpecStatus defines the TaskRun spec status the user can provide

TaskRunSpecStatusMessage
(string alias)

(Appears on:TaskRunSpec)


TaskRunSpecStatusMessage defines human readable status messages for the TaskRun.




Value
Description


"TaskRun cancelled as the PipelineRun it belongs to has been cancelled."
TaskRunCancelledByPipelineMsg indicates that the PipelineRun of which this
TaskRun was a part of has been cancelled.

"TaskRun cancelled as the PipelineRun it belongs to has timed out."
TaskRunCancelledByPipelineTimeoutMsg indicates that the TaskRun was cancelled because the PipelineRun running it timed out.



TaskRunStatus


(Appears on:TaskRun, PipelineRunTaskRunStatus, TaskRunStatusFields)


TaskRunStatus defines the observed state of TaskRun




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





TaskRunStatusFields


TaskRunStatusFields





(Members of TaskRunStatusFields are embedded into this type.)

TaskRunStatusFields inlines the status fields.




TaskRunStatusFields


(Appears on:TaskRunStatus)


TaskRunStatusFields holds the fields of TaskRunâ€™s status.  This is defined
separately and inlined so that other types can readily consume these fields
via duck typing.




Field
Description





podName

string



PodName is the name of the pod responsible for executing this taskâ€™s steps.




startTime


Kubernetes meta/v1.Time




StartTime is the time the build is actually started.




completionTime


Kubernetes meta/v1.Time




CompletionTime is the time the build completed.




steps


[]StepState




(Optional)
Steps describes the state of each build step container.




retriesStatus


[]TaskRunStatus




(Optional)
RetriesStatus contains the history of TaskRunStatus in case of a retry in order to keep record of failures.
All TaskRunStatus stored in RetriesStatus will have no date within the RetriesStatus as is redundant.




results


[]TaskRunResult




(Optional)
Results are the list of results written out by the taskâ€™s containers




artifacts


Artifacts




(Optional)
Artifacts are the list of artifacts written out by the taskâ€™s containers




sidecars


[]SidecarState




The list has one entry per sidecar in the manifest. Each entry is
represents the imageid of the corresponding sidecar.




taskSpec


TaskSpec




TaskSpec contains the Spec from the dereferenced Task definition used to instantiate this TaskRun.




provenance


Provenance




(Optional)
Provenance contains some key authenticated metadata about how a software artifact was built (what sources, what inputs/outputs, etc.).




spanContext

map[string]string



SpanContext contains tracing span context fields




TaskRunStepSpec


(Appears on:PipelineTaskRunSpec, TaskRunSpec)


TaskRunStepSpec is used to override the values of a Step in the corresponding Task.




Field
Description





name

string



The name of the Step to override.




computeResources


Kubernetes core/v1.ResourceRequirements




The resource requirements to apply to the Step.




TaskSpec


(Appears on:Task, EmbeddedTask, TaskRunSpec, TaskRunStatusFields)


TaskSpec defines the desired state of Task.




Field
Description





params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the task. Params
must be supplied as inputs in TaskRuns unless they declare a default
value.




displayName

string



(Optional)
DisplayName is a user-facing name of the task that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the task that may be
used to populate a UI.




steps


[]Step




Steps are the steps of the build; each step is run sequentially with the
source mounted into /workspace.




volumes


[]Kubernetes core/v1.Volume




Volumes is a collection of volumes that are available to mount into the
steps of the build.




stepTemplate


StepTemplate




StepTemplate can be used as the basis for all step containers within the
Task, so that the steps inherit settings on the base container.




sidecars


[]Sidecar




Sidecars are run alongside the Taskâ€™s step containers. They begin before
the steps start and end after the steps complete.




workspaces


[]WorkspaceDeclaration




Workspaces are the volumes that this Task requires.




results


[]TaskResult




Results are values that this Task can output




TimeoutFields


(Appears on:PipelineRunSpec)


TimeoutFields allows granular specification of pipeline, task, and finally timeouts




Field
Description





pipeline


Kubernetes meta/v1.Duration




Pipeline sets the maximum allowed duration for execution of the entire pipeline. The sum of individual timeouts for tasks and finally must not exceed this value.




tasks


Kubernetes meta/v1.Duration




Tasks sets the maximum allowed duration of this pipelineâ€™s tasks




finally


Kubernetes meta/v1.Duration




Finally sets the maximum allowed duration of this pipelineâ€™s finally




WhenExpression


(Appears on:ChildStatusReference, PipelineRunRunStatus, PipelineRunTaskRunStatus, SkippedTask)


WhenExpression allows a PipelineTask to declare expressions to be evaluated before the Task is run
to determine whether the Task should be executed or skipped




Field
Description





input

string



Input is the string for guard checking which can be a static input or an output from a parent Task




operator

k8s.io/apimachinery/pkg/selection.Operator



Operator that represents an Inputâ€™s relationship to the values




values

[]string



Values is an array of strings, which is compared against the input, for guard checking
It must be non-empty




cel

string



(Optional)
CEL is a string of Common Language Expression, which can be used to conditionally execute
the task based on the result of the expression evaluation
More info about CEL syntax: https://github.com/google/cel-spec/blob/master/doc/langdef.md




WhenExpressions
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1.WhenExpression alias)

(Appears on:PipelineTask, Step)


WhenExpressions are used to specify whether a Task should be executed or skipped
All of them need to evaluate to True for a guarded Task to be executed.

WorkspaceBinding


(Appears on:PipelineRunSpec, TaskRunSpec)


WorkspaceBinding maps a Taskâ€™s declared workspace to a Volume.




Field
Description





name

string



Name is the name of the workspace populated by the volume.




subPath

string



(Optional)
SubPath is optionally a directory on the volume which should be used
for this binding (i.e. the volume will be mounted at this sub directory).




volumeClaimTemplate


Kubernetes core/v1.PersistentVolumeClaim




(Optional)
VolumeClaimTemplate is a template for a claim that will be created in the same namespace.
The PipelineRun controller is responsible for creating a unique claim for each instance of PipelineRun.




persistentVolumeClaim


Kubernetes core/v1.PersistentVolumeClaimVolumeSource




(Optional)
PersistentVolumeClaimVolumeSource represents a reference to a
PersistentVolumeClaim in the same namespace. Either this OR EmptyDir can be used.




emptyDir


Kubernetes core/v1.EmptyDirVolumeSource




(Optional)
EmptyDir represents a temporary directory that shares a Taskâ€™s lifetime.
More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
Either this OR PersistentVolumeClaim can be used.




configMap


Kubernetes core/v1.ConfigMapVolumeSource




(Optional)
ConfigMap represents a configMap that should populate this workspace.




secret


Kubernetes core/v1.SecretVolumeSource




(Optional)
Secret represents a secret that should populate this workspace.




projected


Kubernetes core/v1.ProjectedVolumeSource




(Optional)
Projected represents a projected volume that should populate this workspace.




csi


Kubernetes core/v1.CSIVolumeSource




(Optional)
CSI (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers.




WorkspaceDeclaration


(Appears on:TaskSpec)


WorkspaceDeclaration is a declaration of a volume that a Task requires.




Field
Description





name

string



Name is the name by which you can bind the volume at runtime.




description

string



(Optional)
Description is an optional human readable description of this volume.




mountPath

string



(Optional)
MountPath overrides the directory that the volume will be made available at.




readOnly

bool



ReadOnly dictates whether a mounted volume is writable. By default this
field is false and so mounted volumes are writable.




optional

bool



Optional marks a Workspace as not being required in TaskRuns. By default
this field is false and so declared workspaces are required.




WorkspacePipelineTaskBinding


(Appears on:PipelineTask)


WorkspacePipelineTaskBinding describes how a workspace passed into the pipeline should be
mapped to a taskâ€™s declared workspace.




Field
Description





name

string



Name is the name of the workspace as declared by the task




workspace

string



(Optional)
Workspace is the name of the workspace declared by the pipeline




subPath

string



(Optional)
SubPath is optionally a directory on the volume which should be used
for this binding (i.e. the volume will be mounted at this sub directory).




WorkspaceUsage


(Appears on:Sidecar, Step)


WorkspaceUsage is used by a Step or Sidecar to declare that it wants isolated access
to a Workspace defined in a Task.




Field
Description





name

string



Name is the name of the workspace this Step or Sidecar wants access to.




mountPath

string



MountPath is the path that the workspace should be mounted to inside the Step or Sidecar,
overriding any MountPath specified in the Taskâ€™s WorkspaceDeclaration.





tekton.dev/v1alpha1

Package v1alpha1 contains API Schema definitions for the pipeline v1alpha1 API group

Resource Types:

Run

StepAction

VerificationPolicy

PipelineResource

Run


Run represents a single execution of a Custom Task.




Field
Description





apiVersion
string


tekton.dev/v1alpha1





kind
string

Run



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


RunSpec




(Optional)





ref


TaskRef




(Optional)




spec


EmbeddedRunSpec




(Optional)
Spec is a specification of a custom task








params


Params




(Optional)




status


RunSpecStatus




(Optional)
Used for cancelling a run (and maybe more later on)




statusMessage


RunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Used for propagating retries count to custom tasks




serviceAccountName

string



(Optional)




podTemplate


Template




(Optional)
PodTemplate holds pod specific configuration




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which the custom-task times out.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.







status


RunStatus




(Optional)




StepAction


StepAction represents the actionable components of Step.
The Step can only reference it from the cluster or using remote resolution.




Field
Description





apiVersion
string


tekton.dev/v1alpha1





kind
string

StepAction



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


StepActionSpec




(Optional)
Spec holds the desired state of the Step from the client





description

string



(Optional)
Description is a user-facing description of the stepaction that may be
used to populate a UI.




image

string



(Optional)
Image reference name to run for this StepAction.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the container.
Cannot be updated.




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command and the Args will be passed to the Script.




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the stepAction.
Params must be supplied as inputs in Steps unless they declare a defaultvalue.




results


[]StepResult




(Optional)
Results are values that this StepAction can output




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
The value set in StepAction will take precedence over the value from Task.




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.







VerificationPolicy


VerificationPolicy defines the rules to verify Tekton resources.
VerificationPolicy can config the mapping from resources to a list of public
keys, so when verifying the resources we can use the corresponding public keys.




Field
Description





apiVersion
string


tekton.dev/v1alpha1





kind
string

VerificationPolicy



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


VerificationPolicySpec




Spec holds the desired state of the VerificationPolicy.





resources


[]ResourcePattern




Resources defines the patterns of resources sources that should be subject to this policy.
For example, we may want to apply this Policy from a certain GitHub repo.
Then the ResourcesPattern should be valid regex. E.g. If using gitresolver, and we want to config keys from a certain git repo.
ResourcesPattern can be https://github.com/tektoncd/catalog.git, we will use regex to filter out those resources.




authorities


[]Authority




Authorities defines the rules for validating signatures.




mode


ModeType




(Optional)
Mode controls whether a failing policy will fail the taskrun/pipelinerun, or only log the warnings
enforce - fail the taskrun/pipelinerun if verification fails (default)
warn - donâ€™t fail the taskrun/pipelinerun if verification fails but log warnings







PipelineResource


PipelineResource describes a resource that is an input to or output from a
Task.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





apiVersion
string


tekton.dev/v1alpha1





kind
string

PipelineResource



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


PipelineResourceSpec




Spec holds the desired state of the PipelineResource from the client





description

string



(Optional)
Description is a user-facing description of the resource that may be
used to populate a UI.




type

string







params


[]ResourceParam








secrets


[]SecretParam




(Optional)
Secrets to fetch to populate some of resource fields







status


PipelineResourceStatus




(Optional)
Status is used to communicate the observed state of the PipelineResource from
the controller, but was unused as there is no controller for PipelineResource.




Authority


(Appears on:VerificationPolicySpec)


The Authority block defines the keys for validating signatures.




Field
Description





name

string



Name is the name for this authority.




key


KeyRef




Key contains the public key to validate the resource.




EmbeddedRunSpec


(Appears on:RunSpec)


EmbeddedRunSpec allows custom task definitions to be embedded




Field
Description





metadata


PipelineTaskMetadata




(Optional)




spec

k8s.io/apimachinery/pkg/runtime.RawExtension



(Optional)
Spec is a specification of a custom task





-

[]byte



Raw is the underlying serialization of this object.
TODO: Determine how to detect ContentType and ContentEncoding of â€˜Rawâ€™ data.




-

k8s.io/apimachinery/pkg/runtime.Object



Object can hold a representation of this extension - useful for working with versioned
structs.







HashAlgorithm
(string alias)

(Appears on:KeyRef)


HashAlgorithm defines the hash algorithm used for the public key

KeyRef


(Appears on:Authority)


KeyRef defines the reference to a public key




Field
Description





secretRef


Kubernetes core/v1.SecretReference




(Optional)
SecretRef sets a reference to a secret with the key.




data

string



(Optional)
Data contains the inline public key.




kms

string



(Optional)
KMS contains the KMS url of the public key
Supported formats differ based on the KMS system used.
One example of a KMS url could be:
gcpkms://projects/[PROJECT]/locations/[LOCATION]>/keyRings/[KEYRING]/cryptoKeys/[KEY]/cryptoKeyVersions/[KEY_VERSION]
For more examples please refer https://docs.sigstore.dev/cosign/kms_support.
Note that the KMS is not supported yet.




hashAlgorithm


HashAlgorithm




(Optional)
HashAlgorithm always defaults to sha256 if the algorithm hasnâ€™t been explicitly set




ModeType
(string alias)

(Appears on:VerificationPolicySpec)


ModeType indicates the type of a mode for VerificationPolicy

ResourcePattern


(Appears on:VerificationPolicySpec)


ResourcePattern defines the pattern of the resource source




Field
Description





pattern

string



Pattern defines a resource pattern. Regex is created to filter resources based on Pattern
Example patterns:
GitHub resource: https://github.com/tektoncd/catalog.git, https://github.com/tektoncd/*
Bundle resource: gcr.io/tekton-releases/catalog/upstream/git-clone, gcr.io/tekton-releases/catalog/upstream/*
Hub resource: https://artifacthub.io/*,




RunReason
(string alias)

RunReason is an enum used to store all Run reason for the Succeeded condition that are controlled by the Run itself.

RunSpec


(Appears on:Run)


RunSpec defines the desired state of Run




Field
Description





ref


TaskRef




(Optional)




spec


EmbeddedRunSpec




(Optional)
Spec is a specification of a custom task








params


Params




(Optional)




status


RunSpecStatus




(Optional)
Used for cancelling a run (and maybe more later on)




statusMessage


RunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Used for propagating retries count to custom tasks




serviceAccountName

string



(Optional)




podTemplate


Template




(Optional)
PodTemplate holds pod specific configuration




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which the custom-task times out.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.




RunSpecStatus
(string alias)

(Appears on:RunSpec)


RunSpecStatus defines the taskrun spec status the user can provide

RunSpecStatusMessage
(string alias)

(Appears on:RunSpec)


RunSpecStatusMessage defines human readable status messages for the TaskRun.

StepActionObject


StepActionObject is implemented by StepAction

StepActionSpec


(Appears on:StepAction)


StepActionSpec contains the actionable components of a step.




Field
Description





description

string



(Optional)
Description is a user-facing description of the stepaction that may be
used to populate a UI.




image

string



(Optional)
Image reference name to run for this StepAction.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the container.
Cannot be updated.




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command and the Args will be passed to the Script.




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the stepAction.
Params must be supplied as inputs in Steps unless they declare a defaultvalue.




results


[]StepResult




(Optional)
Results are values that this StepAction can output




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
The value set in StepAction will take precedence over the value from Task.




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.




VerificationPolicySpec


(Appears on:VerificationPolicy)


VerificationPolicySpec defines the patterns and authorities.




Field
Description





resources


[]ResourcePattern




Resources defines the patterns of resources sources that should be subject to this policy.
For example, we may want to apply this Policy from a certain GitHub repo.
Then the ResourcesPattern should be valid regex. E.g. If using gitresolver, and we want to config keys from a certain git repo.
ResourcesPattern can be https://github.com/tektoncd/catalog.git, we will use regex to filter out those resources.




authorities


[]Authority




Authorities defines the rules for validating signatures.




mode


ModeType




(Optional)
Mode controls whether a failing policy will fail the taskrun/pipelinerun, or only log the warnings
enforce - fail the taskrun/pipelinerun if verification fails (default)
warn - donâ€™t fail the taskrun/pipelinerun if verification fails but log warnings




PipelineResourceSpec


(Appears on:PipelineResource, PipelineResourceBinding)


PipelineResourceSpec defines an individual resources used in the pipeline.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





description

string



(Optional)
Description is a user-facing description of the resource that may be
used to populate a UI.




type

string







params


[]ResourceParam








secrets


[]SecretParam




(Optional)
Secrets to fetch to populate some of resource fields




PipelineResourceStatus


(Appears on:PipelineResource)


PipelineResourceStatus does not contain anything because PipelineResources on their own
do not have a status
Deprecated: Unused, preserved only for backwards compatibility

ResourceDeclaration


(Appears on:TaskResource)


ResourceDeclaration defines an input or output PipelineResource declared as a requirement
by another type such as a Task or Condition. The Name field will be used to refer to these
PipelineResources within the typeâ€™s definition, and when provided as an Input, the Name will be the
path to the volume mounted containing this PipelineResource as an input (e.g.
an input Resource named workspace will be mounted at /workspace).
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





name

string



Name declares the name by which a resource is referenced in the
definition. Resources may be referenced by name in the definition of a
Taskâ€™s steps.




type

string



Type is the type of this resource;




description

string



(Optional)
Description is a user-facing description of the declared resource that may be
used to populate a UI.




targetPath

string



(Optional)
TargetPath is the path in workspace directory where the resource
will be copied.




optional

bool



Optional declares the resource as optional.
By default optional is set to false which makes a resource required.
optional: true - the resource is considered optional
optional: false - the resource is considered required (equivalent of not specifying it)




ResourceParam


(Appears on:PipelineResourceSpec)


ResourceParam declares a string value to use for the parameter called Name, and is used in
the specific context of PipelineResources.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





name

string







value

string







SecretParam


(Appears on:PipelineResourceSpec)


SecretParam indicates which secret can be used to populate a field of the resource
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





fieldName

string







secretKey

string







secretName

string







RunResult


(Appears on:RunStatusFields)


RunResult used to describe the results of a task




Field
Description





name

string



Name the given name




value

string



Value the given value of the result




RunStatus


(Appears on:Run, RunStatusFields)


RunStatus defines the observed state of Run




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





RunStatusFields


RunStatusFields





(Members of RunStatusFields are embedded into this type.)

RunStatusFields inlines the status fields.




RunStatusFields


(Appears on:RunStatus)


RunStatusFields holds the fields of Runâ€™s status.  This is defined
separately and inlined so that other types can readily consume these fields
via duck typing.




Field
Description





startTime


Kubernetes meta/v1.Time




(Optional)
StartTime is the time the build is actually started.




completionTime


Kubernetes meta/v1.Time




(Optional)
CompletionTime is the time the build completed.




results


[]RunResult




(Optional)
Results reports any output result values to be consumed by later
tasks in a pipeline.




retriesStatus


[]RunStatus




(Optional)
RetriesStatus contains the history of RunStatus, in case of a retry.




extraFields

k8s.io/apimachinery/pkg/runtime.RawExtension



ExtraFields holds arbitrary fields provided by the custom task
controller.





tekton.dev/v1beta1

Package v1beta1 contains API Schema definitions for the pipeline v1beta1 API group

Resource Types:

ClusterTask

CustomRun

Pipeline

PipelineRun

StepAction

Task

TaskRun

ClusterTask


ClusterTask is a Task with a cluster scope. ClusterTasks are used to
represent Tasks that should be publicly addressable from any namespace in the
cluster.
Deprecated: Please use the cluster resolver instead.




Field
Description





apiVersion
string


tekton.dev/v1beta1





kind
string

ClusterTask



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


TaskSpec




(Optional)
Spec holds the desired state of the Task from the client





resources


TaskResources




(Optional)
Resources is a list input and output resource to run the task
Resources are represented in TaskRuns as bindings to instances of
PipelineResources.
Deprecated: Unused, preserved only for backwards compatibility




params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the task. Params
must be supplied as inputs in TaskRuns unless they declare a default
value.




displayName

string



(Optional)
DisplayName is a user-facing name of the task that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the task that may be
used to populate a UI.




steps


[]Step




Steps are the steps of the build; each step is run sequentially with the
source mounted into /workspace.




volumes


[]Kubernetes core/v1.Volume




Volumes is a collection of volumes that are available to mount into the
steps of the build.




stepTemplate


StepTemplate




StepTemplate can be used as the basis for all step containers within the
Task, so that the steps inherit settings on the base container.




sidecars


[]Sidecar




Sidecars are run alongside the Taskâ€™s step containers. They begin before
the steps start and end after the steps complete.




workspaces


[]WorkspaceDeclaration




Workspaces are the volumes that this Task requires.




results


[]TaskResult




Results are values that this Task can output







CustomRun


CustomRun represents a single execution of a Custom Task.




Field
Description





apiVersion
string


tekton.dev/v1beta1





kind
string

CustomRun



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


CustomRunSpec




(Optional)





customRef


TaskRef




(Optional)




customSpec


EmbeddedCustomRunSpec




(Optional)
Spec is a specification of a custom task




params


Params




(Optional)




status


CustomRunSpecStatus




(Optional)
Used for cancelling a customrun (and maybe more later on)




statusMessage


CustomRunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Used for propagating retries count to custom tasks




serviceAccountName

string



(Optional)




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which the custom-task times out.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.







status


CustomRunStatus




(Optional)




Pipeline


Pipeline describes a list of Tasks to execute. It expresses how outputs
of tasks feed into inputs of subsequent tasks.
Deprecated: Please use v1.Pipeline instead.




Field
Description





apiVersion
string


tekton.dev/v1beta1





kind
string

Pipeline



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


PipelineSpec




(Optional)
Spec holds the desired state of the Pipeline from the client





displayName

string



(Optional)
DisplayName is a user-facing name of the pipeline that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the pipeline that may be
used to populate a UI.




resources


[]PipelineDeclaredResource




Deprecated: Unused, preserved only for backwards compatibility




tasks


[]PipelineTask




Tasks declares the graph of Tasks that execute when this Pipeline is run.




params


ParamSpecs




Params declares a list of input parameters that must be supplied when
this Pipeline is run.




workspaces


[]PipelineWorkspaceDeclaration




(Optional)
Workspaces declares a set of named workspaces that are expected to be
provided by a PipelineRun.




results


[]PipelineResult




(Optional)
Results are values that this pipeline can output once run




finally


[]PipelineTask




Finally declares the list of Tasks that execute just before leaving the Pipeline
i.e. either after all Tasks are finished executing successfully
or after a failure which would result in ending the Pipeline







PipelineRun


PipelineRun represents a single execution of a Pipeline. PipelineRuns are how
the graph of Tasks declared in a Pipeline are executed; they specify inputs
to Pipelines such as parameter values and capture operational aspects of the
Tasks execution such as service account and tolerations. Creating a
PipelineRun creates TaskRuns for Tasks in the referenced Pipeline.
Deprecated: Please use v1.PipelineRun instead.




Field
Description





apiVersion
string


tekton.dev/v1beta1





kind
string

PipelineRun



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


PipelineRunSpec




(Optional)





pipelineRef


PipelineRef




(Optional)




pipelineSpec


PipelineSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




resources


[]PipelineResourceBinding




Resources is a list of bindings specifying which actual instances of
PipelineResources to use for the resources the Pipeline has declared
it needs.
Deprecated: Unused, preserved only for backwards compatibility




params


Params




Params is a list of parameter names and values.




serviceAccountName

string



(Optional)




status


PipelineRunSpecStatus




(Optional)
Used for cancelling a pipelinerun (and maybe more later on)




timeouts


TimeoutFields




(Optional)
Time after which the Pipeline times out.
Currently three keys are accepted in the map
pipeline, tasks and finally
with Timeouts.pipeline >= Timeouts.tasks + Timeouts.finally




timeout


Kubernetes meta/v1.Duration




(Optional)
Timeout is the Time after which the Pipeline times out.
Defaults to never.
Refer to Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration
Deprecated: use pipelineRunSpec.Timeouts.Pipeline instead




podTemplate


Template




PodTemplate holds pod specific configuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces holds a set of workspace bindings that must match names
with those declared in the pipeline.




taskRunSpecs


[]PipelineTaskRunSpec




(Optional)
TaskRunSpecs holds a set of runtime specs







status


PipelineRunStatus




(Optional)




StepAction


StepAction represents the actionable components of Step.
The Step can only reference it from the cluster or using remote resolution.




Field
Description





apiVersion
string


tekton.dev/v1beta1





kind
string

StepAction



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


StepActionSpec




(Optional)
Spec holds the desired state of the Step from the client





description

string



(Optional)
Description is a user-facing description of the stepaction that may be
used to populate a UI.




image

string



(Optional)
Image reference name to run for this StepAction.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the container.
Cannot be updated.




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command and the Args will be passed to the Script.




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the stepAction.
Params must be supplied as inputs in Steps unless they declare a defaultvalue.




results


[]StepResult




(Optional)
Results are values that this StepAction can output




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
The value set in StepAction will take precedence over the value from Task.




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.







Task


Task represents a collection of sequential steps that are run as part of a
Pipeline using a set of inputs and producing a set of outputs. Tasks execute
when TaskRuns are created that provide the input parameters and resources and
output resources the Task requires.
Deprecated: Please use v1.Task instead.




Field
Description





apiVersion
string


tekton.dev/v1beta1





kind
string

Task



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


TaskSpec




(Optional)
Spec holds the desired state of the Task from the client





resources


TaskResources




(Optional)
Resources is a list input and output resource to run the task
Resources are represented in TaskRuns as bindings to instances of
PipelineResources.
Deprecated: Unused, preserved only for backwards compatibility




params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the task. Params
must be supplied as inputs in TaskRuns unless they declare a default
value.




displayName

string



(Optional)
DisplayName is a user-facing name of the task that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the task that may be
used to populate a UI.




steps


[]Step




Steps are the steps of the build; each step is run sequentially with the
source mounted into /workspace.




volumes


[]Kubernetes core/v1.Volume




Volumes is a collection of volumes that are available to mount into the
steps of the build.




stepTemplate


StepTemplate




StepTemplate can be used as the basis for all step containers within the
Task, so that the steps inherit settings on the base container.




sidecars


[]Sidecar




Sidecars are run alongside the Taskâ€™s step containers. They begin before
the steps start and end after the steps complete.




workspaces


[]WorkspaceDeclaration




Workspaces are the volumes that this Task requires.




results


[]TaskResult




Results are values that this Task can output







TaskRun


TaskRun represents a single execution of a Task. TaskRuns are how the steps
specified in a Task are executed; they specify the parameters and resources
used to run the steps in a Task.
Deprecated: Please use v1.TaskRun instead.




Field
Description





apiVersion
string


tekton.dev/v1beta1





kind
string

TaskRun



metadata


Kubernetes meta/v1.ObjectMeta




(Optional)
Refer to the Kubernetes API documentation for the fields of the
metadata field.




spec


TaskRunSpec




(Optional)





debug


TaskRunDebug




(Optional)




params


Params




(Optional)




resources


TaskRunResources




(Optional)
Deprecated: Unused, preserved only for backwards compatibility




serviceAccountName

string



(Optional)




taskRef


TaskRef




(Optional)
no more than one of the TaskRef and TaskSpec may be specified.




taskSpec


TaskSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




status


TaskRunSpecStatus




(Optional)
Used for cancelling a TaskRun (and maybe more later on)




statusMessage


TaskRunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Retries represents how many times this TaskRun should be retried in the event of Task failure.




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which one retry attempt times out. Defaults to 1 hour.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




podTemplate


Template




PodTemplate holds pod specific configuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.




stepOverrides


[]TaskRunStepOverride




(Optional)
Overrides to apply to Steps in this TaskRun.
If a field is specified in both a Step and a StepOverride,
the value from the StepOverride will be used.
This field is only supported when the alpha feature gate is enabled.




sidecarOverrides


[]TaskRunSidecarOverride




(Optional)
Overrides to apply to Sidecars in this TaskRun.
If a field is specified in both a Sidecar and a SidecarOverride,
the value from the SidecarOverride will be used.
This field is only supported when the alpha feature gate is enabled.




computeResources


Kubernetes core/v1.ResourceRequirements




Compute resources to use for this TaskRun







status


TaskRunStatus




(Optional)




Algorithm
(string alias)

Algorithm Standard cryptographic hash algorithm

Artifact


(Appears on:Artifacts, StepState)


TaskRunStepArtifact represents an artifact produced or used by a step within a task run.
It directly uses the Artifact type for its structure.




Field
Description





name

string



The artifactâ€™s identifying category name




values


[]ArtifactValue




A collection of values related to the artifact




buildOutput

bool



Indicate if the artifact is a build output or a by-product




ArtifactValue


(Appears on:Artifact)


ArtifactValue represents a specific value or data element within an Artifact.




Field
Description





digest

map[github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.Algorithm]string







uri

string



Algorithm-specific digests for verifying the content (e.g., SHA256)




Artifacts


Artifacts represents the collection of input and output artifacts associated with
a task run or a similar process. Artifacts in this context are units of data or resources
that the process either consumes as input or produces as output.




Field
Description





inputs


[]Artifact








outputs


[]Artifact








ChildStatusReference


(Appears on:PipelineRunStatusFields)


ChildStatusReference is used to point to the statuses of individual TaskRuns and Runs within this PipelineRun.




Field
Description





name

string



Name is the name of the TaskRun or Run this is referencing.




displayName

string



DisplayName is a user-facing name of the pipelineTask that may be
used to populate a UI.




pipelineTaskName

string



PipelineTaskName is the name of the PipelineTask this is referencing.




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




CloudEventCondition
(string alias)

(Appears on:CloudEventDeliveryState)


CloudEventCondition is a string that represents the condition of the event.

CloudEventDelivery


(Appears on:TaskRunStatusFields)


CloudEventDelivery is the target of a cloud event along with the state of
delivery.




Field
Description





target

string



Target points to an addressable




status


CloudEventDeliveryState








CloudEventDeliveryState


(Appears on:CloudEventDelivery)


CloudEventDeliveryState reports the state of a cloud event to be sent.




Field
Description





condition


CloudEventCondition




Current status




sentAt


Kubernetes meta/v1.Time




(Optional)
SentAt is the time at which the last attempt to send the event was made




message

string



Error is the text of error (if any)




retryCount

int32



RetryCount is the number of attempts of sending the cloud event




Combination
(map[string]string alias)

Combination is a map, mainly defined to hold a single combination from a Matrix with key as param.Name and value as param.Value

Combinations
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.Combination alias)

Combinations is a Combination list

ConfigSource


(Appears on:Provenance)


ConfigSource contains the information that can uniquely identify where a remote
built definition came from i.e. Git repositories, Tekton Bundles in OCI registry
and hub.




Field
Description





uri

string



URI indicates the identity of the source of the build definition.
Example: â€œhttps://github.com/tektoncd/catalogâ€




digest

map[string]string



Digest is a collection of cryptographic digests for the contents of the artifact specified by URI.
Example: {â€œsha1â€: â€œf99d13e554ffcb696dee719fa85b695cb5b0f428â€}




entryPoint

string



EntryPoint identifies the entry point into the build. This is often a path to a
build definition file and/or a target label within that file.
Example: â€œtask/git-clone/0.8/git-clone.yamlâ€




CustomRunReason
(string alias)

CustomRunReason is an enum used to store all Run reason for the Succeeded condition that are controlled by the CustomRun itself.

CustomRunSpec


(Appears on:CustomRun)


CustomRunSpec defines the desired state of CustomRun




Field
Description





customRef


TaskRef




(Optional)




customSpec


EmbeddedCustomRunSpec




(Optional)
Spec is a specification of a custom task




params


Params




(Optional)




status


CustomRunSpecStatus




(Optional)
Used for cancelling a customrun (and maybe more later on)




statusMessage


CustomRunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Used for propagating retries count to custom tasks




serviceAccountName

string



(Optional)




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which the custom-task times out.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.




CustomRunSpecStatus
(string alias)

(Appears on:CustomRunSpec)


CustomRunSpecStatus defines the taskrun spec status the user can provide

CustomRunSpecStatusMessage
(string alias)

(Appears on:CustomRunSpec)


CustomRunSpecStatusMessage defines human readable status messages for the TaskRun.

EmbeddedCustomRunSpec


(Appears on:CustomRunSpec)


EmbeddedCustomRunSpec allows custom task definitions to be embedded




Field
Description





metadata


PipelineTaskMetadata




(Optional)




spec

k8s.io/apimachinery/pkg/runtime.RawExtension



(Optional)
Spec is a specification of a custom task





-

[]byte



Raw is the underlying serialization of this object.
TODO: Determine how to detect ContentType and ContentEncoding of â€˜Rawâ€™ data.




-

k8s.io/apimachinery/pkg/runtime.Object



Object can hold a representation of this extension - useful for working with versioned
structs.







EmbeddedTask


(Appears on:PipelineTask)


EmbeddedTask is used to define a Task inline within a Pipelineâ€™s PipelineTasks.




Field
Description





spec

k8s.io/apimachinery/pkg/runtime.RawExtension



(Optional)
Spec is a specification of a custom task





-

[]byte



Raw is the underlying serialization of this object.
TODO: Determine how to detect ContentType and ContentEncoding of â€˜Rawâ€™ data.




-

k8s.io/apimachinery/pkg/runtime.Object



Object can hold a representation of this extension - useful for working with versioned
structs.







metadata


PipelineTaskMetadata




(Optional)




TaskSpec


TaskSpec





(Members of TaskSpec are embedded into this type.)

(Optional)
TaskSpec is a specification of a task




IncludeParams


IncludeParams allows passing in a specific combinations of Parameters into the Matrix.




Field
Description





name

string



Name the specified combination




params


Params




Params takes only Parameters of type "string"
The names of the params must match the names of the params in the underlying Task




InternalTaskModifier


InternalTaskModifier implements TaskModifier for resources that are built-in to Tekton Pipelines.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





stepsToPrepend


[]Step








stepsToAppend


[]Step








volumes


[]Kubernetes core/v1.Volume








Matrix


(Appears on:PipelineTask)


Matrix is used to fan out Tasks in a Pipeline




Field
Description





params


Params




Params is a list of parameters used to fan out the pipelineTask
Params takes only Parameters of type "array"
Each array element is supplied to the PipelineTask by substituting params of type "string" in the underlying Task.
The names of the params in the Matrix must match the names of the params in the underlying Task that they will be substituting.




include


IncludeParamsList




(Optional)
Include is a list of IncludeParams which allows passing in specific combinations of Parameters into the Matrix.




OnErrorType
(string alias)

(Appears on:Step)


OnErrorType defines a list of supported exiting behavior of a container on error

Param


(Appears on:TaskRunInputs)


Param declares an ParamValues to use for the parameter called name.




Field
Description





name

string







value


ParamValue








ParamSpec


ParamSpec defines arbitrary parameters needed beyond typed inputs (such as
resources). Parameter values are provided by users as inputs on a TaskRun
or PipelineRun.




Field
Description





name

string



Name declares the name by which a parameter is referenced.




type


ParamType




(Optional)
Type is the user-specified type of the parameter. The possible types
are currently â€œstringâ€, â€œarrayâ€ and â€œobjectâ€, and â€œstringâ€ is the default.




description

string



(Optional)
Description is a user-facing description of the parameter that may be
used to populate a UI.




properties


map[string]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.PropertySpec




(Optional)
Properties is the JSON Schema properties to support key-value pairs parameter.




default


ParamValue




(Optional)
Default is the value a parameter takes if no input value is supplied. If
default is set, a Task may be executed without a supplied value for the
parameter.




enum

[]string



(Optional)
Enum declares a set of allowed param input values for tasks/pipelines that can be validated.
If Enum is not set, no input validation is performed for the param.




ParamSpecs
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.ParamSpec alias)

(Appears on:PipelineSpec, TaskSpec)


ParamSpecs is a list of ParamSpec

ParamType
(string alias)

(Appears on:ParamSpec, ParamValue, PropertySpec)


ParamType indicates the type of an input parameter;
Used to distinguish between a single string and an array of strings.

ParamValue


(Appears on:Param, ParamSpec, PipelineResult, PipelineRunResult, TaskResult, TaskRunResult)


ResultValue is a type alias of ParamValue




Field
Description





Type


ParamType








StringVal

string



Represents the stored type of ParamValues.




ArrayVal

[]string







ObjectVal

map[string]string







Params
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.Param alias)

(Appears on:RunSpec, CustomRunSpec, IncludeParams, Matrix, PipelineRunSpec, PipelineTask, ResolverRef, Step, TaskRunSpec)


Params is a list of Param

PipelineDeclaredResource


(Appears on:PipelineSpec)


PipelineDeclaredResource is used by a Pipeline to declare the types of the
PipelineResources that it will required to run and names which can be used to
refer to these PipelineResources in PipelineTaskResourceBindings.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





name

string



Name is the name that will be used by the Pipeline to refer to this resource.
It does not directly correspond to the name of any PipelineResources Task
inputs or outputs, and it does not correspond to the actual names of the
PipelineResources that will be bound in the PipelineRun.




type

string



Type is the type of the PipelineResource.




optional

bool



Optional declares the resource as optional.
optional: true - the resource is considered optional
optional: false - the resource is considered required (default/equivalent of not specifying it)




PipelineObject


PipelineObject is implemented by Pipeline

PipelineRef


(Appears on:PipelineRunSpec, PipelineTask)


PipelineRef can be used to refer to a specific instance of a Pipeline.




Field
Description





name

string



Name of the referent; More info: http://kubernetes.io/docs/user-guide/identifiers#names




apiVersion

string



(Optional)
API version of the referent




bundle

string



(Optional)
Bundle url reference to a Tekton Bundle.
Deprecated: Please use ResolverRef with the bundles resolver instead.
The field is staying there for go client backward compatibility, but is not used/allowed anymore.




ResolverRef


ResolverRef




(Optional)
ResolverRef allows referencing a Pipeline in a remote location
like a git repo. This field is only supported when the alpha
feature gate is enabled.




PipelineResourceBinding


(Appears on:PipelineRunSpec, TaskResourceBinding)


PipelineResourceBinding connects a reference to an instance of a PipelineResource
with a PipelineResource dependency that the Pipeline has declared
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





name

string



Name is the name of the PipelineResource in the Pipelineâ€™s declaration




resourceRef


PipelineResourceRef




(Optional)
ResourceRef is a reference to the instance of the actual PipelineResource
that should be used




resourceSpec


PipelineResourceSpec




(Optional)
ResourceSpec is specification of a resource that should be created and
consumed by the task




PipelineResourceInterface


PipelineResourceInterface interface to be implemented by different PipelineResource types
Deprecated: Unused, preserved only for backwards compatibility

PipelineResourceRef


(Appears on:PipelineResourceBinding)


PipelineResourceRef can be used to refer to a specific instance of a Resource
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





name

string



Name of the referent; More info: http://kubernetes.io/docs/user-guide/identifiers#names




apiVersion

string



(Optional)
API version of the referent




PipelineResult


(Appears on:PipelineSpec)


PipelineResult used to describe the results of a pipeline




Field
Description





name

string



Name the given name




type


ResultsType




Type is the user-specified type of the result.
The possible types are â€˜stringâ€™, â€˜arrayâ€™, and â€˜objectâ€™, with â€˜stringâ€™ as the default.
â€˜arrayâ€™ and â€˜objectâ€™ types are alpha features.




description

string



(Optional)
Description is a human-readable description of the result




value


ParamValue




Value the expression used to retrieve the value




PipelineRunReason
(string alias)

PipelineRunReason represents a reason for the pipeline run â€œSucceededâ€ condition

PipelineRunResult


(Appears on:PipelineRunStatusFields)


PipelineRunResult used to describe the results of a pipeline




Field
Description





name

string



Name is the resultâ€™s name as declared by the Pipeline




value


ParamValue




Value is the result returned from the execution of this PipelineRun




PipelineRunRunStatus


(Appears on:PipelineRunStatusFields)


PipelineRunRunStatus contains the name of the PipelineTask for this CustomRun or Run and the CustomRun or Runâ€™s Status




Field
Description





pipelineTaskName

string



PipelineTaskName is the name of the PipelineTask.




status


CustomRunStatus




(Optional)
Status is the CustomRunStatus for the corresponding CustomRun or Run




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




PipelineRunSpec


(Appears on:PipelineRun)


PipelineRunSpec defines the desired state of PipelineRun




Field
Description





pipelineRef


PipelineRef




(Optional)




pipelineSpec


PipelineSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




resources


[]PipelineResourceBinding




Resources is a list of bindings specifying which actual instances of
PipelineResources to use for the resources the Pipeline has declared
it needs.
Deprecated: Unused, preserved only for backwards compatibility




params


Params




Params is a list of parameter names and values.




serviceAccountName

string



(Optional)




status


PipelineRunSpecStatus




(Optional)
Used for cancelling a pipelinerun (and maybe more later on)




timeouts


TimeoutFields




(Optional)
Time after which the Pipeline times out.
Currently three keys are accepted in the map
pipeline, tasks and finally
with Timeouts.pipeline >= Timeouts.tasks + Timeouts.finally




timeout


Kubernetes meta/v1.Duration




(Optional)
Timeout is the Time after which the Pipeline times out.
Defaults to never.
Refer to Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration
Deprecated: use pipelineRunSpec.Timeouts.Pipeline instead




podTemplate


Template




PodTemplate holds pod specific configuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces holds a set of workspace bindings that must match names
with those declared in the pipeline.




taskRunSpecs


[]PipelineTaskRunSpec




(Optional)
TaskRunSpecs holds a set of runtime specs




PipelineRunSpecStatus
(string alias)

(Appears on:PipelineRunSpec)


PipelineRunSpecStatus defines the pipelinerun spec status the user can provide

PipelineRunStatus


(Appears on:PipelineRun)


PipelineRunStatus defines the observed state of PipelineRun




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





PipelineRunStatusFields


PipelineRunStatusFields





(Members of PipelineRunStatusFields are embedded into this type.)

PipelineRunStatusFields inlines the status fields.




PipelineRunStatusFields


(Appears on:PipelineRunStatus)


PipelineRunStatusFields holds the fields of PipelineRunStatusâ€™ status.
This is defined separately and inlined so that other types can readily
consume these fields via duck typing.




Field
Description





startTime


Kubernetes meta/v1.Time




StartTime is the time the PipelineRun is actually started.




completionTime


Kubernetes meta/v1.Time




CompletionTime is the time the PipelineRun completed.




taskRuns


map[string]*github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.PipelineRunTaskRunStatus




(Optional)
TaskRuns is a map of PipelineRunTaskRunStatus with the taskRun name as the key.
Deprecated: use ChildReferences instead. As of v0.45.0, this field is no
longer populated and is only included for backwards compatibility with
older server versions.




runs


map[string]*github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.PipelineRunRunStatus




(Optional)
Runs is a map of PipelineRunRunStatus with the run name as the key
Deprecated: use ChildReferences instead. As of v0.45.0, this field is no
longer populated and is only included for backwards compatibility with
older server versions.




pipelineResults


[]PipelineRunResult




(Optional)
PipelineResults are the list of results written out by the pipeline taskâ€™s containers




pipelineSpec


PipelineSpec




PipelineRunSpec contains the exact spec used to instantiate the run




skippedTasks


[]SkippedTask




(Optional)
list of tasks that were skipped due to when expressions evaluating to false




childReferences


[]ChildStatusReference




(Optional)
list of TaskRun and Run names, PipelineTask names, and API versions/kinds for children of this PipelineRun.




finallyStartTime


Kubernetes meta/v1.Time




(Optional)
FinallyStartTime is when all non-finally tasks have been completed and only finally tasks are being executed.




provenance


Provenance




(Optional)
Provenance contains some key authenticated metadata about how a software artifact was built (what sources, what inputs/outputs, etc.).




spanContext

map[string]string



SpanContext contains tracing span context fields




PipelineRunTaskRunStatus


(Appears on:PipelineRunStatusFields)


PipelineRunTaskRunStatus contains the name of the PipelineTask for this TaskRun and the TaskRunâ€™s Status




Field
Description





pipelineTaskName

string



PipelineTaskName is the name of the PipelineTask.




status


TaskRunStatus




(Optional)
Status is the TaskRunStatus for the corresponding TaskRun




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




PipelineSpec


(Appears on:Pipeline, PipelineRunSpec, PipelineRunStatusFields, PipelineTask)


PipelineSpec defines the desired state of Pipeline.




Field
Description





displayName

string



(Optional)
DisplayName is a user-facing name of the pipeline that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the pipeline that may be
used to populate a UI.




resources


[]PipelineDeclaredResource




Deprecated: Unused, preserved only for backwards compatibility




tasks


[]PipelineTask




Tasks declares the graph of Tasks that execute when this Pipeline is run.




params


ParamSpecs




Params declares a list of input parameters that must be supplied when
this Pipeline is run.




workspaces


[]PipelineWorkspaceDeclaration




(Optional)
Workspaces declares a set of named workspaces that are expected to be
provided by a PipelineRun.




results


[]PipelineResult




(Optional)
Results are values that this pipeline can output once run




finally


[]PipelineTask




Finally declares the list of Tasks that execute just before leaving the Pipeline
i.e. either after all Tasks are finished executing successfully
or after a failure which would result in ending the Pipeline




PipelineTask


(Appears on:PipelineSpec)


PipelineTask defines a task in a Pipeline, passing inputs from both
Params and from the output of previous tasks.




Field
Description





name

string



Name is the name of this task within the context of a Pipeline. Name is
used as a coordinate with the from and runAfter fields to establish
the execution order of tasks relative to one another.




displayName

string



(Optional)
DisplayName is the display name of this task within the context of a Pipeline.
This display name may be used to populate a UI.




description

string



(Optional)
Description is the description of this task within the context of a Pipeline.
This description may be used to populate a UI.




taskRef


TaskRef




(Optional)
TaskRef is a reference to a task definition.




taskSpec


EmbeddedTask




(Optional)
TaskSpec is a specification of a task
Specifying TaskSpec can be disabled by setting
disable-inline-spec feature flag..




when


WhenExpressions




(Optional)
WhenExpressions is a list of when expressions that need to be true for the task to run




retries

int



(Optional)
Retries represents how many times this task should be retried in case of task failure: ConditionSucceeded set to False




runAfter

[]string



(Optional)
RunAfter is the list of PipelineTask names that should be executed before
this Task executes. (Used to force a specific ordering in graph execution.)




resources


PipelineTaskResources




(Optional)
Deprecated: Unused, preserved only for backwards compatibility




params


Params




(Optional)
Parameters declares parameters passed to this task.




matrix


Matrix




(Optional)
Matrix declares parameters used to fan out this task.




workspaces


[]WorkspacePipelineTaskBinding




(Optional)
Workspaces maps workspaces from the pipeline spec to the workspaces
declared in the Task.




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which the TaskRun times out. Defaults to 1 hour.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




pipelineRef


PipelineRef




(Optional)
PipelineRef is a reference to a pipeline definition
Note: PipelineRef is in preview mode and not yet supported




pipelineSpec


PipelineSpec




(Optional)
PipelineSpec is a specification of a pipeline
Note: PipelineSpec is in preview mode and not yet supported
Specifying TaskSpec can be disabled by setting
disable-inline-spec feature flag..




onError


PipelineTaskOnErrorType




(Optional)
OnError defines the exiting behavior of a PipelineRun on error
can be set to [ continue | stopAndFail ]




PipelineTaskInputResource


(Appears on:PipelineTaskResources)


PipelineTaskInputResource maps the name of a declared PipelineResource input
dependency in a Task to the resource in the Pipelineâ€™s DeclaredPipelineResources
that should be used. This input may come from a previous task.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





name

string



Name is the name of the PipelineResource as declared by the Task.




resource

string



Resource is the name of the DeclaredPipelineResource to use.




from

[]string



(Optional)
From is the list of PipelineTask names that the resource has to come from.
(Implies an ordering in the execution graph.)




PipelineTaskMetadata


(Appears on:EmbeddedRunSpec, EmbeddedCustomRunSpec, EmbeddedTask, PipelineTaskRunSpec)


PipelineTaskMetadata contains the labels or annotations for an EmbeddedTask




Field
Description





labels

map[string]string



(Optional)




annotations

map[string]string



(Optional)




PipelineTaskOnErrorType
(string alias)

(Appears on:PipelineTask)


PipelineTaskOnErrorType defines a list of supported failure handling behaviors of a PipelineTask on error

PipelineTaskOutputResource


(Appears on:PipelineTaskResources)


PipelineTaskOutputResource maps the name of a declared PipelineResource output
dependency in a Task to the resource in the Pipelineâ€™s DeclaredPipelineResources
that should be used.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





name

string



Name is the name of the PipelineResource as declared by the Task.




resource

string



Resource is the name of the DeclaredPipelineResource to use.




PipelineTaskParam


PipelineTaskParam is used to provide arbitrary string parameters to a Task.




Field
Description





name

string







value

string







PipelineTaskResources


(Appears on:PipelineTask)


PipelineTaskResources allows a Pipeline to declare how its DeclaredPipelineResources
should be provided to a Task as its inputs and outputs.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





inputs


[]PipelineTaskInputResource




Inputs holds the mapping from the PipelineResources declared in
DeclaredPipelineResources to the input PipelineResources required by the Task.




outputs


[]PipelineTaskOutputResource




Outputs holds the mapping from the PipelineResources declared in
DeclaredPipelineResources to the input PipelineResources required by the Task.




PipelineTaskRun


PipelineTaskRun reports the results of running a step in the Task. Each
task has the potential to succeed or fail (based on the exit code)
and produces logs.




Field
Description





name

string







PipelineTaskRunSpec


(Appears on:PipelineRunSpec)


PipelineTaskRunSpec  can be used to configure specific
specs for a concrete Task




Field
Description





pipelineTaskName

string







taskServiceAccountName

string







taskPodTemplate


Template








stepOverrides


[]TaskRunStepOverride








sidecarOverrides


[]TaskRunSidecarOverride








metadata


PipelineTaskMetadata




(Optional)




computeResources


Kubernetes core/v1.ResourceRequirements




Compute resources to use for this TaskRun




PipelineWorkspaceDeclaration


(Appears on:PipelineSpec)


WorkspacePipelineDeclaration creates a named slot in a Pipeline that a PipelineRun
is expected to populate with a workspace binding.
Deprecated: use PipelineWorkspaceDeclaration type instead




Field
Description





name

string



Name is the name of a workspace to be provided by a PipelineRun.




description

string



(Optional)
Description is a human readable string describing how the workspace will be
used in the Pipeline. It can be useful to include a bit of detail about which
tasks are intended to have access to the data on the workspace.




optional

bool



Optional marks a Workspace as not being required in PipelineRuns. By default
this field is false and so declared workspaces are required.




PropertySpec


(Appears on:ParamSpec, TaskResult)


PropertySpec defines the struct for object keys




Field
Description





type


ParamType








Provenance


(Appears on:PipelineRunStatusFields, StepState, TaskRunStatusFields)


Provenance contains metadata about resources used in the TaskRun/PipelineRun
such as the source from where a remote build definition was fetched.
This field aims to carry minimum amoumt of metadata in *Run status so that
Tekton Chains can capture them in the provenance.




Field
Description





configSource


ConfigSource




Deprecated: Use RefSource instead




refSource


RefSource




RefSource identifies the source where a remote task/pipeline came from.




featureFlags

github.com/tektoncd/pipeline/pkg/apis/config.FeatureFlags



FeatureFlags identifies the feature flags that were used during the task/pipeline run




Ref


(Appears on:Step)


Ref can be used to refer to a specific instance of a StepAction.




Field
Description





name

string



Name of the referenced step




ResolverRef


ResolverRef




(Optional)
ResolverRef allows referencing a StepAction in a remote location
like a git repo.




RefSource


(Appears on:Provenance)


RefSource contains the information that can uniquely identify where a remote
built definition came from i.e. Git repositories, Tekton Bundles in OCI registry
and hub.




Field
Description





uri

string



URI indicates the identity of the source of the build definition.
Example: â€œhttps://github.com/tektoncd/catalogâ€




digest

map[string]string



Digest is a collection of cryptographic digests for the contents of the artifact specified by URI.
Example: {â€œsha1â€: â€œf99d13e554ffcb696dee719fa85b695cb5b0f428â€}




entryPoint

string



EntryPoint identifies the entry point into the build. This is often a path to a
build definition file and/or a target label within that file.
Example: â€œtask/git-clone/0.8/git-clone.yamlâ€




ResolverName
(string alias)

(Appears on:ResolverRef)


ResolverName is the name of a resolver from which a resource can be
requested.

ResolverRef


(Appears on:PipelineRef, Ref, TaskRef)


ResolverRef can be used to refer to a Pipeline or Task in a remote
location like a git repo.




Field
Description





resolver


ResolverName




(Optional)
Resolver is the name of the resolver that should perform
resolution of the referenced Tekton resource, such as â€œgitâ€.




params


Params




(Optional)
Params contains the parameters used to identify the
referenced Tekton resource. Example entries might include
â€œrepoâ€ or â€œpathâ€ but the set of params ultimately depends on
the chosen resolver.




ResultRef


ResultRef is a type that represents a reference to a task run result




Field
Description





pipelineTask

string







result

string







resultsIndex

int







property

string







ResultsType
(string alias)

(Appears on:PipelineResult, TaskResult, TaskRunResult)


ResultsType indicates the type of a result;
Used to distinguish between a single string and an array of strings.
Note that there is ResultType used to find out whether a
RunResult is from a task result or not, which is different from
this ResultsType.

RunObject


RunObject is implemented by CustomRun and Run

Sidecar


(Appears on:TaskSpec)


Sidecar has nearly the same data structure as Step but does not have the ability to timeout.




Field
Description





name

string



Name of the Sidecar specified as a DNS_LABEL.
Each Sidecar in a Task must have a unique name (DNS_LABEL).
Cannot be updated.




image

string



(Optional)
Image name to be used by the Sidecar.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the Sidecarâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




workingDir

string



(Optional)
Sidecarâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




ports


[]Kubernetes core/v1.ContainerPort




(Optional)
List of ports to expose from the Sidecar. Exposing a port here gives
the system additional information about the network connections a
container uses, but is primarily informational. Not specifying a port here
DOES NOT prevent that port from being exposed. Any port which is
listening on the default â€œ0.0.0.0â€ address inside a container will be
accessible from the network.
Cannot be updated.




envFrom


[]Kubernetes core/v1.EnvFromSource




(Optional)
List of sources to populate environment variables in the Sidecar.
The keys defined within a source must be a C_IDENTIFIER. All invalid keys
will be reported as an event when the Sidecar is starting. When a key exists in multiple
sources, the value associated with the last source will take precedence.
Values defined by an Env with a duplicate key will take precedence.
Cannot be updated.




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the Sidecar.
Cannot be updated.




resources


Kubernetes core/v1.ResourceRequirements




(Optional)
Compute Resources required by this Sidecar.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Sidecarâ€™s filesystem.
Cannot be updated.




volumeDevices


[]Kubernetes core/v1.VolumeDevice




(Optional)
volumeDevices is the list of block devices to be used by the Sidecar.




livenessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of Sidecar liveness.
Container will be restarted if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes




readinessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of Sidecar service readiness.
Container will be removed from service endpoints if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes




startupProbe


Kubernetes core/v1.Probe




(Optional)
StartupProbe indicates that the Pod the Sidecar is running in has successfully initialized.
If specified, no other probes are executed until this completes successfully.
If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
This can be used to provide different probe parameters at the beginning of a Podâ€™s lifecycle,
when it might take a long time to load data or warm a cache, than during steady-state operation.
This cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes




lifecycle


Kubernetes core/v1.Lifecycle




(Optional)
Actions that the management system should take in response to Sidecar lifecycle events.
Cannot be updated.




terminationMessagePath

string



(Optional)
Optional: Path at which the file to which the Sidecarâ€™s termination message
will be written is mounted into the Sidecarâ€™s filesystem.
Message written is intended to be brief final status, such as an assertion failure message.
Will be truncated by the node if greater than 4096 bytes. The total message length across
all containers will be limited to 12kb.
Defaults to /dev/termination-log.
Cannot be updated.




terminationMessagePolicy


Kubernetes core/v1.TerminationMessagePolicy




(Optional)
Indicate how the termination message should be populated. File will use the contents of
terminationMessagePath to populate the Sidecar status message on both success and failure.
FallbackToLogsOnError will use the last chunk of Sidecar log output if the termination
message file is empty and the Sidecar exited with an error.
The log output is limited to 2048 bytes or 80 lines, whichever is smaller.
Defaults to File.
Cannot be updated.




imagePullPolicy


Kubernetes core/v1.PullPolicy




(Optional)
Image pull policy.
One of Always, Never, IfNotPresent.
Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/containers/images#updating-images




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Sidecar should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/




stdin

bool



(Optional)
Whether this Sidecar should allocate a buffer for stdin in the container runtime. If this
is not set, reads from stdin in the Sidecar will always result in EOF.
Default is false.




stdinOnce

bool



(Optional)
Whether the container runtime should close the stdin channel after it has been opened by
a single attach. When stdin is true the stdin stream will remain open across multiple attach
sessions. If stdinOnce is set to true, stdin is opened on Sidecar start, is empty until the
first client attaches to stdin, and then remains open and accepts data until the client disconnects,
at which time stdin is closed and remains closed until the Sidecar is restarted. If this
flag is false, a container processes that reads from stdin will never receive an EOF.
Default is false




tty

bool



(Optional)
Whether this Sidecar should allocate a TTY for itself, also requires â€˜stdinâ€™ to be true.
Default is false.




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command or Args.




workspaces


[]WorkspaceUsage




(Optional)
This is an alpha field. You must set the â€œenable-api-fieldsâ€ feature flag to â€œalphaâ€
for this field to be supported.
Workspaces is a list of workspaces from the Task that this Sidecar wants
exclusive access to. Adding a workspace to this list means that any
other Step or Sidecar that does not also request this Workspace will
not have access to it.




restartPolicy


Kubernetes core/v1.ContainerRestartPolicy




(Optional)
RestartPolicy refers to kubernetes RestartPolicy. It can only be set for an
initContainer and must have itâ€™s policy set to â€œAlwaysâ€. It is currently
left optional to help support Kubernetes versions prior to 1.29 when this feature
was introduced.




SidecarState


(Appears on:TaskRunStatusFields)


SidecarState reports the results of running a sidecar in a Task.




Field
Description





ContainerState


Kubernetes core/v1.ContainerState





(Members of ContainerState are embedded into this type.)





name

string







container

string







imageID

string







SkippedTask


(Appears on:PipelineRunStatusFields)


SkippedTask is used to describe the Tasks that were skipped due to their When Expressions
evaluating to False. This is a struct because we are looking into including more details
about the When Expressions that caused this Task to be skipped.




Field
Description





name

string



Name is the Pipeline Task name




reason


SkippingReason




Reason is the cause of the PipelineTask being skipped.




whenExpressions


[]WhenExpression




(Optional)
WhenExpressions is the list of checks guarding the execution of the PipelineTask




SkippingReason
(string alias)

(Appears on:SkippedTask)


SkippingReason explains why a PipelineTask was skipped.

Step


(Appears on:InternalTaskModifier, TaskSpec)


Step runs a subcomponent of a Task




Field
Description





name

string



Name of the Step specified as a DNS_LABEL.
Each Step in a Task must have a unique name.




image

string



(Optional)
Image reference name to run for this Step.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




ports


[]Kubernetes core/v1.ContainerPort




(Optional)
List of ports to expose from the Stepâ€™s container. Exposing a port here gives
the system additional information about the network connections a
container uses, but is primarily informational. Not specifying a port here
DOES NOT prevent that port from being exposed. Any port which is
listening on the default â€œ0.0.0.0â€ address inside a container will be
accessible from the network.
Cannot be updated.
Deprecated: This field will be removed in a future release.




envFrom


[]Kubernetes core/v1.EnvFromSource




(Optional)
List of sources to populate environment variables in the container.
The keys defined within a source must be a C_IDENTIFIER. All invalid keys
will be reported as an event when the container is starting. When a key exists in multiple
sources, the value associated with the last source will take precedence.
Values defined by an Env with a duplicate key will take precedence.
Cannot be updated.




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the container.
Cannot be updated.




resources


Kubernetes core/v1.ResourceRequirements




(Optional)
Compute Resources required by this Step.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.




volumeDevices


[]Kubernetes core/v1.VolumeDevice




(Optional)
volumeDevices is the list of block devices to be used by the Step.




livenessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of container liveness.
Step will be restarted if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
Deprecated: This field will be removed in a future release.




readinessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of container service readiness.
Step will be removed from service endpoints if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
Deprecated: This field will be removed in a future release.




startupProbe


Kubernetes core/v1.Probe




(Optional)
DeprecatedStartupProbe indicates that the Pod this Step runs in has successfully initialized.
If specified, no other probes are executed until this completes successfully.
If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
This can be used to provide different probe parameters at the beginning of a Podâ€™s lifecycle,
when it might take a long time to load data or warm a cache, than during steady-state operation.
This cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
Deprecated: This field will be removed in a future release.




lifecycle


Kubernetes core/v1.Lifecycle




(Optional)
Actions that the management system should take in response to container lifecycle events.
Cannot be updated.
Deprecated: This field will be removed in a future release.




terminationMessagePath

string



(Optional)
Deprecated: This field will be removed in a future release and canâ€™t be meaningfully used.




terminationMessagePolicy


Kubernetes core/v1.TerminationMessagePolicy




(Optional)
Deprecated: This field will be removed in a future release and canâ€™t be meaningfully used.




imagePullPolicy


Kubernetes core/v1.PullPolicy




(Optional)
Image pull policy.
One of Always, Never, IfNotPresent.
Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/containers/images#updating-images




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/




stdin

bool



(Optional)
Whether this container should allocate a buffer for stdin in the container runtime. If this
is not set, reads from stdin in the container will always result in EOF.
Default is false.
Deprecated: This field will be removed in a future release.




stdinOnce

bool



(Optional)
Whether the container runtime should close the stdin channel after it has been opened by
a single attach. When stdin is true the stdin stream will remain open across multiple attach
sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
first client attaches to stdin, and then remains open and accepts data until the client disconnects,
at which time stdin is closed and remains closed until the container is restarted. If this
flag is false, a container processes that reads from stdin will never receive an EOF.
Default is false
Deprecated: This field will be removed in a future release.




tty

bool



(Optional)
Whether this container should allocate a DeprecatedTTY for itself, also requires â€˜stdinâ€™ to be true.
Default is false.
Deprecated: This field will be removed in a future release.




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command and the Args will be passed to the Script.




timeout


Kubernetes meta/v1.Duration




(Optional)
Timeout is the time after which the step times out. Defaults to never.
Refer to Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




workspaces


[]WorkspaceUsage




(Optional)
This is an alpha field. You must set the â€œenable-api-fieldsâ€ feature flag to â€œalphaâ€
for this field to be supported.
Workspaces is a list of workspaces from the Task that this Step wants
exclusive access to. Adding a workspace to this list means that any
other Step or Sidecar that does not also request this Workspace will
not have access to it.




onError


OnErrorType




OnError defines the exiting behavior of a container on error
can be set to [ continue | stopAndFail ]




stdoutConfig


StepOutputConfig




(Optional)
Stores configuration for the stdout stream of the step.




stderrConfig


StepOutputConfig




(Optional)
Stores configuration for the stderr stream of the step.




ref


Ref




(Optional)
Contains the reference to an existing StepAction.




params


Params




(Optional)
Params declares parameters passed to this step action.




results


[]StepResult




(Optional)
Results declares StepResults produced by the Step.
This is field is at an ALPHA stability level and gated by â€œenable-step-actionsâ€ feature flag.
It can be used in an inlined Step when used to store Results to $(step.results.resultName.path).
It cannot be used when referencing StepActions using [v1beta1.Step.Ref].
The Results declared by the StepActions will be stored here instead.




when


WhenExpressions








StepActionObject


StepActionObject is implemented by StepAction

StepActionSpec


(Appears on:StepAction)


StepActionSpec contains the actionable components of a step.




Field
Description





description

string



(Optional)
Description is a user-facing description of the stepaction that may be
used to populate a UI.




image

string



(Optional)
Image reference name to run for this StepAction.
More info: https://kubernetes.io/docs/concepts/containers/images




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the containerâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the container.
Cannot be updated.




script

string



(Optional)
Script is the contents of an executable file to execute.
If Script is not empty, the Step cannot have an Command and the Args will be passed to the Script.




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the stepAction.
Params must be supplied as inputs in Steps unless they declare a defaultvalue.




results


[]StepResult




(Optional)
Results are values that this StepAction can output




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
The value set in StepAction will take precedence over the value from Task.




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.




StepOutputConfig


(Appears on:Step)


StepOutputConfig stores configuration for a step output stream.




Field
Description





path

string



(Optional)
Path to duplicate stdout stream to on containerâ€™s local filesystem.




StepState


(Appears on:TaskRunStatusFields)


StepState reports the results of running a step in a Task.




Field
Description





ContainerState


Kubernetes core/v1.ContainerState





(Members of ContainerState are embedded into this type.)





name

string







container

string







imageID

string







results


[]TaskRunResult








provenance


Provenance








inputs


[]Artifact








outputs


[]Artifact








StepTemplate


(Appears on:TaskSpec)


StepTemplate is a template for a Step




Field
Description





name

string



Default name for each Step specified as a DNS_LABEL.
Each Step in a Task must have a unique name.
Cannot be updated.
Deprecated: This field will be removed in a future release.




image

string



(Optional)
Default image name to use for each Step.
More info: https://kubernetes.io/docs/concepts/containers/images
This field is optional to allow higher level config management to default or override
container images in workload controllers like Deployments and StatefulSets.




command

[]string



(Optional)
Entrypoint array. Not executed within a shell.
The docker imageâ€™s ENTRYPOINT is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the Stepâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




args

[]string



(Optional)
Arguments to the entrypoint.
The imageâ€™s CMD is used if this is not provided.
Variable references $(VAR_NAME) are expanded using the Stepâ€™s environment. If a variable
cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced
to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. â€œ$$(VAR_NAME)â€ will
produce the string literal â€œ$(VAR_NAME)â€. Escaped references will never be expanded, regardless
of whether the variable exists or not. Cannot be updated.
More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell




workingDir

string



(Optional)
Stepâ€™s working directory.
If not specified, the container runtimeâ€™s default will be used, which
might be configured in the container image.
Cannot be updated.




ports


[]Kubernetes core/v1.ContainerPort




(Optional)
List of ports to expose from the Stepâ€™s container. Exposing a port here gives
the system additional information about the network connections a
container uses, but is primarily informational. Not specifying a port here
DOES NOT prevent that port from being exposed. Any port which is
listening on the default â€œ0.0.0.0â€ address inside a container will be
accessible from the network.
Cannot be updated.
Deprecated: This field will be removed in a future release.




envFrom


[]Kubernetes core/v1.EnvFromSource




(Optional)
List of sources to populate environment variables in the Step.
The keys defined within a source must be a C_IDENTIFIER. All invalid keys
will be reported as an event when the container is starting. When a key exists in multiple
sources, the value associated with the last source will take precedence.
Values defined by an Env with a duplicate key will take precedence.
Cannot be updated.




env


[]Kubernetes core/v1.EnvVar




(Optional)
List of environment variables to set in the container.
Cannot be updated.




resources


Kubernetes core/v1.ResourceRequirements




(Optional)
Compute Resources required by this Step.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/




volumeMounts


[]Kubernetes core/v1.VolumeMount




(Optional)
Volumes to mount into the Stepâ€™s filesystem.
Cannot be updated.




volumeDevices


[]Kubernetes core/v1.VolumeDevice




(Optional)
volumeDevices is the list of block devices to be used by the Step.




livenessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of container liveness.
Container will be restarted if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
Deprecated: This field will be removed in a future release.




readinessProbe


Kubernetes core/v1.Probe




(Optional)
Periodic probe of container service readiness.
Container will be removed from service endpoints if the probe fails.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
Deprecated: This field will be removed in a future release.




startupProbe


Kubernetes core/v1.Probe




(Optional)
DeprecatedStartupProbe indicates that the Pod has successfully initialized.
If specified, no other probes are executed until this completes successfully.
If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
This can be used to provide different probe parameters at the beginning of a Podâ€™s lifecycle,
when it might take a long time to load data or warm a cache, than during steady-state operation.
This cannot be updated.
More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
Deprecated: This field will be removed in a future release.




lifecycle


Kubernetes core/v1.Lifecycle




(Optional)
Actions that the management system should take in response to container lifecycle events.
Cannot be updated.
Deprecated: This field will be removed in a future release.




terminationMessagePath

string



(Optional)
Deprecated: This field will be removed in a future release and cannot be meaningfully used.




terminationMessagePolicy


Kubernetes core/v1.TerminationMessagePolicy




(Optional)
Deprecated: This field will be removed in a future release and cannot be meaningfully used.




imagePullPolicy


Kubernetes core/v1.PullPolicy




(Optional)
Image pull policy.
One of Always, Never, IfNotPresent.
Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
Cannot be updated.
More info: https://kubernetes.io/docs/concepts/containers/images#updating-images




securityContext


Kubernetes core/v1.SecurityContext




(Optional)
SecurityContext defines the security options the Step should be run with.
If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/




stdin

bool



(Optional)
Whether this Step should allocate a buffer for stdin in the container runtime. If this
is not set, reads from stdin in the Step will always result in EOF.
Default is false.
Deprecated: This field will be removed in a future release.




stdinOnce

bool



(Optional)
Whether the container runtime should close the stdin channel after it has been opened by
a single attach. When stdin is true the stdin stream will remain open across multiple attach
sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the
first client attaches to stdin, and then remains open and accepts data until the client disconnects,
at which time stdin is closed and remains closed until the container is restarted. If this
flag is false, a container processes that reads from stdin will never receive an EOF.
Default is false
Deprecated: This field will be removed in a future release.




tty

bool



(Optional)
Whether this Step should allocate a DeprecatedTTY for itself, also requires â€˜stdinâ€™ to be true.
Default is false.
Deprecated: This field will be removed in a future release.




TaskBreakpoints


(Appears on:TaskRunDebug)


TaskBreakpoints defines the breakpoint config for a particular Task




Field
Description





onFailure

string



(Optional)
if enabled, pause TaskRun on failure of a step
failed step will not exit




beforeSteps

[]string



(Optional)




TaskKind
(string alias)

(Appears on:TaskRef)


TaskKind defines the type of Task used by the pipeline.

TaskModifier


TaskModifier is an interface to be implemented by different PipelineResources
Deprecated: Unused, preserved only for backwards compatibility

TaskObject


TaskObject is implemented by Task and ClusterTask

TaskRef


(Appears on:RunSpec, CustomRunSpec, PipelineTask, TaskRunSpec)


TaskRef can be used to refer to a specific instance of a task.




Field
Description





name

string



Name of the referent; More info: http://kubernetes.io/docs/user-guide/identifiers#names




kind


TaskKind




TaskKind indicates the Kind of the Task:
1. Namespaced Task when Kind is set to â€œTaskâ€. If Kind is â€œâ€, it defaults to â€œTaskâ€.
2. Cluster-Scoped Task when Kind is set to â€œClusterTaskâ€
3. Custom Task when Kind is non-empty and APIVersion is non-empty




apiVersion

string



(Optional)
API version of the referent
Note: A Task with non-empty APIVersion and Kind is considered a Custom Task




bundle

string



(Optional)
Bundle url reference to a Tekton Bundle.
Deprecated: Please use ResolverRef with the bundles resolver instead.
The field is staying there for go client backward compatibility, but is not used/allowed anymore.




ResolverRef


ResolverRef




(Optional)
ResolverRef allows referencing a Task in a remote location
like a git repo. This field is only supported when the alpha
feature gate is enabled.




TaskResource


(Appears on:TaskResources)


TaskResource defines an input or output Resource declared as a requirement
by a Task. The Name field will be used to refer to these Resources within
the Task definition, and when provided as an Input, the Name will be the
path to the volume mounted containing this Resource as an input (e.g.
an input Resource named workspace will be mounted at /workspace).
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





ResourceDeclaration


ResourceDeclaration





(Members of ResourceDeclaration are embedded into this type.)





TaskResourceBinding


(Appears on:TaskRunInputs, TaskRunOutputs, TaskRunResources)


TaskResourceBinding points to the PipelineResource that
will be used for the Task input or output called Name.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





PipelineResourceBinding


PipelineResourceBinding





(Members of PipelineResourceBinding are embedded into this type.)





paths

[]string



(Optional)
Paths will probably be removed in #1284, and then PipelineResourceBinding can be used instead.
The optional Path field corresponds to a path on disk at which the Resource can be found
(used when providing the resource via mounted volume, overriding the default logic to fetch the Resource).




TaskResources


(Appears on:TaskSpec)


TaskResources allows a Pipeline to declare how its DeclaredPipelineResources
should be provided to a Task as its inputs and outputs.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





inputs


[]TaskResource




Inputs holds the mapping from the PipelineResources declared in
DeclaredPipelineResources to the input PipelineResources required by the Task.




outputs


[]TaskResource




Outputs holds the mapping from the PipelineResources declared in
DeclaredPipelineResources to the input PipelineResources required by the Task.




TaskResult


(Appears on:TaskSpec)


TaskResult used to describe the results of a task




Field
Description





name

string



Name the given name




type


ResultsType




(Optional)
Type is the user-specified type of the result. The possible type
is currently â€œstringâ€ and will support â€œarrayâ€ in following work.




properties


map[string]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.PropertySpec




(Optional)
Properties is the JSON Schema properties to support key-value pairs results.




description

string



(Optional)
Description is a human-readable description of the result




value


ParamValue




(Optional)
Value the expression used to retrieve the value of the result from an underlying Step.




TaskRunConditionType
(string alias)

TaskRunConditionType is an enum used to store TaskRun custom
conditions such as one used in spire results verification

TaskRunDebug


(Appears on:TaskRunSpec)


TaskRunDebug defines the breakpoint config for a particular TaskRun




Field
Description





breakpoints


TaskBreakpoints




(Optional)




TaskRunInputs


TaskRunInputs holds the input values that this task was invoked with.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





resources


[]TaskResourceBinding




(Optional)




params


[]Param




(Optional)




TaskRunOutputs


TaskRunOutputs holds the output values that this task was invoked with.
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





resources


[]TaskResourceBinding




(Optional)




TaskRunReason
(string alias)

TaskRunReason is an enum used to store all TaskRun reason for
the Succeeded condition that are controlled by the TaskRun itself. Failure
reasons that emerge from underlying resources are not included here

TaskRunResources


(Appears on:TaskRunSpec)


TaskRunResources allows a TaskRun to declare inputs and outputs TaskResourceBinding
Deprecated: Unused, preserved only for backwards compatibility




Field
Description





inputs


[]TaskResourceBinding




Inputs holds the inputs resources this task was invoked with




outputs


[]TaskResourceBinding




Outputs holds the inputs resources this task was invoked with




TaskRunResult


(Appears on:StepState, TaskRunStatusFields)


TaskRunStepResult is a type alias of TaskRunResult




Field
Description





name

string



Name the given name




type


ResultsType




(Optional)
Type is the user-specified type of the result. The possible type
is currently â€œstringâ€ and will support â€œarrayâ€ in following work.




value


ParamValue




Value the given value of the result




TaskRunSidecarOverride


(Appears on:PipelineTaskRunSpec, TaskRunSpec)


TaskRunSidecarOverride is used to override the values of a Sidecar in the corresponding Task.




Field
Description





name

string



The name of the Sidecar to override.




resources


Kubernetes core/v1.ResourceRequirements




The resource requirements to apply to the Sidecar.




TaskRunSpec


(Appears on:TaskRun)


TaskRunSpec defines the desired state of TaskRun




Field
Description





debug


TaskRunDebug




(Optional)




params


Params




(Optional)




resources


TaskRunResources




(Optional)
Deprecated: Unused, preserved only for backwards compatibility




serviceAccountName

string



(Optional)




taskRef


TaskRef




(Optional)
no more than one of the TaskRef and TaskSpec may be specified.




taskSpec


TaskSpec




(Optional)
Specifying PipelineSpec can be disabled by setting
disable-inline-spec feature flag..




status


TaskRunSpecStatus




(Optional)
Used for cancelling a TaskRun (and maybe more later on)




statusMessage


TaskRunSpecStatusMessage




(Optional)
Status message for cancellation.




retries

int



(Optional)
Retries represents how many times this TaskRun should be retried in the event of Task failure.




timeout


Kubernetes meta/v1.Duration




(Optional)
Time after which one retry attempt times out. Defaults to 1 hour.
Refer Goâ€™s ParseDuration documentation for expected format: https://golang.org/pkg/time/#ParseDuration




podTemplate


Template




PodTemplate holds pod specific configuration




workspaces


[]WorkspaceBinding




(Optional)
Workspaces is a list of WorkspaceBindings from volumes to workspaces.




stepOverrides


[]TaskRunStepOverride




(Optional)
Overrides to apply to Steps in this TaskRun.
If a field is specified in both a Step and a StepOverride,
the value from the StepOverride will be used.
This field is only supported when the alpha feature gate is enabled.




sidecarOverrides


[]TaskRunSidecarOverride




(Optional)
Overrides to apply to Sidecars in this TaskRun.
If a field is specified in both a Sidecar and a SidecarOverride,
the value from the SidecarOverride will be used.
This field is only supported when the alpha feature gate is enabled.




computeResources


Kubernetes core/v1.ResourceRequirements




Compute resources to use for this TaskRun




TaskRunSpecStatus
(string alias)

(Appears on:TaskRunSpec)


TaskRunSpecStatus defines the TaskRun spec status the user can provide

TaskRunSpecStatusMessage
(string alias)

(Appears on:TaskRunSpec)


TaskRunSpecStatusMessage defines human readable status messages for the TaskRun.

TaskRunStatus


(Appears on:TaskRun, PipelineRunTaskRunStatus, TaskRunStatusFields)


TaskRunStatus defines the observed state of TaskRun




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





TaskRunStatusFields


TaskRunStatusFields





(Members of TaskRunStatusFields are embedded into this type.)

TaskRunStatusFields inlines the status fields.




TaskRunStatusFields


(Appears on:TaskRunStatus)


TaskRunStatusFields holds the fields of TaskRunâ€™s status.  This is defined
separately and inlined so that other types can readily consume these fields
via duck typing.




Field
Description





podName

string



PodName is the name of the pod responsible for executing this taskâ€™s steps.




startTime


Kubernetes meta/v1.Time




StartTime is the time the build is actually started.




completionTime


Kubernetes meta/v1.Time




CompletionTime is the time the build completed.




steps


[]StepState




(Optional)
Steps describes the state of each build step container.




cloudEvents


[]CloudEventDelivery




(Optional)
CloudEvents describe the state of each cloud event requested via a
CloudEventResource.
Deprecated: Removed in v0.44.0.




retriesStatus


[]TaskRunStatus




(Optional)
RetriesStatus contains the history of TaskRunStatus in case of a retry in order to keep record of failures.
All TaskRunStatus stored in RetriesStatus will have no date within the RetriesStatus as is redundant.




resourcesResult

[]github.com/tektoncd/pipeline/pkg/result.RunResult



(Optional)
Results from Resources built during the TaskRun.
This is tomb-stoned along with the removal of pipelineResources
Deprecated: this field is not populated and is preserved only for backwards compatibility




taskResults


[]TaskRunResult




(Optional)
TaskRunResults are the list of results written out by the taskâ€™s containers




sidecars


[]SidecarState




The list has one entry per sidecar in the manifest. Each entry is
represents the imageid of the corresponding sidecar.




taskSpec


TaskSpec




TaskSpec contains the Spec from the dereferenced Task definition used to instantiate this TaskRun.




provenance


Provenance




(Optional)
Provenance contains some key authenticated metadata about how a software artifact was built (what sources, what inputs/outputs, etc.).




spanContext

map[string]string



SpanContext contains tracing span context fields




TaskRunStepOverride


(Appears on:PipelineTaskRunSpec, TaskRunSpec)


TaskRunStepOverride is used to override the values of a Step in the corresponding Task.




Field
Description





name

string



The name of the Step to override.




resources


Kubernetes core/v1.ResourceRequirements




The resource requirements to apply to the Step.




TaskSpec


(Appears on:ClusterTask, Task, EmbeddedTask, TaskRunSpec, TaskRunStatusFields)


TaskSpec defines the desired state of Task.




Field
Description





resources


TaskResources




(Optional)
Resources is a list input and output resource to run the task
Resources are represented in TaskRuns as bindings to instances of
PipelineResources.
Deprecated: Unused, preserved only for backwards compatibility




params


ParamSpecs




(Optional)
Params is a list of input parameters required to run the task. Params
must be supplied as inputs in TaskRuns unless they declare a default
value.




displayName

string



(Optional)
DisplayName is a user-facing name of the task that may be
used to populate a UI.




description

string



(Optional)
Description is a user-facing description of the task that may be
used to populate a UI.




steps


[]Step




Steps are the steps of the build; each step is run sequentially with the
source mounted into /workspace.




volumes


[]Kubernetes core/v1.Volume




Volumes is a collection of volumes that are available to mount into the
steps of the build.




stepTemplate


StepTemplate




StepTemplate can be used as the basis for all step containers within the
Task, so that the steps inherit settings on the base container.




sidecars


[]Sidecar




Sidecars are run alongside the Taskâ€™s step containers. They begin before
the steps start and end after the steps complete.




workspaces


[]WorkspaceDeclaration




Workspaces are the volumes that this Task requires.




results


[]TaskResult




Results are values that this Task can output




TimeoutFields


(Appears on:PipelineRunSpec)


TimeoutFields allows granular specification of pipeline, task, and finally timeouts




Field
Description





pipeline


Kubernetes meta/v1.Duration




Pipeline sets the maximum allowed duration for execution of the entire pipeline. The sum of individual timeouts for tasks and finally must not exceed this value.




tasks


Kubernetes meta/v1.Duration




Tasks sets the maximum allowed duration of this pipelineâ€™s tasks




finally


Kubernetes meta/v1.Duration




Finally sets the maximum allowed duration of this pipelineâ€™s finally




WhenExpression


(Appears on:ChildStatusReference, PipelineRunRunStatus, PipelineRunTaskRunStatus, SkippedTask)


WhenExpression allows a PipelineTask to declare expressions to be evaluated before the Task is run
to determine whether the Task should be executed or skipped




Field
Description





input

string



Input is the string for guard checking which can be a static input or an output from a parent Task




operator

k8s.io/apimachinery/pkg/selection.Operator



Operator that represents an Inputâ€™s relationship to the values




values

[]string



Values is an array of strings, which is compared against the input, for guard checking
It must be non-empty




cel

string



(Optional)
CEL is a string of Common Language Expression, which can be used to conditionally execute
the task based on the result of the expression evaluation
More info about CEL syntax: https://github.com/google/cel-spec/blob/master/doc/langdef.md




WhenExpressions
([]github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1.WhenExpression alias)

(Appears on:PipelineTask, Step)


WhenExpressions are used to specify whether a Task should be executed or skipped
All of them need to evaluate to True for a guarded Task to be executed.

WorkspaceBinding


(Appears on:RunSpec, CustomRunSpec, PipelineRunSpec, TaskRunSpec)


WorkspaceBinding maps a Taskâ€™s declared workspace to a Volume.




Field
Description





name

string



Name is the name of the workspace populated by the volume.




subPath

string



(Optional)
SubPath is optionally a directory on the volume which should be used
for this binding (i.e. the volume will be mounted at this sub directory).




volumeClaimTemplate


Kubernetes core/v1.PersistentVolumeClaim




(Optional)
VolumeClaimTemplate is a template for a claim that will be created in the same namespace.
The PipelineRun controller is responsible for creating a unique claim for each instance of PipelineRun.




persistentVolumeClaim


Kubernetes core/v1.PersistentVolumeClaimVolumeSource




(Optional)
PersistentVolumeClaimVolumeSource represents a reference to a
PersistentVolumeClaim in the same namespace. Either this OR EmptyDir can be used.




emptyDir


Kubernetes core/v1.EmptyDirVolumeSource




(Optional)
EmptyDir represents a temporary directory that shares a Taskâ€™s lifetime.
More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
Either this OR PersistentVolumeClaim can be used.




configMap


Kubernetes core/v1.ConfigMapVolumeSource




(Optional)
ConfigMap represents a configMap that should populate this workspace.




secret


Kubernetes core/v1.SecretVolumeSource




(Optional)
Secret represents a secret that should populate this workspace.




projected


Kubernetes core/v1.ProjectedVolumeSource




(Optional)
Projected represents a projected volume that should populate this workspace.




csi


Kubernetes core/v1.CSIVolumeSource




(Optional)
CSI (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers.




WorkspaceDeclaration


(Appears on:TaskSpec)


WorkspaceDeclaration is a declaration of a volume that a Task requires.




Field
Description





name

string



Name is the name by which you can bind the volume at runtime.




description

string



(Optional)
Description is an optional human readable description of this volume.




mountPath

string



(Optional)
MountPath overrides the directory that the volume will be made available at.




readOnly

bool



ReadOnly dictates whether a mounted volume is writable. By default this
field is false and so mounted volumes are writable.




optional

bool



Optional marks a Workspace as not being required in TaskRuns. By default
this field is false and so declared workspaces are required.




WorkspacePipelineTaskBinding


(Appears on:PipelineTask)


WorkspacePipelineTaskBinding describes how a workspace passed into the pipeline should be
mapped to a taskâ€™s declared workspace.




Field
Description





name

string



Name is the name of the workspace as declared by the task




workspace

string



(Optional)
Workspace is the name of the workspace declared by the pipeline




subPath

string



(Optional)
SubPath is optionally a directory on the volume which should be used
for this binding (i.e. the volume will be mounted at this sub directory).




WorkspaceUsage


(Appears on:Sidecar, Step)


WorkspaceUsage is used by a Step or Sidecar to declare that it wants isolated access
to a Workspace defined in a Task.




Field
Description





name

string



Name is the name of the workspace this Step or Sidecar wants access to.




mountPath

string



MountPath is the path that the workspace should be mounted to inside the Step or Sidecar,
overriding any MountPath specified in the Taskâ€™s WorkspaceDeclaration.




CustomRunResult


(Appears on:CustomRunStatusFields)


CustomRunResult used to describe the results of a task




Field
Description





name

string



Name the given name




value

string



Value the given value of the result




CustomRunStatus


(Appears on:CustomRun, PipelineRunRunStatus, PipelineRunRunStatus, CustomRunStatusFields)


CustomRunStatus defines the observed state of CustomRun




Field
Description





Status


knative.dev/pkg/apis/duck/v1.Status





(Members of Status are embedded into this type.)





CustomRunStatusFields


CustomRunStatusFields





(Members of CustomRunStatusFields are embedded into this type.)

CustomRunStatusFields inlines the status fields.




CustomRunStatusFields


(Appears on:CustomRunStatus)


CustomRunStatusFields holds the fields of CustomRunâ€™s status.  This is defined
separately and inlined so that other types can readily consume these fields
via duck typing.




Field
Description





startTime


Kubernetes meta/v1.Time




(Optional)
StartTime is the time the build is actually started.




completionTime


Kubernetes meta/v1.Time




(Optional)
CompletionTime is the time the build completed.




results


[]CustomRunResult




(Optional)
Results reports any output result values to be consumed by later
tasks in a pipeline.




retriesStatus


[]CustomRunStatus




(Optional)
RetriesStatus contains the history of CustomRunStatus, in case of a retry.




extraFields

k8s.io/apimachinery/pkg/runtime.RawExtension



ExtraFields holds arbitrary fields provided by the custom task
controller.






Generated with gen-crd-api-reference-docs
.


	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Affinity Assistants
Affinity Assistant is a feature to coschedule PipelineRun pods to the same node
based on kubernetes pod affinity so that it possible for the taskruns to execute parallel while sharing volume.
Available Affinity Assistant Modes are coschedule workspaces, coschedule pipelineruns,
isolate pipelinerun and disabled.

ğŸŒ± coschedule pipelineruns and isolate pipelinerun modes are alpha features.
coschedule workspaces is a stable feature



coschedule workspaces - When a PersistentVolumeClaim is used as volume source for a Workspace in a PipelineRun,
all TaskRun pods within the PipelineRun that share the Workspace will be scheduled to the same Node. (Note: Only one pvc-backed workspace can be mounted to each TaskRun in this mode.)


coschedule pipelineruns - All TaskRun pods within the PipelineRun will be scheduled to the same Node.


isolate pipelinerun - All TaskRun pods within the PipelineRun will be scheduled to the same Node,
and only one PipelineRun is allowed to run on a node at a time.


disabled - The Affinity Assistant is disabled. No pod coscheduling behavior.


This means that Affinity Assistant is incompatible with other affinity rules
configured for the TaskRun pods (i.e. other affinity rules specified in custom PodTemplate will be overwritten by Affinity Assistant).
If the PipelineRun has a custom PodTemplate configured, the NodeSelector and Tolerations fields will also be set on the Affinity Assistant pod. The Affinity Assistant
is deleted when the PipelineRun is completed.
Currently, the Affinity Assistant Modes can be configured by the disable-affinity-assistant and coschedule feature flags.
The disable-affinity-assistant feature flag is now deprecated and will be removed in release v0.60. At the time, the Affinity Assistant Modes will be only determined by the coschedule feature flag.
The following chart summarizes the Affinity Assistant Modes with different combinations of the disable-affinity-assistant and coschedule feature flags during migration (when both feature flags are present) and after the migration (when only the coschedule flag is present).

    
        
            disable-affinity-assistant
            coschedule
            behavior during migration
            behavior after migration
        
    
    
        
            false (default)
            disabled
            N/A: invalid
            disabled
        
        
            false (default)
            workspaces (default)
            coschedule workspaces
            coschedule workspaces
        
        
            false (default)
            pipelineruns
            N/A: invalid
            coschedule pipelineruns
        
        
            false (default)
            isolate-pipelinerun
            N/A: invalid
            isolate pipelinerun
        
        
            true
            disabled
            disabled
            disabled
        
        
            true
            workspaces (default)
            disabled
            coschedule workspaces
        
        
            true
            pipelineruns
            coschedule pipelineruns
            coschedule pipelineruns
        
        
            true
            isolate-pipelinerun
            isolate pipelinerun
            isolate pipelinerun
        
    

Note: For users who previously accepted the default behavior (disable-affinity-assistant: false) but now want one of the new features, you need to set disable-affinity-assistant to â€œtrueâ€ and then turn on the new behavior by setting the coschedule flag. For users who previously disabled the affinity assistant but want one of the new features, just set the coschedule flag accordingly.
Note: Affinity Assistant use Inter-pod affinity and anti-affinity
that require substantial amount of processing which can slow down scheduling in large clusters
significantly. We do not recommend using the affinity assistant in clusters larger than several hundred nodes
Note: Pod anti-affinity requires nodes to be consistently labelled, in other words every
node in the cluster must have an appropriate label matching topologyKey. If some or all nodes
are missing the specified topologyKey label, it can lead to unintended behavior.
Note: Any time during the execution of a pipelineRun, if the node with a placeholder Affinity Assistant pod and
the taskRun pods sharing a workspace is cordoned or disabled for scheduling anything new (tainted), the
pipelineRun controller deletes the placeholder pod. The taskRun pods on a cordoned node continues running
until completion. The deletion of a placeholder pod triggers creating a new placeholder pod on any available node
such that the rest of the pipelineRun can continue without any disruption until it finishes.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Workspaces

Overview

Workspaces in Tasks and TaskRuns
Workspaces in Pipelines and PipelineRuns
Optional Workspaces
Isolated Workspaces


Configuring Workspaces

Using Workspaces in Tasks

Isolating Workspaces to Specific Steps or Sidecars
Setting a default TaskRun Workspace Binding
Using Workspace variables in Tasks
Mapping Workspaces in Tasks to TaskRuns
Examples of TaskRun definition using Workspaces


Using Workspaces in Pipelines

Specifying Workspace order in a Pipeline and Affinity Assistants
Specifying Workspaces in PipelineRuns
Example PipelineRun definition using Workspaces


Specifying VolumeSources in Workspaces

Using PersistentVolumeClaims as VolumeSource
Using other types of VolumeSources




Using Persistent Volumes within a PipelineRun
More examples

Overview
Workspaces allow Tasks to declare parts of the filesystem that need to be provided
at runtime by TaskRuns. A TaskRun can make these parts of the filesystem available
in many ways: using a read-only ConfigMap or Secret, an existing PersistentVolumeClaim
shared with other Tasks, create a PersistentVolumeClaim from a provided VolumeClaimTemplate, or simply an emptyDir that is discarded when the TaskRun
completes.
Workspaces are similar to Volumes except that they allow a Task author
to defer to users and their TaskRuns when deciding which class of storage to use.
Workspaces can serve the following purposes:

Storage of inputs and/or outputs
Sharing data among Tasks
A mount point for credentials held in Secrets
A mount point for configurations held in ConfigMaps
A mount point for common tools shared by an organization
A cache of build artifacts that speed up jobs

Workspaces in Tasks and TaskRuns
Tasks specify where a Workspace resides on disk for its Steps. At
runtime, a TaskRun provides the specific details of the Volume that is
mounted into that Workspace.
This separation of concerns allows for a lot of flexibility. For example, in isolation,
a single TaskRun might simply provide an emptyDir volume that mounts quickly
and disappears at the end of the run. In a more complex system, however, a TaskRun
might use a PersistentVolumeClaim which is pre-populated with
data for the Task to process. In both scenarios the Task's
Workspace declaration remains the same and only the runtime
information in the TaskRun changes.
Tasks can also share Workspaces with their Sidecars, though thereâ€™s a little more
configuration involved to add the required volumeMount. This allows for a
long-running process in a Sidecar to share data with the executing Steps of a Task.
Note: If the enable-api-fields feature-flag is set to "beta" then workspaces
will automatically be available to Sidecars too!
Workspaces in Pipelines and PipelineRuns
A Pipeline can use Workspaces to show how storage will be shared through
its Tasks. For example, Task A might clone a source repository onto a Workspace
and Task B might compile the code that it finds in that Workspace. Itâ€™s
the Pipeline's job to ensure that the Workspace these two Tasks use is the
same, and more importantly, that the order in which they access the Workspace is
correct.
PipelineRuns perform mostly the same duties as TaskRuns - they provide the
specific Volume information to use for the Workspaces used by each Pipeline.
PipelineRuns have the added responsibility of ensuring that whatever Volume type they
provide can be safely and correctly shared across multiple Tasks.
Optional Workspaces
Both Tasks and Pipelines can declare a Workspace â€œoptionalâ€. When an optional Workspace
is declared the TaskRun or PipelineRun may omit a Workspace Binding for that Workspace.
The Task or Pipeline behaviour may change when the Binding is omitted. This feature has
many uses:

A Task may optionally accept credentials to run authenticated commands.
A Pipeline may accept optional configuration that changes the linting or compilation
parameters used.
An optional build cache may be provided to speed up compile times.

See the section Using Workspaces in Tasks for more info on
the optional field.
Isolated Workspaces
This is a beta feature. The enable-api-fields feature flag must be set to "beta"
for Isolated Workspaces to function.
Certain kinds of data are more sensitive than others. To reduce exposure of sensitive data Task
authors can isolate Workspaces to only those Steps and Sidecars that require access to
them. The primary use-case for this is credentials but it can apply to any data that should have
its access strictly limited to only specific container images.
See the section Isolating Workspaces to Specific Steps or Sidecars
for more info on this feature.
Configuring Workspaces
This section describes how to configure one or more Workspaces in a TaskRun.
Using Workspaces in Tasks
To configure one or more Workspaces in a Task, add a workspaces list with each entry using the following fields:

name -  (required) A unique string identifier that can be used to refer to the workspace
description - An informative string describing the purpose of the Workspace
readOnly - A boolean declaring whether the Task will write to the Workspace. Defaults to false.
optional - A boolean indicating whether a TaskRun can omit the Workspace. Defaults to false.
mountPath - A path to a location on disk where the workspace will be available to Steps. If a
mountPath is not provided the workspace will be placed by default at /workspace/<name> where <name>
is the workspaceâ€™s unique name.

Note the following:

A Task definition can include as many Workspaces as it needs. It is recommended that Tasks use
at most one writeable Workspace.
A readOnly Workspace will have its volume mounted as read-only. Attempting to write
to a readOnly Workspace will result in errors and failed TaskRuns.

Below is an example Task definition that includes a Workspace called messages to which the Task writes a message:
spec:
  steps:
    - name: write-message
      image: ubuntu
      script: |
        #!/usr/bin/env bash
        set -xe
        if [ "$(workspaces.messages.bound)" == "true" ] ; then
          echo hello! > $(workspaces.messages.path)/message
        fi        
  workspaces:
    - name: messages
      description: |
        The folder where we write the message to. If no workspace
        is provided then the message will not be written.        
      optional: true
      mountPath: /custom/path/relative/to/root
Sharing Workspaces with Sidecars
A Task's Sidecars are also able to access the Workspaces the Task defines but must have their
volumeMount configuration set explicitly. Below is an example Task that shares a Workspace between
its Steps and its Sidecar. In the example a Sidecar sleeps for a short amount of time and then writes
a ready file which the Step is waiting for:
spec:
  workspaces:
    - name: signals
  steps:
    - image: alpine
      script: |
        while [ ! -f "$(workspaces.signals.path)/ready" ]; do
          echo "Waiting for ready file..."
          sleep 1
        done
        echo "Saw ready file!"        
  sidecars:
    - image: alpine
      # Note: must explicitly include volumeMount for the workspace to be accessible in the Sidecar
      volumeMounts:
        - name: $(workspaces.signals.volume)
          mountPath: $(workspaces.signals.path)
      script: |
        sleep 3
        touch "$(workspaces.signals.path)/ready"        
Note: Starting in Pipelines v0.24.0 Sidecars automatically get access to Workspaces. This is a
beta feature and requires Pipelines to have the â€œbetaâ€ feature gate enabled.
If a Sidecar already has a volumeMount at the location expected for a workspace then that workspace is
not bound to the Sidecar. This preserves backwards-compatibility with any existing uses of the volumeMount
trick described above.
Isolating Workspaces to Specific Steps or Sidecars
This is a beta feature. The enable-api-fields feature flag must be set to "beta"
for Isolated Workspaces to function.
To limit access to a Workspace from a subset of a Task's Steps or Sidecars requires
adding a workspaces declaration to those sections. In the following example a Task has several
Steps but only the one that performs a git clone will be able to access the SSH credentials
passed into it:
spec:
  workspaces:
  - name: ssh-credentials
    description: An .ssh directory with keys, known_host and config files used to clone the repo.
  steps:
  - name: clone-repo
    workspaces:
    - name: ssh-credentials # This Step receives the sensitive workspace; the others do not.
    image: git
    script: # git clone ...
  - name: build-source
    image: third-party-source-builder:latest # This image doesn't get access to ssh-credentials.
  - name: lint-source
    image: third-party-source-linter:latest # This image doesn't get access to ssh-credentials.
It can potentially be useful to mount Workspaces to different locations on a per-Step or
per-Sidecar basis and this is also supported:
kind: Task
spec:
  workspaces:
  - name: ws
    mountPath: /workspaces/ws
  steps:
  - name: edit-files-1
    workspaces:
    - name: ws
      mountPath: /foo # overrides mountPath
  - name: edit-files-2
    workspaces:
    - name: ws # no mountPath specified so will use /workspaces/ws
  sidecars:
  - name: watch-files-on-workspace
    workspaces:
    - name: ws
      mountPath: /files # overrides mountPath
Setting a default TaskRun Workspace Binding
An organization may want to specify default Workspace configuration for TaskRuns. This allows users to
use Tasks without having to know the specifics of Workspaces - they can simply rely on the platform
to use the default configuration when a Workspace is missing. To support this Tekton allows a default
Workspace Binding to be specified for TaskRuns. When the TaskRun executes, any Workspaces that
a Task requires but which are not provided by the TaskRun will be bound with the default configuration.
The configuration for the default Workspace Binding is added to the config-defaults ConfigMap, under
the default-task-run-workspace-binding key. For an example, see the Customizing basic execution
parameters section of the install doc.
Note: the default configuration is used for any required Workspace declared by a Task. Optional
Workspaces are not populated with the default binding. This is because a Task's behaviour will typically
differ slightly when an optional Workspace is bound.
Using Workspace variables in Tasks
The following variables make information about Workspaces available to Tasks:

$(workspaces.<name>.path) - specifies the path to a Workspace
where <name> is the name of the Workspace. This will be an
empty string when a Workspace is declared optional and not provided
by a TaskRun.
$(workspaces.<name>.bound) - either true or false, specifies
whether a workspace was bound. Always true if the workspace is required.
$(workspaces.<name>.claim) - specifies the name of the PersistentVolumeClaim used as a volume source for the Workspace
where <name> is the name of the Workspace. If a volume source other than PersistentVolumeClaim is used, an empty string is returned.
$(workspaces.<name>.volume)- specifies the name of the Volume
provided for a Workspace where <name> is the name of the Workspace.

Mapping Workspaces in Tasks to TaskRuns
A TaskRun that executes a Task containing a workspaces list must bind
those workspaces to actual physical Volumes. To do so, the TaskRun includes
its own workspaces list. Each entry in the list contains the following fields:

name - (required) The name of the Workspace within the Task for which the Volume is being provided
subPath - An optional subdirectory on the Volume to store data for that Workspace

The entry must also include one VolumeSource. See Specifying VolumeSources in Workspaces for more information.
Caution:

The Workspaces declared in a Task must be available when executing the associated TaskRun.
Otherwise, the TaskRun will fail.

Examples of TaskRun definition using Workspaces
The following example illustrate how to specify Workspaces in your TaskRun definition,
an emptyDir
is provided for a Taskâ€™s workspace called myworkspace:
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  generateName: example-taskrun-
spec:
  taskRef:
    name: example-task
  workspaces:
    - name: myworkspace # this workspace name must be declared in the Task
      emptyDir: {}      # emptyDir volumes can be used for TaskRuns, 
                        # but consider using a PersistentVolumeClaim for PipelineRuns
For examples of using other types of volume sources, see Specifying VolumeSources in Workspaces.
For a more in-depth example, see Workspaces in a TaskRun.
Using Workspaces in Pipelines
While individual Tasks declare the Workspaces they need to run, the Pipeline decides
which Workspaces are shared among its Tasks. To declare shared Workspaces in a Pipeline,
you must add the following information to your Pipeline definition:

A list of Workspaces that your PipelineRuns will be providing. Use the workspaces field to
specify the target Workspaces in your Pipeline definition as shown below. Each entry in the
list must have a unique name.
A mapping of Workspace names between the Pipeline and the Task definitions.

The example below defines a Pipeline with a Workspace named pipeline-ws1. This
Workspace is bound in two Tasks - first as the output workspace declared by the gen-code
Task, then as the src workspace declared by the commit Task. If the Workspace
provided by the PipelineRun is a PersistentVolumeClaim then these two Tasks can share
data within that Workspace.
spec:
  workspaces:
    - name: pipeline-ws1 # Name of the workspace in the Pipeline
    - name: pipeline-ws2
      optional: true
  tasks:
    - name: use-ws-from-pipeline
      taskRef:
        name: gen-code # gen-code expects a workspace named "output"
      workspaces:
        - name: output
          workspace: pipeline-ws1
    - name: use-ws-again
      taskRef:
        name: commit # commit expects a workspace named "src"
      workspaces:
        - name: src
          workspace: pipeline-ws1
      runAfter:
        - use-ws-from-pipeline # important: use-ws-from-pipeline writes to the workspace first
Include a subPath in the Workspace Binding to mount different parts of the same volume for different Tasks. See a full example of this kind of Pipeline which writes data to two adjacent directories on the same Volume.
The subPath specified in a Pipeline will be appended to any subPath specified as part of the PipelineRun workspace declaration. So a PipelineRun declaring a Workspace with subPath of /foo for a Pipeline who binds it to a Task with subPath of /bar will end up mounting the Volumeâ€™s /foo/bar directory.
Specifying Workspace order in a Pipeline and Affinity Assistants
Sharing a Workspace between Tasks requires you to define the order in which those Tasks
write to or read from that Workspace. Use the runAfter field in your Pipeline definition
to define when a Task should be executed. For more information, see the runAfter documentation.
When a PersistentVolumeClaim is used as volume source for a Workspace in a PipelineRun,
an Affinity Assistant will be created. For more information, see the Affinity Assistants documentation.
Note: When coschedule is set to workspaces or disabled, it is not allowed to bind multiple PersistentVolumeClaim based workspaces to the same TaskRun in a PipelineRun due to potential Availability Zone conflicts.
See more details in Availability Zones.
Specifying Workspaces in PipelineRuns
For a PipelineRun to execute a Pipeline that includes one or more Workspaces, it needs to
bind the Workspace names to volumes using its own workspaces field. Each entry in
this list must correspond to a Workspace declaration in the Pipeline. Each entry in the
workspaces list must specify the following:

name - (required) the name of the Workspace specified in the Pipeline definition for which a volume is being provided.
subPath - (optional) a directory on the volume that will store that Workspace's data. This directory must exist at the
time the TaskRun executes, otherwise the execution will fail.

The entry must also include one VolumeSource. See Using VolumeSources with Workspaces for more information.
Note: If the Workspaces specified by a Pipeline are not provided at runtime by a PipelineRun, that PipelineRun will fail.
You can pass in extra Workspaces if needed depending on your use cases. An example use
case is when your CI system autogenerates PipelineRuns and it has Workspaces it wants to
provide to all PipelineRuns. Because you can pass in extra Workspaces, you donâ€™t have to
go through the complexity of checking each Pipeline and providing only the required Workspaces.
Example PipelineRun definition using Workspaces
In the example below, a volumeClaimTemplate is provided for how a PersistentVolumeClaim should be created for a workspace named
myworkspace declared in a Pipeline. When using volumeClaimTemplate a new PersistentVolumeClaim is created for
each PipelineRun and it allows the user to specify e.g. size and StorageClass for the volume.
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: example-pipelinerun-
spec:
  pipelineRef:
    name: example-pipeline
  workspaces:
    - name: myworkspace # this workspace name must be declared in the Pipeline
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce # access mode may affect how you can use this volume in parallel tasks
          resources:
            requests:
              storage: 1Gi
For examples of using other types of volume sources, see Specifying VolumeSources in Workspaces.
For a more in-depth example, see the Workspaces in PipelineRun YAML sample.
Specifying VolumeSources in Workspaces
You can only use a single type of VolumeSource per Workspace entry. The configuration
options differ for each type. Workspaces support the following fields:
Using PersistentVolumeClaims as VolumeSource
PersistentVolumeClaim volumes are a good choice for sharing data among Tasks within a Pipeline.
Beware that the access mode
configured for the PersistentVolumeClaim effects how you can use the volume for parallel Tasks in a Pipeline. See
Specifying workspace order in a Pipeline and Affinity Assistants for more information about this.
There are two ways of using PersistentVolumeClaims as a VolumeSource.
volumeClaimTemplate
The volumeClaimTemplate is a template of a PersistentVolumeClaim volume,
created for each PipelineRun or TaskRun. When the volume is created from a template in a PipelineRun or TaskRun
it will be deleted when the PipelineRun or TaskRun is deleted.
workspaces:
  - name: myworkspace
    volumeClaimTemplate:
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
persistentVolumeClaim
The persistentVolumeClaim field references an existing persistentVolumeClaim volume. The example exposes only the subdirectory my-subdir from that PersistentVolumeClaim
workspaces:
  - name: myworkspace
    persistentVolumeClaim:
      claimName: mypvc
    subPath: my-subdir
Using other types of VolumeSources
emptyDir
The emptyDir field references an emptyDir volume which holds
a temporary directory that only lives as long as the TaskRun that invokes it. emptyDir volumes are not suitable for sharing data among Tasks within a Pipeline.
However, they work well for single TaskRuns where the data stored in the emptyDir needs to be shared among the Steps of the Task and discarded after execution.
workspaces:
  - name: myworkspace
    emptyDir: {}
configMap
The configMap field references a configMap volume.
Using a configMap as a Workspace has the following limitations:

configMap volume sources are always mounted as read-only. Steps cannot write to them and will error out if they try.
The configMap you want to use as a Workspace must exist prior to submitting the TaskRun.
configMaps are size-limited to 1MB.

workspaces:
  - name: myworkspace
    configmap:
      name: my-configmap
secret
The secret field references a secret volume.
Using a secret volume has the following limitations:

secret volume sources are always mounted as read-only. Steps cannot write to them and will error out if they try.
The secret you want to use as a Workspace must exist prior to submitting the TaskRun.
secret are size-limited to 1MB.

workspaces:
  - name: myworkspace
    secret:
      secretName: my-secret
projected
The projected field references a projected volume.
projected volume workspaces are a beta feature.
Using a projected volume has the following limitations:

projected volume sources are always mounted as read-only. Steps cannot write to them and will error out if they try.
The volumes you want to project as a Workspace must exist prior to submitting the TaskRun.
The following volumes can be projected: configMap, secret, serviceAccountToken and downwardApi

workspaces:
  - name: myworkspace
    projected:
      sources:
        - configMap:
            name: my-configmap
        - secret:
            name: my-secret
csi
The csi field references a csi volume.
csi workspaces are a beta feature.
Using a csi volume has the following limitations:
 

csi volume sources require a volume driver to use, which must correspond to the value by the CSI driver as defined in the CSI spec.

workspaces:
  - name: my-credentials
    csi:
      driver: secrets-store.csi.k8s.io
      readOnly: true
      volumeAttributes:
        secretProviderClass: "vault-database"
Example of CSI workspace using Hashicorp Vault:

Install the required csi driver. eg. secrets-store-csi-driver
Install the vault Provider onto the kubernetes cluster. Reference
Deploy a provider via example
Create a SecretProviderClass Provider using the following yaml
Specify the ServiceAccount via vault:

vault write auth/kubernetes/role/database \
bound_service_account_names=default \
bound_service_account_namespaces=default \
policies=internal-app \
ttl=20m
If you need support for a VolumeSource type not listed above, open an issue or
a pull request.
Using Persistent Volumes within a PipelineRun
When using a workspace with a PersistentVolumeClaim as VolumeSource,
a Kubernetes Persistent Volumes is used within the PipelineRun.
There are some details that are good to know when using Persistent Volumes within a PipelineRun.
Storage Class
PersistentVolumeClaims specify a Storage Class for the underlying Persistent Volume. Storage Classes have specific
characteristics. If a StorageClassName is not specified for your PersistentVolumeClaim, the cluster defined default
Storage Class is used. For regional clusters - clusters that typically consist of Nodes located in multiple Availability
Zones - it is important to know whether your Storage Class is available to all Nodes. Default Storage Classes are typically
only available to Nodes within one Availability Zone. There is usually an option to use a regional Storage Class,
but they have trade-offs, e.g. you need to pay for multiple volumes since they are replicated and your volume may have
substantially higher latency.
Access Modes
A PersistentVolumeClaim specifies an Access Mode.
Available Access Modes are ReadWriteOnce, ReadWriteMany and ReadOnlyMany. What Access Mode you can use depend on
the storage solution that you are using.


ReadWriteOnce is the most commonly available Access Mode. A volume with this Access Mode can only be mounted on one
Node at a time. This can be problematic for a Pipeline that has parallel Tasks that access the volume concurrently.
The Affinity Assistant helps with this problem by scheduling all Tasks that use the same PersistentVolumeClaim to
the same Node.


ReadOnlyMany is read-only and is less common in a CI/CD-pipeline. These volumes often need to be â€œpreparedâ€ with data
in some way before use. Dynamically provided volumes can usually not be used in read-only mode.


ReadWriteMany is the least commonly available Access Mode. If you use this access mode and these volumes are available
to all Nodes within your cluster, you may want to disable the Affinity Assistant.


Availability Zones
Persistent Volumes are â€œzonalâ€ in some cloud providers like GKE (i.e. they live within a single Availability Zone and cannot be accessed from a pod living in another Availability Zone). When using a workspace backed by a PersistentVolumeClaim (typically only available within a Data Center), the TaskRun pods can be scheduled to any Availability Zone in a regional cluster. This results in potential Availability Zone scheduling conflict when two pods requiring the same Volume are scheduled to different Availability Zones (see issue #3480 and #5275).
To avoid such conflict in PipelineRuns, Tekton provides Affinity Assistants which schedule all TaskRun pods or all TaskRun sharing a PersistentVolumeClaim in a PipelineRun to the same Node depending on the coschedule mode.
Specifically, for users use zonal clusters like GKE or use PersistentVolumeClaim in ReadWriteOnce access modes, please set coschedule: workspaces to schedule each of the TaskRun pod to the same zone as the associated PersistentVolumeClaim. In addition, for users want to bind multiple PersistentVolumeClaims to a single TaskRun, please set coschedule: pipelineruns to schedule all TaskRun pods and PersistentVolumeClaim in a PipelineRun to the same zone.
More examples
See the following in-depth examples of configuring Workspaces:

Workspaces in a TaskRun
Workspaces in a PipelineRun
Workspaces from a volumeClaimTemplate in a PipelineRun


	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Matrix

Overview
Configuring a Matrix

Generating Combinations
Explicit Combinations


Concurrency Control
Parameters

Parameters in Matrix.Params
Parameters in Matrix.Include.Params
Specifying both params and matrix in a PipelineTask


Context Variables

Access Matrix Combinations Length
Access Aggregated Results Length


Results

Specifying Results in a Matrix

Results in Matrix.Params
Results in Matrix.Include.Params


Results from fanned out PipelineTasks


Retries
Examples

Matrix Combinations with Matrix.Params only
Matrix Combinations with Matrix.Params and Matrix.Include
PipelineTasks with Tasks
PipelineTasks with Custom Tasks



Overview
Matrix is used to fan out Tasks in a Pipeline. This doc will explain the details of matrix support in
Tekton.
Documentation for specifying Matrix in a Pipeline:

Specifying Matrix in Tasks
Specifying Matrix in Finally Tasks
Specifying Matrix in Custom Tasks


ğŸŒ± Matrix is an beta feature.
The enable-api-fields feature flag can be set to "beta" to specify Matrix in a PipelineTask.

Configuring a Matrix
A Matrix allows you to generate combinations and specify explicit combinations to fan out a PipelineTask.
Generating Combinations
The Matrix.Params is used to generate combinations to fan out a PipelineTask.
    matrix:
      params:
        - name: platform
          value:
          - linux
          - mac
        - name: browser
          value:
          - safari
          - chrome
  ...
Combinations generated
{ "platform": "linux", "browser": "safari" }
{ "platform": "linux", "browser": "chrome"}
{ "platform": "mac", "browser": "safari" }
{ "platform": "mac", "browser": "chrome"}
See another example
Explicit Combinations
The Matrix.Include is used to add explicit combinations to fan out a PipelineTask.
    matrix:
      params:
        - name: platform
          value:
          - linux
          - mac
        - name: browser
          value:
          - safari
          - chrome
      include:
        - name: linux-url
          params:
            - name: platform
              value: linux
            - name: url
              value: some-url
        - name: non-existent-browser
          params:
            - name: browser
              value: "i-do-not-exist"
  ...
The first Matrix.Include clause adds "url": "some-url" only to the original matrix combinations that include "platform": "linux" and the second Matrix.Include clause cannot be added to any original matrix combination without overwriting any params of the original combinations, so it is added as an additional matrix combination:
Combinations generated
{ "platform": "linux", "browser": "safari", "url": "some-url" }
{ "platform": "linux", "browser": "chrome", "url": "some-url"}
{ "platform": "mac", "browser": "safari" }
{ "platform": "mac", "browser": "chrome"}
{ "browser": "i-do-not-exist"}
See another example
The Matrix.Include can also be used without Matrix.Params to generate explicit combinations to fan out a PipelineTask.
    matrix:
        include:
          - name: build-1
            params:
              - name: IMAGE
                value: "image-1"
              - name: DOCKERFILE
                value: "path/to/Dockerfile1"
          - name: build-2
            params:
              - name: IMAGE
                value: "image-2"
              - name: DOCKERFILE
                value: "path/to/Dockerfile2"
          - name: build-3
            params:
              - name: IMAGE
                value: "image-3"
              - name: DOCKERFILE
                value: "path/to/Dockerfile3"
  ...
This configuration allows users to take advantage of Matrix to fan out without having an auto-populated Matrix. Matrix with include section without Params section creates the number of TaskRuns specified in the Include section with the specified Parameters.
Combinations generated
{ "IMAGE": "image-1", "DOCKERFILE": "path/to/Dockerfile1" }
{ "IMAGE": "image-2", "DOCKERFILE": "path/to/Dockerfile2"}
{ "IMAGE": "image-3", "DOCKERFILE": "path/to/Dockerfile3}
DisplayName
Matrix creates multiple taskRuns with the same pipelineTask. Each taskRun has its unique combination params based
on the matrix specifications. These params can now be surfaced and used to configure unique name of each matrix
instance such that it is easier to distinguish all the instances based on their inputs.
pipelineSpec:
  tasks:
    - name: platforms-and-browsers
      displayName: "Platforms and Browsers: $(params.platform) and $(params.browser)"
      matrix:
        params:
          - name: platform
            value:
              - linux
              - mac
              - windows
          - name: browser
            value:
              - chrome
              - safari
              - firefox
      taskRef:
        name: platform-browsers
The displayName is available as part of pipelineRun.status.childReferences with each taskRun.
This allows the clients to consume displayName wherever needed:
[
  {
    "apiVersion": "tekton.dev/v1",
    "displayName": "Platforms and Browsers: linux and chrome",
    "kind": "TaskRun",
    "name": "matrixed-pr-vcx79-platforms-and-browsers-0",
    "pipelineTaskName": "platforms-and-browsers"
  },
  {
    "apiVersion": "tekton.dev/v1",
    "displayName": "Platforms and Browsers: mac and safari",
    "kind": "TaskRun",
    "name": "matrixed-pr-vcx79-platforms-and-browsers-1",
    "pipelineTaskName": "platforms-and-browsers"
  }
]
matrix.include[].name
matrix.include[] section allows specifying a name along with a list of params. This name field is available as
part of the pipelineRun.status.childReferences[].displayName if specified.
displayName and matrix.include[].name can co-exist but matrix.include[].name takes higher precedence. It is also
possible for the pipeline author to specify params in matrix.include[].name which are resolved in the childReferences.
- name: platforms-and-browsers-with-include
  matrix:
    include:
      - name: "Platform: $(params.platform)"
        params:
          - name: platform
            value: linux111
  params:
    - name: browser
      value: chrome
Precedence Order



specification
precedence
childReferences[].displayName




tasks[].displayName
tasks[].displayName
tasks[].displayName


tasks[].matrix.include[].name
tasks[].matrix.include[].name
tasks[].matrix.include[].name


tasks[].displayName and tasks[].matrix.include[].name
tasks[].matrix.include[].name
tasks[].matrix.include[].name



Concurrency Control
The default maximum count of TaskRuns or Runs from a given Matrix is 256. To customize the maximum count of
TaskRuns or Runs generated from a given Matrix, configure the default-max-matrix-combinations-count in
config defaults. When a Matrix in PipelineTask would generate more than the maximum
TaskRuns or Runs, the Pipeline validation would fail.
Note: The matrix combination count includes combinations generated from both Matrix.Params and Matrix.Include.Params.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
data:
  default-service-account: "tekton"
  default-timeout-minutes: "20"
  default-max-matrix-combinations-count: "1024"
  ...
For more information, see installation customizations.
Parameters
Matrix takes in Parameters in two sections:

Matrix.Params: used to generate combinations to fan out the PipelineTask.
Matrix.Include.Params: used to specify specific combinations to fan out the PipelineTask.

Note that:

The names of the Parameters in the Matrix must match the names of the Parameters in the underlying
Task that they will be substituting.
The names of the Parameters in the Matrix must be unique. Specifying the same parameter multiple times
will result in a validation error.
A Parameter can be passed to either the matrix or params field, not both.
If the Matrix has an empty array Parameter, then the PipelineTask will be skipped.

For further details on specifying Parameters in the Pipeline and passing them to
PipelineTasks, see documentation.
Parameters in Matrix.Params
Matrix.Params supports string replacements from Parameters of type String, Array or Object.
tasks:
...
- name: task-4
  taskRef:
    name: task-4
  matrix:
    params:
    - name: param-one
      value:
      - $(params.foo) # string replacement from string param
      - $(params.bar[0]) # string replacement from array param
      - $(params.rad.key) # string replacement from object param
    - name: param-two
      value: $(params.bar) # array replacement from array param
Matrix.Params supports whole array replacements from array Parameters.
tasks:
...
- name: task-4
  taskRef:
    name: task-4
  matrix:
    params:
    - name: param-one
      value: $(params.bar[*]) # whole array replacement from array param
Parameters in Matrix.Include.Params
Matrix.Include.Params takes string replacements from Parameters of type String, Array or Object.
tasks:
...
- name: task-4
  taskRef:
    name: task-4
  matrix:
    include:
      - name: foo-bar-rad
        params:
        - name: foo
          value: $(params.foo) # string replacement from string param
        - name: bar
          value: $(params.bar[0]) # string replacement from array param
        - name: rad
          value: $(params.rad.key) # string replacement from object param
Specifying both params and matrix in a PipelineTask
In the example below, the test Task takes browser and platform Parameters of type
"string". A Pipeline used to run the Task on three browsers (using matrix) and one
platform (using params) would be specified as such and execute three TaskRuns:
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: platform-browser-tests
spec:
  tasks:
  - name: fetch-repository
    taskRef:
      name: git-clone
    ...
  - name: test
    matrix:
      params:
        - name: browser
          value:
            - chrome
            - safari
            - firefox
    params:
      - name: platform
        value: linux
    taskRef:
      name: browser-test
  ...
Context Variables
Similarly to the Parameters in the Params field, the Parameters in the Matrix field will accept
context variables that will be substituted, including:

PipelineRun name, namespace and uid
Pipeline name
PipelineTask retries

The following context variables allow users to access the matrix runtime data. Note: In order to create an ordering dependency, use runAfter or taskResult consumption as part of the same pipelineTask.
Access Matrix Combinations Length
The pipeline authors can access the total number of instances created as part of the matrix using the syntax: tasks.<pipelineTaskName>.matrix.length.
      - name: matrixed-echo-length
        runAfter:
          - matrix-emitting-results
        params:
          - name: matrixlength
            value: $(tasks.matrix-emitting-results.matrix.length)
Access Aggregated Results Length
The pipeline authors can access the length of the array of aggregated results that were
actually produced using the syntax: tasks.<pipelineTaskName>.matrix.<resultName>.length. This will allow users to loop over the results produced.
      - name: matrixed-echo-results-length
        runAfter:
          - matrix-emitting-results
        params:
          - name: matrixlength
            value: $(tasks.matrix-emitting-results.matrix.a-result.length)
See the full example here: pr-with-matrix-context-variables
Results
Specifying Results in a Matrix
Consuming Results from previous TaskRuns or Runs in a Matrix, which would dynamically generate
TaskRuns or Runs from the fanned out PipelineTask, is supported. Producing Results in from a
PipelineTask with a Matrix is not yet supported - see further details.
See the end-to-end example in PipelineRun with Matrix and Results.
Results in Matrix.Params
Matrix.Params supports whole array replacements and string replacements from Results of type String, Array or Object
tasks:
...
- name: task-4
  taskRef:
    name: task-4
  matrix:
    params:
    - name: values
      value: $(tasks.task-4.results.whole-array[*])
tasks:
...
- name: task-5
  taskRef:
    name: task-5
  matrix:
    params:
    - name: values
      value:
        - $(tasks.task-1.results.a-string-result)
        - $(tasks.task-2.results.an-array-result[0])
        - $(tasks.task-3.results.an-object-result.key)
For further information, see the example in PipelineRun with Matrix and Results.
Results in Matrix.Include.Params
Matrix.Include.Params supports string replacements from Results of type String, Array or Object.
tasks:
...
- name: task-4
  taskRef:
    name: task-4
  matrix:
    include:
      - name: foo-bar-duh
      params:
        - name: foo
          value: $(tasks.task-1.results.foo) # string replacement from string result
        - name: bar
          value: $(tasks.task-2.results.bar[0]) # string replacement from array result
        - name: duh
          value: $(tasks.task-2.results.duh.key) # string replacement from object result
Results from fanned out Matrixed PipelineTasks
Emitting Results from fanned out PipelineTasks is now supported. Each fanned out
TaskRun that produces Result of type string will be aggregated into an array
of Results during reconciliation, in which the whole array of Results can be consumed by another pipelineTask using the star notion [*].
Note: A known limitation is not being able to consume a singular result or specific
combinations of results produced by a previous fanned out PipelineTask.



Result Type in taskRef or taskSpec
Parameter Type of Consumer
Specification




string
array
$(tasks.<pipelineTaskName>.results.<resultName>[*])


array
Not Supported
Not Supported


object
Not Supported
Not Supported



apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: platform-browser-tests
spec:
  tasks:
    - name: matrix-emitting-results
        matrix:
          params:
            - name: platform
              value:
                - linux
                - mac
                - windows
            - name: browser
              value:
                - chrome
                - safari
                - firefox
        taskRef:
          name: taskwithresults
          kind: Task
      - name: task-consuming-results
        taskRef:
          name: echoarrayurl
          kind: Task
        params:
          - name: url
            value: $(tasks.matrix-emitting-results.results.report-url[*])
  ...
See the full example pr-with-matrix-emitting-results
Retries
The retries field is used to specify the number of times a PipelineTask should be retried when its TaskRun or
Run fails, see the documentation for further details. When a PipelineTask is fanned out using Matrix,
a given TaskRun or Run executed will be retried as much as the field in the retries field of the PipelineTask.
For example, the PipelineTask in this PipelineRun will be fanned out into three TaskRuns each of which will be
retried once:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: matrixed-pr-with-retries-
spec:
  pipelineSpec:
    tasks:
      - name: matrix-and-params
        matrix:
          params:
            - name: platform
              value:
                - linux
                - mac
                - windows
        params:
          - name: browser
            value: chrome
        retries: 1
        taskSpec:
          params:
            - name: platform
            - name: browser
          steps:
            - name: echo
              image: alpine
              script: |
                echo "$(params.platform) and $(params.browser)"
                exit 1                
Examples
Matrix Combinations with Matrix.Params only
matrix:
  params:
    - name: GOARCH
      value:
        - "linux/amd64"
        - "linux/ppc64le"
        - "linux/s390x"
    - name: version
      value:
        - "go1.17"
        - "go1.18.1"
This matrix specification will result in six taskRuns with the following matrix combinations:
{ "GOARCH": "linux/amd64", "version": "go1.17" }
{ "GOARCH": "linux/amd64", "version": "go1.18.1" }
{ "GOARCH": "linux/ppc64le", "version": "go1.17" }
{ "GOARCH": "linux/ppc64le", "version": "go1.18.1" }
{ "GOARCH": "linux/s390x", "version": "go1.17" }
{ "GOARCH": "linux/s390x", "version": "go1.18.1" }
Letâ€™s expand this use case to showcase a little more complex combinations in the next example.
Matrix Combinations with Matrix.Params and Matrix.Include
Now, letâ€™s introduce include with a couple of Parameters: "package", "flags" and "context":
      matrix:
        params:
          - name: GOARCH
            value:
              - "linux/amd64"
              - "linux/ppc64le"
              - "linux/s390x"
          - name: version
            value:
              - "go1.17"
              - "go1.18.1"
        include:
          - name: common-package
            params:
              - name: package
                value: "path/to/common/package/"
          - name: s390x-no-race
            params:
              - name: GOARCH
                value: "linux/s390x"
              - name: flags
                value: "-cover -v"

          - name: go117-context
            params:
              - name: version
                value: "go1.17"
              - name: context
                value: "path/to/go117/context"
          - name: non-existent-arch
            params:
                - name: GOARCH
                  value: "I-do-not-exist"
The first include clause is added to all the original matrix combintations without overwriting any parameters of
the original combinations:
{ "GOARCH": "linux/amd64", "version": "go1.17", **"package": "path/to/common/package/"** }
{ "GOARCH": "linux/amd64", "version": "go1.18.1", **"package": "path/to/common/package/"** }
{ "GOARCH": "linux/ppc64le", "version": "go1.17", **"package": "path/to/common/package/"** }
{ "GOARCH": "linux/ppc64le", "version": "go1.18.1", **"package": "path/to/common/package/"** }
{ "GOARCH": "linux/s390x", "version": "go1.17", **"package": "path/to/common/package/"** }
{ "GOARCH": "linux/s390x", "version": "go1.18.1", **"package": "path/to/common/package/"** }
The second include clause adds "flags": "-cover -v" only to the original matrix combinations that include
"GOARCH": "linux/s390x":
{ "GOARCH": "linux/s390x", "version": "go1.17", "package": "path/to/common/package/", **"flags": "-cover -v"** }
{ "GOARCH": "linux/s390x", "version": "go1.18.1", "package": "path/to/common/package/", **"flags": "-cover -v"** }
The third include clause adds "context": "path/to/go117/context" only to the original matrix combinations
that include "version": "go1.17":
{ "GOARCH": "linux/amd64", "version": "go1.17", "package": "path/to/common/package/", **"context": "path/to/go117/context"** }
{ "GOARCH": "linux/ppc64le", "version": "go1.17", "package": "path/to/common/package/", **"context": "path/to/go117/context"** }
{ "GOARCH": "linux/s390x", "version": "go1.17", "package": "path/to/common/package/", "flags": "-cover -v", **"context": "path/to/go117/context"** }
The fourth include clause cannot be added to any original matrix combination without overwriting any params of the
original combinations, so it is added as an additional matrix combination:
* { **"GOARCH": "I-do-not-exist"** }
The above specification will result in seven taskRuns with the following matrix combinations:
{ "GOARCH": "linux/amd64", "version": "go1.17", "package": "path/to/common/package/", "context": "path/to/go117/context" }
{ "GOARCH": "linux/amd64", "version": "go1.18.1", "package": "path/to/common/package/" }
{ "GOARCH": "linux/ppc64le", "version": "go1.17", "package": "path/to/common/package/", "context": "path/to/go117/context" }
{ "GOARCH": "linux/ppc64le", "version": "go1.18.1", "package": "path/to/common/package/" }
{ "GOARCH": "linux/s390x", "version": "go1.17", "package": "path/to/common/package/", "flags": "-cover -v", "context": "path/to/go117/context" }
{ "GOARCH": "linux/s390x", "version": "go1.18.1", "package": "path/to/common/package/", "flags": "-cover -v" }
{ "GOARCH": "I-do-not-exist" }
PipelineTasks with Tasks
When a PipelineTask has a Task and a Matrix, the Task will be executed in parallel TaskRuns with
substitutions from combinations of Parameters.
In the example below, nine TaskRuns are created with combinations of platforms (â€œlinuxâ€, â€œmacâ€, â€œwindowsâ€)
and browsers (â€œchromeâ€, â€œsafariâ€, â€œfirefoxâ€).
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: platform-browsers
  annotations:
    description: |
            A task that does something cool with platforms and browsers
spec:
  params:
    - name: platform
    - name: browser
  steps:
    - name: echo
      image: alpine
      script: |
                echo "$(params.platform) and $(params.browser)"
---
# run platform-browsers task with:
#   platforms: linux, mac, windows
#   browsers: chrome, safari, firefox
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: matrixed-pr-
spec:
  serviceAccountName: 'default'
  pipelineSpec:
    tasks:
      - name: platforms-and-browsers
        matrix:
          params:
            - name: platform
              value:
                - linux
                - mac
                - windows
            - name: browser
              value:
                - chrome
                - safari
                - firefox
        taskRef:
          name: platform-browsers
When the above PipelineRun is executed, these are the TaskRuns that are created:
$ tkn taskruns list

NAME                                         STARTED        DURATION     STATUS
matrixed-pr-6lvzk-platforms-and-browsers-8   11 seconds ago   7 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-6   12 seconds ago   7 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-7   12 seconds ago   9 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-4   12 seconds ago   7 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-5   12 seconds ago   6 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-3   13 seconds ago   7 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-1   13 seconds ago   8 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-2   13 seconds ago   8 seconds    Succeeded
matrixed-pr-6lvzk-platforms-and-browsers-0   13 seconds ago   8 seconds    Succeeded
When the above Pipeline is executed, its status is populated with ChildReferences of the above TaskRuns. The
PipelineRun status tracks the status of all the fanned out TaskRuns. This is the PipelineRun after completing
successfully:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: matrixed-pr-
  labels:
    tekton.dev/pipeline: matrixed-pr-6lvzk
  name: matrixed-pr-6lvzk
  namespace: default
spec:
  pipelineSpec:
    tasks:
    - matrix:
        params:
          - name: platform
            value:
              - linux
              - mac
              - windows
          - name: browser
              value:
                - chrome
                - safari
                - firefox
      name: platforms-and-browsers
      taskRef:
        kind: Task
        name: platform-browsers
  serviceAccountName: default
  timeout: 1h0m0s
status:
  pipelineSpec:
    tasks:
      - matrix:
          params:
            - name: platform
              value:
                - linux
                - mac
                - windows
            - name: browser
              value:
                - chrome
                - safari
                - firefox
        name: platforms-and-browsers
        taskRef:
          kind: Task
          name: platform-browsers
  startTime: "2022-06-23T23:01:11Z"
  completionTime: "2022-06-23T23:01:20Z"
  conditions:
    - lastTransitionTime: "2022-06-23T23:01:20Z"
      message: 'Tasks Completed: 1 (Failed: 0, Cancelled 0), Skipped: 0'
      reason: Succeeded
      status: "True"
      type: Succeeded
  childReferences:
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-4
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-6
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-2
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-1
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-7
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-0
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-8
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-3
    pipelineTaskName: platforms-and-browsers
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    name: matrixed-pr-6lvzk-platforms-and-browsers-5
    pipelineTaskName: platforms-and-browsers
To execute this example yourself, run PipelineRun with Matrix.
PipelineTasks with Custom Tasks
When a PipelineTask has a Custom Task and a Matrix, the Custom Task will be executed in parallel Runs with
substitutions from combinations of Parameters.
In the example below, eight Runs are created with combinations of CEL expressions, using the CEL Custom Task.
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: matrixed-pr-
spec:
  serviceAccountName: 'default'
  pipelineSpec:
    tasks:
      - name: platforms-and-browsers
        matrix:
          params:
            - name: type
              value:
                - "type(1)"
                - "type(1.0)"
            - name: colors
              value:
                - "{'blue': '0x000080', 'red': '0xFF0000'}['blue']"
                - "{'blue': '0x000080', 'red': '0xFF0000'}['red']"
            - name: bool
              value:
                - "type(1) == int"
                - "{'blue': '0x000080', 'red': '0xFF0000'}['red'] == '0xFF0000'"
        taskRef:
          apiVersion: cel.tekton.dev/v1alpha1
          kind: CEL
When the above PipelineRun is executed, these Runs are created:
$ k get run.tekton.dev

NAME                                         SUCCEEDED   REASON              STARTTIME   COMPLETIONTIME
matrixed-pr-4djw9-platforms-and-browsers-0   True        EvaluationSuccess   10s         10s
matrixed-pr-4djw9-platforms-and-browsers-1   True        EvaluationSuccess   10s         10s
matrixed-pr-4djw9-platforms-and-browsers-2   True        EvaluationSuccess   10s         10s
matrixed-pr-4djw9-platforms-and-browsers-3   True        EvaluationSuccess   9s          9s
matrixed-pr-4djw9-platforms-and-browsers-4   True        EvaluationSuccess   9s          9s
matrixed-pr-4djw9-platforms-and-browsers-5   True        EvaluationSuccess   9s          9s
matrixed-pr-4djw9-platforms-and-browsers-6   True        EvaluationSuccess   9s          9s
matrixed-pr-4djw9-platforms-and-browsers-7   True        EvaluationSuccess   9s          9s
When the above PipelineRun is executed, its status is populated with ChildReferences of the above Runs. The
PipelineRun status tracks the status of all the fanned out Runs. This is the PipelineRun after completing:
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: matrixed-pr-
  labels:
    tekton.dev/pipeline: matrixed-pr-4djw9
  name: matrixed-pr-4djw9
  namespace: default
spec:
  pipelineSpec:
    tasks:
      - matrix:
          params:
            - name: type
              value:
                - type(1)
                - type(1.0)
            - name: colors
              value:
                - '{''blue'': ''0x000080'', ''red'': ''0xFF0000''}[''blue'']'
                - '{''blue'': ''0x000080'', ''red'': ''0xFF0000''}[''red'']'
            - name: bool
              value:
                - type(1) == int
                - '{''blue'': ''0x000080'', ''red'': ''0xFF0000''}[''red''] == ''0xFF0000'''
        name: platforms-and-browsers
        taskRef:
          apiVersion: cel.tekton.dev/v1alpha1
          kind: CEL
  serviceAccountName: default
  timeout: 1h0m0s
status:
  pipelineSpec:
    tasks:
      - matrix:
          params:
            - name: type
              value:
                - type(1)
                - type(1.0)
            - name: colors
              value:
                - '{''blue'': ''0x000080'', ''red'': ''0xFF0000''}[''blue'']'
                - '{''blue'': ''0x000080'', ''red'': ''0xFF0000''}[''red'']'
            - name: bool
              value:
                - type(1) == int
                - '{''blue'': ''0x000080'', ''red'': ''0xFF0000''}[''red''] == ''0xFF0000'''
        name: platforms-and-browsers
        taskRef:
          apiVersion: cel.tekton.dev/v1alpha1
          kind: CEL
  startTime: "2022-06-28T20:49:40Z"
  completionTime: "2022-06-28T20:49:41Z"
  conditions:
    - lastTransitionTime: "2022-06-28T20:49:41Z"
      message: 'Tasks Completed: 1 (Failed: 0, Cancelled 0), Skipped: 0'
      reason: Succeeded
      status: "True"
      type: Succeeded
  childReferences:
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-1
      pipelineTaskName: platforms-and-browsers
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-2
      pipelineTaskName: platforms-and-browsers
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-3
      pipelineTaskName: platforms-and-browsers
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-4
      pipelineTaskName: platforms-and-browsers
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-5
      pipelineTaskName: platforms-and-browsers
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-6
      pipelineTaskName: platforms-and-browsers
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-7
      pipelineTaskName: platforms-and-browsers
    - apiVersion: tekton.dev/v1alpha1
      kind: Run
      name: matrixed-pr-4djw9-platforms-and-browsers-0
      pipelineTaskName: platforms-and-browsers

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Pipelines in Pipelines

Overview
Specifying pipelineRef in Tasks
Specifying pipelineSpec in Tasks
Specifying Parameters

Overview
A mechanism to define and execute Pipelines in Pipelines, alongside Tasks and Custom Tasks, for a more in-depth background and inspiration, refer to the proposal TEP-0056.

ğŸŒ± Pipelines in Pipelines is an  alpha feature.
The enable-api-fields feature flag must be set to "alpha" to specify pipelineRef or pipelineSpec in a pipelineTask.
This feature is in Preview Only mode and not yet supported/implemented.

Specifying pipelineRef in pipelineTasks
Defining Pipelines in Pipelines at authoring time, by either specifying PipelineRef or PipelineSpec fields to a PipelineTask alongside TaskRef and TaskSpec.
For example, a Pipeline named security-scans which is run within a Pipeline named clone-scan-notify where the PipelineRef is used:
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: security-scans
spec:
  tasks:
    - name: scorecards
      taskRef:
        name: scorecards
    - name: codeql
      taskRef:
        name: codeql
---
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: clone-scan-notify
spec:
  tasks:
    - name: git-clone
      taskRef:
        name: git-clone
    - name: security-scans
      pipelineRef:
        name: security-scans
    - name: notification
      taskRef:
        name: notification
Specifying pipelineSpec in pipelineTasks
The pipelineRef example can be modified to use PipelineSpec instead of PipelineRef to instead embed the Pipeline specification:
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: clone-scan-notify
spec:
  tasks:
    - name: git-clone
      taskRef:
        name: git-clone
    - name: security-scans
      pipelineSpec:
        tasks:
          - name: scorecards
            taskRef:
              name: scorecards
          - name: codeql
            taskRef:
              name: codeql
    - name: notification
      taskRef:
        name: notification
Specifying Parameters
Pipelines in Pipelines consume Parameters in the same way as Tasks in Pipelines
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: clone-scan-notify
spec:
  params:
    - name: repo
      value: $(params.repo)
  tasks:
    - name: git-clone
      params:
        - name: repo
          value: $(params.repo)      
      taskRef:
        name: git-clone
    - name: security-scans
      params:
        - name: repo
          value: $(params.repo)
      pipelineRef:
        name: security-scans
    - name: notification
      taskRef:
        name: notification

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Variable Substitutions Supported by Tasks and Pipelines
This page documents the variable substitutions supported by Tasks and Pipelines.
For instructions on using variable substitutions see the relevant section of the Tasks doc.
Note: Tekton does not escape the contents of variables. Task authors are responsible for properly escaping a variableâ€™s value according to the shell, image or scripting language that the variable will be used in.
Variables available in a Pipeline



Variable
Description




params.<param name>
The value of the parameter at runtime.


params['<param name>']
(see above)


params["<param name>"]
(see above)


params.<param name>[*]
Get the whole param array or object.


params['<param name>'][*]
(see above)


params["<param name>"][*]
(see above)


params.<param name>[i]
Get the i-th element of param array. This is alpha feature, set enable-api-fields to alpha  to use it.


params['<param name>'][i]
(see above)


params["<param name>"][i]
(see above)


params.<object-param-name>[*]
Get the value of the whole object param. This is alpha feature, set enable-api-fields to alpha  to use it.


params.<object-param-name>.<individual-key-name>
Get the value of an individual child of an object param. This is alpha feature, set enable-api-fields to alpha  to use it.


tasks.<taskName>.matrix.length
The length of the Matrix combination count.


tasks.<taskName>.results.<resultName>
The value of the Task's result. Can alter Task execution order within a Pipeline.)


tasks.<taskName>.results.<resultName>[i]
The ith value of the Task's array result. Can alter Task execution order within a Pipeline.)


tasks.<taskName>.results.<resultName>[*]
The array value of the Task's result. Can alter Task execution order within a Pipeline. Cannot be used in script.)


tasks.<taskName>.results.<resultName>.key
The key value of the Task's object result. Can alter Task execution order within a Pipeline.)


tasks.<taskName>.matrix.<resultName>.length
The length of the matrixed Task's results. (Can alter Task execution order within a Pipeline.)


workspaces.<workspaceName>.bound
Whether a Workspace has been bound or not. â€œfalseâ€ if the Workspace declaration has optional: true and the Workspace binding was omitted by the PipelineRun.


context.pipelineRun.name
The name of the PipelineRun that this Pipeline is running in.


context.pipelineRun.namespace
The namespace of the PipelineRun that this Pipeline is running in.


context.pipelineRun.uid
The uid of the PipelineRun that this Pipeline is running in.


context.pipeline.name
The name of this Pipeline .


tasks.<pipelineTaskName>.status
The execution status of the specified pipelineTask, only available in finally tasks. The execution status can be set to any one of the values (Succeeded, Failed, or None) described here.


tasks.<pipelineTaskName>.reason
The execution reason of the specified pipelineTask, only available in finally tasks. The reason can be set to any one of the values (Failed, TaskRunCancelled, TaskRunTimeout, FailureIgnored, etc ) described here.


tasks.status
An aggregate status of all the pipelineTasks under the tasks section (excluding the finally section). This variable is only available in the finally tasks and can have any one of the values (Succeeded, Failed, Completed, or None) described here.


context.pipelineTask.retries
The retries of this PipelineTask.


tasks.<taskName>.outputs.<artifactName>
The value of a specific output artifact of the Task


tasks.<taskName>.inputs.<artifactName>
The value of a specific input artifact of the Task



Variables available in a Task



Variable
Description




params.<param name>
The value of the parameter at runtime.


params['<param name>']
(see above)


params["<param name>"]
(see above)


params.<param name>[*]
Get the whole param array or object.


params['<param name>'][*]
(see above)


params["<param name>"][*]
(see above)


params.<param name>[i]
Get the i-th element of param array. This is alpha feature, set enable-api-fields to alpha  to use it.


params['<param name>'][i]
(see above)


params["<param name>"][i]
(see above)


params.<object-param-name>.<individual-key-name>
Get the value of an individual child of an object param. This is alpha feature, set enable-api-fields to alpha  to use it.


results.<resultName>.path
The path to the file where the Task writes its results data.


results['<resultName>'].path
(see above)


results["<resultName>"].path
(see above)


workspaces.<workspaceName>.path
The path to the mounted Workspace. Empty string if an optional Workspace has not been provided by the TaskRun.


workspaces.<workspaceName>.bound
Whether a Workspace has been bound or not. â€œfalseâ€ if an optionalWorkspace has not been provided by the TaskRun.


workspaces.<workspaceName>.claim
The name of the PersistentVolumeClaim specified as a volume source for the Workspace. Empty string for other volume types.


workspaces.<workspaceName>.volume
The name of the volume populating the Workspace.


credentials.path
The path to credentials injected from Secrets with matching annotations.


context.taskRun.name
The name of the TaskRun that this Task is running in.


context.taskRun.namespace
The namespace of the TaskRun that this Task is running in.


context.taskRun.uid
The uid of the TaskRun that this Task is running in.


context.task.name
The name of this Task.


context.task.retry-count
The current retry number of this Task.


steps.step-<stepName>.exitCode.path
The path to the file where a Stepâ€™s exit code is stored.


steps.step-unnamed-<stepIndex>.exitCode.path
The path to the file where a Stepâ€™s exit code is stored for a step without any name.


artifacts.path
The path to the file where the Task writes its artifacts data.



Fields that accept variable substitutions



CRD
Field




Task
spec.steps[].name


Task
spec.steps[].image


Task
spec.steps[].imagePullPolicy


Task
spec.steps[].command


Task
spec.steps[].args


Task
spec.steps[].script


Task
spec.steps[].onError


Task
spec.steps[].env.value


Task
spec.steps[].env.valueFrom.secretKeyRef.name


Task
spec.steps[].env.valueFrom.secretKeyRef.key


Task
spec.steps[].env.valueFrom.configMapKeyRef.name


Task
spec.steps[].env.valueFrom.configMapKeyRef.key


Task
spec.steps[].volumeMounts.name


Task
spec.steps[].volumeMounts.mountPath


Task
spec.steps[].volumeMounts.subPath


Task
spec.volumes[].name


Task
spec.volumes[].configMap.name


Task
spec.volumes[].configMap.items[].key


Task
spec.volumes[].configMap.items[].path


Task
spec.volumes[].secret.secretName


Task
spec.volumes[].secret.items[].key


Task
spec.volumes[].secret.items[].path


Task
spec.volumes[].persistentVolumeClaim.claimName


Task
spec.volumes[].projected.sources.configMap.name


Task
spec.volumes[].projected.sources.secret.name


Task
spec.volumes[].projected.sources.serviceAccountToken.audience


Task
spec.volumes[].csi.nodePublishSecretRef.name


Task
spec.volumes[].csi.volumeAttributes.* 


Task
spec.sidecars[].name


Task
spec.sidecars[].image


Task
spec.sidecars[].imagePullPolicy


Task
spec.sidecars[].env.value


Task
spec.sidecars[].env.valueFrom.secretKeyRef.name


Task
spec.sidecars[].env.valueFrom.secretKeyRef.key


Task
spec.sidecars[].env.valueFrom.configMapKeyRef.name


Task
spec.sidecars[].env.valueFrom.configMapKeyRef.key


Task
spec.sidecars[].volumeMounts.name


Task
spec.sidecars[].volumeMounts.mountPath


Task
spec.sidecars[].volumeMounts.subPath


Task
spec.sidecars[].command


Task
spec.sidecars[].args


Task
spec.sidecars[].script


Task
spec.workspaces[].mountPath


TaskRun
spec.workspaces[].subPath


TaskRun
spec.workspaces[].persistentVolumeClaim.claimName


TaskRun
spec.workspaces[].configMap.name


TaskRun
spec.workspaces[].configMap.items[].key


TaskRun
spec.workspaces[].configMap.items[].path


TaskRun
spec.workspaces[].secret.secretName


TaskRun
spec.workspaces[].secret.items[].key


TaskRun
spec.workspaces[].secret.items[].path


TaskRun
spec.workspaces[].projected.sources[].secret.name


TaskRun
spec.workspaces[].projected.sources[].secret.items[].key


TaskRun
spec.workspaces[].projected.sources[].secret.items[].path


TaskRun
spec.workspaces[].projected.sources[].configMap.name


TaskRun
spec.workspaces[].projected.sources[].configMap.items[].key


TaskRun
spec.workspaces[].projected.sources[].configMap.items[].path


TaskRun
spec.workspaces[].csi.driver


TaskRun
spec.workspaces[].csi.nodePublishSecretRef.name


Pipeline
spec.tasks[].params[].value


Pipeline
spec.tasks[].conditions[].params[].value


Pipeline
spec.results[].value


Pipeline
spec.tasks[].when[].input


Pipeline
spec.tasks[].when[].values


Pipeline
spec.tasks[].workspaces[].subPath


Pipeline
spec.tasks[].displayName


PipelineRun
spec.workspaces[].subPath


PipelineRun
spec.workspaces[].persistentVolumeClaim.claimName


PipelineRun
spec.workspaces[].configMap.name


PipelineRun
spec.workspaces[].configMap.items[].key


PipelineRun
spec.workspaces[].configMap.items[].path


PipelineRun
spec.workspaces[].secret.secretName


PipelineRun
spec.workspaces[].secret.items[].key


PipelineRun
spec.workspaces[].secret.items[].path


PipelineRun
spec.workspaces[].projected.sources[].secret.name


PipelineRun
spec.workspaces[].projected.sources[].secret.items[].key


PipelineRun
spec.workspaces[].projected.sources[].secret.items[].path


PipelineRun
spec.workspaces[].projected.sources[].configMap.name


PipelineRun
spec.workspaces[].projected.sources[].configMap.items[].key


PipelineRun
spec.workspaces[].projected.sources[].configMap.items[].path


PipelineRun
spec.workspaces[].csi.driver


PipelineRun
spec.workspaces[].csi.nodePublishSecretRef.name




	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Compute Resources in Tekton
Background: Resource Requirements in Kubernetes
Kubernetes allows users to specify CPU, memory, and ephemeral storage constraints
for containers.
Resource requests determine the resources reserved for a pod when itâ€™s scheduled,
and affect likelihood of pod eviction. Resource limits constrain the maximum amount of
a resource a container can use. A container that exceeds its memory limits will be killed,
and a container that exceeds its CPU limits will be throttled.
A podâ€™s effective resource requests and limits
are the higher of:

the sum of all app containers request/limit for a resource
the effective init container request/limit for a resource

This formula exists because Kubernetes runs init containers sequentially and app containers
in parallel. (There is no distinction made between app containers and sidecar containers
in Kubernetes; a sidecar is used in the following example to illustrate this.)
For example, consider a pod with the following containers:



Container
CPU request
CPU limit




init container 1
1
2


init container 2
2
3


app container 1
1
2


app container 2
2
3


sidecar container 1
3
no limit



The sum of all app container CPU requests is 6 (including the sidecar container), which is
greater than the maximum init container CPU request (2). Therefore, the podâ€™s effective CPU
request will be 6.
Since the sidecar container has no CPU limit, this is treated as the highest CPU limit.
Therefore, the pod will have no effective CPU limit.
Task-level Compute Resources Configuration
(beta)
Tekton allows users to specify resource requirements of Steps,
which run sequentially. However, the podâ€™s effective resource requirements are still the
sum of its containersâ€™ resource requirements. This means that when specifying resource
requirements for Step containers, they must be treated as if they are running in parallel.
Tekton adjusts Step resource requirements to comply with LimitRanges.
ResourceQuotas are not currently supported.
Instead of specifying resource requirements on each Step, users can choose to specify resource requirements at the Task-level. If users specify a Task-level resource request, it will ensure that the kubelet reserves only that amount of resources to execute the Taskâ€™s Steps.
If users specify a Task-level resource limit, no Step may use more than that amount of resources.
Each of these details is explained in more depth below.
Some points to note:

Task-level resource requests and limits do not apply to sidecars which can be configured separately.
If only limits are configured in task-level, it will be applied as the task-level requests.
Resource requirements configured in Step or StepTemplate of the referenced Task will be overridden by the task-level requirements.
TaskRun configured with both StepOverrides and task-level requirements will be rejected.

Configure Task-level Compute Resources
Task-level resource requirements can be configured in TaskRun.ComputeResources, or PipelineRun.TaskRunSpecs.ComputeResources.
e.g.
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: foo 
spec:
  computeResources:
    requests:
      cpu: 1 
    limits:
      cpu: 2
The following TaskRun will be rejected, because it configures both stepOverrides and task-level compute resource requirements:
kind: TaskRun
spec:
  stepOverrides:
    - name: foo
      resources:
        requests:
          cpu: 1
  computeResources:
    requests:
      cpu: 2 
kind: PipelineRun
spec:
  taskRunSpecs:
    - pipelineTaskName: foo
      stepOverrides:
        - name: foo 
          resources:
            requests:
              cpu: 1 
      computeResources:
        requests:
          cpu: 2
Configure Resource Requirements with Sidecar
Users can specify compute resources separately for a sidecar while configuring task-level resource requirements on TaskRun.
e.g.
kind: TaskRun
spec:
  sidecarOverrides:
    - name: sidecar
      resources:
        requests:
          cpu: 750m
        limits:
          cpu: 1
  computeResources:
    requests:
      cpu: 2
LimitRange Support
Kubernetes allows users to configure LimitRanges,
which constrain compute resources of pods, containers, or PVCs running in the same namespace.
LimitRanges can:

Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.
Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.
Enforce a ratio between request and limit for a resource in a namespace.
Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.

Tekton applies the resource requirements specified by users directly to the containers
in a Task's pod, unless there is a LimitRange present in the namespace.
Tekton supports LimitRange minimum, maximum, and default resource requirements for containers,
but does not support LimitRange ratios between requests and limits (#4230).
LimitRange types other than â€œContainerâ€ are not considered for purposes of resource requirements.
Tekton doesnâ€™t allow users to configure init containers for a Task, but any default and defaultRequest from a LimitRange
will be applied to the init containers that Tekton injects into a TaskRunâ€™s pod.
Requests
If a Step container does not have requests defined, Tekton will divide a LimitRangeâ€™s defaultRequests by the number of Step containers and apply these requests to the Steps.
This results in a TaskRun with overall requests equal to LimitRange defaultRequests.
If this value is less than the LimitRange minimum, the LimitRange minimum will be used instead.
LimitRange defaultRequests are applied as-is to init containers or Sidecar containers that donâ€™t specify requests.
Containers that do specify requests will not be modified. If these requests are lower than LimitRange minimums, Kubernetes will reject the resulting TaskRunâ€™s pod.
Limits
Tekton does not adjust container limits, regardless of whether a container is a Step, Sidecar, or init container.
If a container does not have limits defined, Kubernetes will apply the LimitRange default to the containerâ€™s limits.
If a container does define limits, and they are less than the LimitRange default, Kubernetes will reject the resulting TaskRunâ€™s pod.
Examples
Consider the following LimitRange:
apiVersion: v1
kind: LimitRange
metadata:
  name: limitrange-example
spec:
  limits:
  - default:  # The default limits
      cpu: 2
    defaultRequest:  # The default requests
      cpu: 1
    max:  # The maximum limits
      cpu: 3
    min:  # The minimum requests
      cpu: 300m
    type: Container
A Task with 2 Steps and no resources specified would result in a pod with the following containers:



Container
CPU request
CPU limit




container 1
500m
2


container 2
500m
2



Here, the default CPU request was divided among the step containers, and this value was used since it was greater
than the minimum request specified by the LimitRange.
The CPU limits are 2 for each container, as this is the default limit specifed in the LimitRange.
If we had a Task with 2 Steps and 1 Sidecar with no resources specified would result in a pod with the following containers:



Container
CPU request
CPU limit




container 1
500m
2


container 2
500m
2


container 3
1
2



For the first two containers, the default CPU request was divided among the step containers, and this value was used since it was greater
than the minimum request specified by the LimitRange. The third container is a sidecar and since it is not a step container gets the full
default CPU request of 1. AS before the CPU limits are 2 for each container, as this is the default limit specifed in the LimitRange.
Now, consider a Task with the following Steps:



Step
CPU request
CPU limit




step 1
200m
2


step 2
1
4



The resulting pod would have the following containers:



Container
CPU request
CPU limit




container 1
300m
2


container 2
1
3



Here, the first Step's request was less than the LimitRange minimum, so the output request is the minimum (300m).
The second Step's request is unchanged. The first Step's limit is less than the maximum, so it is unchanged,
while the second Step's limit is greater than the maximum, so the maximum (3) is used.
Support for multiple LimitRanges
Tekton supports running TaskRuns in namespaces with multiple LimitRanges.
For a given resource, the minumum used will be the largest of any of the LimitRangesâ€™ minimum values,
and the maximum used will be the smallest of any of the LimitRangesâ€™ maximum values.
The minimum resource requirement used will be the largest of any minimum for that resource,
and the maximum resource requirement will be the smallest of any of the maximum values defined.
The default value will be the minimum of any default values defined.
If the resulting default value is less than the resulting minimum value, the default value will be the minimum value.
Itâ€™s possible for multiple LimitRanges to be defined which are not compatible with each other, preventing pods from being scheduled.
Example
Consider a namespaces with the following LimitRanges defined:
apiVersion: v1
kind: LimitRange
metadata:
  name: limitrange-1
spec:
  limits:
  - default:  # The default limits
      cpu: 2
    defaultRequest:  # The default requests
      cpu: 750m
    max:  # The maximum limits
      cpu: 3
    min:  # The minimum requests
      cpu: 500m
    type: Container
apiVersion: v1
kind: LimitRange
metadata:
  name: limitrange-2
spec:
  limits:
  - default:  # The default limits
      cpu: 1.5
    defaultRequest:  # The default requests
      cpu: 1
    max:  # The maximum limits
      cpu: 2.5
    min:  # The minimum requests
      cpu: 300m
    type: Container
A namespace with limitrange-1 and limitrange-2 would be treated as if it contained only the following LimitRange:
apiVersion: v1
kind: LimitRange
metadata:
  name: aggregate-limitrange
spec:
  limits:
  - default:  # The default limits
      cpu: 1.5
    defaultRequest:  # The default requests
      cpu: 750m
    max:  # The maximum limits
      cpu: 2.5
    min:  # The minimum requests
      cpu: 300m
    type: Container
Here, the minimum of the â€œmaxâ€ values is the output â€œmaxâ€ value, and likewise for â€œdefaultâ€ and â€œdefaultRequestâ€.
The maximum of the â€œminâ€ values is the output â€œminâ€ value.
ResourceQuota Support
Kubernetes allows users to define ResourceQuotas,
which restrict the maximum resource requests and limits of all pods running in a namespace.
To deploy Tekton TaskRuns or PipelineRuns in namespaces with ResourceQuotas, compute resource requirements
must be set for all containers in a TaskRunâ€™s pod, including the init containers injected by Tekton.
Step and Sidecar resource requirements can be configured directly through the API, as described in
Task Resource Requirements. To configure resource requirements for Tektonâ€™s init containers,
deploy a LimitRange in the same namespace. The LimitRangeâ€™s default and defaultRequest will be applied to the init containers,
and divided among the Steps and Sidecars, as described in LimitRange Support.
#2933 tracks support for running TaskRuns in a namespace with a ResourceQuota
without having to use LimitRanges.
ResourceQuotas consider the effective resource requests and limits of a pod, which Kubernetes determines by summing the resource requirements
of its containers (under the assumption that they run in parallel). When using LimitRanges to set compute resources for TaskRun pods,
LimitRange default requests are divided among Step containers, meaning that the podâ€™s effective requests reflect the actual requests
that the pod needs. However, LimitRange default limits are not divided among containers, meaning the podâ€™s effective limits are much larger
than the limits applied during execution of any given Step. For example, if a ResourceQuota restricts a namespace to a limit of 10 CPU,
and a user creates a TaskRun with 20 steps with a limit of 1 CPU each, the pod would not be schedulable even though it is
limited to 1 CPU at each point in time. Therefore, it is recommended to use ResourceQuotas to restrict only requests of TaskRun pods,
not limits (tracked in #4976).
Quality of Service (QoS)
By default, pods that run Tekton TaskRuns will have a Quality of Service (QoS)
of â€œBestEffortâ€. If compute resource requirements are set for any Step or Sidecar, the pod will have a â€œBurstableâ€ QoS.
To get a â€œGuaranteedâ€ QoS, a TaskRun pod must have compute resources set for all of its containers, including init containers which are
injected by Tekton, and all containers must have their requests equal to their limits.
This can be achieved by using LimitRanges to apply default requests and limits.
References

LimitRange in k8s docs
Configure default memory requests and limits for a Namespace
Configure default CPU requests and limits for a Namespace
Configure Minimum and Maximum CPU constraints for a Namespace
Configure Minimum and Maximum Memory constraints for a Namespace
Managing Resources for Containers
Kubernetes best practices: Resource requests and limits
Restrict resource consumption with limit ranges


	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\nDocumentation for latest release
    

    
      Go to Nightly Release
    

    
      Go to Latest LTS release
    
  
  



            
            

	
	
	
		

  
    


  
    


  

		
	    
	Pod templates
A Pod template defines a portion of a PodSpec
configuration that Tekton can use as â€œboilerplateâ€ for a Pod that runs your Tasks and Pipelines.
You can specify a Pod template for TaskRuns and PipelineRuns. In the template, you can specify custom values for fields governing
the execution of individual Tasks or for all Tasks executed by a given PipelineRun.
You also have the option to define a global Pod template in your Tekton config using the key default-pod-template.
However, this global template is going to be merged with any templates you specify in your TaskRuns and PipelineRuns.
Except for the env and volumes fields, other fields that exist in both the global template and the TaskRunâ€™s or
PipelineRunâ€™s template will be taken from the TaskRun or PipelineRun.
The env and volumes fields are merged by the name value in the array elements. If the itemâ€™s name is the same, the item from TaskRun or PipelineRun will be used.
See the following for examples of specifying a Pod template:

Specifying a Pod template for a TaskRun
Specifying a Pod template for a PipelineRun

Affinity Assistant Pod templates
The Pod templates specified in the TaskRuns and PipelineRuns also apply to
the affinity assistant Pods
that are created when using Workspaces, but only on select fields.
The supported fields are: tolerations, nodeSelector, securityContext and
imagePullSecrets (see the table below for more details).
Similarily to Pod templates, you have the option to define a global affinity
assistant Pod template in your Tekton config
using the key default-affinity-assistant-pod-template. The merge strategy is
the same as the one described above.
Supported fields
Pod templates support fields listed in the table below.

	
		Field
		Description
	
	
		
			env
			Environment variables defined in the Pod template at TaskRun and PipelineRun level take precedence over the ones defined in steps and stepTemplate
		
		
			nodeSelector
			Must be true for the Pod to fit on a node.
		
		
			tolerations
			Allows (but does not require) the Pods to schedule onto nodes with matching taints.
		
		
			affinity
			Allows constraining the set of nodes for which the Pod can be scheduled based on the labels present on the node.
		
		
			securityContext
			Specifies Pod-level security attributes and common container settings such as runAsUser and selinux.
		
		
			volumes
			Specifies a list of volumes that containers within the Pod can mount. This allows you to specify a volume type for each volumeMount in a Task.
		
		
			runtimeClassName
			Specifies the runtime class for the Pod.
		
		
			automountServiceAccountToken
			Default: true. Determines whether Tekton automatically provides the token for the service account used by the Pod inside containers at a predefined path.
		
		
			dnsPolicy
			Default: ClusterFirst. Specifies the DNS policy
                for the Pod. Legal values are ClusterFirst, Default, and None. Does not support ClusterFirstWithHostNet
                because Tekton Pods cannot run with host networking.
		
		
			dnsConfig
			Specifies additional DNS configuration for the Pod, such as name servers and search domains.
		
		
			enableServiceLinks
			Default: true. Determines whether services in the Pod's namespace are exposed as environment variables to the Pod, similarly to Docker service links.
		
		
			priorityClassName
			Specifies the priority class for the Pod. Allows you to selectively enable preemption on lower-priority workloads.
		
		
			schedulerName
			Specifies the scheduler to use when dispatching the Pod. You can specify different schedulers for different types of
                workloads, such as volcano.sh for machine learning workloads.
		
		
			imagePullSecrets
			Specifies the secret to use when 
                pulling a container image.
		
		
			hostNetwork
			Default: false. Determines whether to use the host network namespace.
		
		
			hostAliases
			Adds entries to a Pod's `/etc/hosts` to provide Pod-level overrides of hostnames. For further info see [Kubernetes' docs for this field](https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/).
		
        
            topologySpreadConstraints
            Specify how Pods are spread across your cluster among topology domains.
        
	

Use imagePullSecrets to lookup entrypoint
If no command is configured in task and imagePullSecrets is configured in podTemplate, the Tekton Controller will look up the entrypoint of image with imagePullSecrets. The Tekton controllerâ€™s service account is given access to secrets by default. See this for reference. If the Tekton controllerâ€™s service account is not granted the access to secrets in different namespace, you need to grant the access via RoleBinding:
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: creds-getter
  namespace: my-ns
rules:
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["creds"]
  verbs: ["get"]
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: creds-getter-binding
  namespace: my-ns
subjects:
- kind: ServiceAccount
  name: tekton-pipelines-controller
  namespace: tekton-pipelines
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: creds-getter
  apiGroup: rbac.authorization.k8s.io

Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License,
and code samples are licensed under the
Apache 2.0 License.

	
		

Feedback
Was this page helpful?
Yes
No\n\n\n\n