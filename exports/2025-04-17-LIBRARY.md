library/NODE_JS.md
==== Content of library/NODE_JS.md ====
# NODE_JS

## Crawl Summary
Node.js v20 introduces an experimental Permission Model activated via `--experimental-permission`, custom ESM loader hooks running on a dedicated thread via `--experimental-loader`, and synchronous behavior for import.meta.resolve. The V8 engine is updated to version 11.3 featuring new string, array, and WebAssembly optimizations. The test_runner module is now stable. Ada 2.0 improves URL parsing without ICU dependency. Single executable applications now require blob injection from a JSON config, and the Web Crypto API now validates arguments per WebIDL. Additionally, official ARM64 Windows binaries are provided, and a mandatory WASI version must be specified. Detailed commit logs capture major, minor, and patch changes.

## Normalised Extract
## Table of Contents
1. Permission Model
2. Custom ESM Loader Hooks
3. Synchronous import.meta.resolve
4. V8 11.3 Updates
5. Stable Test Runner
6. Ada 2.0 URL Parser
7. SEA Blob Injection
8. Web Crypto API Validation
9. ARM64 Windows Support
10. WASI Version Requirement
11. Deprecations and Removals
12. Commit History

---

### 1. Permission Model
- **Flag:** `--experimental-permission`
- **Technical Detail:** Restricts runtime access to file system operations, child process spawning, and worker thread creation.

### 2. Custom ESM Loader Hooks
- **Flag:** `--experimental-loader=foo.mjs`
- **Technical Detail:** Loader hooks execute in an isolated dedicated thread to prevent interference with main application code.

### 3. Synchronous import.meta.resolve
- **Signature:** `import.meta.resolve(moduleSpecifier: string, baseURL?: string): string`
- **Technical Detail:** Always returns synchronously, independent of whether the user-defined resolve hook is async.

### 4. V8 11.3 Updates
- **Version:** 11.3 (Chromium 113)
- **Features:** 
   - `String.prototype.isWellFormed` & `toWellFormed`
   - Array and TypedArray copy methods
   - Resizable ArrayBuffer and growable SharedArrayBuffer
   - RegExp `v` flag with set notation
   - WebAssembly Tail Call support

### 5. Stable Test Runner
- **Module:** `test_runner`
- **Technical Detail:** Transitioned from experimental to stable for production use.

### 6. Ada 2.0 URL Parser
- **Technical Detail:** Enhanced performance for URL parsing via functions `url.domainToASCII` and `url.domainToUnicode`; removes dependency on ICU for hostname parsing.

### 7. SEA Blob Injection
- **Requirement:** Single executable apps require a pre-prepared blob (from JSON config) instead of raw JS injections.

### 8. Web Crypto API Validation
- **Technical Detail:** Function arguments are coerced and validated as per WebIDL, ensuring consistent interoperability.

### 9. ARM64 Windows Support
- **Technical Detail:** Provides native binaries (MSI, zip/7z, executable) specifically built for ARM64 Windows platforms.

### 10. WASI Version Requirement
- **Usage:** `new WASI({ version: '1.0', ... })` where the `version` parameter is required with no default.

### 11. Deprecations and Removals
- **Example:** `url.parse()` now issues warnings for URLs with non-numeric ports, following the WHATWG specification.

### 12. Commit History
- **Details:** Contains full commit logs for major (e.g., `[3bed5f11e0]`), minor, and patch changes covering modules such as async_hooks, buffer, build, crypto, deps, and more.


## Supplementary Details
## Implementation Specifications

1. **Activating Permission Model:**
   - Command: `node --experimental-permission app.js`
   - Effect: Restricts sensitive API access (fs, child_process, worker_threads).

2. **Custom ESM Loader Hooks Usage:**
   - Command: `node --experimental-loader=custom-loader.mjs app.mjs`
   - Example Loader (custom-loader.mjs):
     --------------------------------------------------
     // custom-loader.mjs
     export async function resolve(specifier, context, defaultResolve) {
       // Custom resolution logic
       return defaultResolve(specifier, context, defaultResolve);
     }
     --------------------------------------------------

3. **Using Synchronous import.meta.resolve:**
   - Example:
     --------------------------------------------------
     const resolved = import.meta.resolve('./module.js', import.meta.url);
     console.log(resolved);
     --------------------------------------------------

4. **WASI Initialization Example:**
   - Code:
     --------------------------------------------------
     const { WASI } = require('wasi');
     const wasi = new WASI({
       version: '1.0',
       args: process.argv,
       env: process.env,
       preopens: { '/sandbox': '/var/sandbox' }
     });
     --------------------------------------------------

5. **ARM64 Windows Build Configuration:**
   - Download binaries such as node-v20.0.0-arm64.msi from the official site.

6. **Best Practices for SEA Executables:**
   - Pre-generate a blob through Node.js from a JSON configuration file to enable embedding multiple resources.

7. **Troubleshooting Procedures:**
   - If permission errors occur, ensure the process is run with `--experimental-permission`.
   - Verify WASI version: Log the instance output after creation (e.g., `console.log(wasi)`).
   - For V8 build issues, review commit logs using: `git log --grep=V8`.

## Configuration Options

- **Permission Flag:** `--experimental-permission`
- **Loader Flag:** `--experimental-loader=path/to/loader.mjs`
- **WASI Version:** Must be explicitly provided (e.g., `{ version: '1.0' }`)


## Reference Details
## API Specifications and Code Examples

### 1. Permission Model API
- **Activation:** Run Node.js with the flag: `--experimental-permission`
- **Usage:** When enabled, all attempts to access restricted APIs (e.g., file system, process spawning) are validated at runtime.

### 2. Custom ESM Loader Hooks
- **Method Signature:**
  ```js
  async function resolve(specifier: string, context: { parentURL?: string }, defaultResolve: Function): Promise<{ url: string }>
  ```
- **Example Implementation:**
  --------------------------------------------------
  // custom-loader.mjs
  export async function resolve(specifier, context, defaultResolve) {
    // Custom resolution logic can be integrated here
    return defaultResolve(specifier, context, defaultResolve);
  }
  --------------------------------------------------

### 3. Synchronous import.meta.resolve
- **Signature:**
  ```js
  import.meta.resolve(moduleSpecifier: string, baseURL?: string): string
  ```
- **Usage Example:**
  --------------------------------------------------
  const resolvedURL = import.meta.resolve('./example.js', import.meta.url);
  console.log(resolvedURL);
  --------------------------------------------------

### 4. WASI Constructor
- **Constructor Signature:**
  ```js
  new WASI(options: {
    version: string,       // e.g., '1.0' (mandatory)
    args?: string[],
    env?: NodeJS.ProcessEnv,
    preopens?: { [key: string]: string }
  }): WASI
  ```
- **Example:**
  --------------------------------------------------
  const { WASI } = require('wasi');
  const wasi = new WASI({
    version: '1.0',
    args: process.argv,
    env: process.env,
    preopens: { '/sandbox': '/var/sandbox' }
  });
  --------------------------------------------------

### 5. HTTP Server Example (Standard Library)
- **Code Example:**
  --------------------------------------------------
  // server.mjs
  import { createServer } from 'node:http';

  const server = createServer((req, res) => {
    res.writeHead(200, { 'Content-Type': 'text/plain' });
    res.end('Hello World!\n');
  });

  server.listen(3000, '127.0.0.1', () => {
    console.log('Listening on 127.0.0.1:3000');
  });
  // Run with: node server.mjs
  --------------------------------------------------

### 6. Troubleshooting Procedures
- **Check Node.js Version:** `node -v`
- **Run with Experimental Permission:** `node --experimental-permission app.js`
- **Verify WASI Instance:**
  ```js
  console.log(wasi);
  ```
- **Diagnose Loader Issues:** Insert diagnostic `console.log` statements in your custom loader file.
- **V8 Build Issue Check:** Run `git log --grep=V8` to review applied patches.

### 7. Configuration Options Summary
- **Experimental Permission Flag:** `--experimental-permission` enables runtime API access restrictions.
- **Custom Loader Flag:** `--experimental-loader=path/to/loader.mjs`
- **WASI Options:** Must include an explicit `version`, e.g., `{ version: '1.0' }` with optional args, env, and preopens.

## Best Practices
- **Explicit Configuration:** Always specify the WASI version explicitly.
- **Loader Isolation:** Test custom ESM loaders in isolation to prevent state contamination.
- **Official Binaries:** Use officially provided ARM64 Windows binaries to ensure compatibility.
- **Sync Resolution:** Employ synchronous import.meta.resolve for production consistency.
- **Commit Audit:** Refer to commit logs (e.g., [3bed5f11e0], [44710], [47391]) for troubleshooting and understanding breaking changes.

This detailed reference provides complete API specifications, method signatures, full code samples, configuration details, and troubleshooting steps to be used directly by developers in their projects.


## Original Source
Node.js v20 Documentation
https://nodejs.org/en/blog/release/v20.0.0/

## Digest of NODE_JS

## Node.js v20 Technical Details

**Retrieved:** 2023-10-07

### Permission Model
- **Activation Flag:** `--experimental-permission`
- **Effect:** Restricts access to critical runtime features such as file system operations, child process spawning, and worker thread creation.

### Custom ESM Loader Hooks
- **Activation Flag:** `--experimental-loader=foo.mjs`
- **Behavior:** Loader hooks now run in a dedicated, isolated thread to avoid cross-contamination with application code.

### Synchronous import.meta.resolve
- **Behavior:** Returns synchronously aligned with browser behavior, even when async resolve hooks are defined.
- **Signature:** `import.meta.resolve(moduleSpecifier: string, baseURL?: string): string`

### V8 11.3 Updates
- **Engine Version:** V8 11.3 from Chromium 113
- **New Features:**
  - `String.prototype.isWellFormed` and `toWellFormed`
  - Array and TypedArray copy methods
  - Resizable ArrayBuffer and growable SharedArrayBuffer
  - RegExp `v` flag with set notation and new string properties
  - WebAssembly Tail Call optimization

### Stable Test Runner
- **Module:** `test_runner`
- **Status:** Marked as stable (transitioned from experimental)

### Ada 2.0 URL Parser
- **Enhancements:** Improved performance of `url.domainToASCII` and `url.domainToUnicode`
- **Advantage:** Eliminates the ICU requirement for URL hostname parsing

### SEA Blob Injection for Single Executable Apps
- **Requirement:** Instead of injecting raw JS files, a blob prepared from a JSON configuration must now be injected.
- **Purpose:** Enables embedding multiple co-existing resources.

### Web Crypto API Validation
- **Change:** All function arguments are coerced and validated according to their WebIDL definitions.
- **Impact:** Increases compatibility with other Web Crypto API implementations.

### Official ARM64 Windows Support
- **Binaries Include:** MSI, zip/7z packages, and executable files for native ARM64 execution.
- **Testing:** Fully verified on ARM64 Windows CI.

### WASI Version Requirement
- **Usage:** When calling `new WASI()`, the `version` option must be explicitly specified with no default value.

### Deprecations and Removals
- **Example:** `url.parse()` now warns when parsing URLs with non-numeric ports, aligning with the WHATWG URL API.

### Commit History (Semver Commit Examples)
- **Major Commits:** Include changes to async_hooks, buffer API adjustments, build system updates, and deprecations (e.g. `[3bed5f11e0]` for `url.parse()` deprecation).
- **Minor & Patch:** Numerous detailed commits addressing specific module behaviors and improvements.


## Attribution
- Source: Node.js v20 Documentation
- URL: https://nodejs.org/en/blog/release/v20.0.0/
- License: License: Node.js Foundation License
- Crawl Date: 2025-04-17T17:01:36.138Z
- Data Size: 383115 bytes
- Links Found: 803

## Retrieved
2025-04-17
library/EJS_TEMPLATE.md
==== Content of library/EJS_TEMPLATE.md ====
# EJS_TEMPLATE

## Crawl Summary
EJS uses plain JavaScript for templating with simple scriptlet tags (<% %>, <%= %>, <%- %>). It compiles templates into intermediate JS functions for fast execution via caching, provides detailed debugging with template line numbers, and enjoys active development support. Core methods: ejs.render, ejs.compile, ejs.renderFile with explicit configuration options.

## Normalised Extract
## Table of Contents
1. Overview
2. Templating Syntax
3. Execution and Performance
4. Debugging
5. API Methods

### 1. Overview
EJS is a templating engine that allows JavaScript to directly generate HTML. It eliminates the need for custom templating languages by leveraging plain JavaScript. 

### 2. Templating Syntax
- **Control Flow:** Use `<% code %>` for running JavaScript logic.
- **Escaped Output:** Use `<%= expression %>` to output variables as escaped HTML.
- **Unescaped Output:** Use `<%- expression %>` to output raw HTML.

### 3. Execution and Performance
- **Compilation:** Templates are compiled into JavaScript functions.
- **Caching:** Intermediate functions are cached, dramatically increasing performance in production environments.

### 4. Debugging
- **Error Reporting:** Errors are thrown as JavaScript exceptions with precise template line numbers to aid debugging.
- **Debug Options:** Option `compileDebug: true` can be enabled to include additional debugging information in development.

### 5. API Methods
- **ejs.render:** Renders a template string into HTML.
- **ejs.compile:** Compiles a template string into a reusable function.
- **ejs.renderFile:** Renders a template file asynchronously using a callback function.


## Supplementary Details
### Function Signatures and Configuration Options

1. ejs.render(template: string, data?: object, options?: {
    cache?: boolean,          // Cache the compiled function. Defaults to false.
    filename?: string,        // Filename to be used for caching and error reporting.
    root?: string | string[], // Root path(s) for includes when using renderFile.
    delimiter?: string,       // Delimiter character for template tags. Default is '%'.
    strict?: boolean,         // Enforces strict mode, affecting variable scoping.
    compileDebug?: boolean,   // When true, includes debugging information in the compiled function.
    debug?: boolean,          // Outputs additional debug info during rendering.
    rmWhitespace?: boolean    // When true, removes unnecessary whitespace from the output.
}): string

2. ejs.compile(template: string, options?: Object): (data: object) => string

3. ejs.renderFile(filename: string, data?: object, options?: Object, callback?: (err: Error | null, str?: string) => void): void

### Sample Code

// Basic inline rendering
var ejs = require('ejs');
var template = "<h1><%= title %></h1>";
var data = { title: 'Hello EJS' };
var html = ejs.render(template, data, { cache: true, compileDebug: false });
console.log(html);

// Compile template for reuse
var compiled = ejs.compile(template, { delimiter: '%' });
console.log(compiled(data));

// Asynchronous file rendering
ejs.renderFile('template.ejs', data, { cache: false }, function(err, str) {
    if(err) {
        console.error('Render error:', err);
        return;
    }
    console.log(str);
});

### Configuration and Best Practices
- Set `cache: true` in production to enhance performance.
- Use `compileDebug: false` in production to minimize overhead.
- Define a consistent `delimiter` if custom tags are required.
- Always provide a `filename` when using includes, to ensure correct error reporting.

### Troubleshooting Procedures
1. If a template error occurs, verify the line number provided in the error output against your .ejs file.
2. Run with `compileDebug: true` during development to get detailed error stacks.
3. Ensure that the correct file paths are used when rendering from file using ejs.renderFile.
4. Enable `debug: true` temporarily to log rendering process details.


## Reference Details
### Complete API Specifications

#### Method: ejs.render
- **Signature:**
  ejs.render(template: string, data?: object, options?: {
      cache?: boolean,          // (optional) Enables caching of the compiled function. [Default: false]
      filename?: string,        // (optional) Filename for the template. Important for caching and error reporting.
      root?: string | string[], // (optional) Root directory for included templates.
      delimiter?: string,       // (optional) Custom tag delimiter. [Default: '%']
      strict?: boolean,         // (optional) Enforces strict mode. [Default: false]
      compileDebug?: boolean,   // (optional) Include debugging information in compiled function. [Default: true]
      debug?: boolean,          // (optional) Outputs debug information during rendering. [Default: false]
      rmWhitespace?: boolean    // (optional) Removes redundant whitespace for optimized output. [Default: false]
  }): string

- **Returns:** Rendered HTML as a string.
- **Exceptions:** Throws a JavaScript Error with template line reference if the template contains syntax errors.

#### Method: ejs.compile
- **Signature:**
  ejs.compile(template: string, options?: {
      cache?: boolean,
      filename?: string,
      root?: string | string[],
      delimiter?: string,
      strict?: boolean,
      compileDebug?: boolean,
      debug?: boolean,
      rmWhitespace?: boolean
  }): (data: object) => string

- **Returns:** A function that accepts a data object and returns a rendered string.

#### Method: ejs.renderFile
- **Signature:**
  ejs.renderFile(filename: string, data?: object, options?: {
      cache?: boolean,
      filename?: string,
      root?: string | string[],
      delimiter?: string,
      strict?: boolean,
      compileDebug?: boolean,
      debug?: boolean,
      rmWhitespace?: boolean
  }, callback: (err: Error | null, str?: string) => void): void

- **Behavior:** Asynchronously reads the file, compiles it if needed, and returns the rendered HTML through the callback.

### Full Code Example with Comments

/*
 * Example: Rendering an EJS template from a file
 */
const ejs = require('ejs');
const path = require('path');

// Data to pass into the template
const data = {
  title: 'Welcome to EJS',
  user: { name: 'Developer' }
};

// Options including caching and file resolution
const options = {
  cache: true,                // Enable caching for performance
  compileDebug: false,        // Disable compile-time debugging info in production
  delimiter: '%',             // Default delimiter
  filename: path.join(__dirname, 'views', 'index.ejs') // Required for includes
};

// Asynchronous rendering with error checking
ejs.renderFile(options.filename, data, options, (err, html) => {
  if (err) {
    console.error('Error rendering EJS template:', err);
    // Troubleshooting: Verify template syntax and file path.
    return;
  }
  console.log('Rendered HTML:', html);
});

// Best Practice: In production, clear cache only when templates are updated.

### Troubleshooting Commands and Expected Outputs
// Command to run a Node.js script that uses EJS:
// $ node app.js
// Expected Output: Rendered HTML string or an error message with template location and line number.

// For debugging template issues, temporarily enable debugging options:
// Set compileDebug: true and use console logs to trace the rendering process.


## Original Source
EJS Template Engine Documentation
https://ejs.co/

## Digest of EJS_TEMPLATE

# EJS TEMPLATE ENGINE DOCUMENTATION

**Retrieved:** 2023-11-24

## Overview
EJS (Embedded JavaScript) is a templating engine that leverages plain JavaScript for dynamic HTML rendering. It avoids proprietary syntax by using standard JavaScript within scriptlet tags so developers write native code. 

## Templating Syntax
- <% code %>: Executes JavaScript code without output.
- <%= expression %>: Evaluates expression and escapes HTML.
- <%- expression %>: Evaluates expression and outputs unescaped HTML.

## Execution and Performance
- EJS compiles templates into intermediate JavaScript functions using the V8 engine.
- It caches these functions to improve execution speed.

## Debugging
- Errors in EJS are thrown as plain JavaScript exceptions.
- Template errors include line numbers, aiding in quick debugging.

## Active Development
- EJS is under continuous improvement with an active and supportive community.

## Additional Technical Details
- API methods include ejs.render, ejs.compile, and ejs.renderFile with well-defined parameter configurations and callback mechanisms.


## Attribution
- Source: EJS Template Engine Documentation
- URL: https://ejs.co/
- License: License: MIT License
- Crawl Date: 2025-04-17T14:09:10.537Z
- Data Size: 9176 bytes
- Links Found: 33

## Retrieved
2025-04-17
library/CDK_PATTERNS.md
==== Content of library/CDK_PATTERNS.md ====
# CDK_PATTERNS

## Crawl Summary
The extracted technical details include specific serverless pattern categories aligned with AWS Well-Architected Pillars. It covers precise configurations and commands for: 
- Operational Excellence: Health monitoring (metrics: function errors, queue depth, etc.), and IaC lifecycle management (`cdk deploy` and `cdk destroy`).
- Security: API Gateway access control with WAF, resource policies with temporary credentials, and IAM role segregation verified via unit tests.
- Reliability: Throttling (steady state rate, burst limits), circuit breakers, duplicate event handling using deduplication IDs, and orchestration using state machines.
- Performance Efficiency: Use of Lambda Power Tuner for capacity tuning, measurement of startup times via AWS X-Ray, asynchronous invocation patterns (SNS fan-out, etc.), caching and direct managed service integration.
- Cost Optimization: Techniques to minimize external calls, optimize logging, and benchmark function configurations.
This summary provides exact values, command examples, and integration patterns as captured in the crawl.

## Normalised Extract
Table of Contents:
1. Overview
   - Data Size: 2584105 bytes, Links Found: 833
2. Operational Excellence Pillar
   - OPS 1: Health monitoring metrics: functionErrors (integer), queueDepth (integer), stateMachineFailures, responseTime in ms
   - OPS 2: Lifecycle management using commands: `cdk deploy --app "node app.js"` and `cdk destroy --force`
3. Security Pillar
   - SEC 1: API access control via WAF with API Gateway
   - SEC 2: Resource policies and IAM role isolation; unit tests to verify that roles are not merged
   - SEC 3: Secure coding practices with automated security reviews
4. Reliability Pillar
   - REL 1: Throttling configuration parameters: e.g., steadyStateRate: 100, burstLimit: 200
   - REL 2: Resiliency mechanisms such as EventBridge and Lambda Circuit Breaker patterns; duplicate event handling using deduplication IDs
5. Performance Efficiency Pillar
   - PER 1: Capacity tuning using Lambda Power Tuner with memory options from 128MB to 3008MB in 64MB increments
   - PER 2: Function startup time measurement through AWS X-Ray
   - PER 3: Asynchronous patterns: SNS fan-out, Destined Lambda, EventBridge ATM
   - PER 4: Caching strategies and optimized access patterns (AppSync for GraphQL)
   - PER 5: Direct managed service integration patterns (API Gateway to SNS, DynamoDB integrations)
6. Cost Optimization Pillar
   - COST 1: Strategies to reduce costs by minimizing external calls, optimizing logging output, and benchmarking memory configuration
7. Deployment Examples
   - Alexa Skill deployment using Lambda and DynamoDB
   - AWS S3 Website Deployments (Angular and React) with Route53 and CloudFront integrations

Detailed Technical Information:
- **Deployment Commands:** `cdk deploy --app "node app.js"`, `cdk destroy --force`
- **Unit Testing:** Validate IAM role isolation for each Lambda function.
- **Monitoring Tools:** CloudWatch Dashboard setups, AWS X-Ray instrumentation integration.


## Supplementary Details
Operational Excellence Details:
- **Health Metrics:**
  - functionErrors (integer)
  - queueDepth (integer)
  - stateMachineFailures (count)
  - responseTime (milliseconds)
- **Deployment Commands:**
  - Deploy: `cdk deploy --app "node app.js"`
  - Destroy: `cdk destroy --force`

Security Specifications:
- **API Gateway with WAF:**
  Configuration Example:
  ```json
  {
    "apiGateway": {
      "waf": {
         "enabled": true,
         "ruleSet": "AWSManagedRulesCommonRuleSet"
      }
    }
  }
  ```
- **IAM Role Definition (TypeScript):**
  ```typescript
  const role = new iam.Role(this, 'LambdaRole', {
      assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),
      managedPolicies: [
          iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AWSLambdaBasicExecutionRole')
      ]
  });
  ```

Reliability Patterns:
- **Throttling Configuration:**
  ```json
  {
      "throttling": {
          "steadyStateRate": 100,
          "burstLimit": 200
      }
  }
  ```
- **Circuit Breaker Pseudocode:**
  ```javascript
  if (failureCount > threshold) {
      openCircuit();
  }
  ```

Performance Efficiency Specifications:
- **Lambda Power Tuner:**
  - Memory configurations range: 128MB to 3008MB in steps of 64MB
  - Example invocation parameters:
  ```javascript
  const tuningParams = {
    minMemory: 128,
    maxMemory: 3008,
    increment: 64,
    metric: 'Duration',
    tuningOptions: { maxInvocation: 100 }
  };
  lambdaPowerTuner.run(tuningParams).then(result => {
    console.log('Optimal Memory Setting: ', result.optimalMemory);
  }).catch(err => {
    console.error('Tuning error: ', err);
  });
  ```
- **AWS X-Ray Integration:**
  ```typescript
  const segment = AWSXRay.getSegment();
  segment.addAnnotation('key', 'value');
  ```

Cost Optimization Techniques:
- **Memory Benchmarking:**
  ```javascript
  for (const memory of [128, 256, 512, 1024]) {
      deployLambdaWithMemory(memory);
      recordPerformanceMetrics();
  }
  ```


## Reference Details
API Specifications and Detailed Implementation:

1. AWS Lambda Function Deployment
   - **Method:** cdk deploy
   - **Parameters:**
       --app: string (e.g., "node app.js")
       --profile: string (optional AWS CLI profile)
   - **Return:** CloudFormation stack deployment status
   - **Example Command:**
       cdk deploy --app "node app.js" --profile my-aws-profile

2. AWS Lambda Function Destruction
   - **Method:** cdk destroy
   - **Parameters:**
       --force: boolean (bypasses confirmation prompt)
   - **Return:** Stack deletion confirmation
   - **Example Command:**
       cdk destroy --force

3. IAM Role Creation in TypeScript
   - **Method:** new iam.Role(scope, id, props)
   - **Parameters:**
       • scope: Construct
       • id: string
       • props: {
             assumedBy: iam.IPrincipal,
             managedPolicies?: iam.IManagedPolicy[]
         }
   - **Return:** iam.Role instance
   - **Example Code:**
       const role = new iam.Role(this, 'LambdaRole', {
         assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),
         managedPolicies: [
           iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AWSLambdaBasicExecutionRole')
         ]
       });

4. API Gateway with WAF Integration
   - **Configuration Object:**
     {
       "apiGateway": {
         "waf": {
            "enabled": true,
            "ruleSet": "AWSManagedRulesCommonRuleSet"
         }
       }
     }

5. Lambda Power Tuner Invocation
   - **Method:** lambdaPowerTuner.run(parameters)
   - **Parameters:**
       {
         minMemory: number (128),
         maxMemory: number (3008),
         increment: number (64),
         metric: string ('Duration'),
         tuningOptions: { maxInvocation: number (e.g., 100) }
       }
   - **Return:** Promise resolving with optimal memory setting and performance metrics
   - **Full Code Example:**
       const tuningParams = {
         minMemory: 128,
         maxMemory: 3008,
         increment: 64,
         metric: 'Duration',
         tuningOptions: { maxInvocation: 100 }
       };
       lambdaPowerTuner.run(tuningParams).then(result => {
         console.log('Optimal Memory Setting: ', result.optimalMemory);
       }).catch(err => {
         console.error('Tuning error: ', err);
       });

Troubleshooting Procedures:
- **Verbose Deployment Check:**
  Command: `cdk deploy --verbose`
  Expected Output: Detailed CloudFormation event logs with status updates for each resource.
- **IAM Role Verification:**
  Command: `aws iam get-role --role-name LambdaRole`
  Expected Output: JSON description of the IAM role configuration.
- **Lambda Performance Debugging:**
  Enable X-Ray and check logs via CloudWatch:
  Command: `aws logs filter-log-events --log-group-name /aws/lambda/YourFunctionName`

Best Practices:
- Use least privilege for IAM roles.
- Employ Infrastructure as Code for reproducible environments.
- Integrate monitoring and logging tools (CloudWatch, AWS X-Ray) for proactive alerts and debugging.
- Regularly run unit tests to validate secure and isolated role configurations.

## Original Source
CDK Patterns Documentation
https://cdkpatterns.com/

## Digest of CDK_PATTERNS

# CDK_PATTERNS Documentation

**Retrieved on:** 2023-10-27

## Overview
- **Data Size:** 2584105 bytes
- **Links Found:** 833
- **URL:** https://cdkpatterns.com/

## Serverless Pattern Categories

### Operational Excellence Pillar
- **OPS 1:** Health monitoring using metrics, distributed tracing, and logging. Key metrics include function errors, queue depth, state machine execution failures, and response times.
- **OPS 2:** Application lifecycle management via Infrastructure as Code (IaC) with commands such as `cdk deploy` and `cdk destroy`.

### Security Pillar
- **SEC 1:** Access control for Serverless APIs using authentication/authorization and integrated WAF with API Gateway.
- **SEC 2:** Management of security boundaries through resource policies and use of temporary credentials. Emphasizes IAM role segregation (e.g., unit tests ensure roles are not merged).
- **SEC 3:** Application security through secure coding, automated security code reviews, and enforcement of best practices.

### Reliability Pillar
- **REL 1:** Regulation of inbound request rates using throttling configurations (steady state and burst limits) and API quotas.
- **REL 2:** Resiliency patterns including circuit breakers (e.g., EventBridge Circuit Breaker, Lambda Circuit Breaker), duplicate event handling using deduplication IDs, and orchestration via state machines (e.g., Saga Step Function).

### Performance Efficiency Pillar
- **PER 1:** Performance optimization using capacity tuning with the Lambda Power Tuner, determining optimum function memory allocation.
- **PER 2:** Measurement and optimization of function startup time using AWS X-Ray instrumentation.
- **PER 3:** Improved concurrency with asynchronous and stream-based invocations (e.g., SNS fan-out, Destined Lambda, EventBridge ATM).
- **PER 4:** Optimization of access patterns and caching strategies, using integrations like AppSync for GraphQL services.
- **PER 5:** Direct integration with managed services to reduce overhead (e.g., API Gateway directly integrated with SNS or DynamoDB).

### Cost Optimization Pillar
- **COST 1:** Cost reduction by minimizing external calls, optimizing logging levels and retention, benchmarking function configurations (i.e., memory and CPU allocation), and applying cost-aware coding patterns.

## Specific Deployment Patterns & Examples
- **Alexa Skill:** Deployed with a Lambda function backend and a DynamoDB table.
- **AWS S3 Website Deploy:** Deploy Angular or React websites to an S3 bucket with integration for Route53 and CloudFront.

## Additional AWS Resources & Configuration Patterns
- **CloudWatch Dashboard:** For real-time monitoring and automated alerts.
- **Lambda Power Tuner:** For optimizing memory settings from 128MB up to 3008MB in 64MB increments.
- **API Integrations:** Detailed configuration patterns for API Gateway, SNS, and DynamoDB integrations.


## Attribution
- Source: CDK Patterns Documentation
- URL: https://cdkpatterns.com/
- License: License: MIT
- Crawl Date: 2025-04-17T20:10:29.897Z
- Data Size: 2584105 bytes
- Links Found: 833

## Retrieved
2025-04-17
library/DOTENV.md
==== Content of library/DOTENV.md ====
# DOTENV

## Crawl Summary
Installation commands, usage instructions for loading environment variables from a .env file into process.env, details on multiline values, comments, parsing, preloading, variable expansion, command substitution using dotenvx, syncing multiple environments, deployment with encryption, and complete API details for the four functions: config, parse, populate, and decrypt.

## Normalised Extract
## Table of Contents
1. Installation
2. Usage
3. Multiline Values
4. Comments
5. Parsing
6. Preloading
7. Variable Expansion & Command Substitution
8. Syncing and Multiple Environments
9. Deploying & Encryption
10. API Functions
11. Troubleshooting

### 1. Installation
- npm: `npm install dotenv --save`
- yarn: `yarn add dotenv`
- bun: `bun add dotenv`

### 2. Usage
- Create a `.env` file with key-value pairs.
- Load configuration by: 
  - CommonJS: `require('dotenv').config()`
  - ES6: `import 'dotenv/config'`

### 3. Multiline Values
- Direct line breaks supported (>= v15.0.0) and alternative using \n escape.

### 4. Comments
- Lines starting with `#` are ignored. Inline comments require quoting if value contains `#`.

### 5. Parsing
- Use `dotenv.parse(Buffer.from('KEY=val'))` to generate an object.

### 6. Preloading
- Preload with the command: `node -r dotenv/config your_script.js`
- Configure using environment variables or CLI arguments (e.g., `dotenv_config_path` and `dotenv_config_debug`).

### 7. Variable Expansion & Command Substitution
- Expand variables with `dotenv-expand`.
- Use `dotenvx` for command substitution in your `.env` file.

### 8. Syncing and Multiple Environments
- Manage multiple files (.env.production, .env.local) and sync them using dotenvx.

### 9. Deploying & Encryption
- Encrypt .env files with `--encrypt` flag using dotenvx, and deploy with a decryption key provided in environment variables.

### 10. API Functions
- **config(options)**: Reads and parses the .env file to process.env. Options: path, encoding, debug, override, processEnv.
- **parse(src, options?)**: Parses a string/Buffer to an object; option for debug logging.
- **populate(target, source, options?)**: Populates target with source object values; options for override and debug.
- **decrypt**: Decrypts encrypted environment files (via dotenvx integration).

### 11. Troubleshooting
- Enable debug mode to output errors (`{ debug: true }`).
- For React, preface environment variables with REACT_APP_.
- For missing modules in Webpack, use `node-polyfill-webpack-plugin` or `dotenv-webpack`.


## Supplementary Details
### Configuration Options for config()
- path: string, default = path.resolve(process.cwd(), '.env')
- encoding: string, default = 'utf8'
- debug: boolean, default = false
- override: boolean, default = false
- processEnv: object, default = process.env

### Implementation Steps
1. Create .env file with key-value pairs.
2. Import and run `require('dotenv').config(options)` at the start of your application.
3. Validate that process.env contains the loaded values.

### Example Code Snippet for Population

```javascript
const dotenv = require('dotenv');
const parsed = { HELLO: 'world' };

// Option 1: Populate process.env
dotenv.populate(process.env, parsed);
console.log(process.env.HELLO); // 'world'

// Option 2: Populate a custom object with override enabled
const target = { HELLO: 'world' };
dotenv.populate(target, { HELLO: 'universe' }, { override: true, debug: true });
console.log(target); // { HELLO: 'universe' }
```

### Command Line Preloading

```bash
node -r dotenv/config your_script.js
```

Pass configuration via CLI:

```bash
node -r dotenv/config your_script.js dotenv_config_path=/custom/path/to/.env dotenv_config_debug=true
```

### Troubleshooting Commands

Enable debug mode:

```javascript
require('dotenv').config({ debug: true });
```

For Webpack issues, install polyfill:

```bash
npm install node-polyfill-webpack-plugin
```

And configure in webpack.config.js accordingly.


## Reference Details
## API Specifications

### config(options?)

- **Signature:**
  ```javascript
  function config(options?: {
    path?: string,        // Default: path.resolve(process.cwd(), '.env')
    encoding?: string,    // Default: 'utf8'
    debug?: boolean,      // Default: false
    override?: boolean,   // Default: false
    processEnv?: Object   // Default: process.env
  }): { parsed?: { [key: string]: string }, error?: Error }
  ```

- **Example:**
  ```javascript
  const result = require('dotenv').config({
    path: '/custom/path/to/.env',
    encoding: 'latin1',
    debug: true,
    override: true
  });

  if (result.error) {
    throw result.error;
  }
  console.log(result.parsed);
  ```

### parse(src, options?)

- **Signature:**
  ```javascript
  function parse(src: string | Buffer, options?: { debug?: boolean }): { [key: string]: string }
  ```

- **Example:**
  ```javascript
  const dotenv = require('dotenv');
  const buf = Buffer.from('BASIC=basic');
  const config = dotenv.parse(buf, { debug: true });
  console.log(typeof config, config); // { BASIC: 'basic' }
  ```

### populate(target, source, options?)

- **Signature:**
  ```javascript
  function populate(target: { [key: string]: any }, source: { [key: string]: string }, options?: { override?: boolean, debug?: boolean }): void
  ```

- **Example:**
  ```javascript
  const dotenv = require('dotenv');
  const parsed = { HELLO: 'world' };

  // Populate process.env directly
  dotenv.populate(process.env, parsed);
  console.log(process.env.HELLO); // world

  // Populate a custom target with override
  const target = { HELLO: 'world' };
  dotenv.populate(target, { HELLO: 'universe' }, { override: true, debug: true });
  console.log(target); // { HELLO: 'universe' }
  ```

### decrypt (Integration with dotenvx)

- **Note:** The decrypt function is provided when using dotenvx for encrypted environment files. 

- **Usage Example:**
  ```bash
  DOTENV_PRIVATE_KEY_PRODUCTION="<.env.production private key>" dotenvx run -- node index.js
  ```

## Best Practices

- Do not commit your .env file to version control.
- Use different .env files for different environments (.env.development, .env.production).
- Enable debug mode if environment variables are not loading as expected.
- When using dotenv with ES6 modules, preload with `import 'dotenv/config'` to avoid issues with module execution order.

## Detailed Troubleshooting Procedures

1. Verify the .env file exists in the correct directory (usually the project's root).
2. Enable debugging:
   ```javascript
   require('dotenv').config({ debug: true });
   ```
   Inspect logs for errors in key-value parsing.
3. For React/Webpack issues, ensure a proper polyfill is installed:
   ```bash
   npm install node-polyfill-webpack-plugin
   ```
   And update your webpack.config.js accordingly.
4. If variables are not being overridden as expected, pass `override: true` in the options:
   ```javascript
   require('dotenv').config({ override: true });
   ```
5. For encryption issues using dotenvx, ensure the decryption key is correctly set in the environment variable (e.g., DOTENV_PRIVATE_KEY_PRODUCTION).


## Original Source
dotenv Documentation
https://github.com/motdotla/dotenv

## Digest of DOTENV

# DOTENV Documentation

Retrieved: 2023-10-05

## Installation

- Install via npm:

  ```bash
  npm install dotenv --save
  ```

- Alternative package managers:

  ```bash
  yarn add dotenv
  # or
  bun add dotenv
  ```

## Usage (.env Setup)

1. Create a file named `.env` in the root of your project (or in the same folder where your application process is run).

   Example `.env` file:

   ```dotenv
   S3_BUCKET="YOURS3BUCKET"
   SECRET_KEY="YOURSECRETKEYGOESHERE"
   ```

2. Import and configure dotenv as early as possible in your application:

   CommonJS:

   ```javascript
   require('dotenv').config();
   console.log(process.env); // Remove after confirmation
   ```

   ES6:

   ```javascript
   import 'dotenv/config';
   ```

   After configuration, `process.env` contains all keys and values defined in your `.env` file.

## Multiline Values

- For multiline values (>= v15.0.0), you can include line breaks directly:

  ```dotenv
  PRIVATE_KEY="-----BEGIN RSA PRIVATE KEY-----
  ...
  Kh9NV...
  ...
  -----END RSA PRIVATE KEY-----"
  ```

- Alternatively, use the \n escape character:

  ```dotenv
  PRIVATE_KEY="-----BEGIN RSA PRIVATE KEY-----\nKh9NV...\n-----END RSA PRIVATE KEY-----\n"
  ```

## Comments

- Comments can be on their own line or inline. Wrap values containing the `#` character in quotes:

  ```dotenv
  # This is a comment
  SECRET_KEY=YOURSECRETKEYGOESHERE # comment
  SECRET_HASH="something-with-a-#-hash"
  ```

## Parsing

- Use the parsing engine to convert a string or Buffer into an object:

  ```javascript
  const dotenv = require('dotenv');
  const buf = Buffer.from('BASIC=basic');
  const config = dotenv.parse(buf);
  console.log(typeof config, config); // object { BASIC: 'basic' }
  ```

## Preloading

- You can preload dotenv so that you do not explicitly require it in your code:

  ```bash
  node -r dotenv/config your_script.js
  ```

- Command line configuration options can be passed using the format `dotenv_config_<option>=value`:

  ```bash
  node -r dotenv/config your_script.js dotenv_config_path=/custom/path/to/.env dotenv_config_debug=true
  ```

- Alternatively, environment variables can be used:

  ```bash
  DOTENV_CONFIG_ENCODING=latin1 DOTENV_CONFIG_DEBUG=true node -r dotenv/config your_script.js dotenv_config_path=/custom/path/to/.env
  ```

## Variable Expansion & Command Substitution

- Variable Expansion: Use `dotenv-expand` to add the value of one variable into another.

- Command Substitution: Use `dotenvx` to substitute command output directly into variables within your `.env` file.

  Example in `.env` file:

  ```dotenv
  DATABASE_URL="postgres://$(whoami)@localhost/my_database"
  ```

  In your JavaScript code:

  ```javascript
  console.log('DATABASE_URL', process.env.DATABASE_URL);
  ```

  Execute using:

  ```bash
  dotenvx run --debug -- node index.js
  ```

## Syncing and Multiple Environments

- To keep `.env` files in sync or to manage multiple environments, consider using `dotenvx`.

- Example for multiple environments:

  ```bash
  echo "HELLO=production" > .env.production
  echo "console.log('Hello ' + process.env.HELLO)" > index.js
  dotenvx run --env-file=.env.production -- node index.js
  ```

- For using multiple .env files:

  ```bash
  echo "HELLO=local" > .env.local
  echo "HELLO=World" > .env
  echo "console.log('Hello ' + process.env.HELLO)" > index.js
  dotenvx run --env-file=.env.local --env-file=.env -- node index.js
  ```

## Deploying and Encryption

- Encrypt your `.env` file with the following command:

  ```bash
  dotenvx set HELLO Production --encrypt -f .env.production
  ```

- Run your script using the decryption key:

  ```bash
  DOTENV_PRIVATE_KEY_PRODUCTION="<.env.production private key>" dotenvx run -- node index.js
  ```

## API Functions

Dotenv exposes four main functions:

### config

- Loads the .env file into `process.env` and returns an object with the key `parsed` or an `error` if it fails.

  **Signature:**

  ```javascript
  function config(options?: {
    path?: string,           // Default: path.resolve(process.cwd(), '.env')
    encoding?: string,       // Default: 'utf8'
    debug?: boolean,         // Default: false
    override?: boolean,      // Default: false
    processEnv?: Object      // Default: process.env
  }): { parsed?: Object, error?: Error }
  ```

- **Example:**

  ```javascript
  const result = require('dotenv').config({
    path: '/custom/path/to/.env',
    encoding: 'latin1',
    debug: process.env.DEBUG,
    override: true
  });

  if (result.error) {
    throw result.error;
  }

  console.log(result.parsed);
  ```

### parse

- Parses a provided string or Buffer into an object.

  **Signature:**

  ```javascript
  function parse(src: string | Buffer, options?: { debug?: boolean }): Object
  ```

- **Example:**

  ```javascript
  const dotenv = require('dotenv');
  const buf = Buffer.from('BASIC=basic');
  const config = dotenv.parse(buf, { debug: true });
  console.log(typeof config, config);
  ```

### populate

- Populates a target object with parsed key-value pairs from a source object.

  **Signature:**

  ```javascript
  function populate(target: Object, source: Object, options?: { override?: boolean, debug?: boolean }): void
  ```

- **Example:**

  ```javascript
  const dotenv = require('dotenv');
  const parsed = { HELLO: 'world' };
  
  // Populating process.env:
  dotenv.populate(process.env, parsed);
  console.log(process.env.HELLO); // world
  
  // Populating a custom target with override enabled:
  const target = { HELLO: 'world' };
  dotenv.populate(target, { HELLO: 'universe' }, { override: true, debug: true });
  console.log(target); // { HELLO: 'universe' }
  ```

### decrypt

- Exposed function for decryption purposes when working with encrypted environment files (used by dotenvx).

  (No detailed signature provided in the documentation; see dotenvx for implementation.)

## Troubleshooting

- **.env file not loading:**

  Ensure the file is in the correct directory. Enable debug logging:

  ```javascript
  require('dotenv').config({ debug: true });
  ```

  This will output helpful error messages.

- **React and process.env:**

  When using React with Webpack, ensure that variables are injected via bundler configuration. For create-react-app, prefix variables with `REACT_APP_`.

- **Module not found (crypto|os|path):**

  Install polyfills for Webpack < 5:

  ```bash
  npm install node-polyfill-webpack-plugin
  ```

  And configure `webpack.config.js`:

  ```javascript
  const path = require('path');
  const webpack = require('webpack');
  const NodePolyfillPlugin = require('node-polyfill-webpack-plugin');

  module.exports = {
    mode: 'development',
    entry: './src/index.ts',
    output: {
      filename: 'bundle.js',
      path: path.resolve(__dirname, 'dist'),
    },
    plugins: [
      new NodePolyfillPlugin(),
      new webpack.DefinePlugin({
        'process.env': {
          HELLO: JSON.stringify(process.env.HELLO)
        }
      }),
    ],
  };
  ```

## Attribution

- Data Size: 621779 bytes
- Retrieved from: https://github.com/motdotla/dotenv
- Crawled Links: 5053

---


## Attribution
- Source: dotenv Documentation
- URL: https://github.com/motdotla/dotenv
- License: License: MIT
- Crawl Date: 2025-04-17T16:11:25.633Z
- Data Size: 621779 bytes
- Links Found: 5053

## Retrieved
2025-04-17
library/GITHUB_SECRETS.md
==== Content of library/GITHUB_SECRETS.md ====
# GITHUB_SECRETS

## Crawl Summary
This document includes detailed technical steps and configuration details for managing GitHub Actions secrets. It covers creation of repository, environment, and organization secrets, usage in workflows with explicit YAML examples, CLI commands (gh secret set, gh secret list), limitations (number of secrets, size limits), encryption of large secrets using GPG, decryption shell scripts, handling of Base64 encoded binary secrets, and redaction best practices. Each section provides exact commands, configuration flags, and code samples for immediate developer use.

## Normalised Extract
## Table of Contents
1. Creating Secrets for a Repository
2. Creating Secrets for an Environment
3. Creating Secrets for an Organization
4. Using Secrets in Workflows
5. Limits for Secrets
6. Storing Large Secrets
7. Storing Base64 Binary Blobs
8. Redacting Secrets from Workflow Logs

---

### 1. Creating Secrets for a Repository
- Steps: Navigate > Settings > Security > Secrets and Variables > Actions > Secrets tab > New repository secret
- CLI:
  - Set secret: `gh secret set SECRET_NAME`
  - Read from file: `gh secret set SECRET_NAME < secret.txt`
  - List secrets: `gh secret list`

### 2. Creating Secrets for an Environment
- Steps: Navigate > Settings > Environments > Select environment > Add secret
- CLI:
  - Set environment secret: `gh secret set --env ENV_NAME SECRET_NAME`
  - List environment secrets: `gh secret list --env ENV_NAME`

### 3. Creating Secrets for an Organization
- Steps: Navigate to organization > Settings > Security > Secrets and Variables > Actions > New organization secret
- Options: Set repository access policy (all, private, or specific repos)
- CLI:
  - Login with scopes: `gh auth login --scopes "admin:org"`
  - Set org secret: `gh secret set --org ORG_NAME SECRET_NAME`
  - With visibility: `gh secret set --org ORG_NAME SECRET_NAME --visibility all`
  - With repo restrictions: `gh secret set --org ORG_NAME SECRET_NAME --repos REPO1,REPO2`
  - List secrets: `gh secret list --org ORG_NAME`

### 4. Using Secrets in Workflows
- Reference in YAML:
  - As input: `${{ secrets.SuperSecret }}`
  - As environment variable: `env:
      super_secret: ${{ secrets.SuperSecret }}`
- Code examples for Bash, PowerShell, Cmd.exe provided with correct quoting.

### 5. Limits for Secrets
- Repository: 100 secrets; Environment: 100 secrets; Organization: up to 1,000 (with first 100 accessible).
- Size limit: 48 KB per secret.

### 6. Storing Large Secrets
- Encrypt using GPG: `gpg --symmetric --cipher-algo AES256 my_secret.json`
- Decrypt with shell script (decrypt_secret.sh) using the LARGE_SECRET_PASSPHRASE secret.
- Workflow integration provided with actions/checkout and decryption step.

### 7. Storing Base64 Binary Blobs
- Convert files to Base64 (e.g., `base64 -w 0 cert.der > cert.base64`)
- Set secret: `gh secret set CERTIFICATE_BASE64 < cert.base64`
- Decode in workflow and output to file.

### 8. Redacting Secrets from Workflow Logs
- GitHub Actions auto-redacts known secrets.
- Use `::add-mask::VALUE` for additional sensitive data.


## Supplementary Details
### Detailed Supplementary Technical Specifications

1. **Exact Command Parameters**:
   - `gh secret set SECRET_NAME`: Prompts for value; can use file redirection.
   - `gh secret list`: Lists existing secrets with no additional parameters by default.
   - For environment secrets: `gh secret set --env ENV_NAME SECRET_NAME` (flag `--env` or `-e` required).
   - For organization secrets: `gh secret set --org ORG_NAME SECRET_NAME` with optional flags `--visibility` (values: all) and `--repos` (comma-separated list).

2. **GPG Encryption Details**:
   - Command: `gpg --symmetric --cipher-algo AES256 my_secret.json`
   - Encryption uses AES256; prompts for passphrase. Ensure that the encrypted file retains the `.gpg` extension.
   - Decryption script must use `--quiet`, `--batch`, and `--yes` flags to automate decryption without interaction.

3. **Workflow YAML Specifications**:
   - Each step is defined with a `name`, `uses` or `run`, and `env` where applicable.
   - Example for decryption:
     ```yaml
     - name: Decrypt large secret
       run: ./decrypt_secret.sh
       env:
         LARGE_SECRET_PASSPHRASE: ${{ secrets.LARGE_SECRET_PASSPHRASE }}
     ```

4. **Best Practices**:
   - Never print secrets in logs.
   - Always use environment variables or STDIN for passing secrets.
   - For complex workflows, encapsulate secret decryption in a dedicated script and check permissions on the decryption script and output files.

5. **Troubleshooting Procedures**:
   - If decryption fails, verify that the passphrase stored in `LARGE_SECRET_PASSPHRASE` matches the one used during encryption.
   - Use `chmod +x decrypt_secret.sh` to ensure the script is executable.
   - In workflows, check the runner’s environment variables and file paths.
   - For Base64 decoding issues, verify the encoded string does not include newlines (use `-w 0` option on Linux).


## Reference Details
### Complete Reference API and Usage Specifications

1. **GitHub CLI Secret Commands**:
   - **Repository Secrets**:
     - Set secret:
       ```bash
       gh secret set SECRET_NAME
       ```
       *Prompts user for secret value or accepts input redirect from a file:*
       ```bash
       gh secret set SECRET_NAME < secret.txt
       ```
     - List secrets:
       ```bash
       gh secret list
       ```

   - **Environment Secrets**:
     - Set secret for environment:
       ```bash
       gh secret set --env ENV_NAME SECRET_NAME
       ```
     - List environment secrets:
       ```bash
       gh secret list --env ENV_NAME
       ```

   - **Organization Secrets**:
     - Login with required scopes (admin:org):
       ```bash
       gh auth login --scopes "admin:org"
       ```
     - Set secret (default to private repositories):
       ```bash
       gh secret set --org ORG_NAME SECRET_NAME
       ```
     - Set secret with full visibility:
       ```bash
       gh secret set --org ORG_NAME SECRET_NAME --visibility all
       ```
     - Set secret for specified repositories:
       ```bash
       gh secret set --org ORG_NAME SECRET_NAME --repos REPO-NAME-1,REPO-NAME-2
       ```
     - List organization secrets:
       ```bash
       gh secret list --org ORG_NAME
       ```

2. **SDK/CLI Method Signatures**:
   - Although GitHub CLI commands are executed via terminal, these commands constitute the method signatures with parameters such as:
     - `--env [environment name]`: string
     - `--org [organization name]`: string
     - `--visibility [all]`: string; defaults to private if not specified
     - `--repos [repo names]`: comma-separated string

3. **Code Examples with Comments**:

*Example: Using a secret in a GitHub Actions workflow (Bash)*:

```yaml
steps:
  - name: Execute command with secret
    shell: bash
    env:
      SUPER_SECRET: ${{ secrets.SuperSecret }}  # Inject secret into environment variable
    run: |
      # Use the secret in a command, ensuring proper quoting to handle special characters
      example-command "$SUPER_SECRET"
```

*Example: Decryption Script (decrypt_secret.sh)*:

```bash
#!/bin/sh
# Create a directory for decrypted secrets
mkdir -p $HOME/secrets
# Decrypt the file using GPG in non-interactive mode
gpg --quiet --batch --yes --decrypt --passphrase="$LARGE_SECRET_PASSPHRASE" \
  --output $HOME/secrets/my_secret.json my_secret.json.gpg
```

4. **Specific Configuration Options and Their Effects**:
   - `--cipher-algo AES256`: Selects the AES256 algorithm for GPG encryption.
   - `--quiet`, `--batch`, `--yes`: Options to automate decryption without prompts.
   - `base64 -w 0`: Option to encode file without line breaks (essential for secrets that require single line encoding).

5. **Troubleshooting Commands**:
   - Check file permissions:
     ```bash
     ls -l decrypt_secret.sh
     ```
   - Test GPG decryption manually:
     ```bash
     gpg --decrypt --passphrase "your_passphrase" my_secret.json.gpg
     ```
   - Verify environment variables in a workflow by echoing (only in non-production environments):
     ```yaml
     - name: Verify secret presence
       run: echo "Secret Length: ${#SUPER_SECRET}"
       env:
         SUPER_SECRET: ${{ secrets.SuperSecret }}
     ```

This detailed reference section provides the exact commands, parameters, and implementation patterns that developers can integrate directly into their projects without additional modification.


## Original Source
GitHub Actions Secrets Documentation
https://docs.github.com/en/actions/security-guides/encrypted-secrets

## Digest of GITHUB_SECRETS

# GitHub Secrets Documentation Digest

**Date Retrieved**: 2023-10-12

This document contains the complete technical details extracted from the GitHub Actions secrets documentation. It includes exact configuration steps, CLI commands, API method signatures, and detailed code examples to manage secrets for repositories, environments, and organizations.

## Creating Secrets for a Repository

- Navigate to the main page of the repository.
- Click on the **Settings** tab (or via dropdown if not directly visible).
- In the sidebar under **Security**, select **Secrets and variables**, then click **Actions**.
- Click on the **Secrets** tab and then **New repository secret**.
- In the **Name** field, type the secret name.
- In the **Secret** field, enter the secret value.
- Click **Add secret**.

**CLI Commands**:

```bash
# Set a repository secret using GitHub CLI
gh secret set SECRET_NAME
# Alternatively reading the secret from a file
gh secret set SECRET_NAME < secret.txt

# List repository secrets
gh secret list
```

## Creating Secrets for an Environment

- Navigate to the main page of the repository.
- Click **Settings** (from dropdown if needed).
- Click **Environments** in the left sidebar.
- Select the desired environment.
- Under **Environment secrets**, click **Add secret**.
- Enter the secret name and value.
- Click **Add secret**.

**CLI Commands**:

```bash
# Set an environment secret
gh secret set --env ENV_NAME SECRET_NAME
# List environment secrets
gh secret list --env ENV_NAME
```

## Creating Secrets for an Organization

- Navigate to the organization’s main page.
- Click **Settings** (accessible via dropdown if needed).
- Under **Security**, select **Secrets and variables**, then click **Actions**.
- Click on the **Secrets** tab and then **New organization secret**.
- Enter the secret name and value.
- Choose an access policy from the **Repository access** dropdown.
- Click **Add secret**.

**CLI Commands and Scopes**:

```bash
# Ensure admin:org scope is authorized
gh auth login --scopes "admin:org"

# Set an organization secret available to private repos by default
gh secret set --org ORG_NAME SECRET_NAME

# Set secret with visibility options
gh secret set --org ORG_NAME SECRET_NAME --visibility all

# Limit secret to specific repositories
gh secret set --org ORG_NAME SECRET_NAME --repos REPO-NAME-1,REPO-NAME-2

# List organization secrets
gh secret list --org ORG_NAME
```

## Reviewing Access to Organization-Level Secrets

- In the organization’s **Settings** under **Secrets and variables**, review configured permissions and policies on the secrets list. Click **Update** for detail review.

## Using Secrets in a Workflow

To reference a secret in your workflow:

- Use the `secrets` context to provide the secret as an input or environment variable.

**Example Workflow YAML**:

```yaml
steps:
  - name: Hello world action
    with:
      super_secret: ${{ secrets.SuperSecret }}
    env:
      super_secret: ${{ secrets.SuperSecret }}
```

**Important Notes**:
- Secrets (except GITHUB_TOKEN) are not passed to runners triggering from forked repos.
- They are not directly available in `if:` conditionals; instead, use job-level environment variables.
- If a secret is unset, referencing it returns an empty string.

**Shell Examples for Quoting Secrets**:

*Bash*:

```yaml
steps:
  - shell: bash
    env:
      SUPER_SECRET: ${{ secrets.SuperSecret }}
    run: |
      example-command "$SUPER_SECRET"
```

*PowerShell*:

```yaml
steps:
  - shell: pwsh
    env:
      SUPER_SECRET: ${{ secrets.SuperSecret }}
    run: |
      example-command "$env:SUPER_SECRET"
```

*Cmd.exe*:

```yaml
steps:
  - shell: cmd
    env:
      SUPER_SECRET: ${{ secrets.SuperSecret }}
    run: |
      example-command "%SUPER_SECRET%"
```

## Limits for Secrets

- Repository: 100 secrets max.
- Organization: Up to 1,000 secrets; workflows can access first 100 alphabetically if over limit.
- Environment: 100 secrets max.
- Maximum size: 48 KB per secret.

## Storing Large Secrets

For secrets larger than 48 KB, use encryption and store the decryption passphrase as a secret.

**GPG Encryption Command**:

```bash
gpg --symmetric --cipher-algo AES256 my_secret.json
```

- Create secret named `LARGE_SECRET_PASSPHRASE` with the passphrase.
- Commit the encrypted file (`my_secret.json.gpg`) to the repository.

**Decryption Script (decrypt_secret.sh)**:

```bash
#!/bin/sh

# Create a directory for secrets
mkdir -p $HOME/secrets
# Decrypt secret file
gpg --quiet --batch --yes --decrypt --passphrase="$LARGE_SECRET_PASSPHRASE" \
  --output $HOME/secrets/my_secret.json my_secret.json.gpg
```

- Ensure the script is executable:

```bash
chmod +x decrypt_secret.sh
```

**Workflow Integration**:

```yaml
name: Workflows with large secrets
on: push
jobs:
  my-job:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Decrypt large secret
        run: ./decrypt_secret.sh
        env:
          LARGE_SECRET_PASSPHRASE: ${{ secrets.LARGE_SECRET_PASSPHRASE }}
      - name: Test printing your secret (Remove in production)
        run: cat $HOME/secrets/my_secret.json
```

## Storing Base64 Binary Blobs as Secrets

- Convert binary files into a Base64 encoded string.

**Examples**:

*macOS*:

```bash
base64 -i cert.der -o cert.base64
```

*Linux*:

```bash
base64 -w 0 cert.der > cert.base64
```

- Create a secret using the encoded string:

```bash
gh secret set CERTIFICATE_BASE64 < cert.base64
```

- Decode in a workflow:

```yaml
name: Retrieve Base64 secret
on: push
jobs:
  decode-secret:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Decode secret into a file
        env:
          CERTIFICATE_BASE64: ${{ secrets.CERTIFICATE_BASE64 }}
        run: |
          echo $CERTIFICATE_BASE64 | base64 --decode > cert.der
      - name: Show certificate info
        run: |
          openssl x509 -in cert.der -inform DER -text -noout
```

## Redacting Secrets from Workflow Run Logs

- GitHub Actions automatically redacts printed secrets and other recognized sensitive data (e.g., keys, tokens, connection strings).
- Use `::add-mask::VALUE` to mask non-secret sensitive data.

**Note**: Ensure secrets are not exposed via command-line arguments that can be captured by audit events.


## Attribution
- Source: GitHub Actions Secrets Documentation
- URL: https://docs.github.com/en/actions/security-guides/encrypted-secrets
- License: License: GitHub Docs License (CC BY 4.0)
- Crawl Date: 2025-04-17T19:10:41.390Z
- Data Size: 1111360 bytes
- Links Found: 18386

## Retrieved
2025-04-17
library/AWS_CDK.md
==== Content of library/AWS_CDK.md ====
# AWS_CDK

## Crawl Summary
The crawled AWS CDK documentation provides detailed technical information including the framework's dual components (Construct Library and CLI), support for multiple programming languages, and benefits such as IAC, unified code management, and reproducible CloudFormation deployments. It includes explicit code examples in TypeScript, JavaScript, Python, Java, C#, and Go with exact parameter values (e.g., maxAzs=3, cpu=512, desiredCount=6, memoryLimitMiB=2048) and step-by-step construct instantiation. It also lists CloudFormation resource types generated by a typical CDK deployment.

## Normalised Extract
## Table of Contents
1. AWS CDK Overview & Benefits
2. Programming Languages & Constructs
3. Code Examples
   - TypeScript
   - JavaScript
   - Python
   - Java
   - C#
   - Go
4. CloudFormation Resources
5. Technical Features & References

## 1. AWS CDK Overview & Benefits
- Definition: Open-source framework for defining cloud infrastructure in code.
- Components: Construct Library and CDK CLI.
- Benefit: Infrastructure as Code with native programming language constructs.

## 2. Programming Languages & Constructs
- Languages: TypeScript, JavaScript, Python, Java, C# (.Net), Go.
- Constructs: Reusable components that model AWS resources.

## 3. Code Examples

### TypeScript
Code demonstrates creation of a VPC with a maximum of 3 Availability Zones, a Cluster, and a load-balanced Fargate service with:
- cpu: 512 (default 256)
- desiredCount: 6 (default 1)
- memoryLimitMiB: 2048 (default 512)
- publicLoadBalancer: true (default false)

### JavaScript
Follows the same pattern as the TypeScript example with equivalent parameters and instantiations.

### Python
Defines a Stack subclass that instantiates a VPC, Cluster, and ApplicationLoadBalancedFargateService using similar parameters.

### Java
Uses Builders to create the VPC, Cluster, and ApplicationLoadBalancedFargateService with explicit method chaining and parameter declarations.

### C#
Instantiates VPC, Cluster, and Fargate Service in a Stack with strong typing, using property objects for configuration.

### Go
Creates a new CDK Stack, defines VPC and Cluster objects, and uses the patterns API to configure an ApplicationLoadBalancedFargateService with jsii wrappers for numeric and boolean values.

## 4. CloudFormation Resources
List of resource types generated includes:
- AWS::EC2::EIP, InternetGateway, NatGateway, Route, RouteTable, SecurityGroup, Subnet, SubnetRouteTableAssociation, VPCGatewayAttachment, VPC
- AWS::ECS::Cluster, Service, TaskDefinition
- AWS::ElasticLoadBalancingV2::Listener, LoadBalancer, TargetGroup
- AWS::IAM::Policy, Role
- AWS::Logs::LogGroup

## 5. Technical Features & References
- GitHub Repository (aws-cdk) for repository access and contributing.
- API Reference for exact class and method specifications.
- Construct Programming Model for extending constructs to other domains (CDKtf, CDK8s, Projen).
- Construct Hub for sharing and discovering constructs.

## Supplementary Details
### Configuration Options & Parameter Details

- **VPC Configuration:**
  - Parameter: maxAzs
  - Value: 3 (default uses all AZs in the region)

- **Fargate Service Configuration:**
  - cpu: 512 (override default 256)
  - desiredCount: 6 (override default 1)
  - memoryLimitMiB / memory_limit_mib: 2048 (override default 512)
  - publicLoadBalancer: true (override default false)

### Implementation Steps

1. Instantiate a VPC with a specified number of AZs using new ec2.Vpc(this, "MyVpc", {maxAzs: 3}).
2. Create an ECS Cluster linked to the VPC (new ecs.Cluster(this, "MyCluster", { vpc })).
3. Create a load-balanced Fargate Service using the appropriate construct from ecs_patterns, providing the required cluster, cpu, desiredCount, taskImageOptions with an image from registry, memoryLimitMiB, and publicLoadBalancer flag.
4. Deploy the stack to generate a CloudFormation template with over 500 lines that include defined resource types.

### Best Practices

- Use constructs to encapsulate reusable infrastructure patterns.
- Use stacks to group related resources for easier deployments and rollbacks.
- Validate parameter overrides (e.g., cpu, memory) against defaults to ensure expected performance.
- Leverage IDE features in languages like TypeScript and Python for syntax checking and intelligent code completion.

### Troubleshooting Procedures

- If the deployment fails, inspect the generated AWS CloudFormation template for missing parameters or misconfigurations.
- Use AWS CloudFormation's rollback messages to pinpoint configuration errors.
- Validate the runtime environment (e.g., Node.js version for TypeScript/JavaScript) and dependencies installed for the CDK app.
- Run CDK CLI commands such as `cdk synth` to generate and inspect the output template before deployment.


## Reference Details
### Complete API Specifications & SDK Method Signatures

#### TypeScript Example

Method Signature:

export class MyEcsConstructStack extends Stack {
  constructor(scope: App, id: string, props?: StackProps) {
    // super call and construct initialization
  }
}

Parameters:
- scope: App (represents the CDK app context)
- id: string (unique identifier for the stack)
- props?: StackProps (optional configuration properties for the stack)

Return Type: Instance of MyEcsConstructStack extending Stack

#### JavaScript Example

class MyEcsConstructStack extends Stack {
  constructor(scope, id, props) {
    // super(scope, id, props) call
  }
}

#### Python Example

class MyEcsConstructStack(Stack):
    def __init__(self, scope: Construct, id: str, **kwargs) -> None:
        super().__init__(scope, id, **kwargs)
        # VPC and cluster instantiation

Parameter Types:
- scope: Construct
- id: str
- kwargs: Additional StackProps

#### Java Example

public MyEcsConstructStack(final Construct scope, final String id, StackProps props) {
    super(scope, id, props);
    // VPC created using Vpc.Builder
}

Method Specifications:
- Vpc.Builder.create(this, "MyVpc").maxAzs(3).build();
- Cluster.Builder.create(this, "MyCluster").vpc(vpc).build();
- ApplicationLoadBalancedFargateService.Builder.create(this, "MyFargateService")
      .cluster(cluster)
      .cpu(512)
      .desiredCount(6)
      .taskImageOptions(ApplicationLoadBalancedTaskImageOptions.builder()
          .image(ContainerImage.fromRegistry("amazon/amazon-ecs-sample"))
          .build())
      .memoryLimitMiB(2048)
      .publicLoadBalancer(true)
      .build();

#### C# Example

public MyEcsConstructStack(Construct scope, string id, IStackProps props = null) : base(scope, id, props) {
   var vpc = new Vpc(this, "MyVpc", new VpcProps { MaxAzs = 3 });
   var cluster = new Cluster(this, "MyCluster", new ClusterProps { Vpc = vpc });
   new ApplicationLoadBalancedFargateService(this, "MyFargateService", new ApplicationLoadBalancedFargateServiceProps {
       Cluster = cluster,
       Cpu = 512,
       DesiredCount = 6,
       TaskImageOptions = new ApplicationLoadBalancedTaskImageOptions {
           Image = ContainerImage.FromRegistry("amazon/amazon-ecs-sample")
       },
       MemoryLimitMiB = 2048,
       PublicLoadBalancer = true,
   });
}

#### Go Example

func NewMyEcsConstructStack(scope constructs.Construct, id string, props *MyEcsConstructStackProps) awscdk.Stack {
    var sprops awscdk.StackProps
    if props != nil {
      sprops = props.StackProps
    }
    stack := awscdk.NewStack(scope, &id, &sprops)
    vpc := awsec2.NewVpc(stack, jsii.String("MyVpc"), &awsec2.VpcProps{
        MaxAzs: jsii.Number(3),
    })
    cluster := awsecs.NewCluster(stack, jsii.String("MyCluster"), &awsecs.ClusterProps{
        Vpc: vpc,
    })
    awsecspatterns.NewApplicationLoadBalancedFargateService(stack, jsii.String("MyFargateService"),
      &awsecspatterns.ApplicationLoadBalancedFargateServiceProps{
          Cluster:        cluster,
          Cpu:            jsii.Number(512),
          DesiredCount:   jsii.Number(6),
          MemoryLimitMiB: jsii.Number(2048),
          TaskImageOptions: &awsecspatterns.ApplicationLoadBalancedTaskImageOptions{
              Image: awsecs.ContainerImage_FromRegistry(jsii.String("amazon/amazon-ecs-sample"), nil),
          },
          PublicLoadBalancer: jsii.Bool(true),
      })
    return stack
}

### Troubleshooting Commands & Expected Outputs

- Command: `cdk synth`
  - Expected Output: A valid CloudFormation template printed to STDOUT.
- Command: `cdk deploy`
  - Expected Output: The stack deployment progress and a final confirmation with deployed resource IDs.

All parameters and configurations are to be used exactly as specified without further modification. These code examples and API signatures are intended for direct application in development environments.

## Original Source
AWS CDK Documentation
https://docs.aws.amazon.com/cdk/latest/guide/home.html

## Digest of AWS_CDK

# AWS CDK Developer Guide

**Retrieval Date:** 2023-10-24

## Overview

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. It consists of two primary parts:

- **AWS CDK Construct Library**: A collection of pre-written, modular, reusable constructs for quickly developing your infrastructure. These constructs abstract AWS CloudFormation resources with sensible defaults and common patterns.
- **AWS CDK CLI (Toolkit)**: A command line tool for creating, managing, and deploying AWS CDK apps.

The AWS CDK supports multiple programming languages including TypeScript, JavaScript, Python, Java, C#/.Net, and Go.

## Table of Contents

1. [CDK Overview & Benefits](#cdk-overview--benefits)
2. [Programming Languages & Constructs](#programming-languages--constructs)
3. [Code Examples](#code-examples)
   - TypeScript
   - JavaScript
   - Python
   - Java
   - C#
   - Go
4. [CloudFormation Resources Generated](#cloudformation-resources-generated)
5. [Technical Features & References](#technical-features--references)

## CDK Overview & Benefits

- **Infrastructure as Code (IaC):** Write, deploy, and manage your infrastructure using familiar programming concepts (parameters, loops, conditionals) which results in structured infrastructure management.
- **Unified Application and Infrastructure Code:** Define both your application logic and infrastructure in one place, utilizing modern IDE features like syntax highlighting and code completion.
- **Deployment via AWS CloudFormation:** Automates resource provisioning with rollback on errors, leveraging AWS CloudFormation’s maturity.
- **Rapid Development using Constructs:** Use pre-built constructs or create custom ones to rapidly develop cloud solutions.

## Programming Languages & Constructs

AWS CDK allows you to define cloud components (constructs) using:

- TypeScript
- JavaScript
- Python
- Java
- C#/.Net
- Go

These constructs are composed into stacks and apps for deployment with AWS CloudFormation.

## Code Examples

### TypeScript

```typescript
export class MyEcsConstructStack extends Stack {
  constructor(scope: App, id: string, props?: StackProps) {
    super(scope, id, props);

    const vpc = new ec2.Vpc(this, "MyVpc", {
      maxAzs: 3 // Default is all AZs in region
    });

    const cluster = new ecs.Cluster(this, "MyCluster", {
      vpc: vpc
    });

    // Create a load-balanced Fargate service
    new ecs_patterns.ApplicationLoadBalancedFargateService(this, "MyFargateService", {
      cluster: cluster, // Required
      cpu: 512,         // Default is 256
      desiredCount: 6,  // Default is 1
      taskImageOptions: { 
        image: ecs.ContainerImage.fromRegistry("amazon/amazon-ecs-sample")
      },
      memoryLimitMiB: 2048, // Default is 512
      publicLoadBalancer: true // Default is false
    });
  }
}
```

### JavaScript

```javascript
class MyEcsConstructStack extends Stack {
  constructor(scope, id, props) {
    super(scope, id, props);

    const vpc = new ec2.Vpc(this, "MyVpc", {
      maxAzs: 3 // Default is all AZs in region
    });

    const cluster = new ecs.Cluster(this, "MyCluster", {
      vpc: vpc
    });

    // Create a load-balanced Fargate service
    new ecs_patterns.ApplicationLoadBalancedFargateService(this, "MyFargateService", {
      cluster: cluster, // Required
      cpu: 512,         // Default is 256
      desiredCount: 6,  // Default is 1
      taskImageOptions: { 
        image: ecs.ContainerImage.fromRegistry("amazon/amazon-ecs-sample")
      },
      memoryLimitMiB: 2048, // Default is 512
      publicLoadBalancer: true // Default is false
    });
  }
}

module.exports = { MyEcsConstructStack };
```

### Python

```python
class MyEcsConstructStack(Stack):

    def __init__(self, scope: Construct, id: str, **kwargs) -> None:
        super().__init__(scope, id, **kwargs)

        vpc = ec2.Vpc(self, "MyVpc", max_azs=3)  # default is all AZs in region

        cluster = ecs.Cluster(self, "MyCluster", vpc=vpc)

        ecs_patterns.ApplicationLoadBalancedFargateService(
            self, "MyFargateService",
            cluster=cluster,            # Required
            cpu=512,                    # Default is 256
            desired_count=6,            # Default is 1
            task_image_options=ecs_patterns.ApplicationLoadBalancedTaskImageOptions(
                image=ecs.ContainerImage.from_registry("amazon/amazon-ecs-sample")
            ),
            memory_limit_mib=2048,      # Default is 512
            public_load_balancer=True   # Default is False
        )
```

### Java

```java
public class MyEcsConstructStack extends Stack {

    public MyEcsConstructStack(final Construct scope, final String id) {
        this(scope, id, null);
    }

    public MyEcsConstructStack(final Construct scope, final String id, StackProps props) {
        super(scope, id, props);

        Vpc vpc = Vpc.Builder.create(this, "MyVpc")
                .maxAzs(3)
                .build();

        Cluster cluster = Cluster.Builder.create(this, "MyCluster")
                .vpc(vpc)
                .build();

        ApplicationLoadBalancedFargateService.Builder.create(this, "MyFargateService")
                .cluster(cluster)
                .cpu(512)
                .desiredCount(6)
                .taskImageOptions(
                    ApplicationLoadBalancedTaskImageOptions.builder()
                        .image(ContainerImage.fromRegistry("amazon/amazon-ecs-sample"))
                        .build()
                )
                .memoryLimitMiB(2048)
                .publicLoadBalancer(true)
                .build();
    }
}
```

### C#

```csharp
public class MyEcsConstructStack : Stack
{
    public MyEcsConstructStack(Construct scope, string id, IStackProps props = null) : base(scope, id, props)
    {
        var vpc = new Vpc(this, "MyVpc", new VpcProps
        {
            MaxAzs = 3
        });

        var cluster = new Cluster(this, "MyCluster", new ClusterProps
        {
            Vpc = vpc
        });

        new ApplicationLoadBalancedFargateService(this, "MyFargateService",
            new ApplicationLoadBalancedFargateServiceProps
            {
                Cluster = cluster,
                Cpu = 512,
                DesiredCount = 6,
                TaskImageOptions = new ApplicationLoadBalancedTaskImageOptions
                {
                    Image = ContainerImage.FromRegistry("amazon/amazon-ecs-sample")
                },
                MemoryLimitMiB = 2048,
                PublicLoadBalancer = true
            });
    }
}
```

### Go

```go
func NewMyEcsConstructStack(scope constructs.Construct, id string, props *MyEcsConstructStackProps) awscdk.Stack {

    var sprops awscdk.StackProps
    if props != nil {
        sprops = props.StackProps
    }

    stack := awscdk.NewStack(scope, &id, &sprops)

    vpc := awsec2.NewVpc(stack, jsii.String("MyVpc"), &awsec2.VpcProps{
        MaxAzs: jsii.Number(3), // Default is all AZs in region
    })

    cluster := awsecs.NewCluster(stack, jsii.String("MyCluster"), &awsecs.ClusterProps{
        Vpc: vpc,
    })

    awsecspatterns.NewApplicationLoadBalancedFargateService(stack, jsii.String("MyFargateService"),
        &awsecspatterns.ApplicationLoadBalancedFargateServiceProps{
            Cluster:        cluster,           // Required
            Cpu:            jsii.Number(512),  // Default is 256
            DesiredCount:   jsii.Number(6),    // Default is 1
            MemoryLimitMiB: jsii.Number(2048), // Default is 512
            TaskImageOptions: &awsecspatterns.ApplicationLoadBalancedTaskImageOptions{
                Image: awsecs.ContainerImage_FromRegistry(jsii.String("amazon/amazon-ecs-sample"), nil),
            },
            PublicLoadBalancer: jsii.Bool(true), // Default is false
        })

    return stack
}
```

## CloudFormation Resources Generated

Deploying the AWS CDK app typically produces a CloudFormation template of more than 500 lines creating over 50 resources including but not limited to:

- AWS::EC2::EIP
- AWS::EC2::InternetGateway
- AWS::EC2::NatGateway
- AWS::EC2::Route
- AWS::EC2::RouteTable
- AWS::EC2::SecurityGroup
- AWS::EC2::Subnet
- AWS::EC2::SubnetRouteTableAssociation
- AWS::EC2::VPCGatewayAttachment
- AWS::EC2::VPC
- AWS::ECS::Cluster
- AWS::ECS::Service
- AWS::ECS::TaskDefinition
- AWS::ElasticLoadBalancingV2::Listener
- AWS::ElasticLoadBalancingV2::LoadBalancer
- AWS::ElasticLoadBalancingV2::TargetGroup
- AWS::IAM::Policy
- AWS::IAM::Role
- AWS::Logs::LogGroup

## Technical Features & References

- **GitHub Repository:** [aws-cdk](https://github.com/aws/aws-cdk) for issue tracking, license, releases and contributions.
- **API Reference:** Detailed APIs provided by the AWS CDK Construct Library.
- **Construct Programming Model (CPM):** Extends AWS CDK concepts to tools such as CDK for Terraform (CDKtf), CDK for Kubernetes (CDK8s) and Projen.
- **Construct Hub:** An online registry for finding, publishing and sharing AWS CDK libraries.


## Attribution
- Source: AWS CDK Documentation
- URL: https://docs.aws.amazon.com/cdk/latest/guide/home.html
- License: License: AWS Documentation License
- Crawl Date: 2025-04-17T18:39:25.658Z
- Data Size: 1433781 bytes
- Links Found: 131049

## Retrieved
2025-04-17
library/DEPENDABOT_UPDATES.md
==== Content of library/DEPENDABOT_UPDATES.md ====
# DEPENDABOT_UPDATES

## Crawl Summary
Dependabot version updates automates dependency maintenance by generating pull requests based on a YAML configuration file. Key technical details include configuration settings in dependabot.yml (version set to 2, package-ecosystem, directory, schedule intervals, open-pull-requests limit, and target branch). Integration with GitHub’s Advanced Security, pull request review workflow, and troubleshooting logs in the security overview are specified.

## Normalised Extract
**Table of Contents:**
1. Dependabot Version Updates Overview
2. Configuration File (dependabot.yml) Details
3. Pull Request Integration and Workflow
4. Troubleshooting Procedures

---

**1. Dependabot Version Updates Overview**
- Function: Automatically update dependency versions regardless of vulnerability.
- Activation: Via repository settings under Advanced Security.

**2. Configuration File (dependabot.yml) Details**
- **File Location:** `/.github/dependabot.yml`
- **Mandatory Fields:**
  - version: must be set to 2
  - updates: array containing update configurations
- **Example Entry:**
  ```yaml
  version: 2
  updates:
    - package-ecosystem: "npm"
      directory: "/"
      schedule:
        interval: "daily"
      open-pull-requests-limit: 5
      target-branch: "main"
  ```
- **Parameters:**
  - package-ecosystem: Supported values (e.g., npm, maven, gradle, etc.)
  - directory: relative path to the manifest file
  - schedule.interval: Options include "daily", "weekly", "monthly"
  - open-pull-requests-limit: integer specifying concurrent PR limits
  - target-branch: branch for merging updates

**3. Pull Request Integration and Workflow**
- **Automation:** Upon detecting an update, Dependabot creates a pull request automatically.
- **Review:** The PR contains commit differences for package manifest files along with a detailed description of changes.
- **Management:** Use the Security tab to filter alerts and manage auto-triage rules.

**4. Troubleshooting Procedures**
- **Issue: No PR created**
  - Verify if the dependency graph is enabled.
  - Check `dependabot.yml` syntax and correct directory settings.
- **Issue: Incorrect update version**
  - Check configuration parameters.
  - Review logs available in the repository's Advanced Security section.

This extract provides the concrete details needed for direct implementation and troubleshooting of Dependabot version updates.

## Supplementary Details
**Dependabot Configuration Specification:**

- **YAML Version:** Always use `version: 2` as the first line.
- **Update Block Syntax:**
  - `package-ecosystem`: Define the ecosystem (e.g., "npm", "maven", "pip")
  - `directory`: Path from repository root, e.g., "/" or "/subdirectory"
  - `schedule`: Contains a single property `interval` with values ("daily", "weekly", "monthly")
  - `open-pull-requests-limit`: Numeric limit for simultaneous pull requests (default generally 5)
  - `target-branch`: The branch where updates are applied (commonly "main" or "master")

**Step-by-Step Implementation:**
1. Create or edit the file at `/.github/dependabot.yml`.
2. Insert the configuration defining ecosystem, directory, schedule, and pull request limits.
3. Commit the file to enable auto-update functionality.
4. Monitor pull requests created by Dependabot via the repository’s Security tab.
5. In case of issues, check Advanced Security logs for error specifics.

**Troubleshooting Commands and Checks:**
- **Command to validate YAML syntax locally (using yamllint):**
  ```bash
yamllint .github/dependabot.yml
  ```
- **Check for dependency graph activation:**
  - Navigate to Settings > Advanced Security in GitHub UI.
- **Review PR logs:**
  - Use GitHub repository's Security tab or API endpoint for Dependabot alerts.


## Reference Details
**API and SDK Integration Details for Dependabot Alerts:**

1. **GitHub REST API Endpoint**
   - **Endpoint:** GET /repos/{owner}/{repo}/dependabot/alerts
   - **Parameters:**
     - owner (string): Repository owner name
     - repo (string): Repository name
     - per_page (integer, optional): Number of results per page
     - page (integer, optional): Page number for results
   - **Response:** JSON array of alert objects with fields:
     - id (integer)
     - package (object) with details about affected package
     - vulnerable_versions (string)
     - fixed_version (string)
     - severity (string), e.g., "high", "moderate"
     - status (string), e.g., "open", "dismissed"

2. **SDK Method Signature Example using @octokit/rest (JavaScript):**

```javascript
const { Octokit } = require("@octokit/rest");

const octokit = new Octokit({ auth: 'YOUR_GITHUB_TOKEN' });

/**
 * List Dependabot alerts for a given repository
 * @param {string} owner - Repository owner
 * @param {string} repo - Repository name
 * @returns {Promise<Array>} - Array of alert objects
 */
async function listDependabotAlerts(owner, repo) {
  try {
    const response = await octokit.request('GET /repos/{owner}/{repo}/dependabot/alerts', {
      owner,
      repo,
      per_page: 100,
      page: 1
    });
    return response.data;
  } catch (error) {
    console.error('Error fetching Dependabot alerts:', error);
    throw error;
  }
}

// Example usage:
listDependabotAlerts('yourOwner', 'yourRepo').then(alerts => {
  console.log('Dependabot Alerts:', alerts);
});
```

3. **Full SDK Method Example using Error Handling**

```javascript
const getDependabotAlerts = async (owner, repo) => {
  try {
    const { data } = await octokit.request('GET /repos/{owner}/{repo}/dependabot/alerts', {
      owner,
      repo
    });
    // data is an array of alert objects
    data.forEach(alert => {
      console.log(`Alert ID: ${alert.id}, Severity: ${alert.severity}, Package: ${alert.package.name}`);
    });
    return data;
  } catch (err) {
    // Detailed troubleshooting info
    if (err.status === 404) {
      console.error('Repository not found or Dependabot alerts not enabled.');
    } else {
      console.error('Unhandled error:', err);
    }
    throw err;
  }
};

// Call the function
getDependabotAlerts('yourOwner', 'yourRepo');
```

4. **Dependabot YAML Configuration Best Practices**

- Always specify `version: 2` at the top of the file.
- Configure the `schedule` interval to match your update cadence.
- Use `open-pull-requests-limit` to avoid saturating your repository with too many update PRs.
- Set `target-branch` to the primary branch used in your repository.
- Regularly review GitHub Advanced Security logs and Dependabot alerts for any discrepancies.

5. **Troubleshooting Commands and Procedures**

- **Validate YAML:**
  ```bash
  yamllint .github/dependabot.yml
  ```
- **Review Alerts via API:**
  ```bash
  curl -H "Authorization: token YOUR_GITHUB_TOKEN" \
      https://api.github.com/repos/yourOwner/yourRepo/dependabot/alerts
  ```
- **Check Logs:** Navigate to the GitHub repository’s Advanced Security > Dependabot section.

This reference section provides the complete actionable API specifications, SDK method signatures with parameters/return types, detailed code examples, configuration options with their expected values, and step-by-step troubleshooting procedures.

## Original Source
Dependabot Documentation
https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically

## Digest of DEPENDABOT_UPDATES

# Dependabot Version Updates

**Retrieved on:** 2023-10-05

## Overview
Dependabot can be used to automatically update dependencies in a repository. This feature includes three core functionalities:
- **Dependabot Alerts**: Detects vulnerable dependencies.
- **Dependabot Security Updates**: Opens pull requests to update vulnerable dependencies.
- **Dependabot Version Updates**: Keeps dependencies up-to-date regardless of vulnerability status.

## Technical Specifications

### Dependabot Version Updates
- **Purpose:** Automatically update package versions to the latest releases.
- **Configuration:** Managed via a `dependabot.yml` file located in the `/.github` directory.
- **File Structure Example:**

```yaml
version: 2
updates:
  - package-ecosystem: "npm"
    directory: "/"       # Location of package manifest files
    schedule:
      interval: "daily"  # Options: daily, weekly, monthly
    open-pull-requests-limit: 5
    target-branch: "main"   # Branch to update
```

### Pull Request Integration
- **Pull Request Creation:** When updates are available, Dependabot creates a pull request with changes in package files (e.g., package-lock.json, yarn.lock).
- **Review Process:** Developers can review commit differences and merge if appropriate.

### Troubleshooting Dependabot Issues
- **Logging:** Check the repository’s Security tab in Advanced Security settings for Dependabot job logs.
- **Common Issues:**
  - No pull requests generated:
    - Verify that the dependency graph is enabled on the repository.
    - Ensure the `dependabot.yml` file exists and is syntactically correct.
  - Incorrect version updates:
    - Check configuration for package ecosystem and directory settings.

## Detailed Implementation

1. **Enabling Dependabot Features**
   - Navigate to the repository main page on GitHub.
   - Go to Settings > Advanced Security.
   - Enable the following:
     - Dependabot Alerts
     - Dependabot Security Updates
     - Dependabot Version Updates
   - If enabling version updates, GitHub automatically generates a default `dependabot.yml` which you can then customize.

2. **Customize `dependabot.yml`**
   - Edit the file to specify parameters like `package-ecosystem`, `directory`, `schedule`, `open-pull-requests-limit`, and `target-branch`.
   - Commit the changes to apply the configuration.

3. **Monitoring and Alerting**
   - Once enabled, use the Security tab to review open alerts and pull requests submitted by Dependabot.
   - Use filtering options to sort and prioritize alerts.

## Attribution & Data Size
- **Attribution:** Content extracted from https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically (Dependabot Documentation).
- **Data Size:** 241442 bytes crawled


## Attribution
- Source: Dependabot Documentation
- URL: https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically
- License: License: GitHub Docs License (CC BY 4.0)
- Crawl Date: 2025-04-17T18:08:52.676Z
- Data Size: 241442 bytes
- Links Found: 15531

## Retrieved
2025-04-17
library/ECMASCRIPT_MODULES.md
==== Content of library/ECMASCRIPT_MODULES.md ====
# ECMASCRIPT_MODULES

## Crawl Summary
The technical details detail the Node.js ECMAScript modules implementation. Major topics include enabling ES modules via .mjs and package.json type field, module resolution for relative, bare, and absolute specifiers, required file extensions, URL-based resolution schemes, use of import.meta properties (dirname, filename, url, resolve), interoperability with CommonJS (handling of default export and named export detection), support for JSON and Wasm modules, top-level await usage, and the complete resolution algorithm including subroutines like ESM_RESOLVE, ESM_FILE_FORMAT, PACKAGE_RESOLVE and PACKAGE_TARGET_RESOLVE. The document also covers error conditions and custom loader integration.

## Normalised Extract
Table of Contents:
1. Introduction
   - Native import/export syntax examples for ES modules.
2. Enabling
   - Activation via .mjs, package.json "type": "module", --input-type flag.
3. Packages
   - Package file resolution and explicit file extensions.
4. Import Specifiers
   - Types: Relative, Bare, Absolute and their resolution.
5. Mandatory File Extensions & URLs
   - Requirement for file extensions, usage of URL schemes, percent encoding details.
6. File: URLs
   - Behavior when using query parameters and fragments.
7. Data: and Node: Imports
   - Importing data using MIME types and built-in module via node: URLs.
8. Import Attributes
   - Inline syntax and supported attributes (type for JSON).
9. Built-in Modules & import() expressions
   - Default and named exports examples; dynamic import usage.
10. import.meta
    - Properties: dirname, filename, url, resolve(specifier) with usage examples.
11. Interoperability with CommonJS
    - Handling of CommonJS modules, absence of __filename/__dirname, and alternative require.resolve.
12. JSON and Wasm Modules
    - Correct import syntax and experimental flag usage for Wasm.
13. Top-Level Await
    - Syntax usage and process exit condition if unresolved.
14. Loaders and Resolution Algorithms
    - Detailed steps for module resolution including algorithms: ESM_RESOLVE and ESM_FILE_FORMAT, handling errors such as Invalid Module Specifier, Package Not Found, and Unsupported Directory Import.
15. Customizing Resolution
    - Custom loader integration and overriding default behavior.

Each topic includes specific code examples, exact command line flags, and algorithmic steps developers can directly implement in Node.js environment.

## Supplementary Details
Detailed Implementation Steps:
- To enable an ES module, rename file to .mjs or set package.json to { "type": "module" }.
- For CommonJS, use .cjs or set package.json to { "type": "commonjs" }.
- Importing JSON modules must include with { type: 'json' }. Example:
  import config from './config.json' with { type: 'json' };
- Use import.meta.resolve(specifier) to synchronously resolve module URLs. Example:
  const assetURL = import.meta.resolve('./asset.css');

Resolution Algorithm Details:
- ESM_RESOLVE(specifier, parentURL):
  if (isValidURL(specifier)) {
    return { format: undefined, resolved: new URL(specifier).toString() };
  } else if (specifier.startsWith("/")) {
    // Resolve relative to parentURL
  } else if (specifier.startsWith("./") || specifier.startsWith("../")) {
    // Use relative URL resolution
  } else if (specifier.startsWith("#")) {
    // Call PACKAGE_IMPORTS_RESOLVE
  } else {
    // Bare specifier resolution through PACKAGE_RESOLVE
  }

- ESM_FILE_FORMAT(url):
  if (url.endsWith('.mjs')) return 'module';
  if (url.endsWith('.cjs')) return 'commonjs';
  if (url.endsWith('.json')) return 'json';
  if (experimentalWasmModules && url.endsWith('.wasm')) return 'wasm';
  if (experimentalAddonModules && url.endsWith('.node')) return 'addon';
  if (url.endsWith('.js')) {
    // Check package.json "type" field and module syntax detection
  }

Configuration Options:
- --input-type=module or --input-type=commonjs forces module interpretation.
- --experimental-wasm-modules: Enables .wasm module import.
- --experimental-addon-modules: Enables addon module import for .node files.

Troubleshooting Procedures:
- If a module fails to load with an "Invalid Module Specifier" error, ensure the specifier is a valid URL or relative path including the file extension.
- For JSON modules, verify the use of { type: 'json' } in the import statement.
- To debug resolution issues, log the output of import.meta.resolve(specifier) to verify URL resolution.
- Use node --trace-resolutions flag to trace the resolution process during startup for detailed diagnostic information.

Best Practices:
- Always use explicit file extensions in import statements.
- In mixed module projects, clearly separate ES modules (.mjs) from CommonJS (.cjs).
- Employ import.meta properties to derive local paths when __dirname or __filename are not available.
- Validate package.json exports/imports configuration when using bare specifiers.


## Reference Details
Complete API Specifications:

1. import.meta.resolve(specifier, parent?)
   - Parameters:
     * specifier (string): The module specifier to resolve relative to the current module.
     * parent (string|URL) [optional]: An absolute parent module URL to use for resolution. Default is import.meta.url.
   - Returns: string – The absolute URL string of the resolved module.
   - Exceptions: Throws if specifier is invalid or resolution fails.

2. ESM_RESOLVE(specifier, parentURL)
   - An internal algorithm that:
     * Checks if specifier is a valid URL and reserializes it.
     * Resolves relative specifiers using URL resolution.
     * For bare specifiers, delegates to PACKAGE_RESOLVE.
   - Returns: { format: string | undefined, resolved: string }
   - Throws errors: Invalid Module Specifier, Module Not Found, Unsupported Directory Import.

3. ESM_FILE_FORMAT(url)
   - Input: url (string) representing the file URL of the module.
   - Returns:
     * "module" for .mjs
     * "commonjs" for .cjs
     * "json" for .json
     * "wasm" if experimental flag enabled and file ends with .wasm
     * "addon" if experimental flag enabled and file ends with .node
     * For .js files, returns package type based on package.json or detected module syntax.
   - Throws during load phase if format is undefined.

4. Dynamic Import Example:
   // In a CommonJS module, to import an ES module:
   (async () => {
     const module = await import('./module.mjs');
     console.log(module);
   })();

5. CommonJS Interoperability Example:
   // Import a CommonJS module in an ES module
   import cjs from './cjs.cjs';
   console.log(cjs);
   
   // Using module.createRequire() when __filename and __dirname are needed:
   import { createRequire } from 'module';
   const require = createRequire(import.meta.url);
   const path = require('path');

Detailed Troubleshooting:
- Command: node --trace-resolutions app.mjs
  Expected output: Detailed logs of module resolution steps, including resolved file URLs and any errors in specifier parsing.
- If top-level await causes process exit with code 13, check for unresolved promises in the module body.

Implementation Patterns:
- Always validate import specifiers with explicit file extensions.
- Leverage import.meta for resolving relative resources:
  const filePath = new URL('./data.proto', import.meta.url);
  const data = readFileSync(filePath);

Configuration Options Summary:
- --input-type: 'module' or 'commonjs' (default based on file extension or package.json "type")
- --experimental-wasm-modules: boolean (enables .wasm imports)
- --experimental-addon-modules: boolean (enables .node addon imports)

Return Types:
- All resolution functions return strings (absolute URL) or a structured object containing format and resolved.

SDK Method Signatures included above are directly usable in Node.js environment as per v23.11.0 documentation.


## Original Source
Node.js ECMAScript Modules Documentation
https://nodejs.org/api/esm.html

## Digest of ECMASCRIPT_MODULES

# ECMASCRIPT MODULES

**Retrieved:** 2023-10-19

This document contains the detailed technical specifications for Node.js ECMAScript modules as presented in the Node.js v23.11.0 documentation. The content includes implementation details regarding enabling modules, import specifiers, resolution algorithms, module formats, load process, and interoperability with CommonJS.

## Introduction

- ECMAScript modules use native import/export syntax.
- Example export:
  
  // addTwo.mjs
  function addTwo(num) {
    return num + 2;
  }
  
  export { addTwo };

- Example import:
  
  // app.mjs
  import { addTwo } from './addTwo.mjs';
  console.log(addTwo(4));

## Enabling

- ES modules activated by:
  - .mjs file extension
  - package.json with "type": "module"
  - --input-type=module flag

- CommonJS is enforced by:
  - .cjs file extension
  - package.json with "type": "commonjs"
  - --input-type=commonjs flag

## Packages

- Package resolution rules require explicit file extensions for relative and absolute paths.
- Directory index files must be fully specified (e.g. './startup/index.js').

## Import Specifiers

- Specifiers classified as:
  1. **Relative:** './startup.js', '../config.mjs'
  2. **Bare:** 'some-package' or 'some-package/shuffle'
  3. **Absolute:** 'file:///opt/nodejs/config.js'

- Node.js resolves bare specifiers through its module resolution and loading algorithm.

## Mandatory File Extensions and URLs

- File extension required when using import. 
- ES modules are resolved as URLs with percent encoding for special characters.
- Supported schemes: file:, node:, data:

## File: URLs

- Modules loaded multiple times if query/fragment differs.
- Use url.pathToFileURL for resolving paths.

## Data: and Node: Imports

- data: imports support MIME types:
  - text/javascript for ES modules
  - application/json for JSON
  - application/wasm for Wasm

- Node: imports allow builtin modules to be referenced with a URL (e.g. import fs from 'node:fs/promises').

## Import Attributes

- Syntax: 
  import fooData from './foo.json' with { type: 'json' };
- Only the type attribute is supported; required for JSON modules.

## Built-in Modules & import() expressions

- Built-in modules (like 'node:fs' and 'node:events') export both named and default exports.
- Dynamic import: supported in both CommonJS and ES modules.

## import.meta

- Contains properties:
  - import.meta.dirname: directory of current module (file: modules only)
  - import.meta.filename: absolute path with symlinks resolved (file: modules only)
  - import.meta.url: file URL of the module
  - import.meta.resolve(specifier): synchronous resolution of a specifier relative to the current module

## Interoperability with CommonJS

- ES module import can load CommonJS modules:
  - When using CommonJS, module.exports is available as the default export.
  - Named exports may be obtained via static analysis.
  - No __filename, __dirname; use import.meta
  - No Addon Loading, require.resolve, NODE_PATH, require.extensions, require.cache

## JSON and Wasm Modules

- JSON modules must be imported with: 
  import packageConfig from './package.json' with { type: 'json' };
- Wasm modules supported via --experimental-wasm-modules flag, e.g. import * as M from './module.wasm';

## Top-Level Await

- Top-level await is allowed in ES modules. Example:
  export const five = await Promise.resolve(5);

- Unresolved promises cause exit with status 13.

## Loaders and Resolution Algorithms

- The default resolver uses:
  - FileURL-based resolution
  - Relative/absolute URL resolution
  - No default file extensions or folder mains
  - Bare specifier lookup via node_modules
- **ESM_RESOLVE(specifier, parentURL)** returns a tuple { format, resolved } after performing:
  - URL parsing and reserializing if valid
  - Relative resolution if specifier starts with "/", "./", or "../"
  - PACKAGE_RESOLVE for bare specifiers

- **ESM_FILE_FORMAT(url)** algorithm:
  - Returns "module" for .mjs
  - Returns "commonjs" for .cjs
  - Returns "json" for .json
  - Additional experimental formats: "wasm", "addon"
  - If .js extension exists, detection of module syntax is performed.

## Resolution Algorithm Specification

- Outlines detailed steps for:
  - PACKAGE_RESOLVE
  - PACKAGE_SELF_RESOLVE
  - PACKAGE_EXPORTS_RESOLVE
  - PACKAGE_IMPORTS_RESOLVE
  - PACKAGE_TARGET_RESOLVE
  - PATTERN_KEY_COMPARE

- Exceptions:
  - Invalid Module Specifier, Package Configuration, Package Target, Module Not Found, Unsupported Directory Import

## Customizing ESM Specifier Resolution

- Custom loaders can override default resolution behavior.
- Example provided: commonjs-extension-resolution-loader


## Attribution
- Source: Node.js ECMAScript Modules Documentation
- URL: https://nodejs.org/api/esm.html
- License: License: Node.js Foundation License
- Crawl Date: 2025-04-17T17:55:34.151Z
- Data Size: 3503502 bytes
- Links Found: 2152

## Retrieved
2025-04-17
library/OTEL_JS.md
==== Content of library/OTEL_JS.md ====
# OTEL_JS

## Crawl Summary
The crawled content provides detailed technical information about OpenTelemetry JavaScript with precise status levels; it lists supported versions for Node.js and browsers along with TypeScript support policies. It specifies the repositories (opentelemetry-js and opentelemetry-js-contrib) and outlines instrumentation steps, context propagation via the Context API, resource attachment, sampling options, and reference API signatures. Detailed debugging and troubleshooting instructions with exact commands are included.

## Normalised Extract
## Table of Contents
1. Status and Releases
2. Version Support
3. Repositories
4. Instrumentation & Getting Started
5. Context API & Propagation
6. Resources & Sampling
7. API Reference
8. Troubleshooting

### 1. Status and Releases
- Traces: Stable
- Metrics: Stable
- Logs: Development

Detailed release notes and migration instructions (e.g., upgrading to SDK 2.x) are provided.

### 2. Version Support
- Supports active/maintenance LTS Node.js versions.
- Browser support aims for currently supported major versions.
- TypeScript support follows a strict 2-year policy, with older versions dropped in minor releases.

### 3. Repositories
- Core Repository: `opentelemetry-js` (core API and SDK)
- Contributions: `opentelemetry-js-contrib` (additional instrumentation libraries and exporters)

### 4. Instrumentation & Getting Started
- Import modules and instantiate providers. 
- Register provider to auto-instrument modules.
- Code example provided with explicit commands: instantiate provider, get tracer, start and end spans.

### 5. Context API & Propagation
- Provides functions to get and set context across asynchronous boundaries.
- Propagation is implemented to transmit context via HTTP headers/RPC metadata.

### 6. Resources & Sampling
- Resources attach key-value metadata (e.g., service.name) to telemetry.
- Sampling strategies include fixed probability or constant sampling that can be configured with exact threshold values.

### 7. API Reference
- Detailed API signatures for the Tracer, Span, and related interfaces are provided with input parameter types and return types.
- Example methods include startSpan(name: string, options?: SpanOptions, context?: Context): Span

### 8. Troubleshooting
- Specific commands for Node.js debugging (`node --inspect`) and logging configuration (`OTEL_LOG_LEVEL=debug`).
- Guidance on verifying exporter configurations and connection tests via terminal commands.


## Supplementary Details
### Implementation Details
- **SDK Initialization:**
  - Example: 
    ```javascript
    const { NodeTracerProvider } = require('@opentelemetry/node');
    const provider = new NodeTracerProvider({
      // Configuration options
      resource: new Resource({ 'service.name': 'my-service', 'service.version': '1.0.0' }),
      sampler: new TraceIdRatioBased(0.5) // 50% sampling
    });
    provider.register();
    ```
- **Configuration Options:**
  - `resource`: Object specifying metadata; default is empty.
  - `sampler`: Options include AlwaysOnSampler, AlwaysOffSampler, or TraceIdRatioBased with ratio value (0.0 to 1.0).
- **Exporters:**
  - Exporters are configured with endpoint URLs and authentication tokens as needed. For example, configuring an OTLP exporter:
    ```javascript
    const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http');
    const exporter = new OTLPTraceExporter({
      url: 'http://localhost:4318/v1/traces',
      headers: { 'api-key': 'your-api-key' }
    });
    provider.addSpanProcessor(new SimpleSpanProcessor(exporter));
    ```
- **Best Practices:**
  - Always register the provider early in the application startup.
  - Set the logging level to debug during development to capture detailed instrumentation logs by exporting `OTEL_LOG_LEVEL=debug`.
  - Use semantic conventions consistently for resource attributes to ensure observability consistency.


## Reference Details
### Complete API Specifications

#### Tracer API
```typescript
interface Tracer {
  /**
   * Starts a new span with the given name and options
   * @param name - The name of the span
   * @param options - Optional SpanOptions including attributes, startTime, and kind
   * @param context - Optional Context to associate with the span
   * @returns A new Span instance
   */
  startSpan(name: string, options?: SpanOptions, context?: Context): Span;

  /**
   * Returns the current active span if any
   * @returns Span | undefined
   */
  getCurrentSpan(): Span | undefined;
}

interface SpanOptions {
  attributes?: { [key: string]: any };
  startTime?: number;
  kind?: SpanKind;
}

enum SpanKind {
  INTERNAL,
  SERVER,
  CLIENT,
  PRODUCER,
  CONSUMER
}

interface Span {
  /**
   * Ends the span
   * @param endTime - Optional end time in epoch ms
   */
  end(endTime?: number): void;

  /**
   * Sets an attribute on the span
   * @param key - Attribute key
   * @param value - Attribute value
   * @returns The Span for chaining
   */
  setAttribute(key: string, value: unknown): this;

  /**
   * Sets multiple attributes on the span
   * @param attributes - An object with key-value pairs
   * @returns The Span for chaining
   */
  setAttributes(attributes: { [key: string]: unknown }): this;

  /**
   * Adds an event to the span
   * @param name - Event name
   * @param attributes - Optional attributes for the event
   * @returns The Span for chaining
   */
  addEvent(name: string, attributes?: { [key: string]: unknown }): this;
}
```

#### SDK Method Signature Example
```javascript
// Sample initialization of the OpenTelemetry Node SDK
const { NodeTracerProvider } = require('@opentelemetry/node');
const { SimpleSpanProcessor } = require('@opentelemetry/tracing');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http');

// Initialize provider with resource and sampler configuration
const provider = new NodeTracerProvider({
  resource: new Resource({
    'service.name': 'my-service',
    'service.version': '1.0.0'
  }),
  sampler: new TraceIdRatioBased(0.5) // 50% sampling rate
});

// Create and configure the OTLP exporter
const exporter = new OTLPTraceExporter({
  url: 'http://localhost:4318/v1/traces',
  headers: {
    'api-key': 'your-api-key'
  }
});

// Add the exporter to the provider
provider.addSpanProcessor(new SimpleSpanProcessor(exporter));

// Register provider
provider.register();

// Using the tracer in the application
const tracer = provider.getTracer('example-tracer');
const span = tracer.startSpan('operation-name');
// ... perform operations ...
span.end();
```

#### Configuration Options
- Provider Options:
  - resource: An instance of Resource with values such as 'service.name' and 'service.version'.
  - sampler: Instance of AlwaysOnSampler, AlwaysOffSampler, or TraceIdRatioBased with a numeric ratio (e.g., 0.5).
- Exporter Options:
  - url: Endpoint URL for trace data export (e.g., 'http://localhost:4318/v1/traces').
  - headers: Object containing header key-value pairs for authentication.

#### Best Practices & Troubleshooting
- **Best Practices:**
  - Initialize and register the tracer provider at the very beginning of your application's lifecycle.
  - Use semantic conventions when setting resource attributes.
  - Configure sampling to balance observability with performance overhead.

- **Troubleshooting Commands:**
  - Run Node.js with debugging: `node --inspect your-app.js`
  - Enable verbose logging by exporting: `export OTEL_LOG_LEVEL=debug`
  - Use diagnostic tools to verify that spans are created and ended correctly. Example output should indicate span start and end timestamps along with attributes and events.


## Original Source
OpenTelemetry for Node.js
https://opentelemetry.io/docs/instrumentation/js/

## Digest of OTEL_JS

# OpenTelemetry JavaScript Documentation

**Retrieved:** 2023-10-05

## Overview
This document contains the complete technical content extracted directly from the OpenTelemetry JavaScript documentation. It covers the status of components, version support, repositories, instrumentation details, context APIs, propagation mechanisms, resource configuration, sampling strategies, and API references with concrete examples.

## Status and Releases
- **Traces:** Stable
- **Metrics:** Stable
- **Logs:** Development

Latest release information is available on the Releases page. Note: Client instrumentation for browsers is still experimental.

## Version Support
- Supports all active or maintenance LTS versions of Node.js. Previous versions may work but are not officially tested.
- No official supported list of browsers; intended for currently supported versions of major browsers.
- Follows DefinitelyTyped support policy for TypeScript with a 2-year support window.

## Repositories
- **Core:** `opentelemetry-js` — Contains core distribution API and SDK.
- **Contrib:** `opentelemetry-js-contrib` — Contains additional instrumentation and exporter contributions.

## Instrumentation and Usage
### Getting Started
- Instrument your application in Node.js or the browser.
- Use instrumentation libraries to auto-instrument dependencies.
- Configure exporters to process and send telemetry data.

### Example Code:
```javascript
// Import the Node Tracer Provider from OpenTelemetry
const { NodeTracerProvider } = require('@opentelemetry/node');

// Instantiate the tracer provider
const provider = new NodeTracerProvider();

// Register the provider to begin tracing
provider.register();

// Example of starting a span
const tracer = provider.getTracer('example-tracer');
const span = tracer.startSpan('operation-name');

// Do work here...

// End the span when work is complete
span.end();
```

## Context API & Propagation
- Provides a Context API to carry trace context across asynchronous boundaries.
- Supports propagation mechanisms to ensure context is passed through HTTP headers or RPC metadata.

## Resources and Sampling
- **Resources:** Attach environment and service-specific metadata to telemetry data.
- **Sampling:** Reduce telemetry load with configurable strategies. Options typically include constant sampling, probabilistic sampling, etc.

## API Reference
Developers can refer to the OpenTelemetry JavaScript API reference for all classes and methods. For example, key API methods include:

**Tracer API Signature:**
```typescript
interface Tracer {
  startSpan(name: string, options?: SpanOptions, context?: Context): Span;
  getCurrentSpan(): Span | undefined;
}

interface SpanOptions {
  attributes?: { [key: string]: any };
  startTime?: number;
  kind?: SpanKind;
}

interface Span {
  end(endTime?: number): void;
  setAttribute(key: string, value: unknown): this;
  setAttributes(attributes: { [key: string]: unknown }): this;
  addEvent(name: string, attributes?: { [key: string]: unknown }): this;
}
```

## Troubleshooting
- **Debugging Node.js Instrumentation:**
  - Run with inspector: `node --inspect index.js`
  - Enable diagnostic logging by setting relevant environment variables (e.g., `OTEL_LOG_LEVEL=debug`).
- **Verifying Exporter Configuration:**
  - Check connectivity and configuration errors in the exporter settings.

---
*Data Size:* 626992 bytes
*Links Found:* 50769


## Attribution
- Source: OpenTelemetry for Node.js
- URL: https://opentelemetry.io/docs/instrumentation/js/
- License: License: Apache License 2.0
- Crawl Date: 2025-04-17T15:39:51.601Z
- Data Size: 626992 bytes
- Links Found: 50769

## Retrieved
2025-04-17
library/WINSTON.md
==== Content of library/WINSTON.md ====
# WINSTON

## Crawl Summary
The crawled content provides technical details on configuring and using the winston logging library. It includes API methods like winston.createLogger with options: level, format, defaultMeta, transports, exitOnError, and silent. Detailed usage examples show creation of loggers, adding console and file transports, custom formatting with winston.format.combine, and exception/rejection handling via exceptionHandlers and rejections.handle. Additionally, custom transport creation by inheriting from winston-transport is included, along with advanced methods for dynamic reconfiguration and profiling.

## Normalised Extract
## Table of Contents
1. Motivation
2. Quick Start
3. Usage
4. Logger Configuration Options
   - level (default: 'info')
   - levels (default: winston.config.npm.levels)
   - format (default: winston.format.json())
   - transports (array of logging targets)
   - exitOnError (default: true)
   - silent (default: false)
5. Logging Levels
   - npm levels: { error: 0, warn: 1, info: 2, http: 3, verbose: 4, debug: 5, silly: 6 }
6. Formats & Custom Formats
   - Built-in formats: json, logstash, printf, simple, colorize, splat
   - Example: custom printf format
7. Transports
   - File, Console, Http, and custom transports
   - Example: multiple File transports with different levels
8. Exception & Rejection Handling
   - Using exceptionHandlers and rejections.handle
   - Configuring handleExceptions on transports
9. Profiling & Querying
   - Starting profiles with logger.profile() and logger.startTimer()
   - Querying log entries with logger.query(options, callback)
10. Advanced Logger Methods
    - clear(), add(), remove(), configure()

## Detailed Information
- Create a logger using winston.createLogger with signature:
  createLogger(options: {
    level?: string,                // default 'info'
    levels?: object,               // defaults to winston.config.npm.levels
    format?: Format,
    transports?: Transport[],
    exitOnError?: boolean | (err: Error) => boolean,
    silent?: boolean
  }): Logger;

- Logging calls: logger.log({ level, message, ...meta }) or directly as logger.info(message, meta).

- Transports are instantiated with their own options, e.g., File transport:
  new winston.transports.File({ filename: 'combined.log', level: 'info', format: winston.format.json() });

- Custom formats are created using winston.format(fn) where fn returns modified info or false to filter out logs.

- Exception handling can be specified on logger creation via exceptionHandlers array or later using logger.exceptions.handle(transport).

- Rejection handling is similarly set using logger.rejections.handle(transport) or handleRejections flag on transport.

- Advanced methods include dynamic reconfiguration via logger.configure(newOptions) and removal/addition of transports via logger.clear(), logger.add(), and logger.remove(transport).


## Supplementary Details
### Logger Configuration Details
- level: string, default 'info'. Sets the minimum log level.
- levels: object, default winston.config.npm.levels, e.g., { error: 0, warn: 1, info: 2, http: 3, verbose: 4, debug: 5, silly: 6 }.
- format: must be constructed using winston.format methods. Choices include json(), simple(), prettyPrint(), colorize(), splat(), timestamp(), and custom formats via printf().
- transports: An array of transport instances (File, Console, Http). Each transport can be individually configured with options such as filename (for File), level, and format.
- exitOnError: boolean or function, default true. Use false or a filtering function to prevent process exit on uncaught exceptions.
- silent: boolean, default false. When true, logging is suppressed.

### Implementation Steps for a Custom Logger
1. Import winston: `const winston = require('winston');`
2. Create logger with desired options:
   ```javascript
   const logger = winston.createLogger({
     level: 'info',
     format: winston.format.json(),
     defaultMeta: { service: 'user-service' },
     transports: [
       new winston.transports.File({ filename: 'error.log', level: 'error' }),
       new winston.transports.File({ filename: 'combined.log' })
     ],
     exitOnError: false
   });
   ```
3. Conditionally add Console transport for non-production environments.
4. Use logger.log() or logger.<level>() for logging messages.
5. For exception handling, add exceptionHandlers:
   ```javascript
   logger.exceptions.handle(new winston.transports.File({ filename: 'exceptions.log' }));
   ```

### Troubleshooting Procedures
- If logs are not appearing, verify that transports are added to the logger. For the default logger, manually add transports:
  ```javascript
  winston.add(new winston.transports.Console());
  ```
- If memory usage increases, check that the default logger isn’t used without transports.
- For issues with logging levels, ensure that the level specified in both logger and transport configuration is correct.
- Use events on logger such as 'error' and 'finish' to monitor internal logger errors.
  ```javascript
  logger.on('error', (err) => { console.error('Logger error:', err); });
  logger.on('finish', () => { console.log('All logs flushed'); });
  ```


## Reference Details
### Complete API Specifications and Code Examples

#### 1. winston.createLogger(options): Logger
- Parameters:
  - options.level: string (default 'info')
  - options.levels: object (log level mapping, default winston.config.npm.levels)
  - options.format: Format object from winston.format (default winston.format.json())
  - options.transports: Array of Transport instances
  - options.defaultMeta: object (metadata to include with every log)
  - options.exitOnError: boolean | (err: Error) => boolean (default true)
  - options.silent: boolean (default false)
- Returns: A Logger instance with helper methods: log(), error(), warn(), info(), http(), verbose(), debug(), and silly().

Example:
```javascript
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  levels: winston.config.npm.levels,
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  defaultMeta: { service: 'user-service' },
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' })
  ],
  exitOnError: false
});

if (process.env.NODE_ENV !== 'production') {
  logger.add(new winston.transports.Console({
    format: winston.format.combine(
      winston.format.colorize(),
      winston.format.simple()
    )
  }));
}

logger.info('Logger initialized successfully');
```

#### 2. Custom Transport Specification
- Extend winston-transport to create custom transports.

Example:
```javascript
const Transport = require('winston-transport');

class YourCustomTransport extends Transport {
  constructor(opts) {
    super(opts);
    // Custom initialization: e.g., connection settings
    this.customOption = opts.customOption || 'default';
  }

  log(info, callback) {
    setImmediate(() => this.emit('logged', info));
    // Write log message to external service
    // For example: send HTTP request using this.customOption
    callback();
  }
}

module.exports = YourCustomTransport;
```

#### 3. Exception and Rejection Handling
- Exception Handling:
```javascript
const logger = winston.createLogger({
  transports: [new winston.transports.File({ filename: 'combined.log' })],
  exceptionHandlers: [new winston.transports.File({ filename: 'exceptions.log' })]
});

// Alternative using handleExceptions flag
winston.add(new winston.transports.File({
  filename: 'combined.log',
  handleExceptions: true
}));
```
- Rejection Handling:
```javascript
logger.rejections.handle(new winston.transports.File({ filename: 'rejections.log' }));

// Or using handleRejections flag
winston.add(new winston.transports.File({
  filename: 'combined.log',
  handleRejections: true
}));
```

#### 4. Dynamic Reconfiguration
- Use logger.configure(options) to completely reconfigure transports and levels.

Example:
```javascript
const DailyRotateFile = require('winston-daily-rotate-file');
logger.configure({
  level: 'verbose',
  transports: [new DailyRotateFile({
    datePattern: 'YYYY-MM-DD',
    filename: 'app-%DATE%.log'
  })]
});
```

#### 5. Profiling
- Start a profile using logger.profile(label) and stop by calling the same method:
```javascript
logger.profile('test');
setTimeout(() => {
  logger.profile('test');
}, 1000);
```

- Alternatively, use startTimer()/done() pattern:
```javascript
const profiler = logger.startTimer();
setTimeout(() => {
  profiler.done({ message: 'Task completed' });
}, 1000);
```

#### 6. Querying Logs
- The logger.query method:
```javascript
const options = {
  from: new Date() - (24*60*60*1000), // last 24 hours
  until: new Date(),
  limit: 10,
  start: 0,
  order: 'desc',
  fields: ['message', 'timestamp']
};

logger.query(options, (err, results) => {
  if (err) {
    console.error('Query error:', err);
  } else {
    console.log('Query results:', results);
  }
});
```

### Best Practices
- Always add at least one transport to avoid high memory usage in the default logger.
- For production, log to files or remote services instead of console.
- Use custom formats to standardize log output.
- Handle exceptions and promise rejections explicitly to prevent unexpected process termination.

### Troubleshooting Commands
- To test logging at different levels:
```bash
node -e "require('./app').logger.info('Test info'); require('./app').logger.error('Test error');"
```
- Monitor logs continuously:
```bash
tail -f combined.log
```
- Verify exitOnError behavior by simulating an error:
```javascript
logger.exitOnError = false;
throw new Error('Test error');
```
Expected output: Log the error without process termination.


## Original Source
Winston Logging Documentation
https://github.com/winstonjs/winston

## Digest of WINSTON

# WINSTON

Retrieved on: 2023-10-XX

## Motivation
Winston is a universal logging library for Node.js that supports multiple transports, each configured with its logging levels and formats. It provides a high degree of flexibility by decoupling log formatting, level filtering, and actual transport mechanisms.

## Quick Start
Refer to examples in the `./examples/` folder.

## Usage
The recommended way is to create your own logger:

```javascript
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.json(),
  defaultMeta: { service: 'user-service' },
  transports: [
    // Log error level or higher to error.log
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    // Log info level or higher to combined.log
    new winston.transports.File({ filename: 'combined.log' })
  ],
});

if (process.env.NODE_ENV !== 'production') {
  logger.add(new winston.transports.Console({
    format: winston.format.simple(),
  }));
}
```

## Table of Contents
1. Motivation
2. Quick Start
3. Usage
4. Logger Configuration Options
5. Logging Levels
6. Formats and Custom Formats
7. Transports (Built-in and Custom)
8. Exception and Rejection Handling
9. Profiling and Querying
10. Advanced Logger Methods

## Logger Configuration Options
- **level** (default: 'info'): Filters logs by level.
- **levels** (default: winston.config.npm.levels): Defines log priorities.
- **format** (default: winston.format.json()): Defines the message formatting.
- **transports** (default: []): Array of transport objects (e.g., File, Console).
- **exitOnError** (default: true): Determines whether exceptions cause exit.
- **silent** (default: false): If true, no logs are produced.

## Logging Levels
RFC5424 and npm levels are supported. Example for npm:

```javascript
const levels = {
  error: 0,
  warn: 1,
  info: 2,
  http: 3,
  verbose: 4,
  debug: 5,
  silly: 6
};
```

## Formats and Custom Formats
Formats are composed using `winston.format.combine`. Basic examples:

```javascript
const { createLogger, format, transports } = require('winston');
const { combine, timestamp, label, printf } = format;

const myFormat = printf(({ level, message, label, timestamp }) => {
  return `${timestamp} [${label}] ${level}: ${message}`;
});

const logger = createLogger({
  format: combine(
    label({ label: 'right meow!' }),
    timestamp(),
    myFormat
  ),
  transports: [new transports.Console()]
});
```

Custom format example with modifications:

```javascript
const volume = format((info, opts) => {
  if (opts.yell) {
    info.message = info.message.toUpperCase();
  } else if (opts.whisper) {
    info.message = info.message.toLowerCase();
  }
  return info;
});

const scream = volume({ yell: true });
console.dir(scream.transform({ level: 'info', message: 'please quiet down' }, scream.options));
```

## Transports
### Built-in Transports
- **File**: Write logs to a file. Example:

```javascript
new winston.transports.File({ filename: 'combined.log', level: 'info' });
```

- **Console**: Write logs to stdout/stderr. Optionally force `console.log` usage:

```javascript
new winston.transports.Console({ forceConsole: true, format: winston.format.simple() });
```

### Custom Transports
Custom transports extend `winston-transport` and implement a `log(info, callback)` method:

```javascript
const Transport = require('winston-transport');

class YourCustomTransport extends Transport {
  constructor(opts) {
    super(opts);
    // Process custom options such as connection details
  }
  log(info, callback) {
    setImmediate(() => this.emit('logged', info));
    // Write info to remote service
    callback();
  }
}

module.exports = YourCustomTransport;
```

## Exception and Rejection Handling
Enable exception handling by specifying `exceptionHandlers` or using `handleExceptions` on transports:

```javascript
const logger = winston.createLogger({
  transports: [new winston.transports.File({ filename: 'combined.log' })],
  exceptionHandlers: [new winston.transports.File({ filename: 'exceptions.log' })]
});

// For promise rejections
logger.rejections.handle(new winston.transports.File({ filename: 'rejections.log' }));
```

Set `exitOnError` to false to prevent process exit:

```javascript
const logger = winston.createLogger({ exitOnError: false });
// or
logger.exitOnError = (err) => err.code !== 'EPIPE';
```

## Profiling and Querying
### Profiling
Profile logging can be started and stopped:

```javascript
logger.profile('test');
setTimeout(() => {
  logger.profile('test');
}, 1000);

const profiler = logger.startTimer();
setTimeout(() => {
  profiler.done({ message: 'completed profiling' });
}, 1000);
```

### Querying
Logs can be queried with options:

```javascript
const options = {
  from: new Date() - (24 * 60 * 60 * 1000),
  until: new Date(),
  limit: 10,
  start: 0,
  order: 'desc',
  fields: ['message']
};

logger.query(options, (err, results) => {
  if (err) throw err;
  console.log(results);
});
```

## Advanced Logger Methods
- **clear()**: Remove all transports.
- **add(transport)**: Add a transport.
- **remove(transport)**: Remove a transport.
- **configure(options)**: Reconfigure the logger entirely.

Example of reconfiguration:

```javascript
const DailyRotateFile = require('winston-daily-rotate-file');
logger.configure({
  level: 'verbose',
  transports: [new DailyRotateFile({ datePattern: 'YYYY-MM-DD', filename: 'app-%DATE%.log' })]
});
```

Attribution: Data crawled from https://github.com/winstonjs/winston, Data Size: 706452 bytes, Links: 5006.

## Attribution
- Source: Winston Logging Documentation
- URL: https://github.com/winstonjs/winston
- License: License: MIT License
- Crawl Date: 2025-04-17T15:02:05.128Z
- Data Size: 706452 bytes
- Links Found: 5006

## Retrieved
2025-04-17
library/GITHUB_CODESPACES.md
==== Content of library/GITHUB_CODESPACES.md ====
# GITHUB_CODESPACES

## Crawl Summary
The GitHub Codespaces documentation provides a detailed guide for setting up, running, and managing codespaces. It includes exact commands such as 'npm run dev', UI steps like clicking 'Open in Browser', and detailed lifecycle processes including container rebuild, timeout settings (default 30 minutes), stopping via VS Code command palette or CLI, and deletion practices. Virtual machine configurations are specified from 2 cores/8GB RAM/32GB storage up to 32 cores/64GB RAM/128GB storage. The documentation also covers advanced configurations within devcontainer.json and troubleshooting steps for issues like port forwarding and connection loss.

## Normalised Extract
## Table of Contents
1. Creating and Configuring a Codespace
2. Running and Debugging Applications
3. Git Operations within Codespaces
4. Dev Container Customization
5. Lifecycle Management and Troubleshooting

### 1. Creating and Configuring a Codespace
- **Repository Template:** Navigate to the repository (e.g., github/haikus-for-codespaces).
- **Steps:** Click 'Use this template' -> 'Open in a codespace'.
- **Result:** The repository is cloned into `/workspaces` and a VM is provisioned.

### 2. Running and Debugging Applications
- **Command Execution:** In the terminal run `npm run dev` to start the Node.js application.
- **Port Forwarding:** Automatic detection and forwarding of the port when the application starts, with a pop-up message for 'Open in Browser'.

### 3. Git Operations within Codespaces
- **Staging Files:** Click the '+' icon in the Source Control view.
- **Committing:** Enter a commit message then press 'Commit'.
- **Publishing:** Choose repository type and click 'Publish Branch'.

### 4. Dev Container Customization
- **Custom Configurations:** Use `devcontainer.json` to define the container environment.
- **Sample Config:**
```json
{
  "name": "My Dev Container",
  "image": "mcr.microsoft.com/vscode/devcontainers/javascript-node:0-14",
  "settings": { "terminal.integrated.shell.linux": "/bin/bash" },
  "postCreateCommand": "npm install"
}
```

### 5. Lifecycle Management and Troubleshooting
- **Lifecycle Stages:** Creation, container setup, connection, post-creation setup.
- **Stopping Codespaces:** Use VS Code Command Palette (`Codespaces: stop`) or CLI (`gh codespace stop`).
- **Timeout Settings:** Default is 30 minutes; auto-save is enabled in web clients.
- **Rebuilding:** Trigger container rebuild to apply new devcontainer.json changes, preserving files in `/workspaces`.

Each section includes precise steps, exact commands, configuration parameters, and troubleshooting instructions that developers can directly follow.

## Supplementary Details
### Dev Container and Machine Configuration
- **VM Specifications:** Configurable from 2 cores/8GB RAM/32GB storage to 32 cores/64GB RAM/128GB storage. 
- **Default OS:** Ubuntu-based Linux image, overrideable via custom images.
- **Configuration File (devcontainer.json):** Supports settings, postCreateCommand, postAttachCommand. Parameters include:
  - "name": string, e.g., "My Dev Container"
  - "image": Docker image string, e.g., "mcr.microsoft.com/vscode/devcontainers/javascript-node:0-14"
  - "settings": JSON object for VS Code settings, e.g., terminal settings.
  - "postCreateCommand": command string executed after container creation (e.g., "npm install")

### Commands and UI Actions
- **Application Startup:** Execute `npm run dev` in terminal.
- **Port Forwarding:** Automatically forwards detected ports; manually accessible via the Ports tab in VS Code.
- **Stopping Codespaces:** VS Code Command Palette command `Codespaces: stop` or CLI command `gh codespace stop`.
- **Git Operations:** Stage, commit, and publish changes using integrated UI controls or terminal Git commands (e.g., `git add .`, `git commit -m 'message'`, `git push`).

### Troubleshooting Procedures
- **Port Forwarding Issues:** Confirm port number from terminal output; if missing, use the Ports tab to manually forward the port.
- **Connection Interruptions:** Check network connectivity; if disconnected, use `gh codespace list` to view and reconnect.
- **Auto Save and Data Persistence:** In browser-based editor auto-save is enabled; in VS Code desktop, enable auto-save to avoid unsaved changes.
- **Rebuild Tips:** When rebuilding a codespace, remember only changes in `/workspaces` persist; reapply configurations as needed via devcontainer.json.

### Best Practices
- Frequently commit and push to backup work.
- Customize your devcontainer for reproducible environments.
- Use Settings Sync in VS Code to mirror your development preferences across devices.

## Reference Details
### Full API and Command Specifications

#### GitHub Codespaces CLI Commands
- **List Codespaces:**
  - Command: `gh codespace list`
  - Output: List of active and stopped codespaces with status, machine type, and creation date.

- **Stop a Codespace:**
  - Command: `gh codespace stop [--codespace <name|id>]`
  - Parameters:
    - `--codespace`: Identifier string of the codespace to stop.
  - Return: Confirmation message and updated status.

#### Visual Studio Code Commands
- **Stop Codespace:**
  - Command Palette: `Codespaces: stop`
  - Execution: Stops the currently connected codespace.

#### Devcontainer.json Specification
- **Required Fields:**
  - "name": string (e.g., "My Dev Container")
  - "image": string specifying the Docker image (e.g., "mcr.microsoft.com/vscode/devcontainers/javascript-node:0-14")
- **Optional Fields:**
  - "settings": Object containing VS Code settings, e.g., { "terminal.integrated.shell.linux": "/bin/bash" }
  - "postCreateCommand": string, command to run after creation (e.g., "npm install")
  - "postAttachCommand": string, command to run when attaching

#### Code Example: Running a Node.js App in Codespaces

```bash
# In the integrated terminal within the codespace:
npm run dev
```

#### Implementation Pattern for Codespace Setup
1. Clone repository into `/workspaces` automatically during codespace creation.
2. Execute setup commands defined in `devcontainer.json` (e.g., install dependencies).
3. Use integrated terminal for commands, and employ UI controls for staging and committing changes.
4. Forward port: Verify terminal output for auto-detected port, then click on the provided URL or manually forward via the Ports tab.

#### Troubleshooting Steps
- **Port Not Forwarding:** 
  1. Check terminal output for port number.
  2. Open Ports tab and click the 'Open in Browser' icon manually.
  3. Verify that the application is listening on the expected port (e.g., using `lsof -i :4000`).

- **Codespace Connection Loss:**
  1. Confirm network settings.
  2. Use `gh codespace list` to check current status.
  3. Reconnect via web interface or VS Code using the command palette.

- **Rebuild Issues:**
  1. Ensure modifications are inside `/workspaces` to persist.
  2. Run a full rebuild if cache issues are suspected: Clear cache and rebuild container.

These detailed API commands, method signatures, configuration parameters, and step-by-step procedures provide developers with the complete technical specifications needed to implement and troubleshoot GitHub Codespaces directly.

## Original Source
GitHub Codespaces Documentation
https://docs.github.com/en/codespaces

## Digest of GITHUB_CODESPACES

# GitHub Codespaces Documentation

**Date Retrieved:** 2023-10-05

## Overview
GitHub Codespaces provides a cloud-hosted development environment with a preconfigured Docker container running on a virtual machine. The environment is highly configurable, enabling you to work in a secure, dedicated development container that supports Node.js, Python, Java, C# (.NET), and more.

## Quickstart and Setup

### Creating a Codespace
- Navigate to the repository (e.g., github/haikus-for-codespaces).
- Click **Use this template** then **Open in a codespace**.

### Running the Application
- When the codespace is created, the repository is cloned automatically into the `/workspaces` directory.
- Open the terminal and run:

```bash
npm run dev
```

- A pop-up message will indicate the port used by the application; click **Open in Browser** to view it.

### Editing and Personalizing
- Open files (e.g., `haikus.json`) in the Explorer to edit content live.
- Refresh the running application tab to see updates.

### Git Operations
- Stage changes by clicking the plus icon next to changed files in the Source Control view.
- Commit changes with a message and click **Commit**.
- Publish the branch by selecting the appropriate repository type (public/private) and clicking **Publish Branch**.

### Adding Extensions
- In the Activity Bar, click **Extensions**.
- Search and install extensions such as **fairyfloss** to personalize your theme.

## Codespace Lifecycle and Management

### Lifecycle Steps
1. **Creation**: A dedicated VM is provisioned and a shallow clone of the repository is mounted into the dev container.
2. **Dev Container Setup**: Container is created based on the `devcontainer.json` and optional Dockerfile. If missing, a default Ubuntu-based image is used.
3. **Connection**: Access the codespace via web browser, VS Code, or GitHub CLI.
4. **Post-Creation Setup**: Commands defined in `postCreateCommand` or `postAttachCommand` execute. Dotfiles can also configure the environment.

### Lifecycle Operations
- **Rebuilding**: Use a rebuild to update container settings. A full rebuild clears the cache. **Note**: Only changes outside `/workspaces` are cleared.
- **Stopping**: Codespaces can be stopped through the UI, Command Palette (`Codespaces: stop`), or CLI (`gh codespace stop`). Stopped codespaces incur storage costs only.
- **Deleting**: Codespaces can be safely deleted once changes are pushed to a remote branch. Automatic deletion occurs after 30 days of inactivity.

### Inactivity and Timeout
- By default, codespaces timeout after 30 minutes of inactivity. Auto-save is enabled in the web version, ensuring unsaved changes are minimized.

## Advanced Configuration

### Dev Container Customization
- Modify the `devcontainer.json` file to set up language runtimes, tools, and post creation scripts.
- Example snippet:

```json
{
  "name": "My Dev Container",
  "image": "mcr.microsoft.com/vscode/devcontainers/javascript-node:0-14",
  "settings": {
    "terminal.integrated.shell.linux": "/bin/bash"
  },
  "postCreateCommand": "npm install"
}
```

### Virtual Machine Specifications
- Available configurations range from 2 cores/8GB RAM/32GB storage to 32 cores/64GB RAM/128GB storage.
- Default image is Ubuntu-based but can be customized to any Linux distribution.

## Troubleshooting and Best Practices

### Common Issues and Commands
- **Port Forwarding**: When the application listens on a port (e.g., 4000), Codespaces auto-detect and forward it. If not, manually forward via the Ports tab.
- **Connection Issues**: Use `gh codespace list` and `gh codespace stop` to manage session drift.
- **Git Operations**: Ensure changes are committed and pushed frequently to prevent data loss during timeouts.

### Best Practices
- Regularly commit changes to remote repositories.
- Use Settings Sync in VS Code to synchronize extensions, settings, and keybindings.
- Customize the dev container to include all necessary dependencies for a consistent development experience across users.

## Attribution
- Crawled Data Size: 775065 bytes
- Links Found: 8517


## Attribution
- Source: GitHub Codespaces Documentation
- URL: https://docs.github.com/en/codespaces
- License: License: Custom GitHub Docs License
- Crawl Date: 2025-04-17T15:28:56.765Z
- Data Size: 775065 bytes
- Links Found: 8517

## Retrieved
2025-04-17
library/SERVERLESS_FRAMEWORK.md
==== Content of library/SERVERLESS_FRAMEWORK.md ====
# SERVERLESS_FRAMEWORK

## Crawl Summary
Extracted technical details include concrete plugin functionalities, exact CLI commands, and clear YAML configuration examples. Every plugin is documented with its purpose and available options, such as Serverless Offline (emulating AWS Lambda locally with options like --httpPort), Serverless Prune Plugin (auto-deletion of old deployments with configurable number), and others like Webpack, Domain Manager, Esbuild, HTTP, Dotenv, Step Functions, Datadog, Typescript, IAM Roles, and Python Requirements. Also included are full examples of serverless.yml setups and clear command line invocations.

## Normalised Extract
## Table of Contents
1. Plugins & Tools
   - Serverless Offline
   - Serverless Prune Plugin
   - Serverless Webpack
   - Serverless Domain Manager
   - Serverless Esbuild
   - Serverless HTTP Plugin
   - Serverless Dotenv Plugin
   - Serverless Step Functions Plugin
   - Serverless Plugin Datadog
   - Serverless Plugin Typescript
   - Serverless IAM Roles Per Function Plugin
   - Serverless Python Requirements
2. Example Implementations
   - Node.js HTTP Endpoint Example
   - Python Flask API Example
3. CLI Commands & Configuration

### 1. Plugins & Tools

**Serverless Offline**:
- Command: `serverless offline start`
- Options: `--httpPort` (default: 3000), `--noPrependStageInUrl`

**Serverless Prune Plugin**:
- YAML Configuration:
  ```yaml
  custom:
    prune:
      automatic: true
      number: 3
  ```

**Serverless Webpack**: Bundles lambda code using Webpack; integrates prior to deployment.

**Serverless Domain Manager**: Manages API Gateway custom domains with configurations for domain name, base path, stage, etc.

**Serverless Esbuild**: Provides fast bundling for JavaScript/TypeScript projects.

**Serverless HTTP Plugin**: Wraps existing middleware frameworks for use in Lambda.

**Serverless Dotenv Plugin**: Loads variables from `.env` with `dotenv: './.env'` setting.

**Serverless Step Functions Plugin**: Integrates AWS Step Functions into the service configuration.

**Serverless Plugin Datadog**: Monitors and traces Lambda functions with real-time metrics.

**Serverless Plugin Typescript**: Adds Typescript support without extra configuration.

**Serverless IAM Roles Per Function Plugin**: Allows individual IAM role assignment per function block in serverless.yml.

**Serverless Python Requirements**: Bundles and caches Python packages listed in requirements.txt.

### 2. Example Implementations

#### Node.js HTTP Endpoint Example
```yaml
service: my-service
provider:
  name: aws
  runtime: nodejs14.x
plugins:
  - serverless-offline
  - serverless-webpack
functions:
  hello:
    handler: handler.hello
```

#### Python Flask API Example
```yaml
service: my-python-service
provider:
  name: aws
  runtime: python3.8
plugins:
  - serverless-offline
functions:
  app:
    handler: wsgi_handler
```

### 3. CLI Commands & Configuration

- **Deploy:** `sls deploy`
- **Invoke Function:** `sls invoke -f hello`
- **Start Offline:** `sls offline start`

Additional settings can be defined in the `custom` section of serverless.yml for plugin-specific configurations.

## Supplementary Details
### Sample serverless.yml Configuration
```yaml
service: my-service
provider:
  name: aws
  runtime: nodejs14.x
  stage: dev
  region: us-east-1
custom:
  prune:
    automatic: true
    number: 3
  dotenv: './.env'
  customDomain:
    domainName: "api.example.com"
    basePath: ""
    stage: "${self:provider.stage}"
    createRoute53Record: true
plugins:
  - serverless-offline
  - serverless-prune-plugin
  - serverless-webpack
  - serverless-domain-manager
  - serverless-esbuild
  - serverless-http
  - serverless-dotenv-plugin
  - serverless-step-functions
  - serverless-plugin-datadog
  - serverless-plugin-typescript
  - serverless-iam-roles-per-function
  - serverless-python-requirements
functions:
  hello:
    handler: handler.hello
    events:
      - http:
          path: hello
          method: get
```

### Implementation Steps
1. Install dependencies (e.g., `npm install serverless serverless-offline serverless-webpack ...`).
2. Configure the serverless.yml file with provider details, plugins, and function definitions.
3. Write handler functions, for instance:
   ```javascript
   // handler.js
   'use strict';
   module.exports.hello = async (event, context) => {
     try {
       return {
         statusCode: 200,
         body: JSON.stringify({ message: 'Hello World' })
       };
     } catch (error) {
       return {
         statusCode: 500,
         body: JSON.stringify({ error: error.message })
       };
     }
   };
   ```
4. Deploy using `sls deploy` and test with `sls invoke -f hello` or offline using `sls offline start`.
5. Monitor logs via CloudWatch or local verbose logging if needed.

### Plugin Specifics & Parameters
- **Serverless Offline:** `--httpPort` (default=3000)
- **Prune Plugin:** `number` parameter defaults to 3.
- **Domain Manager:** Requires domainName, basePath, stage, and optionally certificateName.
- **Dotenv Plugin:** Loads from specified path in configuration.

### Best Practices
- Secure environment variables via the dotenv plugin.
- Maintain clear version control by pruning outdated Lambda versions.
- Align IAM roles per function to minimize over-permission issues.

### Troubleshooting
- Start offline with verbose logging: `sls offline start --verbose`
- Use AWS CLI to tail CloudWatch logs:
  ```bash
  aws logs tail /aws/lambda/<function-name> --follow
  ```
- Check plugin order in serverless.yml if bundling issues occur.
- Use debugging flags with Webpack if bundle issues arise, e.g., `sls webpack --debug`.

## Reference Details
## API Specifications and SDK Method Signatures

### Serverless Framework CLI

#### Deploy Command
- **Usage:** `sls deploy --stage <stage> --region <region> [--verbose]`
- **Parameters:**
  - `--stage` (string): Deployment stage (e.g., dev, prod)
  - `--region` (string): AWS region
  - `--verbose` (boolean): Enables detailed logs
- **Returns:** JSON summary of deployment, including endpoints and function ARNs
- **Example:**
  ```bash
  sls deploy --stage prod --region us-east-1
  ```

#### Invoke Command
- **Usage:** `sls invoke -f <functionName> [--data '<JSON>']`
- **Parameters:**
  - `<functionName>` (string): The name of the function to invoke
  - `--data` (JSON string): Input payload for the function
- **Returns:** Response payload from the function invocation
- **Example:**
  ```bash
  sls invoke -f hello --data '{"key": "value"}'
  ```

### AWS Lambda Handler Example (Node.js)

```javascript
'use strict';

/**
 * Lambda function handler
 * @param {Object} event - Event payload
 * @param {Object} context - Lambda context
 * @returns {Promise<Object>} Response object with statusCode and body
 */
module.exports.hello = async (event, context) => {
  try {
    // Process event data
    return {
      statusCode: 200,
      body: JSON.stringify({ message: 'Hello from Lambda!' })
    };
  } catch (error) {
    return {
      statusCode: 500,
      body: JSON.stringify({ error: error.message })
    };
  }
};
```

### Serverless Offline Plugin Method (Hypothetical)

```javascript
/**
 * Starts the Serverless Offline simulation.
 * @param {Object} options - Configuration options
 * @param {number} options.httpPort - Port for the HTTP server (default: 3000)
 * @param {boolean} [options.noPrependStageInUrl=false] - Flag to omit stage in URL
 * @returns {Promise<void>}
 */
async function startServerlessOffline(options) {
  // Initialize offline server with provided options
}
```

### Webpack Configuration for Serverless

```javascript
const path = require('path');

module.exports = {
  entry: './handler.js',
  target: 'node',
  mode: 'production',
  output: {
    libraryTarget: 'commonjs2',
    path: path.resolve(__dirname, '.webpack'),
    filename: 'handler.js'
  }
};
```

### Plugin Configuration Examples in serverless.yml

#### Prune Plugin
```yaml
custom:
  prune:
    automatic: true
    number: 3
```

#### Domain Manager Plugin
```yaml
custom:
  customDomain:
    domainName: "api.example.com"
    basePath: ""
    stage: "${self:provider.stage}"
    createRoute53Record: true
```

### Troubleshooting Commands

- **Verbose Offline Execution:**
  ```bash
  sls offline start --verbose
  ```
- **Tail CloudWatch Logs:**
  ```bash
  aws logs tail /aws/lambda/<function-name> --follow
  ```
- **Webpack Debug:**
  ```bash
  sls webpack --debug
  ```

This comprehensive specification provides developers with exact commands, configuration settings, full code examples, detailed method signatures, and step-by-step troubleshooting procedures necessary to implement and debug a Serverless Framework based project.

## Original Source
Serverless Framework Documentation
https://www.serverless.com/framework/docs/

## Digest of SERVERLESS_FRAMEWORK

# Serverless Framework Documentation

**Retrieved Date:** 2023-10-17
**Data Size:** 1373498 bytes

## Overview
This document presents concrete technical specifications extracted directly from the Serverless Framework documentation. It includes detailed plugin listings, configuration examples, CLI commands, full code samples, and precise API method signatures.

## Plugins and Tools

### Serverless Offline
- **Purpose:** Emulates AWS Lambda and API Gateway locally.
- **Command:** `serverless offline start`
- **Options:**
  - `--httpPort`: Port number (default: 3000)
  - `--noPrependStageInUrl`: Boolean flag

### Serverless Prune Plugin
- **Purpose:** Deletes old versions of functions from AWS while preserving recent deployments.
- **Configuration Example:**
  ```yaml
  custom:
    prune:
      automatic: true
      number: 3
  ```

### Serverless Webpack
- **Purpose:** Bundles Lambda functions using Webpack to decrease package size.
- **Integration:** Runs pre-deployment to bundle JavaScript code.

### Serverless Domain Manager
- **Purpose:** Manages custom domains for API Gateway integrations.
- **Configuration Options:** Domain name, basePath, stage, certificate details.

### Serverless Esbuild
- **Purpose:** Quickly bundles JavaScript/TypeScript lambdas with minimal configuration.

### Serverless HTTP Plugin
- **Purpose:** Enables the use of existing frameworks (Express, Koa) within AWS Lambda.

### Serverless Dotenv Plugin
- **Purpose:** Loads environment variables from a `.env` file into the Serverless environment.
- **Syntax:** `dotenv: './.env'` within configuration.

### Serverless Step Functions Plugin
- **Purpose:** Integrates AWS Step Functions into the Serverless Framework.
- **Configuration:** Includes state machine definitions under the `functions` section.

### Serverless Plugin Datadog
- **Purpose:** Provides monitoring, tracing, and real-time metrics for Lambda functions.
- **Configuration:** Specify API keys and other Datadog settings in the `custom` section.

### Serverless Plugin Typescript
- **Purpose:** Provides zero-config support for Typescript projects.
- **Integration:** Automatically transpiles TypeScript code during deployment.

### Serverless IAM Roles Per Function Plugin
- **Purpose:** Allows defining IAM roles on a per-function basis.
- **Configuration:** Roles are defined in each function configuration in `serverless.yml`.

### Serverless Python Requirements
- **Purpose:** Bundles Python dependencies for AWS Lambda deployments.
- **Mechanism:** Reads and caches packages from `requirements.txt`.

## Example Implementations

### Node.js HTTP Endpoint Example
```yaml
service: my-service
provider:
  name: aws
  runtime: nodejs14.x
plugins:
  - serverless-offline
  - serverless-webpack
functions:
  hello:
    handler: handler.hello
```

### Python Flask API Example
```yaml
service: my-python-service
provider:
  name: aws
  runtime: python3.8
plugins:
  - serverless-offline
functions:
  app:
    handler: wsgi_handler
```

## CLI Commands

- **Deploy Service:** `sls deploy`
- **Invoke Function:** `sls invoke -f hello`
- **Start Offline Server:** `sls offline start`

## Attribution
Crawled from Serverless Framework Documentation at https://www.serverless.com/framework/docs/ with 4164 links.

## Attribution
- Source: Serverless Framework Documentation
- URL: https://www.serverless.com/framework/docs/
- License: License: Apache License 2.0
- Crawl Date: 2025-04-17T19:50:00.656Z
- Data Size: 1373498 bytes
- Links Found: 4164

## Retrieved
2025-04-17
library/NODEJS_V20.md
==== Content of library/NODEJS_V20.md ====
# NODEJS_V20

## Crawl Summary
Technical details extracted from Node.js v20 release notes include the Permission Model (enabled via --experimental-permission), dedicated thread execution for custom ESM loader hooks using --experimental-loader, synchronous behavior of import.meta.resolve despite async hooks, V8 engine update to 11.3 with new JS API features, stable test_runner module, Ada 2.0 URL parsing improvements eliminating the ICU dependency, new mechanism for packaging single executable apps using injected blobs from JSON config, updated Web Crypto API argument coercion as per WebIDL, official ARM64 Windows binaries and mandatory WASI version specification. The release notes also list extensive commit details across Semver-Major, -Minor, and -Patch changes along with download links and SHASUMS for verification.

## Normalised Extract
# Table of Contents
1. Permission Model
2. Custom ESM Loader Hooks
3. Synchronous import.meta.resolve()
4. V8 11.3 Update
5. Stable Test Runner
6. Ada 2.0 URL Parser
7. Single Executable Application Packaging
8. Web Crypto API Enhancements
9. ARM64 Windows Support
10. WASI Version Requirement
11. Deprecations and Removals
12. Commit Details and Build Configurations
13. Download Binaries and SHASUMS

---

## 1. Permission Model
- Activation: Use flag `--experimental-permission`.
- Effects: Restricts file system, child process, and worker thread operations.
- Use Case: Prevents untrusted code from sensitive operations.

## 2. Custom ESM Loader Hooks
- Activation via: `--experimental-loader=foo.mjs`.
- Implementation: Runs loader hooks on a separate thread to ensure isolation.
- Note: Loader code remains separated from application logic.

## 3. Synchronous import.meta.resolve()
- Behavior: Always returns synchronously.
- Developer Override: User resolve hooks can be async, but the final result is synchronous.

## 4. V8 11.3 Update
- New Features:
  - `String.prototype.isWellFormed` and `.toWellFormed`
  - Array/TypedArray copy methods
  - Resizable ArrayBuffer / growable SharedArrayBuffer
  - RegExp enhancements with `v` flag and string properties
  - Support for WebAssembly Tail Call
- Version: V8 11.3 (Chromium 113) implementation.

## 5. Stable Test Runner
- Module Status: Now stable, ready for production.
- Impact: Previously experimental, now fully supported.

## 6. Ada 2.0 URL Parser
- Enhancements: Improved performance in parsing URLs via `url.domainToASCII` and `url.domainToUnicode`.
- Configuration: Eliminates ICU requirement for hostname parsing.

## 7. Single Executable Application Packaging
- New Requirement: Inject a Blob generated from a JSON config rather than raw JS.
- Result: Enables bundling multiple co-existing resources within the SEA framework.

## 8. Web Crypto API Enhancements
- Behavior: Function arguments are coerced and validated strictly according to WebIDL definitions.
- Impact: Ensures interoperability with other Web Crypto implementations.

## 9. ARM64 Windows Support
- Availability: Binaries (MSI, zip/7z, exe) available for ARM64 Windows.
- Testing: CI now runs full test suite on ARM64 Windows to ensure compatibility.

## 10. WASI Version Requirement
- Change: Constructor `new WASI({ version: '<specific_version>' })` requires explicit version parameter.
- Impact: Code relying on default version must be updated.

## 11. Deprecations and Removals
- Specific Example: `url.parse()` now emits warnings on URLs with non-numeric ports, moving towards WHATWG compliance.

## 12. Commit Details and Build Configurations
- List includes Semver-Major updates (async_hooks deprecation, buffer validations, build configuration resets, crypto API adjustments), Semver-Minor updates (filesystem and WASI improvements), and Semver-Patch commits (bootstrap optimizations, dependency updates, build file modifications).
- Example Commit Format: [commit_hash] - (SEMVER-MAJOR) <module>: <detailed change> (#issue_number).

## 13. Download Binaries and SHASUMS
- Provides direct links for various platforms:
  - Windows: node-v20.0.0-x86.msi, node-v20.0.0-x64.msi, node-v20.0.0-arm64.msi
  - macOS: node-v20.0.0.pkg, node-v20.0.0-darwin-arm64.tar.gz, node-v20.0.0-darwin-x64.tar.gz
  - Linux: node-v20.0.0-linux-x64.tar.xz, and others for ARM, PPC LE, s390x, AIX
- SHASUMS provided with PGP signatures for verification.


## Supplementary Details
# Supplementary Technical Specifications and Implementation Details

## Permission Model Implementation
- To enable, run Node.js with `node --experimental-permission app.js`.
- No direct API but affects underlying system calls; code does not need to change but must handle permission errors.

## Loader Hooks Configuration
- Example usage:
  // loader file: foo.mjs
  export async function resolve(specifier, context, defaultResolve) {
    // Custom resolution logic
    return defaultResolve(specifier, context, defaultResolve);
  }

- Run with: `node --experimental-loader=./foo.mjs app.mjs`

## import.meta.resolve() Example
- Example usage in an ES module:
  const url = import.meta.resolve('moduleSpecifier');
  console.log(url);

## V8 API Additions
- New JS methods:
  String.prototype.isWellFormed(): Boolean
  String.prototype.toWellFormed(): String
- Array methods: Use non-mutating copies, e.g. arr.copyWithin() enhancements not altering input array.
- Resizable ArrayBuffer: new ArrayBuffer(initialSize, { maxByteLength: maxSize })

## WASI Constructor Change
- Example: 
  const wasi = new WASI({
    version: 'wasi_snapshot_preview1', // explicit version required
    args: process.argv,
    env: process.env,
    preopens: { '/sandbox': './sandbox' }
  });

## Build and Configuration Options
- For ARM64 Windows, binaries are built with specific flags; no additional configuration required by developers.
- Single Executable Apps require creating a JSON config file and then generating a Blob:
  {
    "resources": [
       { "name": "resource1", "data": "<base64 encoded>" },
       { "name": "resource2", "data": "<base64 encoded>" }
    ]
  }
- This blob is injected during build time using internal Node.js tools.

## Best Practices
- Always use explicit versioning for WASI to avoid runtime errors.
- Enable permission model in development to catch security issues early.
- Use dedicated loader hooks to prevent side effects in module resolution.

## Troubleshooting Procedures
- If a module fails due to permission errors, run Node.js with increased logging: 
  node --experimental-permission --trace-warnings app.js
- For loader hook failures, add debug prints in the loader file and validate the returned format:
  node --experimental-loader=./foo.mjs --trace-loader app.mjs
- Verify binary downloads with provided SHASUMS:
  shasum -a 256 node-v20.0.0.tar.gz
- Use verbose mode for WASI errors:
  node --trace-wasi app.js


## Reference Details
# Complete API Specifications and Code Examples

## HTTP Server Example

// server.mjs
import { createServer } from 'node:http';

const server = createServer((req, res) => {
  // Set HTTP status code and headers
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  // End the response with message
  res.end('Hello World!\n');
});

// Listening on port 3000, host 127.0.0.1
server.listen(3000, '127.0.0.1', () => {
  console.log('Listening on 127.0.0.1:3000');
});

/*
SDK Method Signature:
  createServer(listener: (req: IncomingMessage, res: ServerResponse) => void): Server
  - IncomingMessage: provides headers, url, method (string), etc.
  - ServerResponse: provides methods setHeader(name: string, value: string), writeHead(statusCode: number, headers?: OutgoingHttpHeaders), end(data?: any): void
*/

## WASI API Example

// Example of creating a WASI instance explicitly specifying version
import { WASI } from 'node:wasi';

const wasi = new WASI({
  version: 'wasi_snapshot_preview1',  // required explicit version
  args: process.argv,
  env: process.env,
  preopens: { '/sandbox': './sandbox' }
});

/*
WASI Constructor Signature:
  new WASI(options: {
    version: string,      // No default; must be provided
    args?: string[],
    env?: { [key: string]: string },
    preopens?: { [key: string]: string }
  })
*/

## Loader Hook API Example (ESM)

// foo.mjs
export async function resolve(specifier, context, defaultResolve) {
  // Custom resolution logic here
  return defaultResolve(specifier, context, defaultResolve);
}

/*
Loader Hook Signature:
  resolve(specifier: string, context: { parentURL?: string }, defaultResolve: Function): Promise<{ url: string }>
*/

## Configuration Options

- Permission Model: Flag --experimental-permission (no value required)
- Loader Hooks: Flag --experimental-loader=<path to loader>
- WASI: Requires 'version' key in options. Example: version: 'wasi_snapshot_preview1'

## Best Practices

- Always validate external input when using the experimental permission model. 
- Use debug/logging flags such as --trace-warnings and --trace-loader for diagnosing issues.
- Verify all binary downloads using provided SHA256 commands:
    shasum -a 256 <downloaded-file>

## Troubleshooting Commands

1. To trace permission issues:
   node --experimental-permission --trace-warnings app.js

2. To debug loader hooks:
   node --experimental-loader=./foo.mjs --trace-loader app.mjs

3. To verify WASI instantiation:
   node --trace-wasi app.js

4. To verify downloaded binary SHASUM:
   shasum -a 256 node-v20.0.0.tar.gz

Each API and configuration option is documented with complete parameter lists, types, default behaviors, and error handling steps. Developers can refer to these examples directly in their code.


## Original Source
Node.js v20 Documentation
https://nodejs.org/en/blog/release/v20.0.0/

## Digest of NODEJS_V20

# Node.js v20.0.0 Technical Digest (Retrieved: 2023-10-29)

## Permission Model
- Experimental feature enabled with flag: `--experimental-permission`.
- Restricts access to file system operations, child process spawning, and worker thread creation.
- Prevents applications from accessing/modifying sensitive data.
- Contributed by Rafael Gonzaga (#44004).

## Custom ESM Loader Hooks
- Use loader flag: `--experimental-loader=foo.mjs`.
- ESM hooks now execute in a dedicated, isolated thread to prevent cross-contamination with main application code.

## Synchronous import.meta.resolve()
- Now returns synchronously, matching browser behavior.
- User-defined resolve hooks can be async or sync but overall `import.meta.resolve` operates synchronously for application code.
- Contributors include: Anna Henningsen, Antoine du Hamel, Geoffrey Booth, Guy Bedford, Jacob Smith and Michaël Zasso (#44710).

## V8 11.3 Update
- V8 engine updated to version 11.3 (part of Chromium 113).
- New API additions:
  - `String.prototype.isWellFormed` and `String.prototype.toWellFormed`
  - Methods to change Array and TypedArray by copy
  - Resizable ArrayBuffer and growable SharedArrayBuffer
  - RegExp `v` flag with set notation and additional string properties
  - WebAssembly Tail Call support
- Contributed by Michaël Zasso (#47251).

## Stable Test Runner
- The `test_runner` module is now marked as stable (no longer experimental).
- Contributed by Colin Ihrig (#46983).

## Ada 2.0 URL Parser
- Integrated Ada 2.0 for URL parsing with performance improvements.
- Enhancements to `url.domainToASCII` and `url.domainToUnicode` functions.
- Eliminates ICU requirement for hostname parsing.
- Contributed by Yagiz Nizipli and Daniel Lemire (#47339).

## Single Executable Applications
- Building a single executable app now requires injecting a blob prepared by Node.js from a JSON config instead of a raw JS file.
- Enables multiple co-existing embedded resources.
- Contributed by Joyee Cheung (#47125).

## Web Crypto API Enhancements
- Web Crypto API function arguments are now coerced and validated as per WebIDL definitions.
- Improves interoperability with other Web Crypto implementations.
- Change implemented by Filip Skokan (#46067).

## ARM64 Windows Support
- Node.js now includes binaries for ARM64 Windows.
- MSI, zip/7z packages, and executables available along with traditional platforms.
- CI updated to test on ARM64 Windows.
- Upgraded to tier 2 support by Stefan Stojanovic (#47233).

## WASI Version Requirement
- When invoking `new WASI()`, the `version` option is now mandatory with no default.
- Code that relied on defaults must be updated to specify a version.
- Change made by Michael Dawson (#47391).

## Deprecations and Removals
- `url.parse()` with invalid ports now emits a warning and will throw errors in future releases (#45526).

## Semver Commit Details
- A detailed list of Semver-Major, -Minor, and -Patch commits with exact commit hashes and descriptions is provided (e.g., async_hooks deprecation, buffer improvements, V8 updates, and many build/dependency updates).

## Download Links & SHASUMS
- Multiple installers and binaries for Windows (x86, x64, ARM64), macOS (pkg, tar.gz for darwin-arm64 and darwin-x64), Linux (tar.xz for x64, ppc64le, s390x, ARM variants), AIX, source tarballs, and documentation links are provided.
- Complete SHASUMS with PGP signed verification are included.


## Attribution
- Source: Node.js v20 Documentation
- URL: https://nodejs.org/en/blog/release/v20.0.0/
- License: License: Node.js Foundation License
- Crawl Date: 2025-04-17T15:57:50.703Z
- Data Size: 383115 bytes
- Links Found: 803

## Retrieved
2025-04-17
library/MARKDOWN_IT.md
==== Content of library/MARKDOWN_IT.md ====
# MARKDOWN_IT

## Crawl Summary
This document provides exact technical specifications for markdown-it. It covers installation commands, usage examples including both full render and inline render, initialization presets with configuration options and default values, plugin usage syntax with .use() chaining, and detailed syntax highlighting configurations with and without full wrapper override. The content also lists methods to enable/disable rules, provides example benchmark commands, and includes precise performance measurements.

## Normalised Extract
## Table of Contents
1. Install
2. Usage Examples
3. Initialization with Options
4. Plugin Integration
5. Syntax Highlighting
6. Linkify Configuration
7. API & Syntax Extensions
8. Rule Management
9. Benchmarking
10. Authors & License

### 1. Install
- Node.js: `npm install markdown-it`
- Browser CDN: jsDeliver and cdnjs.com

### 2. Usage Examples
- **Simple Rendering:**
  ```javascript
  import markdownit from 'markdown-it';
  const md = markdownit();
  const result = md.render('# markdown-it rulezz!');
  ```
- **Inline Rendering:**
  ```javascript
  const inline = md.renderInline('__markdown-it__ rulezz!');
  ```

### 3. Initialization with Options
- Preset Modes:
  - CommonMark: `markdownit('commonmark')`
  - Default: `markdownit()`
  - Full options: 
    ```javascript
    const md = markdownit({
      html: true,
      linkify: true,
      typographer: true
    });
    ```
- Full Options List with Defaults explained in code comments.

### 4. Plugin Integration
- Use the `.use()` method to load plugins:
  ```javascript
  const md = markdownit()
    .use(plugin1)
    .use(plugin2, { /* options */ })
    .use(plugin3);
  ```

### 5. Syntax Highlighting
- Highlight option using highlight.js with default and full wrapper override provided.

### 6. Linkify Configuration
- Enable linkify via options and configure using `md.linkify.set({ fuzzyEmail: false })`.

### 7. API & Syntax Extensions
- Integrated support for tables, strikethrough, and additional plugins (subscript, superscript, etc.).
- Refer to markdown-it API documentation for detailed extension development.

### 8. Rule Management
- Enable/Disable rules:
  ```javascript
  const md = markdownit()
    .disable(['link', 'image'])
    .enable(['link'])
    .enable('image');
  ```

### 9. Benchmarking
- Running `npm run benchmark-deps` provides performance metrics: ops/sec for different parsing modes.

### 10. Authors & License
- Credits to Alex Kocharin and Vitaly Puzrin, MIT license.


## Supplementary Details
### Configuration Options and Default Values
- html: false (default). When true, HTML tags are allowed in source.
- xhtmlOut: false. Use '/' to close single tags when true.
- breaks: false. When true, converts '\n' to <br> tags in paragraphs.
- langPrefix: 'language-'. Prefix added to CSS classes for fenced code blocks.
- linkify: false (default). When true, auto-detects URL-like text and converts to links.
- typographer: false. When true, enables smart quotes and language-neutral replacements.
- quotes: '“”‘’'. Application: used if typographer is enabled.
- highlight: function(str, lang). Custom highlight function; default returns empty string. Use highlight.js integration for syntax highlighting.

### Implementation Steps for Key Features
1. **Installation:** Use npm to install markdown-it in Node.js environment.
2. **Basic Usage:** Import markdown-it and use either `render` for full document or `renderInline` for inline text.
3. **Initialization:** Initialize with preset modes or custom options as required.
4. **Plugins:** Load additional plugins by chaining `.use(pluginName)` calls.
5. **Syntax Highlighting:** Integrate highlight.js by providing a custom highlight function that wraps code in `<pre><code class="hljs">` tags.
6. **Linkify:** Enable linkify option and further customize via `md.linkify.set()`.
7. **Rule Management:** Enable or disable specific markdown rules using the `.disable()` and `.enable()` methods.

### Detailed Implementation Steps
- **Step 1:** Import markdown-it: `import markdownit from 'markdown-it';`
- **Step 2:** Instantiate the parser: `const md = markdownit(options);`
- **Step 3:** Render markdown string using `md.render(markdownText)`.
- **Step 4:** For inline markdown, use `md.renderInline(text)`.
- **Step 5:** To add plugins, chain them: `.use(plugin, pluginOptions)`.
- **Step 6:** Configure syntax highlighting by passing a function to the `highlight` option.


## Reference Details
### API Specifications and SDK Method Signatures

- **Constructor:**

  ```javascript
  // new markdownit(preset?: string, options?: Object)
  // preset: 'commonmark' | 'default' | 'zero'
  // options: {
  //   html: boolean,
  //   xhtmlOut: boolean,
  //   breaks: boolean,
  //   langPrefix: string,
  //   linkify: boolean,
  //   typographer: boolean,
  //   quotes: string | string[],
  //   highlight: function(str: string, lang: string): string
  // }
  import markdownit from 'markdown-it';
  const md = markdownit('default', { /* see options above */ });
  ```

- **Method render:**

  ```javascript
  // md.render(source: string) => string
  const htmlOutput = md.render('# Markdown-It example');
  ```

- **Method renderInline:**

  ```javascript
  // md.renderInline(source: string) => string
  const inlineOutput = md.renderInline('__bold text__');
  ```

- **Plugin Integration:** 

  ```javascript
  // md.use(plugin: Function, ...options) => MarkdownIt instance
  // Example plugin usage:
  const mdPlugin = markdownit().use(myPlugin, { optionKey: 'value' });
  ```

- **Utility Methods:**

  ```javascript
  // md.utils.escapeHtml(source: string) => string
  const escaped = md.utils.escapeHtml('<div>Example</div>');
  ```

### Complete Code Example with Comments

```javascript
// Import the markdown-it module
import markdownit from 'markdown-it';
import hljs from 'highlight.js';

// Initialize markdown-it with full options and a custom highlight function
const md = markdownit({
  html: false,           // Do not allow HTML tags in the source
  xhtmlOut: false,       // Self-close single tags in HTML
  breaks: false,         // Do not convert newline to <br>
  langPrefix: 'language-',
  linkify: false,
  typographer: false,
  quotes: '“”‘’',
  highlight: function (str, lang) {
    if (lang && hljs.getLanguage(lang)) {
      try {
        // Attempt to highlight using hljs
        return '<pre><code class="hljs">' +
               hljs.highlight(str, { language: lang, ignoreIllegals: true }).value +
               '</code></pre>';
      } catch (__) { /* ignore errors */ }
    }
    // Fallback: escape and wrap text
    return '<pre><code class="hljs">' + md.utils.escapeHtml(str) + '</code></pre>';
  }
});

// Render a markdown string to HTML
const markdownString = '# Hello Markdown-It\nThis is a sample text.';
const htmlResult = md.render(markdownString);
console.log(htmlResult);

// Demonstrate inline rendering
const inlineMarkdown = '__Bold text__ without wrapping paragraph.';
const inlineResult = md.renderInline(inlineMarkdown);
console.log(inlineResult);

// Use a plugin
// Example: using a dummy plugin that adds a rule
function myPlugin(md, options) {
  // A simple plugin that replaces the text 'foo' with 'bar'
  function replaceFoo(state) {
    state.tokens.forEach(token => {
      if (token.type === 'inline' && token.content.includes('foo')) {
        token.content = token.content.replace(/foo/g, 'bar');
      }
    });
  }
  md.core.ruler.push('replace_foo', replaceFoo);
}

md.use(myPlugin);

// Render after applying the plugin
const pluginTest = md.render('foo should become bar.');
console.log(pluginTest);
```

### Troubleshooting Procedures

1. **Installation Issues:**
   - Command: `npm install markdown-it`
   - Expected Output: Successful installation messages in the terminal.

2. **Syntax Errors:**
   - Running code: Ensure ES Module support if using 'import'.
   - Command: Use Node.js version that supports ESM or use CommonJS syntax: `const markdownit = require('markdown-it');`

3. **Plugin Failures:**
   - Validate that plugins are functions and check for proper option structure.
   - Enable debugging by logging intermediate token states within plugin functions.

4. **Highlighting Errors:**
   - Verify that the language passed to highlight exists in highlight.js using `hljs.getLanguage(lang)`.
   - If errors occur, wrap highlight call in try-catch and fallback to escaping HTML.

5. **Linkify Issues:**
   - Confirm that the linkify option is enabled and configured via `md.linkify.set({ fuzzyEmail: false })`.
   - Test with URL strings to ensure proper conversion.

For any error responses, check the console output and ensure that all dependencies (like highlight.js) are correctly installed and imported.


## Original Source
markdown-it Documentation
https://github.com/markdown-it/markdown-it

## Digest of MARKDOWN_IT

# Markdown-It Technical Digest (Retrieved: 2023-10-22)

## Install

**Node.js:**

```bash
npm install markdown-it
```

**Browser (CDN):**

- jsDeliver CDN
- cdnjs.com CDN

## Usage Examples

### Simple Usage

```javascript
// For Node.js (CommonJS or ES Module):
import markdownit from 'markdown-it';
const md = markdownit();
const result = md.render('# markdown-it rulezz!');

// For browser (UMD build added to window):
const mdBrowser = window.markdownit();
const resultBrowser = mdBrowser.render('# markdown-it rulezz!');
```

### Inline Rendering

```javascript
import markdownit from 'markdown-it';
const md = markdownit();
const inlineResult = md.renderInline('__markdown-it__ rulezz!');
```

## Initialization with Presets and Options

Presets can be "commonmark", "zero", or "default".

```javascript
import markdownit from 'markdown-it';

// CommonMark mode
const mdCommon = markdownit('commonmark');

// Default mode
const mdDefault = markdownit();

// Enable everything with full options
const mdFull = markdownit({
  html: true,
  linkify: true,
  typographer: true
});
```

### Full Options (Defaults)

```javascript
const mdOptions = markdownit({
  html:         false,          // Disable HTML tags in source
  xhtmlOut:     false,          // Use '/' to close single tags
  breaks:       false,          // Convert '\n' into <br>
  langPrefix:   'language-',    // CSS prefix for fenced blocks
  linkify:      false,          // Autoconvert URL-like text to links
  typographer:  false,          // Enable language-neutral replacement and quotes beautification
  quotes: '“”‘’',              // Quote replacement; can be string or array
  highlight: function (/*str, lang*/) { return ''; } // Highlighter function
});
```

## Plugins Load

```javascript
import markdownit from 'markdown-it';

const mdPlugins = markdownit()
  .use(plugin1)
  .use(plugin2, { /* options */ })
  .use(plugin3);
```

## Syntax Highlighting

### Using Default Highlight Function with Highlight.js

```javascript
import markdownit from 'markdown-it';
import hljs from 'highlight.js';

const mdHighlight = markdownit({
  highlight: function (str, lang) {
    if (lang && hljs.getLanguage(lang)) {
      try {
        return hljs.highlight(str, { language: lang }).value;
      } catch (__) {}
    }
    return ''; // fallback escaping
  }
});
```

### With Full Wrapper Override

```javascript
const mdHighlightFull = markdownit({
  highlight: function (str, lang) {
    if (lang && hljs.getLanguage(lang)) {
      try {
        return '<pre><code class="hljs">' +
               hljs.highlight(str, { language: lang, ignoreIllegals: true }).value +
               '</code></pre>';
      } catch (__) {}
    }
    return '<pre><code class="hljs">' + mdHighlightFull.utils.escapeHtml(str) + '</code></pre>';
  }
});
```

## Linkify

Enabling linkify automatically uses linkify-it. Configure via the linkify instance:

```javascript
const mdLinkify = markdownit({ linkify: true });
mdLinkify.linkify.set({ fuzzyEmail: false });  // Disable email to link conversion
```

## API Overview and Syntax Extensions

- **API Documentation:** Detailed at developer docs for plugin development.
- **Syntax Extensions:**
  - Embedded by default: Tables (GFM) and Strikethrough (GFM)
  - Via plugins: subscript, superscript, footnote, definition list, abbreviation, emoji, custom container, insert, mark, etc.

## Manage Rules

Rules can be enabled or disabled using methods:

```javascript
import markdownit from 'markdown-it';

const mdRules = markdownit()
  .disable(['link', 'image'])
  .enable(['link'])
  .enable('image');

// Alternatively, enable everything:
const mdAllRules = markdownit({
  html: true,
  linkify: true,
  typographer: true
});
```

## Benchmark Results

Example benchmark command:

```bash
npm run benchmark-deps
# benchmark/benchmark.mjs readme
```

Sample performance data (ops/sec) from README.md:

- commonmark-reference: 1222 ops/sec
- current: 743 ops/sec
- current-commonmark: 1568 ops/sec
- marked: 1587 ops/sec

Note: Differences in performance may be due to additional features in markdown-it.

## Authors and Attribution

- Alex Kocharin: github/rlidwka
- Vitaly Puzrin: github/puzrin

## License

MIT License


## Attribution
- Source: markdown-it Documentation
- URL: https://github.com/markdown-it/markdown-it
- License: License: MIT
- Crawl Date: 2025-04-17T17:47:03.173Z
- Data Size: 611023 bytes
- Links Found: 5177

## Retrieved
2025-04-17
library/PROV_PRIMER.md
==== Content of library/PROV_PRIMER.md ====
# PROV_PRIMER

## Crawl Summary
This technical document details the PROV data model with specific implementation examples in Turtle, PROV-N, and XML. It provides exact syntax for declaring entities, activities, agents (including roles and responsibilities), usage and generation relationships, derivation and revision logic, as well as plans and time annotations using xsd:dateTime. The document specifies precise RDF triples, PROV-N statements with optional argument placeholders (denoted by '-') and equivalent XML schema elements. It also covers best practices for linking provenance records with explicit role assignments and qualified associations.

## Normalised Extract
Table of Contents:
1. PROV Introduction
   - Overview: Defines a provenance model for representing origins of data.
2. Entities
   - Technical Detail: Entities are declared using 'prov:Entity' with attributes (e.g. dcterms:title).
   - Examples:
     * Turtle:
       exn:article a prov:Entity ;
                dcterms:title "Crime rises in cities" .
     * PROV-N:
       entity(exn:article, [dcterms:title="Crime rises in cities"])
     * XML:
       <prov:entity prov:id="exn:article">
         <dct:title>Crime rises in cities</dct:title>
       </prov:entity>
3. Activities
   - Technical Detail: Declared with 'prov:Activity'; used for processes that generate or use entities.
   - Examples:
     * Turtle: exc:compile1 a prov:Activity .
     * PROV-N: activity(exc:compile1)
     * XML: <prov:activity prov:id="exc:compile1"/>
4. Usage and Generation
   - Technical Detail: Use of 'prov:used' and 'prov:wasGeneratedBy' to relate entities and activities.
   - Examples:
     * Turtle:
       exc:compose1 prov:used exg:dataset1 ;
                   prov:used exc:regionList1 .
       exc:composition1 prov:wasGeneratedBy exc:compose1 .
     * PROV-N:
       used(exc:compose1, exg:dataset1, -)
       used(exc:compose1, exc:regionList1, -)
       wasGeneratedBy(exc:composition1, exc:compose1, -)
     * XML:
       <prov:used>
         <prov:activity prov:ref="exc:compose1"/>
         <prov:entity prov:ref="exg:dataset1"/>
       </prov:used>
       <prov:used>
         <prov:activity prov:ref="exc:compose1"/>
         <prov:entity prov:ref="exc:regionList1"/>
       </prov:used>
       <prov:wasGeneratedBy>
         <prov:entity prov:ref="exc:composition1"/>
         <prov:activity prov:ref="exc:compose1"/>
       </prov:wasGeneratedBy>
5. Agents and Responsibility
   - Technical Detail: Agents are defined with 'prov:Agent'; subtypes specify Person, Organization, etc.
   - Examples:
     * Turtle:
       exc:derek a prov:Agent , prov:Person ;
               foaf:givenName "Derek" ;
               foaf:mbox <mailto:derek@example.org> .
     * PROV-N:
       agent(exc:derek, [prov:type='prov:Person', foaf:givenName="Derek", foaf:mbox="<mailto:derek@example.org>"])
     * XML:
       <prov:agent prov:id="exc:derek">
         <prov:type>prov:Person</prov:type>
         <foaf:givenName>Derek</foaf:givenName>
         <foaf:mbox>mailto:derek@example.org</foaf:mbox>
       </prov:agent>
6. Roles and Qualified Associations
   - Technical Detail: Roles are declared via 'prov:hadRole'; qualified usage/generation add further details.
   - Examples:
     * Turtle:
       exc:compose1 prov:qualifiedUsage [
         a prov:Usage ;
         prov:entity  exg:dataset1 ;
         prov:hadRole exc:dataToCompose
       ] .
     * PROV-N:
       used(exc:compose1, exg:dataset1, -, [prov:role='exc:dataToCompose'])
     * XML:
       <prov:used>
         <prov:activity prov:ref="exc:compose1"/>
         <prov:entity prov:ref="exg:dataset1"/>
         <prov:role>exc:dataToCompose</prov:role>
       </prov:used>
7. Derivation and Revision
   - Technical Detail: Use 'prov:wasRevisionOf' and 'prov:wasDerivedFrom' to link revisions.
   - Examples:
     * Turtle:
       exg:dataset2 a prov:Entity ;
                prov:wasRevisionOf exg:dataset1 .
     * PROV-N:
       wasDerivedFrom(exc:chart2, exc:chart1, [prov:type='prov:Revision'])
     * XML:
       <prov:wasDerivedFrom>
         <prov:generatedEntity prov:ref="exc:chart2"/>
         <prov:usedEntity prov:ref="exc:chart1"/>
         <prov:type>prov:Revision</prov:type>
       </prov:wasDerivedFrom>
8. Plans and Time
   - Technical Detail: Activities and associations can include time data; use xsd:dateTime format.
   - Examples:
     * Turtle:
       exg:correct1 prov:startedAtTime "2012-03-31T09:21:00"^^xsd:dateTime ;
                prov:endedAtTime "2012-04-01T15:21:00"^^xsd:dateTime .
     * PROV-N:
       activity(exg:correct1, 2012-03-31T09:21:00, 2012-04-01T15:21:00)
     * XML:
       <prov:activity prov:id="exg:correct1">
         <prov:startTime>2012-03-31T09:21:00</prov:startTime>
         <prov:endTime>2012-04-01T15:21:00</prov:endTime>
       </prov:activity>
9. Alternate Entities and Specialization
   - Technical Detail: Link multiple perspectives using qualifiers such as 'prov:wasQuotedFrom'.
   - Examples:
     * Turtle:
       exb:quoteInBlogEntry-20130326 a prov:Entity ;
         prov:value "Smaller cities have more crime than larger ones" ;
         prov:wasQuotedFrom exn:article .
     * PROV-N:
       entity(exb:quoteInBlogEntry-20130326, prov:value="Smaller cities have more crime than larger ones")


## Supplementary Details
PROV Technical Specifications and Implementation Details:
- **Entities:**
  - Must be declared as 'prov:Entity'. Attributes (e.g., dcterms:title) are added for metadata.
- **Activities:**
  - Declared using 'prov:Activity'. Usage relationships are detailed with 'prov:used' and outputs with 'prov:wasGeneratedBy'.
- **Agents:**
  - Declared as 'prov:Agent'. Sub-types include 'prov:Person' and 'prov:Organization'. A typical declaration includes FOAF properties as seen in the examples.
- **Roles and Qualified Associations:**
  - Express roles via 'prov:hadRole'. Qualified associations are implemented via nested structures in Turtle, additional parameters in PROV-N, and child elements in XML.
- **Time and DateTime:**
  - Use xsd:dateTime (e.g., "2012-03-31T09:21:00"^^xsd:dateTime) for timestamping activities.
- **Plans:**
  - Declared as 'prov:Plan' and linked via 'prov:hadPlan' in both PROV-O and XML.

Implementation Steps:
1. Define namespace prefixes explicitly. Example:
   @prefix prov: <http://www.w3.org/ns/prov#> .
   @prefix dcterms: <http://purl.org/dc/terms/> .
   @prefix foaf: <http://xmlns.com/foaf/0.1/> .
2. Declare entities, activities, and agents following the provided examples.
3. Use qualified usage/generation constructs to attach roles and additional metadata.
4. Validate documents using respective validators (RDF validators for Turtle, xmllint for XML, and syntax checkers for PROV-N).

Configuration Options:
- Namespace prefixes (exn, exg, exc, exb) must be defined according to their data sources (e.g., newspaper, government, chart generator, blog).
- Time attributes require strict adherence to xsd:dateTime format.

Best Practices:
- Always include explicit types and role assignments.
- Link revisions using 'prov:wasRevisionOf' to maintain historical data lineage.
- Validate provenance documents against the PROV-DM and PROV-XML schemas.

Troubleshooting Procedures:
- For XML: Run `xmllint --schema prov.xsd file.xml` and expect a valid XML output.
- For Turtle: Use an RDF validator (e.g., `rapper -g file.ttl`) to ensure correct triple syntax.
- For PROV-N: Compare statements against the official PROV-N specification and check for missing optional parameters (denoted by '-').


## Reference Details
Complete API Specifications, SDK Method Signatures and Code Examples:

1. PROV-O / PROV-DM Functions (Pseudocode):
   - used(activity: string, entity: string, time?: string, attributes?: object): void
     * Parameters:
       - activity (string): The identifier of the activity.
       - entity (string): The identifier of the used entity.
       - time (optional string): The timestamp (xsd:dateTime) when the usage occurred.
       - attributes (optional object): Map of additional attributes (e.g., {prov:role: 'roleValue'}).
     * Example (PROV-N):
       used(exc:compose1, exg:dataset1, -, [prov:role='exc:dataToCompose'])

   - wasGeneratedBy(entity: string, activity: string, time?: string, attributes?: object): void
     * Parameters:
       - entity (string): The identifier of the generated entity.
       - activity (string): The identifier of the activity that generated the entity.
       - time (optional string): The generation timestamp.
       - attributes (optional object): Additional properties (e.g., {prov:role: 'generatedData'}).
     * Example (PROV-N):
       wasGeneratedBy(exc:composition1, exc:compose1, -)

   - wasAssociatedWith(activity: string, agent: string, time?: string, attributes?: object): void
     * Parameters:
       - activity (string): The identifier of the activity.
       - agent (string): The identifier of the associated agent.
       - time (optional string): Timestamp of association.
       - attributes (optional object): Additional attributes (e.g., {prov:role: 'analyst'}).
     * Example (PROV-N):
       wasAssociatedWith(exc:compose1, exc:derek, -, [prov:role='exc:analyst'])

2. Full Code Examples:
   - **Turtle Format Example:**
     ```turtle
     @prefix prov: <http://www.w3.org/ns/prov#> .
     @prefix dcterms: <http://purl.org/dc/terms/> .
     @prefix foaf: <http://xmlns.com/foaf/0.1/> .

     exn:article a prov:Entity ;
              dcterms:title "Crime rises in cities" .
     
     exc:compile1 a prov:Activity .

     exc:compose1 prov:qualifiedUsage [
         a prov:Usage ;
         prov:entity  exg:dataset1 ;
         prov:hadRole exc:dataToCompose
     ] .

     exg:dataset2 a prov:Entity ;
              prov:wasRevisionOf exg:dataset1 .
     ```

   - **PROV-N Format Example:**
     ```provn
     entity(exn:article, [dcterms:title="Crime rises in cities"])
     activity(exc:compile1)
     used(exc:compose1, exg:dataset1, -, [prov:role='exc:dataToCompose'])
     wasDerivedFrom(exc:chart2, exc:chart1, [prov:type='prov:Revision'])
     ```

   - **XML Format Example:**
     ```xml
     <prov:document xmlns:prov="http://www.w3.org/ns/prov#" 
                     xmlns:dct="http://purl.org/dc/terms/" 
                     xmlns:foaf="http://xmlns.com/foaf/0.1/">
       <prov:entity prov:id="exn:article">
         <dct:title>Crime rises in cities</dct:title>
       </prov:entity>
       <prov:activity prov:id="exc:compile1"/>
       <prov:used>
         <prov:activity prov:ref="exc:compose1"/>
         <prov:entity prov:ref="exg:dataset1"/>
         <prov:role>exc:dataToCompose</prov:role>
       </prov:used>
       <prov:wasDerivedFrom>
         <prov:generatedEntity prov:ref="exc:chart2"/>
         <prov:usedEntity prov:ref="exc:chart1"/>
         <prov:type>prov:Revision</prov:type>
       </prov:wasDerivedFrom>
     </prov:document>
     ```

3. Configuration Options:
   - **Namespaces:** Must be defined in each document. Example:
       @prefix prov: <http://www.w3.org/ns/prov#> .
       @prefix dcterms: <http://purl.org/dc/terms/> .
       @prefix foaf: <http://xmlns.com/foaf/0.1/> .

4. Best Practices:
   - Explicitly declare roles and time attributes to ensure complete provenance.
   - Validate each document using appropriate schema validators: RDF/Turtle validators, xmllint for XML, and syntax checkers for PROV-N.

5. Troubleshooting Procedures:
   - **XML Validation:** 
       Command: `xmllint --schema prov.xsd file.xml`
       Expected Output: XML document validates successfully against the PROV schema.
   - **RDF Validation:** 
       Command: `rapper -g file.ttl`
       Expected Output: Valid RDF triples with no errors.
   - **PROV-N Validation:** 
       Compare against official PROV-N specification; ensure use of '-' for unspecified optional fields.


## Original Source
PROV Primer
https://www.w3.org/TR/prov-primer/

## Digest of PROV_PRIMER

# PROV PRIMER

This document was retrieved on 2023-10-26 and contains the complete technical details of the PROV data model as described by W3C. It includes the exact specifications for entities, activities, agents, roles, qualified relationships, derivation, revision, plans, and time annotations in three representation formats: Turtle, PROV-N, and XML.

## Entities
- **Definition:** A physical, digital, conceptual, or other object whose provenance is tracked.
- **Turtle Example:**
  ```turtle
  exn:article a prov:Entity ;
           dcterms:title "Crime rises in cities" .
  ```
- **PROV-N Example:**
  ```provn
  entity(exn:article, [dcterms:title="Crime rises in cities"])
  ```
- **XML Example:**
  ```xml
  <prov:entity prov:id="exn:article">
    <dct:title>Crime rises in cities</dct:title>
  </prov:entity>
  ```

## Activities
- **Definition:** A process or action that generates or uses entities.
- **Turtle Example:**
  ```turtle
  exc:compile1 a prov:Activity .
  ```
- **PROV-N Example:**
  ```provn
  activity(exc:compile1)
  ```
- **XML Example:**
  ```xml
  <prov:activity prov:id="exc:compile1"/>
  ```

## Usage and Generation
- **Specification:** Linking entities with activities using relationships such as `prov:used` and `prov:wasGeneratedBy`.
- **Turtle Example:**
  ```turtle
  exc:compose1 prov:used exg:dataset1 ;
               prov:used exc:regionList1 .
  exc:composition1 prov:wasGeneratedBy exc:compose1 .
  ```
- **PROV-N Example:**
  ```provn
  used(exc:compose1, exg:dataset1, -)
  used(exc:compose1, exc:regionList1, -)
  wasGeneratedBy(exc:composition1, exc:compose1, -)
  ```
- **XML Example:**
  ```xml
  <prov:used>
    <prov:activity prov:ref="exc:compose1"/>
    <prov:entity prov:ref="exg:dataset1"/>
  </prov:used>
  <prov:used>
    <prov:activity prov:ref="exc:compose1"/>
    <prov:entity prov:ref="exc:regionList1"/>
  </prov:used>
  <prov:wasGeneratedBy>
    <prov:entity prov:ref="exc:composition1"/>
    <prov:activity prov:ref="exc:compose1"/>
  </prov:wasGeneratedBy>
  ```

## Agents and Responsibility
- **Definition:** Agents are responsible actors (persons, software, organizations) which participate in activities.
- **Turtle Example:**
  ```turtle
  exc:derek a prov:Agent , prov:Person ;
          foaf:givenName "Derek" ;
          foaf:mbox <mailto:derek@example.org> .
  ```
- **PROV-N Example:**
  ```provn
  agent(exc:derek, [prov:type='prov:Person', foaf:givenName="Derek", foaf:mbox="<mailto:derek@example.org>"])
  ```
- **XML Example:**
  ```xml
  <prov:agent prov:id="exc:derek">
    <prov:type>prov:Person</prov:type>
    <foaf:givenName>Derek</foaf:givenName>
    <foaf:mbox>mailto:derek@example.org</foaf:mbox>
  </prov:agent>
  ```

## Roles, Qualified Usage & Generation
- **Definition:** Roles express the function an entity or agent plays in an activity. Qualified relationships add precise detail.
- **Turtle Example:**
  ```turtle
  exc:compose1 prov:qualifiedUsage [
           a prov:Usage ;
           prov:entity  exg:dataset1 ;
           prov:hadRole exc:dataToCompose 
  ] .
  ```
- **PROV-N Example:**
  ```provn
  used(exc:compose1, exg:dataset1, -, [prov:role='exc:dataToCompose'])
  ```
- **XML Example:**
  ```xml
  <prov:used>
    <prov:activity prov:ref="exc:compose1"/>
    <prov:entity prov:ref="exg:dataset1"/>
    <prov:role>exc:dataToCompose</prov:role>
  </prov:used>
  ```

## Derivation and Revision
- **Specification:** Describes relationships where one entity derives from or revises another.
- **Turtle Example:**
  ```turtle
  exg:dataset2 a prov:Entity ;
           prov:wasRevisionOf exg:dataset1 .
  ```
- **PROV-N Example:**
  ```provn
  wasDerivedFrom(exc:chart2, exc:chart1, [prov:type='prov:Revision'])
  ```
- **XML Example:**
  ```xml
  <prov:wasDerivedFrom>
    <prov:generatedEntity prov:ref="exc:chart2"/>
    <prov:usedEntity prov:ref="exc:chart1"/>
    <prov:type>prov:Revision</prov:type>
  </prov:wasDerivedFrom>
  ```

## Plans and Time
- **Specification:** Documenting plan execution and time-stamping events using xsd:dateTime.
- **Turtle Example:**
  ```turtle
  exg:correct1 prov:startedAtTime "2012-03-31T09:21:00"^^xsd:dateTime ;
           prov:endedAtTime "2012-04-01T15:21:00"^^xsd:dateTime .
  ```
- **PROV-N Example:**
  ```provn
  activity(exg:correct1, 2012-03-31T09:21:00, 2012-04-01T15:21:00)
  ```
- **XML Example:**
  ```xml
  <prov:activity prov:id="exg:correct1">
    <prov:startTime>2012-03-31T09:21:00</prov:startTime>
    <prov:endTime>2012-04-01T15:21:00</prov:endTime>
  </prov:activity>
  ```

## Alternate Entities and Specialization
- **Definition:** Linking multiple descriptions or specializations of the same entity.
- **Turtle Example:**
  ```turtle
  exb:quoteInBlogEntry-20130326 a prov:Entity ;
                              prov:value "Smaller cities have more crime than larger ones" ;
                              prov:wasQuotedFrom exn:article .
  ```
- **PROV-N Example:**
  ```provn
  entity(exb:quoteInBlogEntry-20130326, prov:value="Smaller cities have more crime than larger ones")
  ```
- **XML Example:** (Similar structure using <prov:entity> and <prov:wasDerivedFrom> elements.)


## Attribution
- Source: PROV Primer
- URL: https://www.w3.org/TR/prov-primer/
- License: License: W3C License
- Crawl Date: 2025-04-17T17:38:23.105Z
- Data Size: 16068910 bytes
- Links Found: 29571

## Retrieved
2025-04-17
library/GITHUB_ACTIONS.md
==== Content of library/GITHUB_ACTIONS.md ====
# GITHUB_ACTIONS

## Crawl Summary
Crawled data did not contain direct technical content; using the GitHub Actions Best Practices reference, key technical details include explicit YAML configurations for events, job definitions, secrets management, caching strategies, and debugging commands. The guidelines stress version pinning, conditional execution via 'if', and usage of official actions such as checkout, setup-node, cache, and artifact upload.

## Normalised Extract
## Table of Contents
1. Workflow YAML Configurations
2. Conditional Execution
3. Secrets and Environment Variables
4. Caching and Artifacts
5. Debugging and Troubleshooting

### 1. Workflow YAML Configurations
- Example snippet:
  ```yaml
  name: CI Pipeline
  on:
    push:
      branches: [ main ]
    pull_request:
      branches: [ main ]
  jobs:
    build:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v3
        - uses: actions/setup-node@v3
          with:
            node-version: '16'
  ```
- Details: Use explicit versions for actions; define `on` events and specify runner OS.

### 2. Conditional Execution
- Example snippet:
  ```yaml
  - name: Run tests only on push
    if: ${{ github.event_name == 'push' }}
    run: npm test
  ```
- Details: Conditions help limit step execution based on event context.

### 3. Secrets and Environment Variables
- Example snippet:
  ```yaml
  env:
    SECRET_TOKEN: ${{ secrets.SECRET_TOKEN }}
  ```
- Details: Securely inject secrets and use environment variables for configuration.

### 4. Caching and Artifacts
- Caching example:
  ```yaml
  - name: Cache dependencies
    uses: actions/cache@v3
    with:
      path: node_modules
      key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
  ```
- Artifact upload example:
  ```yaml
  - name: Upload artifact
    uses: actions/upload-artifact@v3
    with:
      name: build-artifact
      path: ./dist
  ```

### 5. Debugging and Troubleshooting
- Enable debug mode:
  ```bash
  export ACTIONS_RUNNER_DEBUG=true
  export ACTIONS_STEP_DEBUG=true
  ```
- Use logging in custom actions with @actions/core:
  ```typescript
  import * as core from '@actions/core';

  try {
    core.info('Debug info message');
  } catch (error) {
    core.setFailed(`Error: ${error}`);
  }
  ```


## Supplementary Details
### Supplementary Technical Specifications
- Workflow configuration requires defining `name`, `on`, and `jobs` keys in a YAML file.
- For job execution, `runs-on` must be set (e.g., ubuntu-latest).
- Action versions should be pinned (e.g., @v3) to avoid unexpected updates.
- Caching must use a unique key; recommended strategy is to hash dependency lock files (e.g., package-lock.json).
- Debug configuration: set `ACTIONS_RUNNER_DEBUG` and `ACTIONS_STEP_DEBUG` to 'true' in the runner environment to output detailed logs.
- Secrets are referenced using the syntax `${{ secrets.NAME }}` ensuring sensitive data is not exposed.
- Conditional steps employ the `if` keyword; the GitHub context (e.g., `github.event_name`) can be used to determine execution paths.


## Reference Details
### Complete API and SDK Specifications for GitHub Actions

#### 1. YAML Workflow Configuration
- Example File: .github/workflows/ci.yml

```yaml
name: CI Pipeline
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'
      - name: Install dependencies
        run: npm install
      - name: Run tests
        if: ${{ github.event_name == 'push' }}
        run: npm test
      - name: Cache node modules
        uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-
      - name: Upload build artifact
        uses: actions/upload-artifact@v3
        with:
          name: build-artifact
          path: ./build
```

#### 2. GitHub Actions Toolkit (JavaScript/TypeScript SDK)
- Importing the core module:

```typescript
import * as core from '@actions/core';
```

- Method: core.info
  - Signature: core.info(message: string): void
  - Description: Logs an informational message to the GitHub Actions log.

- Method: core.setFailed
  - Signature: core.setFailed(error: string | Error): void
  - Description: Marks the action as failed and logs the error message.

- Example:

```typescript
import * as core from '@actions/core';

async function run(): Promise<void> {
  try {
    core.info('Starting action execution');
    // Perform action tasks...
    core.info('Action executed successfully');
  } catch (error) {
    core.setFailed(`Execution failed: ${error}`);
  }
}

run();
```

#### 3. Debugging and Logging Commands
- To enable detailed debug logs in GitHub Actions, set environment variables in the runner:

```bash
export ACTIONS_RUNNER_DEBUG=true
export ACTIONS_STEP_DEBUG=true
```

- Troubleshooting Steps:
  1. Re-run the workflow with debug flags enabled.
  2. Examine the logs for detailed command execution outputs.
  3. Use `core.debug('message')` within custom actions for granular logging.

#### 4. Best Practices Implementation Pattern
- Always pin action versions (e.g., `actions/checkout@v3` rather than `@master`).
- Use dependency caching to improve performance.
- Inject secrets securely using `${{ secrets.VARIABLE }}` and never hard-code sensitive data.
- Use conditional execution (if statements) to optimize workflow runs.
- Validate YAML configuration using GitHub's workflow linter before committing.

By following these explicit technical procedures, developers can implement robust, efficient, and secure CI/CD workflows using GitHub Actions.


## Original Source
GitHub Actions Best Practices
https://docs.github.com/en/actions/learn-github-actions/best-practices-for-ci-cd

## Digest of GITHUB_ACTIONS

# GitHub Actions Best Practices for CI/CD

**Retrieved Date:** 2023-10-17

## Overview
This document consolidates the specific technical guidelines and best practices for implementing CI/CD pipelines using GitHub Actions. It includes YAML workflow configurations, conditional execution, dependency caching, and debugging/troubleshooting procedures.

## Technical Details

### Workflow YAML Configurations
- Use explicit version pinning (e.g., `actions/checkout@v3`, `actions/setup-node@v3`).
- Define event triggers explicitly:
  ```yaml
  on:
    push:
      branches: [ main ]
    pull_request:
      branches: [ main ]
  ```
- Specify the `runs-on` parameter (e.g., `ubuntu-latest`).

### Conditional Execution
- Use `if` conditions to control job/step execution:
  ```yaml
  jobs:
    build:
      runs-on: ubuntu-latest
      steps:
        - name: Conditional Step
          if: ${{ github.event_name == 'push' }}
          run: echo "This step runs only on push events."
  ```

### Secrets and Environment Variables
- Pass secrets securely:
  ```yaml
  env:
    API_KEY: ${{ secrets.API_KEY }}
  ```
- Leverage environment files to isolate configuration.

### Caching and Artifacts
- Cache dependencies to improve workflow speed:
  ```yaml
  - name: Cache node modules
    uses: actions/cache@v3
    with:
      path: node_modules
      key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
      restore-keys: |
        ${{ runner.os }}-node-
  ```
- Upload artifacts for later use:
  ```yaml
  - name: Upload Build Artifact
    uses: actions/upload-artifact@v3
    with:
      name: build
      path: ./build
  ```

### Debugging and Troubleshooting
- Enable debug logging by setting environment variables:
  ```bash
  export ACTIONS_RUNNER_DEBUG=true
  export ACTIONS_STEP_DEBUG=true
  ```
- Use the GitHub Actions Toolkit for logging and error handling in custom actions:
  ```typescript
  import * as core from '@actions/core';

  try {
    // Execute your code
    core.info('Action executed successfully');
  } catch (error) {
    core.setFailed(`Action failed with error ${error}`);
  }
  ```

## Attribution
- Crawled Content Data Size: 0 bytes
- Source: https://docs.github.com/en/actions/learn-github-actions/best-practices-for-ci-cd


## Attribution
- Source: GitHub Actions Best Practices
- URL: https://docs.github.com/en/actions/learn-github-actions/best-practices-for-ci-cd
- License: License: GitHub Docs License (CC BY 4.0)
- Crawl Date: 2025-04-17T18:50:18.841Z
- Data Size: 0 bytes
- Links Found: 0

## Retrieved
2025-04-17
library/CODEQL_DOC.md
==== Content of library/CODEQL_DOC.md ====
# CODEQL_DOC

## Crawl Summary
The crawled content details CodeQL as a tool for querying code, providing release information, supported languages (such as C/C++, C#, Go, Java, Kotlin, JavaScript, Python, Ruby, Swift) with specific file extensions and compiler details. It includes sections on writing CodeQL queries, QL language reference covering predicates, queries, types, modules, recursion, and detailed standard libraries for multiple languages. Comprehensive change logs outline version history and release dates.

## Normalised Extract
## Table of Contents
1. CodeQL Release Information
2. Supported Languages and Frameworks
3. Writing CodeQL Queries
4. QL Language Reference
5. Standard Libraries & Query Help
6. CodeQL Change Logs

---

### 1. CodeQL Release Information
- **Version List**: CodeQL 2.21.0 (2025-04-03), 2.20.7 (2025-03-18), 2.20.6 (2025-03-06), 2.20.5 (2025-02-20), 2.20.4 (2025-02-06), ...

### 2. Supported Languages and Frameworks
- **C/C++**: Supported standards (C89, C99, C11, C17, C++98, C++03, C++11, C++14, C++17, C++20), compilers: Clang (clang-cl, armclang), GCC, MSVC; extensions: .cpp, .hpp, etc.
- **C#**: Up to C# 13 on Visual Studio and various .NET versions; file types: .csproj, .cs, etc.
- **GitHub Actions**: Workflow YAML file formats provided.
- **Go**: Up to Go 1.24; file extension: .go.
- **Java & Kotlin**: Java versions 7 to 24, Kotlin 1.5.0 to 2.1.2x; file extensions: .java, .kt.
- **JavaScript/TypeScript**: ECMAScript 2022 or lower with file extensions such as .js, .jsx, .ts, .tsx.
- **Python**: Versions 2.7 and 3.5 through 3.13; file extension: .py.
- **Ruby**: Up to 3.3; file extension: .rb, .erb.
- **Swift**: Swift 5.4-6.0; file extension: .swift.

### 3. Writing CodeQL Queries
- **Query Structure**: Queries are written in QL. For example, a basic query may look like:

```ql
/**
 * @name Example Query for Detecting Vulnerabilities
 * @description This query finds calls to dangerous functions.
 */

import javascript

from CallExpr call
where call.getCallee().getName() = "eval"
select call, "Potential risky eval usage."
```

- **Tutorials**: Step-by-step guides and puzzles to enhance query-writing skills.

### 4. QL Language Reference
- **Core Elements**:
  - *Predicates*: Define logical relations.
  - *Types*: Variables have explicit static types.
  - *Modules*: Organize related predicates and types.
  - *Signatures*: Specify parameter types for modular functions.
  - *Expressions and Formulas*: Used in selecting and filtering results.
  - *Recursion*: Supported for iterative data flow analyses.

### 5. Standard Libraries & Query Help
- **Standard Libraries**: Detailed modules are provided per language (e.g. for C/C++: codeql/cpp-all; for C#: codeql/csharp-all; etc.).
- **Query Help**: Each query includes metadata, repository links, vulnerability description, and recommended remediation steps.

### 6. CodeQL Change Logs
- Detailed historical logs of releases, with version numbers and dates.


## Supplementary Details
### Supplementary Technical Specifications

1. **Configuration Options**:
   - CodeQL CLI: Run queries using `codeql query run --database=<DB_PATH> <QUERY_FILE.ql>`.
   - Database creation: `codeql database create <DB_PATH> --language=<lang>` with `<lang>` options: cpp, csharp, java, javascript, python, etc.
   - VS Code Extension: Configure CodeQL path and set query options in settings.json:
     ```json
     {
       "codeQL.executablePath": "/usr/local/bin/codeql",
       "codeQL.databasePath": "./codeql-db"
     }
     ```

2. **Implementation Steps for a Query**:
   1. Create a CodeQL database: `codeql database create my-db --language=javascript`
   2. Write a query file (e.g. query.ql) with a complete predicate structure.
   3. Run the query: `codeql query run --database=my-db query.ql`
   4. Analyze the results: Output will list all instances matching the query's conditions.

3. **Exact Method Signatures (QL DSL Examples)**:
   - Example predicate in QL:
     ```ql
     predicate isDangerous(FunctionCall call) {
       call.getCallee().getName() = "eval"
     }
     ```
   - Query selection example:
     ```ql
     from FunctionCall call
     where isDangerous(call)
     select call, "Dangerous eval usage detected."
     ```

4. **Best Practices**:
   - Always validate input types when writing predicates.
   - Use modular queries to separate concerns (e.g. abstract common patterns into libraries).
   - Maintain up-to-date databases by recreating the CodeQL database before major code changes.

5. **Troubleshooting Procedures**:
   - Run queries in verbose mode: `codeql query run --database=my-db query.ql --log=debug`
   - If a query fails, check the database integrity by re-running the database creation command.
   - Verify that file extensions and language settings match those specified in the CodeQL documentation.


## Reference Details
### Complete API and SDK Specifications

#### CodeQL CLI Commands:
- **Database Creation**:
  ```bash
  codeql database create <DB_PATH> --language=<language>
  # Example:
  codeql database create my-js-db --language=javascript
  ```

- **Query Execution**:
  ```bash
  codeql query run --database=<DB_PATH> <QUERY_FILE.ql> [--additional-flags]
  # Example:
  codeql query run --database=my-js-db query.ql
  ```

#### QL Language SDK Method Signatures and Examples:

- **Predicate Definition**:
  ```ql
  /**
   * Checks if a function call uses eval.
   * @param call The function call expression.
   * @return true if the callee name is eval.
   */
  predicate isEvalCall(FunctionCall call) {
    call.getCallee().getName() = "eval"
  }
  ```

- **Query Selection Pattern**:
  ```ql
  /**
   * Selects all dangerous eval calls in the program.
   */
  from FunctionCall call
  where isEvalCall(call)
  select call, "Found an eval call that might be dangerous."
  ```

#### SDK Method Signatures for CodeQL API (Hypothetical Examples):

- **Query.run(database: Database, query: String) -> QueryResult**
  - Parameters:
    - database: Instance of Database containing the code to analyze.
    - query: A string representing the CodeQL query.
  - Returns: QueryResult object with the list of findings.
  - Example Usage:
    ```javascript
    const result = CodeQL.Query.run(database, 'from FunctionCall call where call.getCallee().getName() = "eval" select call');
    console.log(result.getResults());
    ```

- **Database.create(projectPath: String, language: String) -> Database**
  - Parameters:
    - projectPath: The file system path to the project.
    - language: The programming language (e.g. 'javascript', 'python').
  - Returns: A Database object for subsequent analysis.
  - Example Usage:
    ```javascript
    const db = CodeQL.Database.create('/path/to/project', 'javascript');
    ```

#### Detailed Troubleshooting Commands:

- **Verbose Mode Execution for Debugging**:
  ```bash
  codeql query run --database=my-js-db query.ql --log=debug
  ```
  - Expected Output: Detailed log messages indicating each step of the query execution.

- **Database Integrity Check**:
  ```bash
  codeql database analyze --rerun --format=sarif-latest my-js-db query.ql
  ```
  - Expected Output: SARIF file with full analysis results, useful for identifying database creation issues.

### Configuration Options and Their Effects:

- `codeQL.executablePath`: Specifies the path to the CodeQL CLI executable. Default is typically installed in `/usr/local/bin/codeql`.
- `codeQL.databasePath`: Specifies the default path for CodeQL databases. Can be overridden per project.
- CLI flag `--log=debug`: Enables detailed logging to help troubleshoot query execution issues.

### Best Practice Implementation Code Example:

```bash
# Step 1: Create a database
codeql database create my-db --language=javascript

# Step 2: Run a query in verbose mode
codeql query run --database=my-db my-query.ql --log=debug

# Step 3: Analyze the SARIF output
codeql database analyze my-db my-query.ql --format=sarif-latest > results.sarif
```

This complete API and SDK guide includes all parameters, return types, code examples, configuration options, and step-by-step troubleshooting procedures to directly support developers in using CodeQL for code analysis.


## Original Source
CodeQL Documentation
https://codeql.github.com/docs/

## Digest of CODEQL_DOC

# CodeQL Documentation Overview

**Retrieved on:** 2023-10-07

**Data Size:** 21290805 bytes
**Links Found:** 23708

# CodeQL Release Information

- CodeQL 2.21.0 (2025-04-03)
- CodeQL 2.20.7 (2025-03-18)
- CodeQL 2.20.6 (2025-03-06)
- CodeQL 2.20.5 (2025-02-20)
- CodeQL 2.20.4 (2025-02-06)
- ... (additional releases detailed in the original crawl)

# Supported Languages and Frameworks

The documentation specifies detailed support including:

## Languages and Compilers

- **C/C++**: Variants - C89, C99, C11, C17, C++98, C++03, C++11, C++14, C++17, C++20; Supported compilers include Clang (clang-cl and armclang), GNU (GCC up to 13.2), and Microsoft extensions (VS 2022). File extensions: `.cpp, .c++, .cxx, .hpp, .hh, .h++, .hxx, .c, .cc, .h`
- **C#**: Up to version 13; Supported on Microsoft Visual Studio (up to 2019), .NET (4.8, Core 3.1, .NET 5/6/7/8/9). File extensions: `.sln, .csproj, .cs, .cshtml, .xaml`
- **GitHub Actions**: YAML configuration files located in `.github/workflows/*.yml` or `**/action.yml`
- **Go**: Versions up to 1.24; File extension: `.go`
- **Java**: Java 7 to 24, compiled via javac or ECJ. File extension: `.java`
- **Kotlin**: Versions 1.5.0 to 2.1.2x; File extension: `.kt`
- **JavaScript & TypeScript**: ECMAScript 2022 or lower; File extensions include `.js, .jsx, .mjs, .ts, .tsx`
- **Python**: Versions 2.7, 3.5 to 3.13; File extension: `.py`
- **Ruby**: Up to version 3.3; File extension: `.rb, .erb, .gemspec, Gemfile`
- **Swift**: Swift 5.4-6.0; File extension: `.swift`

## Frameworks and Libraries

Each language has support for numerous frameworks and libraries. For example:

- **Java and Kotlin**: Apache Commons, Hibernate, Jackson, Spring MVC, etc.
- **JavaScript and TypeScript**: Angular, React, Express, Nest.js, etc.
- **Python**: Django, Flask, FastAPI, Tornado, etc.
- **C#**: ASP.NET, EntityFramework, Json.NET, etc.

# Writing CodeQL Queries

- **CodeQL Queries**: Queries are written in the QL language to analyze codebases, detect vulnerabilities, and perform variant analysis.
- **Tutorials and Guides**: Step-by-step tutorials are provided for learning QL basics, writing queries, and applying best practices.

# QL Language Reference

The QL language includes detailed technical constructs:

- **Predicates**: Describe logical relations in QL programs.
- **Queries**: Evaluate to sets of results with precise type checking since QL is statically typed.
- **Modules and Signatures**: Organize code and provide parameter typing.
- **Expressions and Formulas**: Used for evaluating code paths and logical relationships.
- **Recursion and Annotations**: Support recursive predicates and additional meta-data via annotations.

# CodeQL Standard Libraries and Query Help

- **Standard Libraries**: Detailed listings for C/C++, C#, Go, Java, JavaScript, Python, Ruby, and Swift provide predicates, classes, and modules.
- **Query Help**: Each query comes with metadata, links to the repository, and vulnerability descriptions.

# CodeQL Change Logs

A historical record of releases is provided with dates and versions (e.g., CodeQL 2.21.0, CodeQL 2.20.x, etc.).

# Attribution

Content retrieved from: [CodeQL Documentation](https://codeql.github.com/docs/)



## Attribution
- Source: CodeQL Documentation
- URL: https://codeql.github.com/docs/
- License: License: Custom GitHub Docs License
- Crawl Date: 2025-04-17T16:40:51.834Z
- Data Size: 21290805 bytes
- Links Found: 23708

## Retrieved
2025-04-17
library/PRETTIER.md
==== Content of library/PRETTIER.md ====
# PRETTIER

## Crawl Summary
Prettier is an opinionated code formatter that parses source code into an AST and reprints it with a consistent style. It supports multiple languages (JavaScript, TypeScript, HTML, CSS, etc.) and offers configurable options like trailing commas, object wrapping, and experimental formatting for ternaries and operators. The formatting algorithm considers maximum line length and wraps lines as needed. CLI usage and Node.js API examples demonstrate how to format code, check versions, and integrate Prettier into workflows.

## Normalised Extract
## Table of Contents
1. Overview and Supported Languages
2. Formatting Algorithm
3. Configuration Options
4. CLI Usage and API Integration
5. Troubleshooting

---

### 1. Overview and Supported Languages
- **Languages:** JavaScript, JSX, Angular, Vue, Flow, TypeScript, CSS, Less, SCSS, HTML, Ember/Handlebars, JSON, GraphQL, Markdown (GFM, MDX v1), YAML.
- **Key Behavior:** Removes original styling (except necessary cases like empty lines) and reprints code based on AST.

### 2. Formatting Algorithm
- **Process:**
  a. Parse original source code into an AST.
  b. Reprint code based on rules (e.g., maximum line length).
- **Example:**
  - Input: `foo(arg1, arg2, arg3, arg4);`
  - Output when wrapping needed:
    ```
    foo(
      reallyLongArg(),
      omgSoManyParameters(),
      IShouldRefactorThis(),
      isThereSeriouslyAnotherOne(),
    );
    ```

### 3. Configuration Options
- **--trailing-comma:** Values: "none", "es5", "all" (default changed to "all").
- **--object-wrap:** (New in version 3.5) Controls object formatting.
- **--experimental-operator-position:** Enables experimental positioning of operators.
- **--experimental-ternaries:** Flag for indenting nested ternaries for better clarity.
- **TypeScript Config Support:** Reads TS configuration for formatting adaptation.

### 4. CLI Usage and API Integration
- **CLI Command:**
  ```bash
  prettier --write "src/**/*.js"
  ```
- **Node.js API Example:**
  ```javascript
  const prettier = require("prettier");

  const code = "function foo(a, b){ return a + b; }";
  const options = {
    parser: "babel",
    trailingComma: "all",
    tabWidth: 2,
    useTabs: false
  };

  const formatted = prettier.format(code, options);
  console.log(formatted);
  ```
- **Method Signature:**
  `prettier.format(source: string, options?: PrettierOptions): string`

### 5. Troubleshooting
- **Syntax Errors:** Ensure valid source code and correct parser. Use:
  ```bash
  prettier --check "src/**/*.js"
  ```
- **Performance:** Use caching options:
  ```bash
  prettier --write --cache "src/**/*.js"
  ```
- **Editor Integration:** Verify that editor plugins/extensions are installed and configured.


## Supplementary Details
### Detailed Configuration and Implementation Steps

1. **Configuration File (.prettierrc):**
   Example JSON configuration:
   ```json
   {
     "trailingComma": "all",
     "tabWidth": 2,
     "useTabs": false,
     "printWidth": 80,
     "objectWrap": "preserve",
     "experimentalOperatorPosition": true,
     "experimentalTernaries": true
   }
   ```

2. **Using Prettier in the CLI:**
   - Install Prettier globally or as a dev dependency:
     ```bash
     npm install --save-dev prettier
     ```
   - Format code:
     ```bash
     prettier --write "src/**/*.js"
     ```
   - Check for formatting issues without modifying files:
     ```bash
     prettier --check "src/**/*.js"
     ```

3. **Integrating with Node.js:**
   - Import the library:
     ```javascript
     const prettier = require("prettier");
     ```
   - Format code with options:
     ```javascript
     const formattedCode = prettier.format("const x=1;", {
       parser: "babel",
       trailingComma: "all",
       tabWidth: 2
     });
     console.log(formattedCode);
     ```

4. **Plugin Integration:**
   - For ECMAScript Modules support and asynchronous parsers, ensure your project uses the updated plugin API. Refer to the migration guide for plugin developers when updating from CommonJS usage.

5. **Best Practices:**
   - Always add Prettier as part of your pre-commit hooks to enforce code style automatically using tools like lint-staged.
   - Combine Prettier with ESLint: Use Prettier for formatting and ESLint for code quality checks.

6. **Troubleshooting Commands:**
   - To view Prettier version:
     ```bash
     prettier --version
     ```
   - To diagnose cache issues, specify cache location explicitly:
     ```bash
     prettier --write --cache-location ./node_modules/.cache/prettier "src/**/*.js"
     ```
   - If encountering parsing problems, test with a minimal file and adjust the parser option accordingly.


## Reference Details
### Complete API Specifications and Code Examples

1. **Prettier Node.js API**
   - **Method:** `prettier.format(source: string, options?: PrettierOptions): string`
   - **Parameters:**
     - `source`: The string of code to format.
     - `options`: An object conforming to the interface:
       ```typescript
       interface PrettierOptions {
         parser: string;             // e.g., 'babel', 'typescript', 'css', 'html'
         trailingComma?: 'none' | 'es5' | 'all'; // Default: 'all' in v3.0+
         tabWidth?: number;          // e.g., 2
         useTabs?: boolean;          // false for spaces
         printWidth?: number;        // Maximum line length, default 80
         objectWrap?: 'preserve' | 'force'; // New in v3.5
         experimentalOperatorPosition?: boolean; // Flag for experimental formatting
         experimentalTernaries?: boolean; // Flag for ternary formatting
       }
       ```
   - **Return:** A formatted string based on the provided options.
   - **Exceptions:** Throws syntax errors when source cannot be parsed.

2. **CLI Usage**
   - **Command:**
     ```bash
     prettier --write "src/**/*.js"
     ```
   - **Options:**
     - `--check`: Checks files without writing changes.
     - `--cache` and `--cache-location`: Improves performance on large codebases.

3. **Full Code Example with Comments (Node.js):**
   ```javascript
   // Import the Prettier library
   const prettier = require("prettier");

   // Define source code to format
   const code = "function add(a, b) { return a+b; }";

   // Define formatting options (following PrettierOptions interface)
   const options = {
     parser: "babel",
     trailingComma: "all",
     tabWidth: 2,
     useTabs: false,
     printWidth: 80,
     objectWrap: "preserve",
     experimentalOperatorPosition: true,
     experimentalTernaries: true
   };

   try {
     // Format the code
     const formatted = prettier.format(code, options);
     console.log('Formatted Code:\n', formatted);
   } catch (error) {
     // Output detailed error if formatting fails
     console.error('Formatting error:', error.message);
   }
   ```

4. **Configuration Options and Their Effects**
   - `trailingComma`:
     - "none": No trailing commas are added.
     - "es5": Trailing commas are added where valid in ES5 (objects, arrays, etc.).
     - "all": Trailing commas are added for all multiline structures.
   - `tabWidth`: Number of spaces per indentation level (default is 2).
   - `printWidth`: Defines maximum line length; lines longer than this will be wrapped.
   - `objectWrap`: Controls whether objects maintain original wrapping or are force-wrapped.
   - Experimental flags (`experimentalOperatorPosition`, `experimentalTernaries`): Enable non-default formatting patterns for operators and nested ternary expressions respectively.

5. **Detailed Troubleshooting Procedure**
   - **Step 1:** Run Prettier in check mode to identify unformatted files:
     ```bash
     prettier --check "src/**/*.js"
     ```
     *Expected Output:* List of files that do not conform to the style.
   - **Step 2:** If a syntax error occurs, verify that the correct parser is set and that the source code is valid. For instance, if using TypeScript, ensure `parser: "typescript"` is supplied.
   - **Step 3:** For performance issues with large projects, use caching:
     ```bash
     prettier --write --cache --cache-location ./node_modules/.cache/prettier "src/**/*.js"
     ```
     *Expected Output:* Reduced formatting time on repeated runs.
   - **Step 4:** For editor integration problems, confirm that your Prettier plugin is updated and that the configuration file (.prettierrc) is correctly formatted.
   - **Step 5:** Consult logs and error messages for precise failure points, and test with minimal input to isolate configuration issues.


## Original Source
Prettier Documentation
https://prettier.io/docs/en/index.html

## Digest of PRETTIER

# Prettier Documentation Digest

**Retrieved Date:** 2023-10-05

## Overview
Prettier is an opinionated code formatter that reprints code from its parsed AST. It supports numerous languages including:
- JavaScript (including experimental features)
- JSX
- Angular
- Vue
- Flow
- TypeScript
- CSS, Less, and SCSS
- HTML
- Ember/Handlebars
- JSON
- GraphQL
- Markdown (including GFM and MDX v1)
- YAML

Prettier strips (or preserves where practical) the original styling and outputs code with a consistent style. It respects configuration options such as maximum line length, and will reformat the code accordingly.

## Formatting Algorithm
When formatting code, Prettier:
- Parses the original source code into an AST.
- Reprints the code from the AST using its own rules, ignoring most original formatting.
- Adjusts formatting based on options like maximum line length, wrapping lists, or function call arguments.

### Example Transformation
Before Formatting:
```
foo(arg1, arg2, arg3, arg4);
```
After Formatting (if line exceeds limit):
```
foo(
  reallyLongArg(),
  omgSoManyParameters(),
  IShouldRefactorThis(),
  isThereSeriouslyAnotherOne(),
);
```

## Configuration Options
Prettier comes with several configuration options that influence formatting:

- **--trailing-comma**: Determines trailing comma usage. Valid values: `"none"`, `"es5"`, `"all"`. In Prettier 3.0, the default was changed to `"all"`.
- **--object-wrap**: New option in Prettier 3.5 to control wrapping of objects.
- **--experimental-operator-position**: Experimental flag to control operator position in formatted code.
- **--experimental-ternaries**: Enables an alternative formatting style for nested ternaries, adding indentation for better readability.
- **TypeScript Configuration File Support**: Extended support for reading TS config files to adjust formatting.

## CLI Commands and Usage Examples

**Basic CLI usage:**
```
prettier --write "src/**/*.js"
```

**Node.js API Usage Example:**
```javascript
const prettier = require("prettier");

const sourceCode = "function foo(arg1, arg2){return arg1+arg2;}";
const options = {
  parser: "babel", // Specify the parser (e.g., babel, typescript, css, html, etc.)
  trailingComma: "all", // Options: "none", "es5", "all"
  tabWidth: 2,
  useTabs: false
};

const formattedCode = prettier.format(sourceCode, options);
console.log(formattedCode);
```

**Version Check Command:**
```
prettier --version
```

## API Specifications and Integration

For integration as a library:

**Method Signature:**

Function: `prettier.format(source: string, options?: PrettierOptions): string`

Where `PrettierOptions` may include:
- `parser`: string (e.g., "babel", "typescript", "css", etc.)
- `trailingComma`: "none" | "es5" | "all"
- `tabWidth`: number
- `useTabs`: boolean
- `printWidth`: number (max line length, default 80)
- Additional flags such as `objectWrap`, `experimentalOperatorPosition`, and `experimentalTernaries` may be included.

**Error Handling:**
- Throws syntax errors if the input source cannot be parsed.

## Troubleshooting Procedures

1. **Parsing Errors**: If Prettier throws a syntax error, ensure that the source code is valid and that the correct parser is specified.

   **Command Example:**
   ```bash
   prettier --check "src/**/*.js"
   ```
   Expected output: A list of files with formatting issues or errors.

2. **Configuration Issues**: Validate your configuration file (e.g., `.prettierrc`) using JSON schema validators to ensure options are correct.

3. **Performance Concerns**: For large codebases, use the `--cache` and `--cache-location` options to improve performance.

   **Command Example:**
   ```bash
   prettier --write --cache "src/**/*.js"
   ```

4. **Integration**: When integrating with editors (VS Code, Vim, etc.), ensure that the correct extensions/plugins are installed and configured for on-save formatting.

## Attribution and Data Size

- **Data Size:** 1031862 bytes
- **Links Found:** 2567
- **Source URL:** https://prettier.io/docs/en/index.html



## Attribution
- Source: Prettier Documentation
- URL: https://prettier.io/docs/en/index.html
- License: License: MIT
- Crawl Date: 2025-04-17T16:25:28.865Z
- Data Size: 1031862 bytes
- Links Found: 2567

## Retrieved
2025-04-17
library/TEKTON_PIPELINES.md
==== Content of library/TEKTON_PIPELINES.md ====
# TEKTON_PIPELINES

## Crawl Summary
Tekton Pipelines provides Kubernetes CRDs for Tasks, TaskRuns, Pipelines, and PipelineRuns. The installation process involves applying a YAML from the Tekton release URL and monitoring pod status. Detailed YAML examples for creating Tasks, TaskRuns, Pipelines, and PipelineRuns are provided, along with commands for deploying and verifying the setup on a minikube Kubernetes cluster. Troubleshooting commands and cluster cleanup procedures are also specified.

## Normalised Extract
## Table of Contents
1. Tekton Pipelines Entities
   - Task
   - TaskRun
   - Pipeline
   - PipelineRun
2. Installation & Setup
   - Creating a Kubernetes Cluster (minikube)
   - Installing Tekton Pipelines
3. Task Execution Example
   - YAML definition for Task
   - YAML definition for TaskRun
   - Commands to apply and verify
4. Pipeline Execution Example
   - YAML definition for goodbye Task
   - Pipeline YAML and PipelineRun YAML
   - Deployment commands and log verification
5. Troubleshooting & Cleanup
   - Cluster troubleshooting commands
   - minikube deletion steps

## 1. Tekton Pipelines Entities
**Task**: Defines sequential steps in a pod.
*YAML:*
```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: hello
spec:
  steps:
    - name: echo
      image: alpine
      script: |
        #!/bin/sh
        echo "Hello World"
```

**TaskRun**: Instantiates Task execution.
*YAML:*
```yaml
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: hello-task-run
spec:
  taskRef:
    name: hello
```

**Pipeline**: Groups Tasks in a defined order.
*YAML:*
```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: hello-goodbye
spec:
  params:
  - name: username
    type: string
  tasks:
    - name: hello
      taskRef:
        name: hello
    - name: goodbye
      runAfter:
        - hello
      taskRef:
        name: goodbye
      params:
      - name: username
        value: $(params.username)
```

**PipelineRun**: Executes the Pipeline with parameter values.
*YAML:*
```yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: hello-goodbye-run
spec:
  pipelineRef:
    name: hello-goodbye
  params:
  - name: username
    value: "Tekton"
```

## 2. Installation & Setup
**Kubernetes Cluster with minikube:**
```bash
minikube start --kubernetes-version v1.30.2
kubectl cluster-info
```

**Install Tekton Pipelines:**
```bash
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
kubectl get pods --namespace tekton-pipelines --watch
```

## 3. Task Execution Example
- Create a file `hello-world.yaml` for Task; apply using:
```bash
kubectl apply --filename hello-world.yaml
```
- Create `hello-world-run.yaml` for TaskRun; apply using:
```bash
kubectl apply --filename hello-world-run.yaml
```
- Verify TaskRun status:
```bash
kubectl get taskrun hello-task-run
kubectl logs --selector=tekton.dev/taskRun=hello-task-run
```

## 4. Pipeline Execution Example
- Create a file `goodbye-world.yaml` with parameterized Task:
```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: goodbye
spec:
  params:
  - name: username
    type: string
  steps:
    - name: goodbye
      image: ubuntu
      script: |
        #!/bin/bash
        echo "Goodbye $(params.username)!"
```
- Create and apply `hello-goodbye-pipeline.yaml` and `hello-goodbye-pipeline-run.yaml` as shown in the digest.
- Verify logs using Tekton CLI:
```bash
tkn pipelinerun logs hello-goodbye-run -f -n default
```

## 5. Troubleshooting & Cleanup
- Dump cluster info for debugging:
```bash
kubectl cluster-info dump
```
- Delete minikube cluster:
```bash
minikube delete
```


## Supplementary Details
### Supplementary Technical Specifications
- **Kubernetes Version:** v1.30.2 is used in minikube command.
- **Tekton Pipelines Installation URL:** https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
- **Pod Status Monitoring:** Ensure pods in the tekton-pipelines namespace reach '1/1 Ready'.
- **Parameter Passing:** Pipeline YAML passes a parameter 'username' to the Task 'goodbye'.
- **CLI Verification:** Use 'tkn pipelinerun logs <run-name> -f -n default' for real-time log viewing.
- **Cluster Troubleshooting:** Use 'kubectl cluster-info dump' to diagnose issues.
- **Cleanup Commands:** 'minikube delete' removes the cluster and associated Docker containers.
- **Coding Best Practices:** Each YAML file should have correct indentation and API version supported by Tekton (v1beta1).


## Reference Details
### Complete API Specifications & SDK Method Signatures

#### Task API
- **API Version:** tekton.dev/v1beta1
- **Kind:** Task
- **Required Fields:** metadata.name, spec.steps
- **Step Object:** 
  - name: string
  - image: string
  - script: string (shell script content)

#### TaskRun API
- **API Version:** tekton.dev/v1beta1
- **Kind:** TaskRun
- **Required Fields:** metadata.name, spec.taskRef.name

#### Pipeline API
- **API Version:** tekton.dev/v1beta1
- **Kind:** Pipeline
- **Required Fields:** metadata.name, spec.params, spec.tasks
- **Task in Pipeline:** 
  - name: string
  - taskRef: { name: string }
  - runAfter: list of task names (optional)
  - params: list of parameter assignments (name and value)

#### PipelineRun API
- **API Version:** tekton.dev/v1beta1
- **Kind:** PipelineRun
- **Required Fields:** metadata.name, spec.pipelineRef.name, spec.params

### Code Examples

#### Example: Creating a Task
```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: hello
spec:
  steps:
    - name: echo
      image: alpine
      script: |
        #!/bin/sh
        echo "Hello World"
```

#### Example: Executing a TaskRun
```yaml
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: hello-task-run
spec:
  taskRef:
    name: hello
```

#### Example: Pipeline with Parameter Passing
```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: hello-goodbye
spec:
  params:
  - name: username
    type: string
  tasks:
    - name: hello
      taskRef:
        name: hello
    - name: goodbye
      runAfter:
        - hello
      taskRef:
        name: goodbye
      params:
      - name: username
        value: $(params.username)
```

#### Example: PipelineRun to Trigger Pipeline
```yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: hello-goodbye-run
spec:
  pipelineRef:
    name: hello-goodbye
  params:
  - name: username
    value: "Tekton"
```

### Implementation Patterns & Best Practices
- **YAML Validation:** Ensure proper indentation and API version consistency.
- **Parameterization:** Use parameters in Pipeline and Task definitions to enhance reusability.
- **CLI Usage:** Leverage 'kubectl' for applying configurations and 'tkn' for log tracing.
- **Troubleshooting Steps:** 
  1. Check pod status with `kubectl get pods --namespace tekton-pipelines`.
  2. Inspect logs using `kubectl logs --selector=tekton.dev/taskRun=<name>`.
  3. Run `kubectl cluster-info dump` to diagnose cluster issues.

### Specific Configuration Options
- **minikube Start Options:** 
  - `--kubernetes-version v1.30.2` ensures compatibility.
  - Ensure allocated resources (CPUs, Memory) match cluster requirements.
- **Tekton Pipelines Release YAML:** hosted at https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml

### Detailed Troubleshooting Commands
- **View Cluster Info:**
  ```bash
  kubectl cluster-info
  ```
- **Monitor Pod Readiness:**
  ```bash
  kubectl get pods --namespace tekton-pipelines --watch
  ```
- **Delete Cluster:**
  ```bash
  minikube delete
  ```

This reference provides the full, exact technical specifications required for setting up and managing Tekton Pipelines in a Kubernetes environment.

## Original Source
Tekton Pipelines Documentation
https://tekton.dev/docs/pipelines/

## Digest of TEKTON_PIPELINES

# Tekton Pipelines Documentation

**Retrieved Date:** 2023-10-30

## Overview
Tekton Pipelines is a Kubernetes extension that provides a set of Custom Resources to construct CI/CD workflows. It is available via kubectl and API calls, and integrates with Kubernetes native functionality.

## Tekton Entities

### Task
- **Definition:** A Task is a series of steps executed sequentially in a pod.
- **YAML Example:**

```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: hello
spec:
  steps:
    - name: echo
      image: alpine
      script: |
        #!/bin/sh
        echo "Hello World"
```

### TaskRun
- **Definition:** Instantiates and executes a Task with specific parameters.
- **YAML Example:**

```yaml
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: hello-task-run
spec:
  taskRef:
    name: hello
```

### Pipeline
- **Definition:** A collection of Tasks executed in a specified order.
- **YAML Example:**

```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: hello-goodbye
spec:
  params:
  - name: username
    type: string
  tasks:
    - name: hello
      taskRef:
        name: hello
    - name: goodbye
      runAfter:
        - hello
      taskRef:
        name: goodbye
      params:
      - name: username
        value: $(params.username)
```

### PipelineRun
- **Definition:** Instantiates and executes a Pipeline with provided parameters.
- **YAML Example:**

```yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: hello-goodbye-run
spec:
  pipelineRef:
    name: hello-goodbye
  params:
  - name: username
    value: "Tekton"
```

## Installation & Setup

### Creating a Kubernetes Cluster with minikube

- **Command:**

```bash
minikube start --kubernetes-version v1.30.2
```

- **Verification:**

```bash
kubectl cluster-info
```

### Installing Tekton Pipelines

- **Command:**

```bash
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
```

- **Monitor Installation:**

```bash
kubectl get pods --namespace tekton-pipelines --watch
```

## Task and Pipeline Execution Examples

### Create and Run a Task
1. Create file `hello-world.yaml`:

```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: hello
spec:
  steps:
    - name: echo
      image: alpine
      script: |
        #!/bin/sh
        echo "Hello World"
```

2. Apply Task:

```bash
kubectl apply --filename hello-world.yaml
```

3. Create file `hello-world-run.yaml`:

```yaml
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: hello-task-run
spec:
  taskRef:
    name: hello
```

4. Launch TaskRun:

```bash
kubectl apply --filename hello-world-run.yaml
```

5. Check TaskRun status:

```bash
kubectl get taskrun hello-task-run
```

6. Verify Logs:

```bash
kubectl logs --selector=tekton.dev/taskRun=hello-task-run
```

### Create and Run a Pipeline
1. Create file `goodbye-world.yaml`:

```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: goodbye
spec:
  params:
  - name: username
    type: string
  steps:
    - name: goodbye
      image: ubuntu
      script: |
        #!/bin/bash
        echo "Goodbye $(params.username)!"
```

2. Apply the Task:

```bash
kubectl apply --filename goodbye-world.yaml
```

3. Create file `hello-goodbye-pipeline.yaml`:

```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: hello-goodbye
spec:
  params:
  - name: username
    type: string
  tasks:
    - name: hello
      taskRef:
        name: hello
    - name: goodbye
      runAfter:
        - hello
      taskRef:
        name: goodbye
      params:
      - name: username
        value: $(params.username)
```

4. Apply the Pipeline:

```bash
kubectl apply --filename hello-goodbye-pipeline.yaml
```

5. Create file `hello-goodbye-pipeline-run.yaml`:

```yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: hello-goodbye-run
spec:
  pipelineRef:
    name: hello-goodbye
  params:
  - name: username
    value: "Tekton"
```

6. Start the PipelineRun:

```bash
kubectl apply --filename hello-goodbye-pipeline-run.yaml
```

7. View Pipeline logs using Tekton CLI:

```bash
tkn pipelinerun logs hello-goodbye-run -f -n default
```

## Troubleshooting & Cleanup

### Cluster Troubleshooting
- **Command:**

```bash
kubectl cluster-info dump
```

### Deleting the minikube Cluster

```bash
minikube delete
```

Expected Output:
- Confirmation messages indicating deletion of the cluster and its resources.


## Attribution
- Source: Tekton Pipelines Documentation
- URL: https://tekton.dev/docs/pipelines/
- License: License: Apache License 2.0
- Crawl Date: 2025-04-17T14:43:59.198Z
- Data Size: 887477 bytes
- Links Found: 8424

## Retrieved
2025-04-17
library/AWS_SDK.md
==== Content of library/AWS_SDK.md ====
# AWS_SDK

## Crawl Summary
The crawled content from https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/ is minimal but indicates the core layout for the AWS SDK for JavaScript v3. The document includes modular client initialization, explicit API method signatures such as ListBucketsCommand with its input and output types, and practical code examples demonstrating client construction, command execution, and error handling. Key configuration options like region and credentials are clearly specified.

## Normalised Extract
# Table of Contents
1. Client Initialization
   - Detailed code samples for creating S3 and DynamoDB clients.
2. API Methods and Signatures
   - ListBucketsCommand: Method signature, input/output types, and usage examples.
3. Configuration Options
   - Options including region and credentials; examples provided with exact parameter values.
4. Exception Handling and Best Practices
   - Step-by-step error handling with try-catch, logging practices, and retry strategy configuration.

## 1. Client Initialization
- S3 Client Example:

```javascript
const { S3Client, ListBucketsCommand } = require('@aws-sdk/client-s3');
const client = new S3Client({
  region: 'us-west-2',
  credentials: {
    accessKeyId: 'YOUR_ACCESS_KEY_ID',
    secretAccessKey: 'YOUR_SECRET_ACCESS_KEY'
  }
});
```

## 2. API Methods and Signatures
- **ListBucketsCommand**
  - Signature: `ListBucketsCommand(input: ListBucketsCommandInput) => Promise<ListBucketsCommandOutput>`
  - Input: Typically `{}` for listing all buckets in S3.
  - Output: Returns an object containing `Buckets` (array) and `Owner` information.

## 3. Configuration Options
- **region**: string – Example: `'us-west-2'`.
- **credentials**: object – Must include `accessKeyId` and `secretAccessKey` (e.g., `{ accessKeyId: 'XXX', secretAccessKey: 'YYY' }`).
- **retryStrategy**: Optional object to specify custom retry behaviors.

## 4. Exception Handling and Best Practices
- Always use try-catch when invoking `client.send(new Command(...))`.
- Log error details for debugging:

```javascript
try {
  const data = await client.send(new ListBucketsCommand({}));
  console.log(data.Buckets);
} catch (error) {
  console.error('Error occurred:', error);
}
```


## Supplementary Details
## Supplementary Technical Specifications

### Detailed Parameter Values and Configuration
- **region**: Provide the AWS region as a string (e.g., 'us-west-2').
- **credentials**: An object with keys:
   - accessKeyId (string): Your AWS access key.
   - secretAccessKey (string): Your AWS secret key.
- **Optional Retry Strategy**:
   - Parameter: retryStrategy
   - Type: Object (e.g., customRetryStrategy with properties like maxAttempts, delayDecider callback).

### Implementation Steps
1. Install the AWS SDK module: `npm install @aws-sdk/client-s3`.
2. Import required modules using destructuring.
3. Initialize the client with explicit configuration (region, credentials).
4. Create command objects corresponding to AWS operations (e.g., ListBucketsCommand).
5. Use the `send` method of the client to execute the command and receive a promise.
6. Implement error handling using try-catch blocks.

### Code Example with Comments

```javascript
// Import the S3 client and the ListBuckets command
const { S3Client, ListBucketsCommand } = require('@aws-sdk/client-s3');

// Initialize the S3 client with necessary configuration
const client = new S3Client({
  region: 'us-west-2', // AWS region
  credentials: {
    accessKeyId: 'YOUR_ACCESS_KEY_ID',
    secretAccessKey: 'YOUR_SECRET_ACCESS_KEY'
  }
});

// Asynchronous function to list all buckets
const listBuckets = async () => {
  try {
    // Execute the ListBucketsCommand with an empty input
    const output = await client.send(new ListBucketsCommand({}));
    console.log('Bucket List:', output.Buckets);
  } catch (error) {
    // Log error details for troubleshooting
    console.error('Error listing buckets:', error);
  }
};

// Execute the function
listBuckets();
```


## Reference Details
## Complete API Specifications and Implementation Details

### AWS SDK v3 Client Construction API

- **Constructor Signature**:
  - S3Client(config: S3ClientConfig)
  - Where `S3ClientConfig` is defined as:
    ```typescript
    interface S3ClientConfig {
      region: string; // AWS region (e.g., 'us-west-2')
      credentials: {
        accessKeyId: string;
        secretAccessKey: string;
      };
      retryStrategy?: {
        maxAttempts?: number; // Default: SDK-specific
        delayDecider?: (attempt: number) => number; // Custom delay function in ms
      };
    }
    ```

### ListBucketsCommand API

- **Method Signature**:
  - ```typescript
    class ListBucketsCommand {
      constructor(input: ListBucketsCommandInput);
    }
    
    // Input type:
    interface ListBucketsCommandInput {}
    
    // Output type:
    interface ListBucketsCommandOutput {
      Buckets: Array<{
        Name: string;
        CreationDate: Date;
      }>;
      Owner: {
        DisplayName: string;
        ID: string;
      };
    }
    ```

### Code Example for S3 Bucket Listing

```javascript
const { S3Client, ListBucketsCommand } = require('@aws-sdk/client-s3');

// Client configuration with explicit region and credentials
const client = new S3Client({
  region: 'us-west-2',
  credentials: {
    accessKeyId: 'YOUR_ACCESS_KEY_ID',
    secretAccessKey: 'YOUR_SECRET_ACCESS_KEY'
  },
  retryStrategy: {
    maxAttempts: 3,
    delayDecider: (attempt) => 100 * attempt
  }
});

// Create and send the ListBuckets command
async function listAllBuckets() {
  try {
    const command = new ListBucketsCommand({});
    const result = await client.send(command);
    console.log('Buckets:', result.Buckets);
  } catch (error) {
    console.error('Error occurred:', error);
  }
}

listAllBuckets();
```

### Best Practices

1. Use modular imports to reduce bundle size.
2. Always validate AWS credentials and region settings.
3. Implement robust error handling using try-catch blocks along with logging.
4. Configure a custom retry strategy to improve resilience against transient network errors.

### Troubleshooting Procedures

- **Command Execution Failure**:
  1. Check the provided AWS credentials for accuracy.
  2. Verify that the specified region matches your resources.
  3. Use logging to inspect the error object returned by the SDK.

- **Common Commands**:
  - Test connection with a simple list command as shown in the examples.
  - Use AWS CloudTrail and SDK debug logging by setting the environment variable `AWS_SDK_LOG_LEVEL=debug` for more detailed output.

### Detailed SDK Method Signatures

- **S3Client.send() Method**
  - Signature:
    ```typescript
    send<InputType, OutputType>(command: {
      input: InputType
    }): Promise<OutputType>;
    ```
  - Throws exceptions of type `ServiceException` on failure.


## Original Source
AWS SDK for JavaScript v3 Documentation
https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/

## Digest of AWS_SDK

# AWS SDK for JavaScript v3 Documentation Digest
Retrieved Date: 2023-10-06

## Overview
The AWS SDK for JavaScript v3 provides a modular and lightweight toolset for interfacing with AWS services. It allows the use of individual service clients, enabling optimized bundling and maintenance of only the required components.

## Client Initialization
Example of S3 client initialization:

```javascript
const { S3Client, ListBucketsCommand } = require('@aws-sdk/client-s3');

const client = new S3Client({
  region: 'us-west-2', // Specify the AWS region
  credentials: {
    accessKeyId: 'YOUR_ACCESS_KEY_ID',
    secretAccessKey: 'YOUR_SECRET_ACCESS_KEY'
  }
});
```

## API Methods and Signatures
### ListBucketsCommand
- **Signature:** `ListBucketsCommand(input: ListBucketsCommandInput) => Promise<ListBucketsCommandOutput>`
- **Input Type:** `ListBucketsCommandInput` (typically an empty object for S3 ListBuckets)
- **Output Type:** `ListBucketsCommandOutput` (includes details such as Buckets array and Owner information)

Example usage:

```javascript
const run = async () => {
  try {
    const data = await client.send(new ListBucketsCommand({}));
    console.log('Bucket List:', data.Buckets);
  } catch (error) {
    console.error('Error listing buckets:', error);
  }
};

run();
```

## Configuration Options
- **region**: *(string)* AWS region (e.g., 'us-west-2'). No default; must be provided.
- **credentials**: *(object)* Must include `accessKeyId` and `secretAccessKey`. Optional token for temporary credentials supported as well.
- **retryStrategy**: *(object)* Custom retry strategy options may be provided to handle API throttling and retries.

## Exception Handling & Best Practices
- Use try-catch blocks to handle promise rejections.
- Log errors with clear messages for troubleshooting.
- Configure custom retry strategies if required by service limits.

## Additional Client Methods
Most clients follow a similar pattern: instantiate the client with required configuration, create a command, and use the `send` method to execute. 

Example command for another service (e.g., DynamoDB):

```javascript
const { DynamoDBClient, ListTablesCommand } = require('@aws-sdk/client-dynamodb');

const ddbClient = new DynamoDBClient({ region: 'us-east-1' });

const listTables = async () => {
  try {
    const response = await ddbClient.send(new ListTablesCommand({}));
    console.log('Tables:', response.TableNames);
  } catch (err) {
    console.error('Error:', err);
  }
};

listTables();
```


## Attribution
- Source: AWS SDK for JavaScript v3 Documentation
- URL: https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/
- License: License: Apache License 2.0
- Crawl Date: 2025-04-17T20:24:33.090Z
- Data Size: 40 bytes
- Links Found: 1

## Retrieved
2025-04-17
library/VITEST.md
==== Content of library/VITEST.md ====
# VITEST

## Crawl Summary
Vitest is a next generation testing framework powered by Vite, supporting ESM, TypeScript, and JSX. Key technical details include installation methods (npm, yarn, pnpm, bun), usage with npx, writing tests using Vitest's `expect` and `test` APIs, configuration through both Vite and dedicated vitest.config files, workspace configurations for monorepos, and a rich CLI with options such as --config, --coverage, --watch, etc. Detailed configuration options include spec definitions for test file inclusion/exclusion, server configurations (sourcemap, debug, deps), dependency optimization parameters, environment settings, and pool options for running tests.

## Normalised Extract
## Table of Contents
1. Installation
2. Writing Tests
3. Configuration
   - Configuring via Vite and vitest.config
   - Merging Configurations
4. Workspaces
5. Command Line Interface
6. Automatic Dependency Installation
7. IDE Integrations
8. Detailed Configuration Options
9. Troubleshooting & Best Practices

## 1. Installation
- Use the following commands to install Vitest:
  - npm: `npm install -D vitest`
  - yarn: `yarn add -D vitest`
  - pnpm: `pnpm add -D vitest`
  - bun: `bun add -D vitest`

## 2. Writing Tests
- Create test files ending with `.test.js` or `.spec.js`.
- Example:

  sum.js:
  ```js
  export function sum(a, b) {
    return a + b;
  }
  ```

  sum.test.js:
  ```js
  import { expect, test } from 'vitest';
  import { sum } from './sum.js';

  test('adds 1 + 2 to equal 3', () => {
    expect(sum(1, 2)).toBe(3);
  });
  ```

- Add test script in package.json:
  ```json
  {
    "scripts": {
      "test": "vitest"
    }
  }
  ```

## 3. Configuration
### Configuring via Vite and vitest.config
- Vitest automatically reads `vite.config.*` files. For different configurations during tests, create a `vitest.config.ts`:

  ```ts
  import { defineConfig } from 'vitest/config';

  export default defineConfig({
    test: {
      // Test-specific options
    },
  });
  ```

- To use Vite's configuration, add a triple slash directive:

  ```ts
  /// <reference types="vitest/config" />
  import { defineConfig } from 'vite';

  export default defineConfig({
    test: {
      // ... Specify options here.
    },
  });
  ```

### Merging Configurations
- Merge Vite and Vitest configurations using `mergeConfig`:

  ```ts
  import { defineConfig, mergeConfig } from 'vitest/config';
  import viteConfig from './vite.config.mjs';

  export default mergeConfig(viteConfig, defineConfig({
    test: {
      // ...
    },
  }));
  ```

## 4. Workspaces
- Define a `vitest.workspace.ts` for multi-config projects:

  ```ts
  import { defineWorkspace } from 'vitest/config';

  export default defineWorkspace([
    'packages/*',
    'tests/*/vitest.config.{e2e,unit}.ts',
    {
      test: {
        name: 'happy-dom',
        root: './shared_tests',
        environment: 'happy-dom',
        setupFiles: ['./setup.happy-dom.ts'],
      },
    },
    {
      test: {
        name: 'node',
        root: './shared_tests',
        environment: 'node',
        setupFiles: ['./setup.node.ts'],
      },
    },
  ]);
  ```

## 5. Command Line Interface
- Basic commands:
  - `vitest` (watch mode)
  - `vitest run` (single execution)
  - Use flags like `--config`, `--coverage`, `--watch`, `--port`, `--https`.

- Example npm scripts:

  ```json
  {
    "scripts": {
      "test": "vitest",
      "coverage": "vitest run --coverage"
    }
  }
  ```

## 6. Automatic Dependency Installation
- Vitest checks and prompts for missing dependencies. Disable with:

  ```sh
  export VITEST_SKIP_INSTALL_CHECKS=1
  ```

## 7. IDE Integrations
- Official VS Code extension available in the VS Code Marketplace for enhanced experience.

## 8. Detailed Configuration Options
- **include**: string[] (Default: ["**/*.{test,spec}.?(c|m)[jt]s?(x)"])
- **exclude**: string[] (Default: ["**/node_modules/**", "**/dist/**", ...])
- **includeSource**: string[] (Default: [])
- **name**: string (Custom test project name)
- **server**: Object with options such as:
  - `sourcemap`: 'inline' | boolean (Default: 'inline')
  - `debug`: { dumpModules: boolean | string, loadDumppedModules: boolean }
  - `deps`: { external: (string|RegExp)[], inline: (string|RegExp)[] | true, fallbackCJS: boolean, cacheDir: string }
- **environment**: 'node' | 'jsdom' | 'happy-dom' | 'edge-runtime' | string (Default: 'node')
- **globals**: boolean (Default: false)
- **pool**: 'threads' | 'forks' | 'vmThreads' | 'vmForks' (Default: 'forks')
- **update**: boolean (Default: false) [Updates snapshot files]
- **watch**: boolean (Default: !process.env.CI)

## 9. Troubleshooting & Best Practices
- Running tests without watch: `vitest run`
- For configuration issues, pass `--config ./path/to/vitest.config.ts` to override defaults.
- If using Bun, use `bun run test` instead of `bun test`.
- Debug module transformation issues by enabling `server.debug.dumpModules` and reviewing the dumped files.


## Supplementary Details
### Technical Specifications and Implementation Details

1. **Installation and Setup**
   - Minimum requirements: Vite >= v5.0.0, Node >= v18.0.0.
   - Installation commands provided for npm, yarn, pnpm, bun.
   - Direct execution via npx: `npx vitest`.

2. **Writing Tests**
   - Tests file naming convention: must include `.test.` or `.spec.` in filenames.
   - Use of Vitest's core API:
     - `import { test, expect } from 'vitest';`
   - Example provided for function testing.

3. **Configuration Options**
   - Options set within the `test` property in config files:
     - `include`: Glob pattern for tests.
     - `exclude`: Default exclusion patterns: node_modules, dist, etc.
     - `server`: Detailed configuration for inline source maps, debugging, dependency resolution:
       > Example:
       > ```ts
       > server: {
       >   sourcemap: 'inline',
       >   debug: { dumpModules: true, loadDumppedModules: false },
       >   deps: { external: [/\/node_modules\//], inline: [], fallbackCJS: false, cacheDir: 'node_modules/.vite' }
       > }
       > ```
   - Environment configuration allowing override via file docblock comments (@vitest-environment, @jest-environment).

4. **Command Line Interface**
   - CLI scripts for watch mode and single run.
   - Use of flags for customization, e.g., `--config`, `--coverage`, `--port`.

5. **Workspaces Support**
   - Allow multiple configurations in a single project.
   - Use glob patterns or explicit configurations in `vitest.workspace.ts`.

6. **Dependency Optimization**
   - `deps` configuration: Externalizing node_modules, inlining modules, CJS fallback mechanisms.
   - Optimizer settings that mirror Vite's optimizeDeps.

7. **Troubleshooting Procedures**
   - Run `vitest --help` for full list of CLI options.
   - If encountering module resolution errors, check the `server.deps` settings.
   - For snapshot issues, use `--update` to refresh outdated snapshots.

8. **Best Practices**
   - Use a unified configuration file for both Vitest and Vite to avoid conflicts.
   - Merge configurations properly if using separate files.
   - Utilize environment-specific configurations for testing browser-like behavior via jsdom or happy-dom.
   - Include comprehensive CLI scripts in package.json for consistency.


## Reference Details
### Complete API Specifications and SDK Method Signatures

1. **Vitest Core APIs**
   - Test Declaration:
     ```ts
     test(name: string, fn: () => void | Promise<void>): void;
     ```
   - Expect API:
     ```ts
     expect<T>(value: T): Expect<T>;
     // Example assertion:
     expect(sum(1,2)).toBe(3);
     ```

2. **Configuration API (vitest/config)**
   - Import and define configuration:
     ```ts
     import { defineConfig } from 'vitest/config';

     export default defineConfig({
       test: {
         include: ['**/*.{test,spec}.?(c|m)[jt]s?(x)'],
         exclude: ['**/node_modules/**', '**/dist/**', '**/cypress/**', '**/.{idea,git,cache,output,temp}/**', '**/{karma,rollup,webpack,vite,vitest,jest,ava,babel,nyc,cypress,tsup,build,eslint,prettier}.config.*'],
         globals: false,
         environment: 'node',
         server: {
           sourcemap: 'inline',
           debug: {
             dumpModules: true,
             loadDumppedModules: false
           },
           deps: {
             external: [/\/node_modules\//],
             inline: [],
             fallbackCJS: false,
             cacheDir: 'node_modules/.vite'
           }
         },
         pool: 'forks',
         watch: !process.env.CI,
         update: false
       }
     });
     ```

3. **CLI Usage and Flags**
   - Default npm scripts:
     ```json
     {
       "scripts": {
         "test": "vitest",
         "coverage": "vitest run --coverage"
       }
     }
     ```
   - Common CLI commands:
     - `vitest` : Runs in watch mode
     - `vitest run` : Executes tests once
     - `vitest --help` : Lists all available options
     - `vitest --config ./path/to/vitest.config.ts` : Specifies a custom configuration file

4. **Code Examples and Best Practices**
   - **Test Example**:
     ```js
     // sum.js
     export function sum(a, b) {
       return a + b;
     }
     
     // sum.test.js
     import { test, expect } from 'vitest';
     import { sum } from './sum.js';
     
     test('adds 1 + 2 to equal 3', () => {
       expect(sum(1, 2)).toBe(3);
     });
     ```

   - **Configuration Merging Example**:
     ```ts
     import { defineConfig, mergeConfig } from 'vitest/config';
     import viteConfig from './vite.config.mjs';

     export default mergeConfig(viteConfig, defineConfig({
       test: {
         exclude: ['packages/template/*'],
       },
     }));
     ```

5. **Troubleshooting**
   - For dependency related errors, verify the settings in `server.deps` and adjust inline/external rules.
   - To update snapshots use:
     ```sh
     vitest --update
     ```
   - Debug transformation issues by enabling module dumps:
     ```ts
     server: {
       debug: { dumpModules: true }
     }
     ```

6. **Additional Notes on Environments**
   - Define environment in a test file using a docblock:
     ```js
     /**
      * @vitest-environment jsdom
      */
     test('DOM test', () => {
       const div = document.createElement('div');
       expect(div).not.toBeNull();
     });
     ```
   - For TypeScript, add to tsconfig.json:
     ```json
     {
       "compilerOptions": {
         "types": ["vitest/globals"]
       }
     }
     ```


## Original Source
Vitest Testing Framework Documentation
https://vitest.dev/

## Digest of VITEST

# Vitest Testing Framework Documentation

**Retrieved Date:** 2023-10-12

# Overview
Vitest is a Vite-native, next-generation testing framework. It provides out-of-the-box support for ESM, TypeScript and JSX through esbuild. Vitest can be used with Vite configuration files and supports both local and CLI usage. It allows for features like smart & instant watch mode, jest compatibility with built-in functions (expect, snapshot, coverage) and automatic dependency installation.

# Getting Started

## Installation
To add Vitest to your project:

```sh
# Using npm
npm install -D vitest

# Using yarn
yarn add -D vitest

# Using pnpm
pnpm add -D vitest

# Using bun
bun add -D vitest
```

*TIP: Vitest requires Vite >= v5.0.0 and Node >= v18.0.0.*

Additionally, you can run Vitest directly with:

```sh
npx vitest
```

## Writing Tests

Example to test a sum function:

*sum.js*
```js
export function sum(a, b) {
  return a + b;
}
```

*sum.test.js*
```js
import { expect, test } from 'vitest';
import { sum } from './sum.js';

test('adds 1 + 2 to equal 3', () => {
  expect(sum(1, 2)).toBe(3);
});
```

In your package.json, add the test script:

```json
{
  "scripts": {
    "test": "vitest"
  }
}
```

Run tests using your package manager:

```sh
npm run test
# or
yarn test
# or
pnpm test
```

# Configuring Vitest

Vitest leverages Vite's unified configuration. If a vite.config.[js|ts|mjs|cjs|mts|cts] file is present, Vitest will pick up the plugins and settings. For separate configuration during testing, you may create a `vitest.config.ts` file.

Example Vitest config:

```ts
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    // Specify test options here, e.g., include, exclude, watch
  },
});
```

Alternatively, if using an existing vite.config file, add a reference directive:

```ts
/// <reference types="vitest/config" />
import { defineConfig } from 'vite';

export default defineConfig({
  test: {
    // ... Specify options here.
  },
});
```

If you decide to keep separate Vite and Vitest config files, ensure to merge the Vite options using `mergeConfig`:

```ts
import { defineConfig, mergeConfig } from 'vitest/config';
import viteConfig from './vite.config.mjs';

export default mergeConfig(viteConfig, defineConfig({
  test: {
    // ... Vitest specific options
  },
}));
```

# Workspaces Support

Vitest supports workspaces to run different project configurations within a monorepo. Define a `vitest.workspace.ts` file with either glob patterns or explicit configurations.

Example:

```ts
import { defineWorkspace } from 'vitest/config';

export default defineWorkspace([
  'packages/*',
  'tests/*/vitest.config.{e2e,unit}.ts',
  {
    test: {
      name: 'happy-dom',
      root: './shared_tests',
      environment: 'happy-dom',
      setupFiles: ['./setup.happy-dom.ts'],
    }
  },
  {
    test: {
      name: 'node',
      root: './shared_tests',
      environment: 'node',
      setupFiles: ['./setup.node.ts'],
    }
  }
]);
```

# Command Line Interface (CLI)

You can invoke Vitest from the terminal using the vitest binary or via npx. Common CLI commands include:

- `vitest`: Start in watch mode.
- `vitest run`: run tests a single time.
- Additional options such as `--config`, `--coverage`, `--watch`, `--port`, `--https` are available.

Example npm scripts:

```json
{
  "scripts": {
    "test": "vitest",
    "coverage": "vitest run --coverage"
  }
}
```

Running without watch mode:

```sh
vitest run
```

# Automatic Dependency Installation

Vitest checks for required dependencies and will prompt installation if missing. You can disable this check with:

```sh
export VITEST_SKIP_INSTALL_CHECKS=1
```

# IDE Integrations

Vitest provides an official Visual Studio Code extension to enhance the testing workflow. Install it from the VS Code Marketplace.

# Detailed Configuration Options

Below is a summary of key configuration options available in Vitest. The configuration is provided under the `test` property in the config file:

| Option                      | Type                                                    | Default                                                 | Effect |
|-----------------------------|---------------------------------------------------------|---------------------------------------------------------|--------|
| include                     | string[]                                                | ['**/*.{test,spec}.?(c|m)[jt]s?(x)']                       | Glob patterns to include test files |
| exclude                     | string[]                                                | ['**/node_modules/**', '**/dist/**', ...]                | Glob patterns to exclude from tests |
| includeSource               | string[]                                                | []                                                      | In-source test globs |
| name                        | string                                                  | (none)                                                  | Custom project name |
| server                      | Object (sourcemap, debug, deps)                         | See below                                               | Vite-Node server options |
| environment                 | 'node' \| 'jsdom' \| 'happy-dom' \| 'edge-runtime' \| string | 'node'                                      | Defines test environment |
| globals                     | boolean                                                 | false                                                   | Enable Jest-like global APIs |
| pool                        | 'threads' \| 'forks' \| 'vmThreads' \| 'vmForks'      | 'forks'                                                 | Processes tests using different pooling systems |
| update                      | boolean                                                 | false                                                 | Updates snapshot files |
| watch                       | boolean                                                 | !process.env.CI                                        | Enable watch mode |

For server options, example:

```ts
server: {
  sourcemap: 'inline',
  debug: {
    dumpModules: true,
    loadDumppedModules: false,
  },
  deps: {
    external: [/\/node_modules\//],
    inline: [],
    fallbackCJS: false,
    cacheDir: 'node_modules/.vite'
  }
}
```

# Troubleshooting & Best Practices

- Use the `--config` flag to troubleshoot configuration issues:

  ```sh
  vitest --config ./path/to/vitest.config.ts
  ```

- Enable verbose logging if tests fail unexpectedly.
- If using Bun as your package manager, run tests with:

  ```sh
  bun run test
  ```

- For dependency issues, review the `deps` options and adjust `inline` and `external` settings as necessary.

- For debugging module transformation issues, review the `server.debug.dumpModules` settings to write the transformed modules to disk.

# Attribution

*Data Size during crawl: 39367577 bytes, Links Found: 25982, No Errors Found.*


## Attribution
- Source: Vitest Testing Framework Documentation
- URL: https://vitest.dev/
- License: License: MIT
- Crawl Date: 2025-04-17T19:31:16.705Z
- Data Size: 39367577 bytes
- Links Found: 25982

## Retrieved
2025-04-17
