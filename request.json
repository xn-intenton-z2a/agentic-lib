{
  "model": "o4-mini",
  "messages": [
    {
      "role": "system",
      "content": "You are a documentation updater that returns updated README and documentation file contents to ensure they accurately reflect the current state of the codebase. You can update multiple files by specifying their paths and contents in the updatedFiles object. Answer strictly with a JSON object that adheres to the provided function schema."
    },
    {
      "role": "user",
      "content": "\nYou are providing updates to the README.md file and other documentation files to ensure they accurately reflect the current state of the codebase and emphasize content that delivers substantial user value and addresses core implementation needs.\n\nThe README is the primary focus, but other documentation files can be updated as well if needed. Source files (srcFiles) and test files (testFiles) should NOT be updated.\n\nWhen updating the README:\n1. Preserve existing README content that delivers substantial user value, even if it describes features not yet implemented\n2. Update the README if it conflicts with current source code, tests, or documentation, prioritizing content that directly enhances the product's primary purpose\n3. If documentation files are out of date compared to the source code or tests, update them to be consistent, focusing on high-impact information that enables immediate application rather than superficial descriptions\n4. Ensure documentation clearly communicates the core functionality and value proposition of the product, prioritizing content that helps users solve real problems\n\nApply the contributing guidelines to your response and when suggesting enhancements consider the tone and direction of the contributing guidelines. Focus on documentation improvements that deliver measurable value to users rather than cosmetic changes or excessive detail on edge cases.\n\nYou may only change the files provided in the prompt context. You can update multiple files by specifying their paths and contents in the updatedFiles object. Each file will be checked against the allowedFilepathPatterns before being written.\n\nConsider the following when refining your response:\n  * Current feature names and specifications in the repository\n  * Source file content (for context only)\n  * Test file content (for context only)\n  * Documentation file content\n  * README file content\n  * MISSION file content\n  * Contributing file content\n  * Dependencies file content\n  * Dependency install output\n  * Issue details (if any)\n  * Build output\n  * Test output\n  * Main execution output\n  * Agent configuration file content\n\nCurrent feature names and specifications (for context, read only):\nCURRENT_FEATURES_START\nfind: ‘features/’: No such file or directory\nnone\nCURRENT_FEATURES_END\n\nSource files (for context only, DO NOT UPDATE):\nSOURCE_FILES_START\nFile: sandbox/source/server.js\nimport http from 'http';\nimport crypto from 'crypto';\nimport { URL } from 'url';\nimport { z } from 'zod';\nimport dotenv from 'dotenv';\nimport MarkdownIt from 'markdown-it';\nimport markdownItGithub from 'markdown-it-github';\n\ndotenv.config();\n\n// Configuration\nconst PORT = process.env.PORT ? parseInt(process.env.PORT, 10) : 3000;\nconst CORS_ALLOWED_ORIGINS = process.env.CORS_ALLOWED_ORIGINS || '*';\nconst RATE_LIMIT_REQUESTS = process.env.RATE_LIMIT_REQUESTS ? parseInt(process.env.RATE_LIMIT_REQUESTS, 10) : 60;\nconst METRICS_USER = process.env.METRICS_USER;\nconst METRICS_PASS = process.env.METRICS_PASS;\nconst DOCS_USER = process.env.DOCS_USER;\nconst DOCS_PASS = process.env.DOCS_PASS;\n\n// Metrics storage\nconst metrics = {\n  http_requests_total: {},\n  http_request_failures_total: {},\n};\n\n// Rate limiter per IP\nconst rateLimiters = new Map();\n\n// Minimal OpenAPI spec for available endpoints\nconst openApiSpec = {\n  openapi: '3.0.0',\n  info: { title: 'Agentic-lib Server API', version: '1.0.0' },\n  paths: {\n    '/health': { get: { responses: { '200': { description: 'OK' } } } },\n    '/metrics': { get: { responses: { '200': { description: 'Prometheus metrics' } } } },\n    '/openapi.json': { get: { responses: { '200': { description: 'OpenAPI JSON' } } } },\n    '/docs': { get: { responses: { '200': { description: 'Interactive Docs' } } } }\n  }\n};\n\n// Utility functions\nfunction recordRequest(method, route, status) {\n  const key = `${method}_${route}_${status}`;\n  metrics.http_requests_total[key] = (metrics.http_requests_total[key] || 0) + 1;\n}\n\nfunction recordFailure(route) {\n  metrics.http_request_failures_total[route] = (metrics.http_request_failures_total[route] || 0) + 1;\n}\n\nfunction checkRateLimit(ip) {\n  const now = Date.now();\n  const windowMs = 60000;\n  const limit = RATE_LIMIT_REQUESTS;\n  let entry = rateLimiters.get(ip);\n  if (!entry) {\n    entry = { tokens: limit - 1, last: now };\n    rateLimiters.set(ip, entry);\n    return true;\n  }\n  const elapsed = now - entry.last;\n  const refill = Math.floor(elapsed / windowMs) * limit;\n  if (refill > 0) {\n    entry.tokens = Math.min(entry.tokens + refill, limit);\n    entry.last = now;\n  }\n  if (entry.tokens > 0) {\n    entry.tokens -= 1;\n    return true;\n  }\n  return false;\n}\n\nfunction basicAuth(req, user, pass) {\n  const auth = req.headers.authorization;\n  if (!auth || !auth.startsWith('Basic ')) return false;\n  const creds = Buffer.from(auth.slice(6), 'base64').toString('utf8');\n  const [u, p] = creds.split(':');\n  return u === user && p === pass;\n}\n\nfunction sendJson(res, statusCode, data) {\n  const body = JSON.stringify(data);\n  res.writeHead(statusCode, {\n    'Content-Type': 'application/json',\n    'Access-Control-Allow-Origin': CORS_ALLOWED_ORIGINS,\n  });\n  res.end(body);\n}\n\nfunction sendText(res, statusCode, data) {\n  res.writeHead(statusCode, {\n    'Content-Type': 'text/plain',\n    'Access-Control-Allow-Origin': CORS_ALLOWED_ORIGINS,\n  });\n  res.end(data);\n}\n\nasync function handler(req, res) {\n  const ip = req.socket.remoteAddress || 'unknown';\n  if (!checkRateLimit(ip)) {\n    recordFailure('rate_limit');\n    res.writeHead(429);\n    return res.end('Too Many Requests');\n  }\n  const method = req.method;\n  const parsedUrl = new URL(req.url || '', `http://${req.headers.host}`);\n  const route = parsedUrl.pathname;\n  try {\n    if (method === 'GET' && route === '/health') {\n      const data = { status: 'ok', uptime: process.uptime(), timestamp: new Date().toISOString() };\n      sendJson(res, 200, data);\n      recordRequest(method, 'health', 200);\n    } else if (method === 'GET' && route === '/metrics') {\n      if (METRICS_USER && METRICS_PASS && !basicAuth(req, METRICS_USER, METRICS_PASS)) {\n        recordFailure('metrics_auth');\n        res.writeHead(401, { 'WWW-Authenticate': 'Basic realm=\"Metrics\"' });\n        return res.end('Unauthorized');\n      }\n      let out = '';\n      for (const key in metrics.http_requests_total) {\n        const [m, r, s] = key.split('_');\n        out += `http_requests_total{method=\\\"${m}\\\",route=\\\"${r}\\\",status=\\\"${s}\\\"} ${metrics.http_requests_total[key]}\\n`;\n      }\n      for (const r in metrics.http_request_failures_total) {\n        out += `http_request_failures_total{route=\\\"${r}\\\"} ${metrics.http_request_failures_total[r]}\\n`;\n      }\n      sendText(res, 200, out);\n      recordRequest(method, 'metrics', 200);\n    } else if (method === 'GET' && route === '/openapi.json') {\n      sendJson(res, 200, openApiSpec);\n      recordRequest(method, 'openapi', 200);\n    } else if (method === 'GET' && route === '/docs') {\n      if (DOCS_USER && DOCS_PASS && !basicAuth(req, DOCS_USER, DOCS_PASS)) {\n        recordFailure('docs_auth');\n        res.writeHead(401, { 'WWW-Authenticate': 'Basic realm=\"Docs\"' });\n        return res.end('Unauthorized');\n      }\n      const md = new MarkdownIt().use(markdownItGithub);\n      const mdContent = '```json\\n' + JSON.stringify(openApiSpec, null, 2) + '\\n```';\n      const html = md.render(mdContent);\n      res.writeHead(200, {\n        'Content-Type': 'text/html',\n        'Access-Control-Allow-Origin': CORS_ALLOWED_ORIGINS,\n      });\n      res.end(html);\n      recordRequest(method, 'docs', 200);\n    } else {\n      recordFailure(route);\n      res.writeHead(404);\n      res.end('Not Found');\n    }\n  } catch (err) {\n    recordFailure(route);\n    res.writeHead(500);\n    res.end('Internal Server Error');\n  }\n}\n\nexport function startServer(options = {}) {\n  const port = options.port || PORT;\n  const server = http.createServer(handler);\n  server.listen(port);\n  console.log(`Server started on port ${port}`);\n  return server;\n}\n\nFile: src/lib/main.js\n#!/usr/bin/env node\n// src/lib/main.js\n\n// Initialize global callCount to support test mocks that reference it\nif (typeof globalThis.callCount === \"undefined\") {\n  globalThis.callCount = 0;\n}\n\nimport { fileURLToPath } from \"url\";\nimport { z } from \"zod\";\nimport dotenv from \"dotenv\";\n\n// ---------------------------------------------------------------------------------------------------------------------\n// Environment configuration from .env file or environment variables or test values.\n// ---------------------------------------------------------------------------------------------------------------------\n\ndotenv.config();\n\nif (process.env.VITEST || process.env.NODE_ENV === \"development\") {\n  process.env.GITHUB_API_BASE_URL = process.env.GITHUB_API_BASE_URL || \"https://api.github.com.test/\";\n  process.env.OPENAI_API_KEY = process.env.OPENAI_API_KEY || \"key-test\";\n}\n\nconst configSchema = z.object({\n  GITHUB_API_BASE_URL: z.string().optional(),\n  OPENAI_API_KEY: z.string().optional(),\n});\n\nexport const config = configSchema.parse(process.env);\n\n// Global verbose mode flag\nconst VERBOSE_MODE = false;\n// Global verbose stats flag\nconst VERBOSE_STATS = false;\n\n// Helper function to format log entries\nfunction formatLogEntry(level, message, additionalData = {}) {\n  return {\n    level,\n    timestamp: new Date().toISOString(),\n    message,\n    ...additionalData,\n  };\n}\n\nexport function logConfig() {\n  const logObj = formatLogEntry(\"info\", \"Configuration loaded\", {\n    config: {\n      GITHUB_API_BASE_URL: config.GITHUB_API_BASE_URL,\n      OPENAI_API_KEY: config.OPENAI_API_KEY,\n    },\n  });\n  console.log(JSON.stringify(logObj));\n}\nlogConfig();\n\n// ---------------------------------------------------------------------------------------------------------------------\n// Utility functions\n// ---------------------------------------------------------------------------------------------------------------------\n\nexport function logInfo(message) {\n  const additionalData = VERBOSE_MODE ? { verbose: true } : {};\n  const logObj = formatLogEntry(\"info\", message, additionalData);\n  console.log(JSON.stringify(logObj));\n}\n\nexport function logError(message, error) {\n  const additionalData = { error: error ? error.toString() : undefined };\n  if (VERBOSE_MODE && error && error.stack) {\n    additionalData.stack = error.stack;\n  }\n  const logObj = formatLogEntry(\"error\", message, additionalData);\n  console.error(JSON.stringify(logObj));\n}\n\n// ---------------------------------------------------------------------------------------------------------------------\n// AWS Utility functions\n// ---------------------------------------------------------------------------------------------------------------------\n\nexport function createSQSEventFromDigest(digest) {\n  return {\n    Records: [\n      {\n        eventVersion: \"2.0\",\n        eventSource: \"aws:sqs\",\n        eventTime: new Date().toISOString(),\n        eventName: \"SendMessage\",\n        body: JSON.stringify(digest),\n      },\n    ],\n  };\n}\n\n// ---------------------------------------------------------------------------------------------------------------------\n// SQS Lambda Handlers\n// ---------------------------------------------------------------------------------------------------------------------\n\nexport async function digestLambdaHandler(sqsEvent) {\n  logInfo(`Digest Lambda received event: ${JSON.stringify(sqsEvent)}`);\n\n  // If event.Records is an array, use it. Otherwise, treat the event itself as one record.\n  const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];\n\n  // Array to collect the identifiers of the failed records\n  const batchItemFailures = [];\n\n  for (const [index, sqsEventRecord] of sqsEventRecords.entries()) {\n    try {\n      const digest = JSON.parse(sqsEventRecord.body);\n      logInfo(`Record ${index}: Received digest: ${JSON.stringify(digest)}`);\n    } catch (error) {\n      // If messageId is missing, generate a fallback identifier including record index\n      const recordId =\n        sqsEventRecord.messageId || `fallback-${index}-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n      logError(`Error processing record ${recordId} at index ${index}`, error);\n      logError(`Invalid JSON payload. Error: ${error.message}. Raw message: ${sqsEventRecord.body}`);\n      batchItemFailures.push({ itemIdentifier: recordId });\n    }\n  }\n\n  // Return the list of failed messages so that AWS SQS can attempt to reprocess them.\n  return {\n    batchItemFailures,\n    handler: \"src/lib/main.digestLambdaHandler\",\n  };\n}\n\n// ---------------------------------------------------------------------------------------------------------------------\n// CLI Helper Functions\n// ---------------------------------------------------------------------------------------------------------------------\n\n// Function to generate CLI usage instructions\nfunction generateUsage() {\n  return `\nUsage:\n  --help                     Show this help message and usage instructions.\n  --digest                   Run a full bucket replay simulating an SQS event.\n  --version                  Show version information with current timestamp.\n`;\n}\n\n// Process the --help flag\nfunction processHelp(args) {\n  if (args.includes(\"--help\")) {\n    console.log(generateUsage());\n    return true;\n  }\n  return false;\n}\n\n// Process the --version flag\nasync function processVersion(args) {\n  if (args.includes(\"--version\")) {\n    try {\n      const { readFileSync } = await import(\"fs\");\n      const packageJsonPath = new URL(\"../../package.json\", import.meta.url);\n      const packageJson = JSON.parse(readFileSync(packageJsonPath, \"utf8\"));\n      const versionInfo = {\n        version: packageJson.version,\n        timestamp: new Date().toISOString(),\n      };\n      console.log(JSON.stringify(versionInfo));\n    } catch (error) {\n      logError(\"Failed to retrieve version\", error);\n    }\n    return true;\n  }\n  return false;\n}\n\n// Process the --digest flag\nasync function processDigest(args) {\n  if (args.includes(\"--digest\")) {\n    const exampleDigest = {\n      key: \"events/1.json\",\n      value: \"12345\",\n      lastModified: new Date().toISOString(),\n    };\n    const sqsEvent = createSQSEventFromDigest(exampleDigest);\n    await digestLambdaHandler(sqsEvent);\n    return true;\n  }\n  return false;\n}\n\n// ---------------------------------------------------------------------------------------------------------------------\n// Main CLI\n// ---------------------------------------------------------------------------------------------------------------------\n\nexport async function main(args = process.argv.slice(2)) {\n  if (processHelp(args)) {\n    if (VERBOSE_STATS) {\n      console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));\n    }\n    return;\n  }\n  if (await processVersion(args)) {\n    if (VERBOSE_STATS) {\n      console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));\n    }\n    return;\n  }\n  if (await processDigest(args)) {\n    if (VERBOSE_STATS) {\n      console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));\n    }\n    return;\n  }\n\n  console.log(\"No command argument supplied.\");\n  console.log(generateUsage());\n  if (VERBOSE_STATS) {\n    console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));\n  }\n}\n\n// if (import.meta.url.endsWith(process.argv[1])) {\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\n  main().catch((err) => {\n    logError(\"Fatal error in main execution\", err);\n    process.exit(1);\n  });\n}\n\n\nSOURCE_FILES_END\n\nTest files (for context only, DO NOT UPDATE):\nTEST_FILES_START\nFile: sandbox/tests/server.test.js\nimport { describe, test, expect, beforeAll, afterAll } from 'vitest';\nimport http from 'http';\nimport { startServer } from '../source/server.js';\n\nlet server;\nlet base;\n\nbeforeAll(() => {\n  server = startServer({ port: 0 });\n  const addr = server.address();\n  const port = typeof addr === 'object' && addr !== null ? addr.port : 3000;\n  base = `http://127.0.0.1:${port}`;\n});\n\nafterAll(() => {\n  server.close();\n});\n\nfunction request(path) {\n  return new Promise((resolve, reject) => {\n    http.get(base + path, (res) => {\n      let data = '';\n      res.on('data', (chunk) => (data += chunk));\n      res.on('end', () => resolve({ statusCode: res.statusCode, body: data }));\n    }).on('error', reject);\n  });\n}\n\ndescribe('Server', () => {\n  test('/health returns status ok', async () => {\n    const { statusCode, body } = await request('/health');\n    expect(statusCode).toBe(200);\n    const json = JSON.parse(body);\n    expect(json.status).toBe('ok');\n    expect(typeof json.uptime).toBe('number');\n    expect(typeof json.timestamp).toBe('string');\n  });\n\n  test('/metrics returns metrics without auth when not configured', async () => {\n    const { statusCode, body } = await request('/metrics');\n    expect(statusCode).toBe(200);\n    expect(body).toContain('http_requests_total');\n  });\n});\n\nFile: tests/unit/main.test.js\nimport { describe, test, expect, vi, beforeAll, beforeEach, afterEach } from \"vitest\";\n\n// Ensure that the global callCount is reset before tests that rely on it\nbeforeAll(() => {\n  globalThis.callCount = 0;\n});\n\n// Reset callCount before each test in agenticHandler tests\nbeforeEach(() => {\n  globalThis.callCount = 0;\n});\n\n// Clear all mocks after each test to tidy up\nafterEach(() => {\n  vi.clearAllMocks();\n});\n\n// Use dynamic import for the module to ensure mocks are applied correctly\nlet agenticLib;\n\n// Default mock for openai used by tests that don't override it\nvi.mock(\"openai\", () => {\n  return {\n    Configuration: (config) => config,\n    OpenAIApi: class {\n      async createChatCompletion() {\n        const dummyResponse = { fixed: \"true\", message: \"dummy success\", refinement: \"none\" };\n        return {\n          data: {\n            choices: [{ message: { content: JSON.stringify(dummyResponse) } }]\n          }\n        };\n      }\n    }\n  };\n});\n\n// Re-import the module after setting up the default mock\nbeforeAll(async () => {\n  agenticLib = await import(\"../../src/lib/main.js\");\n});\n\ndescribe(\"Main Module Import\", () => {\n  test(\"should be non-null\", async () => {\n    const mainModule = await import(\"../../src/lib/main.js\");\n    expect(mainModule).not.toBeNull();\n  });\n});\n\n\nFile: tests/unit/module-index.test.js\n// tests/unit/module-index.test.js\n// src/lib/main.js\n//\n// This file is part of the Example Suite for `agentic-lib` see: https://github.com/xn-intenton-z2a/agentic-lib\n// This file is licensed under the MIT License. For details, see LICENSE-MIT\n//\n\nimport { describe, test, expect } from \"vitest\";\nimport anything from \"@src/index.js\";\n\ndescribe(\"Index Module Exports\", () => {\n  test(\"module index should be defined\", () => {\n    expect(anything).toBeUndefined();\n  });\n});\n\n\nTEST_FILES_END\n\nDocumentation files (to be updated if necessary):\nDOCS_FILES_START\nFile: sandbox/docs/SERVER.md\n# HTTP Server Feature\n\n## Mission Alignment\n\nThe HTTP server provides critical observability endpoints (health, metrics, OpenAPI schema, docs) to support continuous monitoring and self-hosted documentation in line with our mission to enable autonomous, agentic workflows. For mission details, see [Mission Statement](../MISSION.md).\n\nThis document describes how to launch the built-in HTTP server provided by agentic-lib, exposing key endpoints.\n\n## Programmatic Usage\n\nImport the `startServer` function and call it in your Node.js application:\n\n```js\nimport { startServer } from \"@xn-intenton-z2a/agentic-lib\";\n\n// Optionally, pass configuration options:\nconst options = {\n  port: process.env.PORT || 3000,\n  // CORS_ALLOWED_ORIGINS, RATE_LIMIT_REQUESTS, METRICS_USER, METRICS_PASS, DOCS_USER, DOCS_PASS\n};\n\nstartServer(options);\n```\n\n## Endpoints\n\n- **GET /health**  \n  Liveness probe. Returns JSON:\n  ```json\n  { \"status\": \"ok\", \"uptime\": <seconds>, \"timestamp\": \"<ISO>\" }\n  ```\n\n- **GET /metrics**  \n  Prometheus-formatted metrics. Exposes:\n  - `http_requests_total{method,route,status}`\n  - `http_request_failures_total{route}`\n  Protected by Basic Auth if `METRICS_USER`/`METRICS_PASS` are set.\n\n- **GET /openapi.json**  \n  Returns the OpenAPI 3.0 schema for all endpoints.\n\n- **GET /docs**  \n  Renders the OpenAPI schema as HTML via Markdown. Protected by Basic Auth if `DOCS_USER`/`DOCS_PASS` are set.\n\n## Configuration\n\nEnvironment variables:\n\n- `PORT` (default `3000`)\n- `CORS_ALLOWED_ORIGINS` (default `*`)\n- `RATE_LIMIT_REQUESTS` (requests per minute, default `60`)\n- `METRICS_USER`, `METRICS_PASS` (for `/metrics` Basic Auth)\n- `DOCS_USER`, `DOCS_PASS` (for `/docs` Basic Auth)\n\n## Rate Limiting\n\nIP-based token bucket with the following behavior:\n\n- Each IP has a token bucket of `RATE_LIMIT_REQUESTS` tokens per minute.\n- Exceeding the limit returns `429 Too Many Requests`.\n\n\nFile: sandbox/docs/SQS_OVERVIEW.md\n# Deep Dive: SQS Overview\n\n## Mission Alignment\n\nThe SQS utilities support robust, event-driven architectures and automated message processing, empowering continuous, autonomous agentic workflows. For mission details, see [Mission Statement](../MISSION.md).\n\nA comprehensive guide to Amazon SQS features, configuration, and best practices.\n\n## 1. Encryption Options\n\nDefault SSE-SQS managed encryption  \nSSE-KMS custom KMS key: specify `KmsMasterKeyId` = key ARN or alias\n\n## 2. Queue Types and Delivery Semantics\n\n- **Standard**: at-least-once, unlimited requests/sec  \n- **FIFO**: exactly-once, high-throughput mode, ordering\n\n## 3. Message Retention Configuration\n\nSetQueueAttributes Attributes:  \n`MessageRetentionPeriod`: integer, 60 to 1,209,600  \nDefault: 345,600\n\n## 4. Visibility Timeout and Message Lifecycle\n\nReceiveMessage hides message for `VisibilityTimeout`  \nDeleteMessage removes message permanently  \n`VisibilityTimeout`: seconds (0 to 43,200)\n\n## 5. Large Message Handling\n\nMax payload: 256 KB  \nUse S3 or DynamoDB for >256 KB, SQS stores pointer only\n\n## 6. Dead-Letter Queue Configuration\n\nOn source queue:  \n```json\n{\n  \"RedrivePolicy\": {\n    \"deadLetterTargetArn\": \"<DLQ_ARN>\",\n    \"maxReceiveCount\": \"<count>\"\n  }\n}\n```\nOn DLQ:  \n```json\n{\n  \"RedriveAllowPolicy\": {\n    \"allowPolicyType\": \"allowAll\" | \"byQueue\" | \"denyAll\",\n    \"queueArns\": [\"arn1\", ...]\n  }\n}\n```\n\n## 7. Dead-Letter Queue Retention Behavior\n\n- **Standard**: original enqueue timestamp applies  \n- **FIFO**: timestamp resets on move  \n- `ApproximateAgeOfOldestMessage` metric interpretation changes accordingly  \n\n**Recommendation**: Set DLQ retention period longer than source queue retention.\n\n\nDOCS_FILES_END\n\nREADME file (primary focus, to be updated): sandbox/README.md\nREADME_FILE_START\n## Mission Statement\n\nThis project is guided by the core mission of agentic-lib to enable autonomous, continuous agentic interactions through issues, branches, and pull requests. For full mission details, see [Mission Statement](../MISSION.md).\n\n# agentic-lib\n\nagentic-lib is a drop-in JavaScript SDK for autonomous GitHub workflows. Inspired by our mission to enable continuous, agentic interactions through issues, branches, and pull requests, this library provides core utilities to configure environments, handle AWS SQS events, power CLI-driven workflows, and optionally launch a self-hosted HTTP server for health, metrics, and documentation.\n\nWith agentic-lib, you can seamlessly integrate environment validation, structured logging, AWS utilities, Lambda handlers, CLI and programmatic workflows into your Node.js projects, ensuring reproducible, testable, and maintainable automation.\n\n## Key Features\n\n- **Environment configuration** (dotenv + Zod)  \n  Mission Alignment: Validates and loads environment variables to ensure consistent, reproducible conditions essential for autonomous workflows.  \n- **Logging helpers** (logInfo, logError)  \n  Mission Alignment: Provides structured, consistent logs to enable transparent audit trails for agentic operations.  \n- **AWS utilities** (createSQSEventFromDigest)  \n  Mission Alignment: Simplifies SQS event creation for seamless integration into continuous, event-driven workflows.  \n- **Lambda handler** (digestLambdaHandler)  \n  Mission Alignment: Automates message processing and error handling to maintain continuous, autonomous system reliability.  \n- **HTTP Server** (startServer function with `/health`, `/metrics`, `/openapi.json`, `/docs` endpoints)  \n  Mission Alignment: Exposes self-hosted endpoints for observability and documentation, supporting ongoing, autonomous monitoring.  \n- **CLI flags**: `--help`, `--version`, `--digest`  \n  Mission Alignment: Offers intuitive CLI interfaces to drive agentic workflows directly from the command line.\n\n## Installation\n\nInstall from npm:\n\n```bash\nnpm install @xn-intenton-z2a/agentic-lib\n```\n\nOr clone this repository and install dependencies:\n\n```bash\ngit clone https://github.com/xn-intenton-z2a/agentic-lib.git\ncd agentic-lib\nnpm install\n```\n\n## CLI Usage Examples\n\n### Show Help\n\n```bash\nnpx agentic-lib --help\n```\n\n### Show Version\n\n```bash\nnpx agentic-lib --version\n```\n\n### Run Digest Simulation\n\n```bash\nnpx agentic-lib --digest\n```\n\n## Programmatic Usage\n\n### Lambda Handler\n\n```js\nimport { createSQSEventFromDigest, digestLambdaHandler } from \"@xn-intenton-z2a/agentic-lib\";\n\nconst event = createSQSEventFromDigest({\n  key: \"path\",\n  value: \"123\",\n  lastModified: new Date().toISOString(),\n});\n\ndigestLambdaHandler(event).then(() => console.log(\"Processed\"));\n```\n\n### HTTP Server\n\nImport and start the built-in HTTP server to expose health, metrics, OpenAPI spec, and interactive docs:\n\n```js\nimport { startServer } from \"@xn-intenton-z2a/agentic-lib\";\n\n// Optional configuration: port, CORS origins, rate limits, auth credentials\nstartServer({ port: 3000 });\n```\n\nSee [docs/SERVER.md](./docs/SERVER.md) for detailed information on endpoints and configuration.\n\n## Testing\n\nRun the full test suite using Vitest:\n\n```bash\nnpm test\n```\n\n## Links\n\n- [Mission Statement](../MISSION.md)\n- [Contributing Guide](../CONTRIBUTING.md)\n- [License](../LICENSE.md)\n- [GitHub Repository](https://github.com/xn-intenton-z2a/agentic-lib)\n- [HTTP Server Docs](./docs/SERVER.md)\n- [Deep Dive: SQS Overview](./docs/SQS_OVERVIEW.md)\n\nREADME_FILE_END\n\nMISSION file (for context, read only): MISSION.md\nMISSION_FILE_START\n# Mission Statement\n\n**agentic‑lib** Is a JavaScript library which can be used as a drop in JS implementation or wholesale replacement for \nthe steps, jobs, and re-usable workflows below in this repository. It is designed to be used in a GitHub Actions \nworkflow to enable your repository to operate in an “agentic” manner. In our system, autonomous workflows communicate\nthrough branches and issues to continuously review, fix, update, and evolve your code. Each workflow is designed to be\ninvoked using GitHub’s `workflow_call` event, so they can be composed together like an SDK.\n\nMISSION_FILE_END\n\nContributing file (for context, read only): CONTRIBUTING.md\nCONTRIBUTING_FILE_START\n# agentic‑lib\n\nThis document outlines our guidelines for human and automated contributions, ensuring that our core library remains \nrobust, testable, and efficient in powering our reusable GitHub Workflows.\n\n## How to Contribute\n\nThe guidelines below apply to human or automated contributions:\n\n1. **Report Issues or Ideas:**\n    - Open an issue on GitHub to share bug reports, feature requests, or any improvements you envision.\n    - Clear descriptions and reproducible steps are highly appreciated.\n\n2. **Submit Pull Requests:**\n    - Implement your changes and push them to a new branch, ensuring you follow the \n      existing coding style and standards.\n    - Add tests to cover any new functionality.\n    - Update documentation if your changes affect usage or workflow behavior.\n    - Submit your pull request for review.\n\n## Guidelines\n\n- **Features:**\n    - Clear Objective & Scope: Define the feature with a concise description outlining its purpose, scope, and the specific problem it solves for the end user.\n    - Value Proposition: Articulate the tangible benefits of the feature, including improved functionality, performance, or user experience.\n    - Success Criteria & Requirements: List measurable success criteria and requirements, including performance benchmarks, usability standards, and stability expectations, to guide development and testing.\n    - Testability & Stability: Ensure the feature can be verified through both automated tests and user acceptance criteria. Specify any necessary rollback or fail-safe mechanisms to maintain system stability.\n    - Dependencies & Constraints: Identify any dependencies (external libraries, APIs, etc.), assumptions, and limitations that could impact feature delivery or future enhancements.\n    - User Scenarios & Examples: Provide illustrative use cases or scenarios that demonstrate how the feature will be used in real-world situations, making it easier for both developers and stakeholders to understand its impact.\n    - Verification & Acceptance: Define clear verification steps and acceptance criteria to ensure the feature meets its intended requirements. This should include detailed plans for unit tests, integration tests, manual user acceptance tests, and code reviews. Specify measurable outcomes that must be achieved for the feature to be considered successfully delivered and stable.\n\n- **Code Quality:**\n    - Ensure there are tests that cover your changes and any likely new cases they introduce.\n    - When making a change remain consistent with the existing code style and structure.\n    - When adding new functionality, consider if some unused or superseded code should be removed.\n\n- **Compatibility:**\n    - Ensure your code runs on Node 20 and adheres to ECMAScript Module (ESM) standards.\n    - Tests use vitest and competing test frameworks should not be added.\n    - Mocks in tests must not interfere with other tests.\n\n- **Testing:**\n    - The command `npm test` should invoke the tests added for the new functionality (and pass).\n    - If you add new functionality, ensure it is covered by tests.\n\n- **Documentation:**\n    - When making a change to the main source file, review the readme to see if it needs to be updated and if so, update it.\n    - Where the source exports a function, consider that part of the API of the library and document it in the readme.\n    - Where the source stands-up an HTTP endpoint, consider that part of the API of the library and document it in the readme.\n    - Include usage examples including inline code usage and CLI and HTTP invocation, API references.\n\n- **README:**\n    - The README should begin with something inspired by the mission statement and describe the current state of the repository (rather than the journey)\n    - The README should include a link to MISSION.md, CONTRIBUTING.md, LICENSE.md.\n    - The README should include a link to the intentïon `agentic-lib` GitHub Repository which is https://github.com/xn-intenton-z2a/agentic-lib.\n\n## Sandbox mode\n\nPlease note that the automation features of this repository are in sandbox mode. This means that\nautomated changes should only be applied to the sandbox paths which are shown below:\n```yaml\npaths:\n  targetTestsPath:\n    path: 'sandbox/tests/'\n    permissions: [ 'write' ]\n  targetSourcePath:\n    path: 'sandbox/source/'\n    permissions: [ 'write' ]\n  documentationPath:\n    path: 'sandbox/docs/'\n    permissions: [ 'write' ]\n  readmeFilepath:\n    path: 'sandbox/README.md'\n    permissions: [ 'write' ]\n```\n\nCONTRIBUTING_FILE_END\n\nDependencies file (for context, read only): package.json\nDEPENDENCIES_FILE_START\n{\n  \"name\": \"@xn-intenton-z2a/agentic-lib\",\n  \"version\": \"6.6.5-0\",\n  \"description\": \"Agentic-lib Agentic Coding Systems SDK powering automated GitHub workflows.\",\n  \"type\": \"module\",\n  \"main\": \"src/lib/main.js\",\n  \"scripts\": {\n    \"build\": \"echo \\\"Nothing to build\\\"\",\n    \"formatting\": \"prettier --check\",\n    \"formatting-fix\": \"prettier --write\",\n    \"linting\": \"eslint\",\n    \"linting-json\": \"eslint --format=@microsoft/eslint-formatter-sarif\",\n    \"linting-fix\": \"eslint --fix\",\n    \"update-to-minor\": \"npx npm-check-updates --upgrade --enginesNode --target minor --verbose --install always\",\n    \"update-to-greatest\": \"npx npm-check-updates --upgrade --enginesNode --target greatest --verbose --install always --reject \\\"alpha\\\"\",\n    \"test\": \"vitest tests/unit/*.test.js sandbox/tests/*.test.js\",\n    \"test:unit\": \"vitest --coverage tests/unit/*.test.js sandbox/tests/*.test.js\",\n    \"start\": \"node src/lib/main.js\"\n  },\n  \"keywords\": [],\n  \"author\": \"https://github.com/xn-intenton-z2a\",\n  \"license\": \"GPL-3.0, MIT\",\n  \"dependencies\": {\n    \"@aws-sdk/client-lambda\": \"^3.804.0\",\n    \"@xn-intenton-z2a/s3-sqs-bridge\": \"^0.24.0\",\n    \"chalk\": \"^5.4.1\",\n    \"change-case\": \"^5.4.4\",\n    \"dayjs\": \"^1.11.13\",\n    \"dotenv\": \"^16.5.0\",\n    \"ejs\": \"^3.1.10\",\n    \"figlet\": \"^1.8.1\",\n    \"js-yaml\": \"^4.1.0\",\n    \"lodash\": \"^4.17.21\",\n    \"minimatch\": \"^10.0.1\",\n    \"openai\": \"^4.97.0\",\n    \"seedrandom\": \"^3.0.5\",\n    \"zod\": \"^3.24.4\"\n  },\n  \"devDependencies\": {\n    \"@microsoft/eslint-formatter-sarif\": \"^3.1.0\",\n    \"@vitest/coverage-v8\": \"^3.1.3\",\n    \"aws-cdk\": \"^2.1013.0\",\n    \"eslint\": \"^9.25.0\",\n    \"eslint-config-google\": \"^0.14.0\",\n    \"eslint-config-prettier\": \"^8.10.0\",\n    \"eslint-plugin-import\": \"^2.31.0\",\n    \"eslint-plugin-prettier\": \"^5.4.0\",\n    \"eslint-plugin-promise\": \"^7.2.1\",\n    \"eslint-plugin-react\": \"^7.37.5\",\n    \"eslint-plugin-security\": \"^3.0.1\",\n    \"eslint-plugin-sonarjs\": \"^3.0.2\",\n    \"figlet\": \"^1.8.1\",\n    \"markdown-it\": \"^14.1.0\",\n    \"markdown-it-github\": \"^0.5.0\",\n    \"npm-check-updates\": \"^18.0.1\",\n    \"prettier\": \"^3.5.3\",\n    \"vitest\": \"^3.1.3\"\n  },\n  \"engines\": {\n    \"node\": \">=20.0.0\"\n  },\n  \"files\": [\n    \"package.json\"\n  ],\n  \"publishConfig\": {\n    \"registry\": \"https://npm.pkg.github.com\"\n  }\n}\n\nDEPENDENCIES_FILE_END   \n\nDependencies install from command: npm install\nDEPENDENCIES_INSTALL_START\nnpm warn deprecated inflight@1.0.6: This module is not supported, and leaks memory. Do not use it. Check out lru-cache if you want a good and tested way to coalesce async requests by a key value, which is much more comprehensive and powerful.\nnpm warn deprecated @humanwhocodes/config-array@0.13.0: Use @eslint/config-array instead\nnpm warn deprecated rimraf@3.0.2: Rimraf versions prior to v4 are no longer supported\nnpm warn deprecated @humanwhocodes/object-schema@2.0.3: Use @eslint/object-schema instead\nnpm warn deprecated glob@7.2.3: Glob versions prior to v9 are no longer supported\nnpm warn deprecated node-domexception@1.0.0: Use your platform's native DOMException instead\nnpm warn deprecated eslint@8.57.1: This version is no longer supported. Please see https://eslint.org/version-support for other options.\n\nadded 605 packages, and audited 606 packages in 5s\n\n162 packages are looking for funding\n  run `npm fund` for details\n\nfound 0 vulnerabilities\nDEPENDENCIES_INSTALL_END    \n\nBuild output from command: npm run build\nBUILD_OUTPUT_START\n\n> @xn-intenton-z2a/agentic-lib@6.6.5-0 build\n> echo \"Nothing to build\"\n\nNothing to build\nBUILD_OUTPUT_END      \n\nTest output from command: npm test\nTEST_OUTPUT_START\n\n> @xn-intenton-z2a/agentic-lib@6.6.5-0 test\n> vitest tests/unit/*.test.js sandbox/tests/*.test.js\n\n\n\u001b[1m\u001b[46m RUN \u001b[49m\u001b[22m \u001b[36mv3.1.3 \u001b[39m\u001b[90m/home/runner/work/agentic-lib/agentic-lib\u001b[39m\n\n \u001b[32m✓\u001b[39m tests/unit/module-index.test.js \u001b[2m(\u001b[22m\u001b[2m1 test\u001b[22m\u001b[2m)\u001b[22m\u001b[32m 2\u001b[2mms\u001b[22m\u001b[39m\n\u001b[90mstdout\u001b[2m | tests/unit/main.test.js\n\u001b[22m\u001b[39m{\"level\":\"info\",\"timestamp\":\"2025-05-18T08:49:33.300Z\",\"message\":\"Configuration loaded\",\"config\":{\"GITHUB_API_BASE_URL\":\"https://api.github.com.test/\",\"OPENAI_API_KEY\":\"key-test\"}}\n\n \u001b[32m✓\u001b[39m tests/unit/main.test.js \u001b[2m(\u001b[22m\u001b[2m1 test\u001b[22m\u001b[2m)\u001b[22m\u001b[32m 71\u001b[2mms\u001b[22m\u001b[39m\n\u001b[90mstdout\u001b[2m | sandbox/tests/server.test.js\n\u001b[22m\u001b[39mServer started on port 3000\n\n \u001b[32m✓\u001b[39m sandbox/tests/server.test.js \u001b[2m(\u001b[22m\u001b[2m2 tests\u001b[22m\u001b[2m)\u001b[22m\u001b[32m 35\u001b[2mms\u001b[22m\u001b[39m\n\n\u001b[2m Test Files \u001b[22m \u001b[1m\u001b[32m3 passed\u001b[39m\u001b[22m\u001b[90m (3)\u001b[39m\n\u001b[2m      Tests \u001b[22m \u001b[1m\u001b[32m4 passed\u001b[39m\u001b[22m\u001b[90m (4)\u001b[39m\n\u001b[2m   Start at \u001b[22m 08:49:32\n\u001b[2m   Duration \u001b[22m 421ms\u001b[2m (transform 96ms, setup 0ms, collect 200ms, tests 108ms, environment 1ms, prepare 257ms)\u001b[22m\nTEST_OUTPUT_END            \n\nMain execution output from command: npm run start\nMAIN_OUTPUT_START\n\n> @xn-intenton-z2a/agentic-lib@6.6.5-0 start\n> node src/lib/main.js\n\n{\"level\":\"info\",\"timestamp\":\"2025-05-18T08:49:33.597Z\",\"message\":\"Configuration loaded\",\"config\":{}}\nNo command argument supplied.\n\nUsage:\n  --help                     Show this help message and usage instructions.\n  --digest                   Run a full bucket replay simulating an SQS event.\n  --version                  Show version information with current timestamp.\nMAIN_OUTPUT_END    \n\nAgent configuration file:\nAGENT_CONFIG_FILE_START\n# Which agentic-lib workflow schedule should be used?\nschedule: schedule-3\n\n# Mapping for from symbolic keys to filepaths for access by agentic-lib workflows with limits and access permissions\npaths:\n  # Filepaths for elaborator workflows\n  missionFilepath:\n    path: 'MISSION.md'\n  librarySourcesFilepath:\n    path: 'sandbox/SOURCES.md'\n    permissions: [ 'write' ]\n    limit: 8\n  libraryDocumentsPath:\n    path: 'sandbox/library/'\n    permissions: [ 'write' ]\n    limit: 32\n  featuresPath:\n    path: 'sandbox/features/'\n    permissions: [ 'write' ]\n    limit: 1\n\n  # Filepaths for engineer workflows\n  contributingFilepath:\n    path: 'CONTRIBUTING.md'\n  targetTestsPath:\n    path: 'sandbox/tests/'\n    permissions: [ 'write' ]\n  otherTestsPaths:\n    paths: [ 'tests/unit/' ]\n  targetSourcePath:\n    path: 'sandbox/source/'\n    permissions: [ 'write' ]\n  otherSourcePaths:\n    paths: [ 'src/lib/' ]\n  dependenciesFilepath:\n    path: 'package.json'\n  documentationPath:\n    path: 'sandbox/docs/'\n    permissions: [ 'write' ]\n\n  # Filepaths for maintainer workflows\n  formattingFilepath:\n    path: '.prettierrc'\n  lintingFilepath:\n    path: 'eslint.config.js'\n  readmeFilepath:\n    path: 'sandbox/README.md'\n    permissions: [ 'write' ]\n\n# Execution commands\nbuildScript: 'npm run build'\ntestScript: 'npm test'\nmainScript: 'npm run start'\n\n# How many issues should be available to be picked up?\nfeatureDevelopmentIssuesWipLimit: 2\nmaintenanceIssuesWipLimit: 1\n\n# How many attempts should be made to work on an issue?\nattemptsPerBranch: 2\nattemptsPerIssue: 2\n\n# Web publishing\ndocRoot: 'public'\n\n# Sandbox configuration\nsandbox:\n  sandboxReset: 'true'\n  sandboxPath: 'sandbox'\n\n# Repository seeding\n#seeding:\n#  repositoryReseed: 'true'\n#  missionFilepath: 'seeds/zero-MISSION.md'\n#  sourcePath: 'seeds/zero-main.js'\n#  testsPath: 'seeds/zero-main.test.js'\n#  dependenciesFilepath: 'seeds/zero-package.json'\n#  readmeFilepath: 'seeds/zero-README.md'\n\nintentionBot:\n  intentionFilepath: 'intentïon.md'\n\nAGENT_CONFIG_FILE_END\n\nPlease produce updated versions of the README and documentation files to ensure they accurately reflect the current state of the codebase.\nRemember:\n1. The README is the primary focus, but other documentation files can be updated as well if needed\n2. Source files (srcFiles) and test files (testFiles) should NOT be updated\n3. Preserve existing README content even if it describes features not yet implemented\n4. Update the README if it conflicts with current source code, tests, or documentation\n5. If documentation files are out of date compared to the source code or tests, update them to be consistent\n\nIf there are no changes required, please provide the original content and state that no changes are necessary in the message.\n\nPaths in (updatedFile01Filepath, updatedFile02Filepath, etc...) must begin with one of: sandbox/SOURCES.md;sandbox/library/;sandbox/features/;sandbox/tests/;sandbox/source/;sandbox/docs/;sandbox/README.md\n\nAnswer strictly with a JSON object following this schema:\n{\n  \"message\": \"A short sentence explaining the changes applied (or why no changes were applied) suitable for a commit message or PR text.\",\n  \"updatedFile01Filepath\": \"sandbox/README.md\",\n  \"updatedFile01Contents\": \"The entire new content of the README file, with all necessary changes applied, if any.\",\n  \"updatedFile02Filepath\":  \"sandbox/docs/USAGE.md\",\n  \"updatedFile02Contents\": \"The entire new content of the file, with all necessary changes applied, if any.\",\n  \"updatedFile03Filepath\": \"unused\",\n  \"updatedFile03Contents\": \"unused\",\n  \"updatedFile04Filepath\": \"unused\",\n  \"updatedFile04Contents\": \"unused\",\n  \"updatedFile05Filepath\": \"unused\",\n  \"updatedFile05Contents\": \"unused\",\n  \"updatedFile06Filepath\": \"unused\",\n  \"updatedFile06Contents\": \"unused\",\n  \"updatedFile07Filepath\": \"unused\",\n  \"updatedFile07Contents\": \"unused\",\n  \"updatedFile08Filepath\": \"unused\",\n  \"updatedFile08Contents\": \"unused\",\n  \"updatedFile09Filepath\": \"unused\",\n  \"updatedFile09Contents\": \"unused\",\n  \"updatedFile10Filepath\": \"unused\",\n  \"updatedFile10Contents\": \"unused\",\n  \"updatedFile11Filepath\": \"unused\",\n  \"updatedFile11Contents\": \"unused\",\n  \"updatedFile12Filepath\": \"unused\",\n  \"updatedFile12Contents\": \"unused\",\n  \"updatedFile13Filepath\": \"unused\",\n  \"updatedFile13Contents\": \"unused\",\n  \"updatedFile14Filepath\": \"unused\",\n  \"updatedFile14Contents\": \"unused\",\n  \"updatedFile15Filepath\": \"unused\",\n  \"updatedFile15Contents\": \"unused\",\n  \"updatedFile16Filepath\": \"unused\",\n  \"updatedFile16Contents\": \"unused\"\n}\n\nYou can include up to 16 files using the updatedFileXXName and updatedFileXXContents pairs (where XX is a number from 01 to 16)\nWhere a file name and contents slot is not used, populate tha name with \"unused\" and the contents with \"unused\".\nNever truncate the files, when returning a file, always return the entire file content.\n\nEnsure valid JSON.\n"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "applyReadmeUpdate",
        "description": "Return updated versions of README and documentation files along with a commit message. Use the provided file contents and supporting context to generate the updates.",
        "parameters": {
          "type": "object",
          "properties": {
            "message": {
              "type": "string",
              "description": "A short sentence explaining the changes applied (or why no changes were applied) suitable for a commit message or PR text."
            },
            "updatedFile01Filepath": {
              "type": "string",
              "description": "Path to the first file to update"
            },
            "updatedFile01Contents": {
              "type": "string",
              "description": "The entire new content of the first file, with all necessary changes applied"
            },
            "updatedFile02Filepath": {
              "type": "string",
              "description": "Path to the second file to update"
            },
            "updatedFile02Contents": {
              "type": "string",
              "description": "The entire new content of the second file, with all necessary changes applied"
            },
            "updatedFile03Filepath": {
              "type": "string",
              "description": "Path to the third file to update"
            },
            "updatedFile03Contents": {
              "type": "string",
              "description": "The entire new content of the third file, with all necessary changes applied"
            },
            "updatedFile04Filepath": {
              "type": "string",
              "description": "Path to the fourth file to update"
            },
            "updatedFile04Contents": {
              "type": "string",
              "description": "The entire new content of the fourth file, with all necessary changes applied"
            },
            "updatedFile05Filepath": {
              "type": "string",
              "description": "Path to the fifth file to update"
            },
            "updatedFile05Contents": {
              "type": "string",
              "description": "The entire new content of the fifth file, with all necessary changes applied"
            },
            "updatedFile06Filepath": {
              "type": "string",
              "description": "Path to the sixth file to update"
            },
            "updatedFile06Contents": {
              "type": "string",
              "description": "The entire new content of the sixth file, with all necessary changes applied"
            },
            "updatedFile07Filepath": {
              "type": "string",
              "description": "Path to the seventh file to update"
            },
            "updatedFile07Contents": {
              "type": "string",
              "description": "The entire new content of the seventh file, with all necessary changes applied"
            },
            "updatedFile08Filepath": {
              "type": "string",
              "description": "Path to the eighth file to update"
            },
            "updatedFile08Contents": {
              "type": "string",
              "description": "The entire new content of the eighth file, with all necessary changes applied"
            },
            "updatedFile09Filepath": {
              "type": "string",
              "description": "Path to the ninth file to update"
            },
            "updatedFile09Contents": {
              "type": "string",
              "description": "The entire new content of the ninth file, with all necessary changes applied"
            },
            "updatedFile10Filepath": {
              "type": "string",
              "description": "Path to the tenth file to update"
            },
            "updatedFile10Contents": {
              "type": "string",
              "description": "The entire new content of the tenth file, with all necessary changes applied"
            },
            "updatedFile11Filepath": {
              "type": "string",
              "description": "Path to the eleventh file to update"
            },
            "updatedFile11Contents": {
              "type": "string",
              "description": "The entire new content of the eleventh file, with all necessary changes applied"
            },
            "updatedFile12Filepath": {
              "type": "string",
              "description": "Path to the twelfth file to update"
            },
            "updatedFile12Contents": {
              "type": "string",
              "description": "The entire new content of the twelfth file, with all necessary changes applied"
            },
            "updatedFile13Filepath": {
              "type": "string",
              "description": "Path to the thirteenth file to update"
            },
            "updatedFile13Contents": {
              "type": "string",
              "description": "The entire new content of the thirteenth file, with all necessary changes applied"
            },
            "updatedFile14Filepath": {
              "type": "string",
              "description": "Path to the fourteenth file to update"
            },
            "updatedFile14Contents": {
              "type": "string",
              "description": "The entire new content of the fourteenth file, with all necessary changes applied"
            },
            "updatedFile15Filepath": {
              "type": "string",
              "description": "Path to the fifteenth file to update"
            },
            "updatedFile15Contents": {
              "type": "string",
              "description": "The entire new content of the fifteenth file, with all necessary changes applied"
            },
            "updatedFile16Filepath": {
              "type": "string",
              "description": "Path to the sixteenth file to update"
            },
            "updatedFile16Contents": {
              "type": "string",
              "description": "The entire new content of the sixteenth file, with all necessary changes applied"
            }
          },
          "required": [
            "message",
            "updatedFile01Filepath",
            "updatedFile01Contents",
            "updatedFile02Filepath",
            "updatedFile02Contents",
            "updatedFile03Filepath",
            "updatedFile03Contents",
            "updatedFile04Filepath",
            "updatedFile04Contents",
            "updatedFile05Filepath",
            "updatedFile05Contents",
            "updatedFile06Filepath",
            "updatedFile06Contents",
            "updatedFile07Filepath",
            "updatedFile07Contents",
            "updatedFile08Filepath",
            "updatedFile08Contents",
            "updatedFile09Filepath",
            "updatedFile09Contents",
            "updatedFile10Filepath",
            "updatedFile10Contents",
            "updatedFile11Filepath",
            "updatedFile11Contents",
            "updatedFile12Filepath",
            "updatedFile12Contents",
            "updatedFile13Filepath",
            "updatedFile13Contents",
            "updatedFile14Filepath",
            "updatedFile14Contents",
            "updatedFile15Filepath",
            "updatedFile15Contents",
            "updatedFile16Filepath",
            "updatedFile16Contents"
          ],
          "additionalProperties": false
        },
        "strict": true
      }
    }
  ]
}