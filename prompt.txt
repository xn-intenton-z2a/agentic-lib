
You are providing the entire new content of source files, test files, documentation files, and other necessary
files with all necessary changes applied to deliver the resolution to an issue. Focus on high-impact, 
functional solutions that address core issues rather than superficial changes or excessive code polishing.
Implement as much as you can and refer to the projects features and mission statement when expanding the code
beyond the scope of the original issue. Implement whole features and do not leave stubbed out or pretended code.

Apply the contributing guidelines to your response, and when suggesting enhancements, consider the tone and direction
of the contributing guidelines. Prioritize changes that deliver user value and maintain the integrity
of the codebase's primary purpose.

Do as much as you can all at once.

Follow the linting guidelines and the formatting guidelines from the included config.


You must only add, remove, or change the files in the target writable locations. You can update multiple
files by specifying their paths and contents in the enumerated updatedFile01Filepath updatedFile02Contents response
attribute, a second file would use updatedFile01Filepath updatedFile02Contents and so on to 16. Each file will
be checked against the write permission in the Agent configuration file before being written. Feel free to
add new files as long as they are in the target writable locations. You can also remove files, but only if
they are in the target writable locations. To delete a file, set the updated file contents to "delete".

The target writable locations for your output are: sandbox/SOURCES.md;sandbox/library/;sandbox/features/;sandbox/tests/;sandbox/source/;sandbox/docs/;sandbox/README.md
Other file will be supplied in the context but only the paths above should be written to.

Only provide new or updated content for the target source files in sandbox/source.
Only delete or update the target source files in sandbox/source.
Only provide new or updated content for the target test files in sandbox/tests.
Only delete or update the target test files in sandbox/tests.
Only update dependency file package.json.
Only update the target documentation files in sandbox/docs.

Follow the attached Formatting file content and Linting file content.

Consider the following when refining your response:
* Current feature names and specifications in the repository
* Source file content
* Test file content
* Documentation file content
* README file content
* MISSION file content
* Contributing file content
* Dependencies file content
* Formatting file content
* Linting file content
* Agent configuration file content
* Issue details
* Dependency list
* Build output
* Test output
* Main execution output

Current feature names and specifications (for context, read only):
CURRENT_FEATURES_START
find: ‘features/’: No such file or directory
none
CURRENT_FEATURES_END

Source files (write new files or update files in sandbox/source as necessary):
(Multiple files from both in writable locations and not.)
SOURCE_FILE_START Filepath: sandbox/source/server.js
import http from "http";
import { URL } from "url";
import { z } from "zod";
import dotenv from "dotenv";
import MarkdownIt from "markdown-it";
import markdownItGithub from "markdown-it-github";

dotenv.config();

// Validate environment variables
const envSchema = z.object({
  PORT: z
    .preprocess((val) => (val === undefined ? undefined : String(val)), z.string().regex(/^\d+$/).default("3000").transform((s) => parseInt(s, 10))),
  CORS_ALLOWED_ORIGINS: z.string().default("*"),
  RATE_LIMIT_REQUESTS: z
    .preprocess((val) => (val === undefined ? undefined : String(val)), z.string().regex(/^\d+$/).default("60").transform((s) => parseInt(s, 10))),
  METRICS_USER: z.string().optional(),
  METRICS_PASS: z.string().optional(),
  DOCS_USER: z.string().optional(),
  DOCS_PASS: z.string().optional(),
});
let env;
try {
  env = envSchema.parse(process.env);
} catch (err) {
  console.error("Invalid or missing environment variables for server:", err.errors);
  process.exit(1);
}

// Configuration
const PORT = env.PORT;
const CORS_ALLOWED_ORIGINS = env.CORS_ALLOWED_ORIGINS;
const RATE_LIMIT_REQUESTS = env.RATE_LIMIT_REQUESTS;
const METRICS_USER = env.METRICS_USER;
const METRICS_PASS = env.METRICS_PASS;
const DOCS_USER = env.DOCS_USER;
const DOCS_PASS = env.DOCS_PASS;

// Metrics storage
const metrics = {
  http_requests_total: {},
  http_request_failures_total: {},
  http_request_duration_seconds: [],
};

// Rate limiter per IP
const rateLimiters = new Map();

// Minimal OpenAPI spec for available endpoints
const openApiSpec = {
  openapi: "3.0.0",
  info: { title: "Agentic-lib Server API", version: "1.0.0" },
  paths: {
    "/health": { get: { responses: { "200": { description: "OK" } } } },
    "/ready": { get: { responses: { "200": { description: "Ready" } } } },
    "/metrics": {
      get: { responses: { "200": { description: "Prometheus metrics including http_request_duration_seconds histogram" } } },
    },
    "/openapi.json": { get: { responses: { "200": { description: "OpenAPI JSON" } } } },
    "/docs": { get: { responses: { "200": { description: "Interactive Docs" } } } },
  },
};

// Utility functions
function recordRequest(method, route, status) {
  const key = `${method}_${route}_${status}`;
  metrics.http_requests_total[key] = (metrics.http_requests_total[key] || 0) + 1;
}

function recordFailure(route) {
  metrics.http_request_failures_total[route] =
    (metrics.http_request_failures_total[route] || 0) + 1;
}

function recordDuration(method, route, status, duration) {
  metrics.http_request_duration_seconds.push({ method, route, status, duration });
}

function checkRateLimit(ip) {
  const now = Date.now();
  const windowMs = 60000;
  const limit = RATE_LIMIT_REQUESTS;
  let entry = rateLimiters.get(ip);
  if (!entry) {
    entry = { tokens: limit - 1, last: now };
    rateLimiters.set(ip, entry);
    return true;
  }
  const elapsed = now - entry.last;
  const refill = Math.floor(elapsed / windowMs) * limit;
  if (refill > 0) {
    entry.tokens = Math.min(entry.tokens + refill, limit);
    entry.last = now;
  }
  if (entry.tokens > 0) {
    entry.tokens -= 1;
    return true;
  }
  return false;
}

function basicAuth(req, user, pass) {
  const auth = req.headers.authorization;
  if (!auth || !auth.startsWith("Basic ")) return false;
  const creds = Buffer.from(auth.slice(6), "base64").toString("utf8");
  const [u, p] = creds.split(":");
  return u === user && p === pass;
}

function sendJson(res, statusCode, data) {
  const body = JSON.stringify(data);
  res.writeHead(statusCode, {
    "Content-Type": "application/json",
    "Access-Control-Allow-Origin": CORS_ALLOWED_ORIGINS,
  });
  res.end(body);
}

function sendText(res, statusCode, data) {
  res.writeHead(statusCode, {
    "Content-Type": "text/plain",
    "Access-Control-Allow-Origin": CORS_ALLOWED_ORIGINS,
  });
  res.end(data);
}

async function handler(req, res) {
  const ip = req.socket.remoteAddress || "unknown";
  const method = req.method;
  const parsedUrl = new URL(req.url || "", `http://${req.headers.host}`);
  const route = parsedUrl.pathname;
  const start = process.hrtime();
  let status = 200;

  if (!checkRateLimit(ip)) {
    status = 429;
    recordFailure("rate_limit");
    res.writeHead(429);
    res.end("Too Many Requests");
  } else {
    try {
      if (method === "GET" && route === "/health") {
        const data = { status: "ok", uptime: process.uptime(), timestamp: new Date().toISOString() };
        sendJson(res, 200, data);
        status = 200;
        recordRequest(method, "health", status);
      } else if (method === "GET" && route === "/ready") {
        const data = { status: "ready", timestamp: new Date().toISOString() };
        sendJson(res, 200, data);
        status = 200;
        recordRequest(method, "ready", status);
      } else if (method === "GET" && route === "/metrics") {
        if (METRICS_USER && METRICS_PASS && !basicAuth(req, METRICS_USER, METRICS_PASS)) {
          status = 401;
          recordFailure("metrics_auth");
          res.writeHead(401, { "WWW-Authenticate": "Basic realm=\"Metrics\"" });
          return res.end("Unauthorized");
        }
        let out = "";
        for (const key in metrics.http_requests_total) {
          const [m, r, s] = key.split("_");
          out += `http_requests_total{method=\"${m}\",route=\"${r}\",status=\"${s}\"} ${metrics.http_requests_total[key]}\n`;
        }
        for (const r in metrics.http_request_failures_total) {
          out += `http_request_failures_total{route=\"${r}\"} ${metrics.http_request_failures_total[r]}\n`;
        }
        for (const entry of metrics.http_request_duration_seconds) {
          out += `http_request_duration_seconds{method=\"${entry.method}\",route=\"${entry.route}\",status=\"${entry.status}\"} ${entry.duration}\n`;
        }
        sendText(res, 200, out);
        status = 200;
        recordRequest(method, "metrics", status);
      } else if (method === "GET" && route === "/openapi.json") {
        sendJson(res, 200, openApiSpec);
        status = 200;
        recordRequest(method, "openapi", status);
      } else if (method === "GET" && route === "/docs") {
        if (DOCS_USER && DOCS_PASS && !basicAuth(req, DOCS_USER, DOCS_PASS)) {
          status = 401;
          recordFailure("docs_auth");
          res.writeHead(401, { "WWW-Authenticate": "Basic realm=\"Docs\"" });
          return res.end("Unauthorized");
        }
        const md = new MarkdownIt().use(markdownItGithub);
        const mdContent = "```json\n" + JSON.stringify(openApiSpec, null, 2) + "\n```";
        const html = md.render(mdContent);
        res.writeHead(200, {
          "Content-Type": "text/html",
          "Access-Control-Allow-Origin": CORS_ALLOWED_ORIGINS,
        });
        res.end(html);
        status = 200;
        recordRequest(method, "docs", status);
      } else {
        status = 404;
        recordFailure(route);
        res.writeHead(404);
        res.end("Not Found");
      }
    } catch (err) {
      status = 500;
      recordFailure(route);
      res.writeHead(500);
      res.end("Internal Server Error");
    }
  }

  // Record duration for each request except /metrics to avoid recursive metrics recording
  const [sec, nanosec] = process.hrtime(start);
  const duration = sec + nanosec / 1e9;
  if (route !== "/metrics") {
    recordDuration(method, route, status, duration);
  }
}

export function startServer(options = {}) {
  const port = options.port || PORT;
  const server = http.createServer(handler);
  server.listen(port);
  console.log(`Server started on port ${port}`);
  return server;
}

SOURCE_FILE_END


SOURCE_FILE_START Filepath: src/lib/main.js
#!/usr/bin/env node
// src/lib/main.js

// Initialize global callCount to support test mocks that reference it
if (typeof globalThis.callCount === "undefined") {
  globalThis.callCount = 0;
}

import { fileURLToPath } from "url";
import { z } from "zod";
import dotenv from "dotenv";

// ---------------------------------------------------------------------------------------------------------------------
// Environment configuration from .env file or environment variables or test values.
// ---------------------------------------------------------------------------------------------------------------------

dotenv.config();

if (process.env.VITEST || process.env.NODE_ENV === "development") {
  process.env.GITHUB_API_BASE_URL = process.env.GITHUB_API_BASE_URL || "https://api.github.com.test/";
  process.env.OPENAI_API_KEY = process.env.OPENAI_API_KEY || "key-test";
}

const configSchema = z.object({
  GITHUB_API_BASE_URL: z.string().optional(),
  OPENAI_API_KEY: z.string().optional(),
});

export const config = configSchema.parse(process.env);

// Global verbose mode flag
const VERBOSE_MODE = false;
// Global verbose stats flag
const VERBOSE_STATS = false;

// Helper function to format log entries
function formatLogEntry(level, message, additionalData = {}) {
  return {
    level,
    timestamp: new Date().toISOString(),
    message,
    ...additionalData,
  };
}

export function logConfig() {
  const logObj = formatLogEntry("info", "Configuration loaded", {
    config: {
      GITHUB_API_BASE_URL: config.GITHUB_API_BASE_URL,
      OPENAI_API_KEY: config.OPENAI_API_KEY,
    },
  });
  console.log(JSON.stringify(logObj));
}
logConfig();

// ---------------------------------------------------------------------------------------------------------------------
// Utility functions
// ---------------------------------------------------------------------------------------------------------------------

export function logInfo(message) {
  const additionalData = VERBOSE_MODE ? { verbose: true } : {};
  const logObj = formatLogEntry("info", message, additionalData);
  console.log(JSON.stringify(logObj));
}

export function logError(message, error) {
  const additionalData = { error: error ? error.toString() : undefined };
  if (VERBOSE_MODE && error && error.stack) {
    additionalData.stack = error.stack;
  }
  const logObj = formatLogEntry("error", message, additionalData);
  console.error(JSON.stringify(logObj));
}

// ---------------------------------------------------------------------------------------------------------------------
// AWS Utility functions
// ---------------------------------------------------------------------------------------------------------------------

export function createSQSEventFromDigest(digest) {
  return {
    Records: [
      {
        eventVersion: "2.0",
        eventSource: "aws:sqs",
        eventTime: new Date().toISOString(),
        eventName: "SendMessage",
        body: JSON.stringify(digest),
      },
    ],
  };
}

// ---------------------------------------------------------------------------------------------------------------------
// SQS Lambda Handlers
// ---------------------------------------------------------------------------------------------------------------------

export async function digestLambdaHandler(sqsEvent) {
  logInfo(`Digest Lambda received event: ${JSON.stringify(sqsEvent)}`);

  // If event.Records is an array, use it. Otherwise, treat the event itself as one record.
  const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];

  // Array to collect the identifiers of the failed records
  const batchItemFailures = [];

  for (const [index, sqsEventRecord] of sqsEventRecords.entries()) {
    try {
      const digest = JSON.parse(sqsEventRecord.body);
      logInfo(`Record ${index}: Received digest: ${JSON.stringify(digest)}`);
    } catch (error) {
      // If messageId is missing, generate a fallback identifier including record index
      const recordId =
        sqsEventRecord.messageId || `fallback-${index}-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      logError(`Error processing record ${recordId} at index ${index}`, error);
      logError(`Invalid JSON payload. Error: ${error.message}. Raw message: ${sqsEventRecord.body}`);
      batchItemFailures.push({ itemIdentifier: recordId });
    }
  }

  // Return the list of failed messages so that AWS SQS can attempt to reprocess them.
  return {
    batchItemFailures,
    handler: "src/lib/main.digestLambdaHandler",
  };
}

// ---------------------------------------------------------------------------------------------------------------------
// CLI Helper Functions
// ---------------------------------------------------------------------------------------------------------------------

// Function to generate CLI usage instructions
function generateUsage() {
  return `
Usage:
  --help                     Show this help message and usage instructions.
  --digest                   Run a full bucket replay simulating an SQS event.
  --version                  Show version information with current timestamp.
`;
}

// Process the --help flag
function processHelp(args) {
  if (args.includes("--help")) {
    console.log(generateUsage());
    return true;
  }
  return false;
}

// Process the --version flag
async function processVersion(args) {
  if (args.includes("--version")) {
    try {
      const { readFileSync } = await import("fs");
      const packageJsonPath = new URL("../../package.json", import.meta.url);
      const packageJson = JSON.parse(readFileSync(packageJsonPath, "utf8"));
      const versionInfo = {
        version: packageJson.version,
        timestamp: new Date().toISOString(),
      };
      console.log(JSON.stringify(versionInfo));
    } catch (error) {
      logError("Failed to retrieve version", error);
    }
    return true;
  }
  return false;
}

// Process the --digest flag
async function processDigest(args) {
  if (args.includes("--digest")) {
    const exampleDigest = {
      key: "events/1.json",
      value: "12345",
      lastModified: new Date().toISOString(),
    };
    const sqsEvent = createSQSEventFromDigest(exampleDigest);
    await digestLambdaHandler(sqsEvent);
    return true;
  }
  return false;
}

// ---------------------------------------------------------------------------------------------------------------------
// Main CLI
// ---------------------------------------------------------------------------------------------------------------------

export async function main(args = process.argv.slice(2)) {
  if (processHelp(args)) {
    if (VERBOSE_STATS) {
      console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));
    }
    return;
  }
  if (await processVersion(args)) {
    if (VERBOSE_STATS) {
      console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));
    }
    return;
  }
  if (await processDigest(args)) {
    if (VERBOSE_STATS) {
      console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));
    }
    return;
  }

  console.log("No command argument supplied.");
  console.log(generateUsage());
  if (VERBOSE_STATS) {
    console.log(JSON.stringify({ callCount: globalThis.callCount, uptime: process.uptime() }));
  }
}

// if (import.meta.url.endsWith(process.argv[1])) {
if (process.argv[1] === fileURLToPath(import.meta.url)) {
  main().catch((err) => {
    logError("Fatal error in main execution", err);
    process.exit(1);
  });
}

SOURCE_FILE_END



Test files (write new files or update files in sandbox/tests as necessary):
(Multiple files from both in writable locations and not.)
TEST_FILE_START File: sandbox/tests/server.test.js
import { describe, test, expect, beforeAll, afterAll } from 'vitest';
import http from 'http';
import { startServer } from '../source/server.js';

let server;
let base;

beforeAll(() => {
  server = startServer({ port: 0 });
  const addr = server.address();
  const port = typeof addr === 'object' && addr !== null ? addr.port : 3000;
  base = `http://127.0.0.1:${port}`;
});

afterAll(() => {
  server.close();
});

function request(path) {
  return new Promise((resolve, reject) => {
    http
      .get(base + path, (res) => {
        let data = '';
        res.on('data', (chunk) => (data += chunk));
        res.on('end', () => resolve({ statusCode: res.statusCode, body: data }));
      })
      .on('error', reject);
  });
}

describe('Server', () => {
  test('/health returns status ok', async () => {
    const { statusCode, body } = await request('/health');
    expect(statusCode).toBe(200);
    const json = JSON.parse(body);
    expect(json.status).toBe('ok');
    expect(typeof json.uptime).toBe('number');
    expect(typeof json.timestamp).toBe('string');
  });

  test('/metrics returns metrics without auth when not configured', async () => {
    const { statusCode, body } = await request('/metrics');
    expect(statusCode).toBe(200);
    expect(body).toContain('http_requests_total');
  });

  test('/ready returns status ready', async () => {
    const { statusCode, body } = await request('/ready');
    expect(statusCode).toBe(200);
    const json = JSON.parse(body);
    expect(json.status).toBe('ready');
    expect(typeof json.timestamp).toBe('string');
    expect(new Date(json.timestamp).toString()).not.toBe('Invalid Date');
  });

  test('metrics includes http_request_duration_seconds after calls', async () => {
    await request('/health');
    await request('/ready');
    const { statusCode, body } = await request('/metrics');
    expect(statusCode).toBe(200);
    const lines = body.split('\n');
    const durationLines = lines.filter((line) =>
      line.startsWith('http_request_duration_seconds'),
    );
    expect(durationLines.length).toBeGreaterThanOrEqual(2);
    durationLines.forEach((line) => {
      expect(line).toMatch(
        /^http_request_duration_seconds\{method="GET",route="\/(health|ready)",status="200"\} \d+\.?\d*$/,
      );
    });
  });
});

TEST_FILE_END


TEST_FILE_START File: tests/unit/main.test.js
import { describe, test, expect, vi, beforeAll, beforeEach, afterEach } from "vitest";

// Ensure that the global callCount is reset before tests that rely on it
beforeAll(() => {
  globalThis.callCount = 0;
});

// Reset callCount before each test in agenticHandler tests
beforeEach(() => {
  globalThis.callCount = 0;
});

// Clear all mocks after each test to tidy up
afterEach(() => {
  vi.clearAllMocks();
});

// Use dynamic import for the module to ensure mocks are applied correctly
let agenticLib;

// Default mock for openai used by tests that don't override it
vi.mock("openai", () => {
  return {
    Configuration: (config) => config,
    OpenAIApi: class {
      async createChatCompletion() {
        const dummyResponse = { fixed: "true", message: "dummy success", refinement: "none" };
        return {
          data: {
            choices: [{ message: { content: JSON.stringify(dummyResponse) } }]
          }
        };
      }
    }
  };
});

// Re-import the module after setting up the default mock
beforeAll(async () => {
  agenticLib = await import("../../src/lib/main.js");
});

describe("Main Module Import", () => {
  test("should be non-null", async () => {
    const mainModule = await import("../../src/lib/main.js");
    expect(mainModule).not.toBeNull();
  });
});

TEST_FILE_END


TEST_FILE_START File: tests/unit/module-index.test.js
// tests/unit/module-index.test.js
// src/lib/main.js
//
// This file is part of the Example Suite for `agentic-lib` see: https://github.com/xn-intenton-z2a/agentic-lib
// This file is licensed under the MIT License. For details, see LICENSE-MIT
//

import { describe, test, expect } from "vitest";
import anything from "@src/index.js";

describe("Index Module Exports", () => {
  test("module index should be defined", () => {
    expect(anything).toBeUndefined();
  });
});

TEST_FILE_END



Documentation files (write new files or update files in sandbox/docs as necessary):
(Multiple files from both in writable locations and not.)
DOCUMENTATION_FILE_START File: sandbox/docs/SERVER.md
# HTTP Server Feature

## Mission Alignment

The HTTP server provides critical observability endpoints (health, readiness, metrics, OpenAPI schema, docs) to support continuous monitoring and self-hosted documentation in line with our mission to enable autonomous, agentic workflows. For mission details, see [Mission Statement](../MISSION.md).

This document describes how to launch the built-in HTTP server provided by agentic-lib, exposing key endpoints.

## Programmatic Usage

Import the `startServer` function and call it in your Node.js application:

```js
import { startServer } from "@xn-intenton-z2a/agentic-lib";

// Optionally, pass configuration options:
const options = {
  port: process.env.PORT || 3000,
  // CORS_ALLOWED_ORIGINS, RATE_LIMIT_REQUESTS, METRICS_USER, METRICS_PASS, DOCS_USER, DOCS_PASS
};

startServer(options);
```

## Endpoints

- **GET /health**  
  Liveness probe. Returns JSON:
  ```json
  { "status": "ok", "uptime": <seconds>, "timestamp": "<ISO>" }
  ```

- **GET /ready**  
  Readiness probe. Returns JSON:
  ```json
  { "status": "ready", "timestamp": "<ISO>" }
  ```

- **GET /metrics**  
  Prometheus-formatted metrics. Exposes:
  - `http_requests_total{method,route,status}`
  - `http_request_failures_total{route}`
  - `http_request_duration_seconds{method,route,status}` (request duration histogram)
  Protected by Basic Auth if `METRICS_USER`/`METRICS_PASS` are set.

- **GET /openapi.json**  
  Returns the OpenAPI 3.0 schema for all endpoints.

- **GET /docs**  
  Renders the OpenAPI schema as HTML via Markdown. Protected by Basic Auth if `DOCS_USER`/`DOCS_PASS` are set.

## Configuration

Environment variables:

- `PORT` (default `3000`)
- `CORS_ALLOWED_ORIGINS` (default `*`)
- `RATE_LIMIT_REQUESTS` (requests per minute, default `60`)
- `METRICS_USER`, `METRICS_PASS` (for `/metrics` Basic Auth)
- `DOCS_USER`, `DOCS_PASS` (for `/docs` Basic Auth)

## Rate Limiting

IP-based token bucket with the following behavior:

- Each IP has a token bucket of `RATE_LIMIT_REQUESTS` tokens per minute.
- Exceeding the limit returns `429 Too Many Requests`.

DOCUMENTATION_FILE_END


DOCUMENTATION_FILE_START File: sandbox/docs/SQS_OVERVIEW.md
# SQS Utilities

## Mission Alignment

The SQS utilities in agentic-lib provide programmatic helpers to simulate, process, and handle AWS SQS events, supporting continuous, event-driven agentic workflows. For mission details, see [Mission Statement](../MISSION.md).

## Utilities Provided

- **createSQSEventFromDigest(digest)**  
  Generates an AWS SQS event wrapper for a given `digest` object. Useful for testing or simulating Lambda invocations.

- **digestLambdaHandler(sqsEvent)**  
  Processes SQS event records, parsing message bodies, logging successes and errors, and returning a `batchItemFailures` list for failed messages, aligning with AWS Lambda SQS batch failure handling.

## Usage Examples

### Creating an SQS Event

```js
import { createSQSEventFromDigest } from "@xn-intenton-z2a/agentic-lib";

const digest = {
  key: "events/1.json",
  value: "12345",
  lastModified: new Date().toISOString(),
};
const event = createSQSEventFromDigest(digest);
```

### Handling SQS Events in a Lambda Function

```js
import { digestLambdaHandler } from "@xn-intenton-z2a/agentic-lib";

export async function handler(event) {
  const result = await digestLambdaHandler(event);
  // Return result.batchItemFailures to SQS for retries of failed messages
  return { batchItemFailures: result.batchItemFailures };
}
```

## Error Handling

On JSON parse errors or invalid message bodies, the handler logs detailed errors and includes the failed record identifiers in `batchItemFailures`, allowing AWS SQS to retry processing.

DOCUMENTATION_FILE_END



README file (for context, read only): sandbox/README.md
README_FILE_START
## Mission Statement

This project is guided by the core mission of agentic-lib to enable autonomous, continuous agentic interactions through issues, branches, and pull requests. For full mission details, see [Mission Statement](../MISSION.md).

# agentic-lib

`agentic-lib` is a drop-in JavaScript SDK for autonomous GitHub workflows. Inspired by our mission to enable continuous, agentic interactions through issues, branches, and pull requests, this library provides core utilities to configure environments, handle AWS SQS events, power CLI-driven workflows, and optionally launch a self-hosted HTTP server for health, readiness, metrics, and documentation.

With `agentic-lib`, you can seamlessly integrate environment validation, structured logging, AWS utilities, Lambda handlers, CLI and programmatic workflows into your Node.js projects, ensuring reproducible, testable, and maintainable automation.

## Key Features

- **Environment configuration** (dotenv + Zod)  
  Mission Alignment: Validates and loads environment variables to ensure consistent, reproducible conditions essential for autonomous workflows.
- **Logging helpers** (logInfo, logError)  
  Mission Alignment: Provides structured, consistent logs to enable transparent audit trails for agentic operations.
- **AWS utilities** (createSQSEventFromDigest)  
  Mission Alignment: Simplifies SQS event creation for seamless integration into continuous, event-driven workflows.
- **Lambda handler** (digestLambdaHandler)  
  Mission Alignment: Automates message processing and error handling to maintain continuous, autonomous system reliability.
- **HTTP Server** (startServer function with `/health`, `/ready`, `/metrics`, `/openapi.json`, `/docs` endpoints, with configurable rate limiting and Basic Auth support)  
  Mission Alignment: Exposes self-hosted endpoints for observability, readiness checks, metrics (including request duration histogram), and interactive documentation, supporting ongoing, autonomous monitoring.
- **Request duration histogram** (`http_request_duration_seconds{method,route,status}`)  
  Mission Alignment: Exposes request duration metrics for performance monitoring.
- **CLI flags**: `--help`, `--version`, `--digest`  
  Mission Alignment: Offers intuitive CLI interfaces to drive agentic workflows directly from the command line.

README_FILE_END

MISSION file (for context, read only): MISSION.md
MISSION_FILE_START
# Mission Statement

**agentic‑lib** Is a JavaScript library which can be used as a drop in JS implementation or wholesale replacement for 
the steps, jobs, and re-usable workflows below in this repository. It is designed to be used in a GitHub Actions 
workflow to enable your repository to operate in an “agentic” manner. In our system, autonomous workflows communicate
through branches and issues to continuously review, fix, update, and evolve your code. Each workflow is designed to be
invoked using GitHub’s `workflow_call` event, so they can be composed together like an SDK.

MISSION_FILE_END

Contributing file (for context, read only): CONTRIBUTING.md
CONTRIBUTING_FILE_START
# agentic‑lib

This document outlines our guidelines for human and automated contributions, ensuring that our core library remains 
robust, testable, and efficient in powering our reusable GitHub Workflows.

## How to Contribute

The guidelines below apply to human or automated contributions:

1. **Report Issues or Ideas:**
    - Open an issue on GitHub to share bug reports, feature requests, or any improvements you envision.
    - Clear descriptions and reproducible steps are highly appreciated.

2. **Submit Pull Requests:**
    - Implement your changes and push them to a new branch, ensuring you follow the 
      existing coding style and standards.
    - Add tests to cover any new functionality.
    - Update documentation if your changes affect usage or workflow behavior.
    - Submit your pull request for review.

## Guidelines

- **Features:**
    - Clear Objective & Scope: Define the feature with a concise description outlining its purpose, scope, and the specific problem it solves for the end user.
    - Value Proposition: Articulate the tangible benefits of the feature, including improved functionality, performance, or user experience.
    - Success Criteria & Requirements: List measurable success criteria and requirements, including performance benchmarks, usability standards, and stability expectations, to guide development and testing.
    - Testability & Stability: Ensure the feature can be verified through both automated tests and user acceptance criteria. Specify any necessary rollback or fail-safe mechanisms to maintain system stability.
    - Dependencies & Constraints: Identify any dependencies (external libraries, APIs, etc.), assumptions, and limitations that could impact feature delivery or future enhancements.
    - User Scenarios & Examples: Provide illustrative use cases or scenarios that demonstrate how the feature will be used in real-world situations, making it easier for both developers and stakeholders to understand its impact.
    - Verification & Acceptance: Define clear verification steps and acceptance criteria to ensure the feature meets its intended requirements. This should include detailed plans for unit tests, integration tests, manual user acceptance tests, and code reviews. Specify measurable outcomes that must be achieved for the feature to be considered successfully delivered and stable.

- **Code Quality:**
    - Ensure there are tests that cover your changes and any likely new cases they introduce.
    - When making a change remain consistent with the existing code style and structure.
    - When adding new functionality, consider if some unused or superseded code should be removed.

- **Compatibility:**
    - Ensure your code runs on Node 20 and adheres to ECMAScript Module (ESM) standards.
    - Tests use vitest and competing test frameworks should not be added.
    - Mocks in tests must not interfere with other tests.

- **Testing:**
    - The command `npm test` should invoke the tests added for the new functionality (and pass).
    - If you add new functionality, ensure it is covered by tests.

- **Documentation:**
    - When making a change to the main source file, review the readme to see if it needs to be updated and if so, update it.
    - Where the source exports a function, consider that part of the API of the library and document it in the readme.
    - Where the source stands-up an HTTP endpoint, consider that part of the API of the library and document it in the readme.
    - Include usage examples including inline code usage and CLI and HTTP invocation, API references.

- **README:**
    - The README should begin with something inspired by the mission statement and describe the current state of the repository (rather than the journey)
    - The README should include a link to MISSION.md, CONTRIBUTING.md, LICENSE.md.
    - The README should include a link to the intentïon `agentic-lib` GitHub Repository which is https://github.com/xn-intenton-z2a/agentic-lib.

## Sandbox mode

Please note that the automation features of this repository are in sandbox mode. This means that
automated changes should only be applied to the sandbox paths which are shown below:
```yaml
paths:
  targetTestsPath:
    path: 'sandbox/tests/'
    permissions: [ 'write' ]
  targetSourcePath:
    path: 'sandbox/source/'
    permissions: [ 'write' ]
  documentationPath:
    path: 'sandbox/docs/'
    permissions: [ 'write' ]
  readmeFilepath:
    path: 'sandbox/README.md'
    permissions: [ 'write' ]
```

CONTRIBUTING_FILE_END

Dependencies file (for context, read only): package.json
DEPENDENCIES_FILE_START
{
  "name": "@xn-intenton-z2a/agentic-lib",
  "version": "6.6.5-0",
  "description": "Agentic-lib Agentic Coding Systems SDK powering automated GitHub workflows.",
  "type": "module",
  "main": "src/lib/main.js",
  "scripts": {
    "build": "echo \"Nothing to build\"",
    "formatting": "prettier --check",
    "formatting-fix": "prettier --write",
    "linting": "eslint",
    "linting-json": "eslint --format=@microsoft/eslint-formatter-sarif",
    "linting-fix": "eslint --fix",
    "update-to-minor": "npx npm-check-updates --upgrade --enginesNode --target minor --verbose --install always",
    "update-to-greatest": "npx npm-check-updates --upgrade --enginesNode --target greatest --verbose --install always --reject \"alpha\"",
    "test": "vitest tests/unit/*.test.js sandbox/tests/*.test.js",
    "test:unit": "vitest --coverage tests/unit/*.test.js sandbox/tests/*.test.js",
    "start": "node src/lib/main.js"
  },
  "keywords": [],
  "author": "https://github.com/xn-intenton-z2a",
  "license": "GPL-3.0, MIT",
  "dependencies": {
    "@aws-sdk/client-lambda": "^3.804.0",
    "@xn-intenton-z2a/s3-sqs-bridge": "^0.24.0",
    "chalk": "^5.4.1",
    "change-case": "^5.4.4",
    "dayjs": "^1.11.13",
    "dotenv": "^16.5.0",
    "ejs": "^3.1.10",
    "figlet": "^1.8.1",
    "js-yaml": "^4.1.0",
    "lodash": "^4.17.21",
    "minimatch": "^10.0.1",
    "openai": "^4.97.0",
    "seedrandom": "^3.0.5",
    "zod": "^3.24.4"
  },
  "devDependencies": {
    "@microsoft/eslint-formatter-sarif": "^3.1.0",
    "@vitest/coverage-v8": "^3.1.3",
    "aws-cdk": "^2.1013.0",
    "eslint": "^9.25.0",
    "eslint-config-google": "^0.14.0",
    "eslint-config-prettier": "^8.10.0",
    "eslint-plugin-import": "^2.31.0",
    "eslint-plugin-prettier": "^5.4.0",
    "eslint-plugin-promise": "^7.2.1",
    "eslint-plugin-react": "^7.37.5",
    "eslint-plugin-security": "^3.0.1",
    "eslint-plugin-sonarjs": "^3.0.2",
    "figlet": "^1.8.1",
    "markdown-it": "^14.1.0",
    "markdown-it-github": "^0.5.0",
    "npm-check-updates": "^18.0.1",
    "prettier": "^3.5.3",
    "vitest": "^3.1.3"
  },
  "engines": {
    "node": ">=20.0.0"
  },
  "files": [
    "package.json"
  ],
  "publishConfig": {
    "registry": "https://npm.pkg.github.com"
  }
}

DEPENDENCIES_FILE_END

Formatting file (for context, read only): .prettierrc
FORMATTING_FILE_START
{
  "singleQuote": false,
  "trailingComma": "all",
  "printWidth": 120,
  "tabWidth": 2,
  "useTabs": false,
  "quoteProps": "consistent",
  "overrides": [
    {
      "files": ".prettierrc",
      "options": { "parser": "json" }
    }
  ]
}

FORMATTING_FILE_END

Linting file (for context, read only): eslint.config.js
LINTING_FILE_START
import js from "@eslint/js";
import google from "eslint-config-google";
import eslintPluginPrettierRecommended from "eslint-plugin-prettier/recommended";
import globals from "globals";
import promise from "eslint-plugin-promise";
import security from "eslint-plugin-security";
import sonarjs from "eslint-plugin-sonarjs";
import react from "eslint-plugin-react";
import importPlugin from "eslint-plugin-import";

const modifiedGoogleConfig = { ...google, rules: { ...google.rules } };
delete modifiedGoogleConfig.rules["valid-jsdoc"];
delete modifiedGoogleConfig.rules["require-jsdoc"];

/** @type {import('eslint').Linter.FlatConfig[]} */
export default [
  js.configs.recommended,
  modifiedGoogleConfig,
  eslintPluginPrettierRecommended,
  {
    plugins: {
      promise,
      security,
      sonarjs,
      react,
      import: importPlugin,
    },
    languageOptions: {
      ecmaVersion: 2023,
      sourceType: "module",
      globals: {
        ...globals.node,
      },
    },
    rules: {
      "prettier/prettier": "error",
      ...promise.configs.recommended.rules,
      "promise/avoid-new": "warn",
      "promise/no-new-statics": "error",
      "promise/valid-params": "error",
      "promise/prefer-await-to-then": "warn",

      ...sonarjs.configs.recommended.rules,
      "sonarjs/no-nested-conditional": "warn",
      "sonarjs/pseudo-random": "warn",
      "sonarjs/sonar-no-fallthrough": "off",
      "sonarjs/os-command": "off",
      "sonarjs/todo-tag": "off",
      "sonarjs/no-commented-code": "off",

      // Enabled non-recommended rules (SonarJS)
      "sonarjs/no-inverted-boolean-check": "warn",
      "sonarjs/no-useless-catch": "warn",

      // Local customizations
      "no-unused-vars": ["error", { argsIgnorePattern: "^_" }],
      "no-extra-semi": 2,
      "object-curly-newline": ["error", { consistent: true }],
      "array-element-newline": ["error", "consistent", { multiline: true, minItems: 10 }],
      "import/newline-after-import": ["error", { count: 1 }],
      "camelcase": "off",
      "import/no-amd": "error",
      "import/no-commonjs": "error",
      "import/no-import-module-exports": "error",
      "import/no-cycle": "error",
      "import/no-dynamic-require": "error",
      "import/no-self-import": "off",
      "import/no-unresolved": "off",
      "import/no-useless-path-segments": "error",
      "import/no-duplicates": "error",
      "sonarjs/fixme-tag": "warn",
    },
  },
  {
    files: ["**/*.js"],
    ignores: ["**/tests/**/*.js", "**/*.test.js", "eslint.config.js"],
    rules: {
      ...security.configs.recommended.rules,
      "security/detect-non-literal-regexp": "off",
    },
  },
  {
    settings: {
      react: {
        version: "18", // With no react installed we can't use "detect"
      },
    },
  },
  {
    ignores: ["build/", "coverage/", "dist/", "exports/", "node_modules/", "eslint.config.js"],
  },
];

LINTING_FILE_END

Agent configuration file (for context, read only):
AGENT_CONFIG_FILE_START
# Which agentic-lib workflow schedule should be used?
schedule: schedule-3

# Mapping for from symbolic keys to filepaths for access by agentic-lib workflows with limits and access permissions
paths:
  # Filepaths for elaborator workflows
  missionFilepath:
    path: 'MISSION.md'
  librarySourcesFilepath:
    path: 'sandbox/SOURCES.md'
    permissions: [ 'write' ]
    limit: 8
  libraryDocumentsPath:
    path: 'sandbox/library/'
    permissions: [ 'write' ]
    limit: 32
  featuresPath:
    path: 'sandbox/features/'
    permissions: [ 'write' ]
    limit: 1

  # Filepaths for engineer workflows
  contributingFilepath:
    path: 'CONTRIBUTING.md'
  targetTestsPath:
    path: 'sandbox/tests/'
    permissions: [ 'write' ]
  otherTestsPaths:
    paths: [ 'tests/unit/' ]
  targetSourcePath:
    path: 'sandbox/source/'
    permissions: [ 'write' ]
  otherSourcePaths:
    paths: [ 'src/lib/' ]
  dependenciesFilepath:
    path: 'package.json'
  documentationPath:
    path: 'sandbox/docs/'
    permissions: [ 'write' ]

  # Filepaths for maintainer workflows
  formattingFilepath:
    path: '.prettierrc'
  lintingFilepath:
    path: 'eslint.config.js'
  readmeFilepath:
    path: 'sandbox/README.md'
    permissions: [ 'write' ]

# Execution commands
buildScript: 'npm run build'
testScript: 'npm test'
mainScript: 'npm run start'

# How many issues should be available to be picked up?
featureDevelopmentIssuesWipLimit: 2
maintenanceIssuesWipLimit: 1

# How many attempts should be made to work on an issue?
attemptsPerBranch: 2
attemptsPerIssue: 2

# Web publishing
docRoot: 'public'

# Sandbox configuration
sandbox:
  sandboxReset: 'true'
  sandboxPath: 'sandbox'

# Repository seeding
#seeding:
#  repositoryReseed: 'true'
#  missionFilepath: 'seeds/zero-MISSION.md'
#  sourcePath: 'seeds/zero-main.js'
#  testsPath: 'seeds/zero-main.test.js'
#  dependenciesFilepath: 'seeds/zero-package.json'
#  readmeFilepath: 'seeds/zero-README.md'

intentionBot:
  intentionFilepath: 'intentïon.md'

AGENT_CONFIG_FILE_END

Issue details:
ISSUE_START
title: Add OpenAI usage metrics collection and /openai-usage endpoint
description:
Title: Add OpenAI Usage Metrics Collection and /openai-usage Endpoint

Description:
Enhance the built-in HTTP server (sandbox/source/server.js) to track and expose real-time OpenAI API usage metrics. This will allow users to monitor request counts, failures, and token consumption for all OpenAI ChatCompletion operations directly through a Prometheus-style endpoint.

Acceptance Criteria:

1. Extend In-Memory Metrics:
   - Add three new counters to the `metrics` object:
     • `openai_requests_total{endpoint, status}`
     • `openai_request_failures_total{endpoint}`
     • `openai_tokens_consumed_total{model, endpoint}`

2. Instrument OpenAI Calls:
   - In all functions that call OpenAI ChatCompletion (e.g., `generateChatCompletion`, `refineIssueDescription`, and any direct `client.chat.completions.create` calls):
     • On success, increment `openai_requests_total{endpoint="chat.completions",status="200"}`.
     • On failure, increment `openai_request_failures_total{endpoint="chat.completions"}`.
     • On success, read the returned token usage from the OpenAI response and increment `openai_tokens_consumed_total{model="<model>",endpoint="chat.completions"}` by the number of tokens consumed.

3. New `/openai-usage` Route:
   - Implement `GET /openai-usage` in `handler`:
     • Return HTTP 200 with Prometheus-formatted text including all three OpenAI counters.
     • If `METRICS_USER` and `METRICS_PASS` are set, enforce Basic Auth:
       - Missing or invalid credentials → HTTP 401 with `WWW-Authenticate` header.
       - Valid credentials → HTTP 200.
     • Record requests in `http_requests_total` labeled `{method="GET",route="openai_usage",status="<code>"}`.

4. OpenAPI Spec & Docs Update:
   - Add `/openai-usage` under `paths` in the `openApiSpec` constant with a 200 response description.
   - Update `sandbox/docs/SERVER.md`:
     • Document the new endpoint, its output format, and auth behavior.
   - Update `sandbox/README.md`:
     • Add a bullet under **Key Features** for the OpenAI usage metrics endpoint.

5. Test Coverage:
   - Add or extend tests in `sandbox/tests/server.test.js` to verify:
     • GET `/openai-usage` without auth returns 200 and includes `openai_requests_total`, `openai_request_failures_total`, and `openai_tokens_consumed_total` lines when no auth is configured.
     • With `METRICS_USER`/`METRICS_PASS` set but no credentials → HTTP 401 and correct `WWW-Authenticate` header.
     • With valid credentials → HTTP 200 and all OpenAI metrics present.
     • Metrics counters increment correctly after sample ChatCompletion calls (can simulate via a test helper).

6. Verification Steps:
   - Run `npm test` to ensure all unit and integration tests pass.
   - Start the server locally and use `curl` or similar to confirm `/openai-usage` returns properly formatted metrics and enforces auth when configured.

This feature aligns with our core mission of providing observability and autonomy in agentic workflows by exposing detailed usage metrics for OpenAI operations.
comments:
Author:github-actions[bot], Created:2025-05-18T14:33:20Z, Comment: Workflow name: flow-feature-maintenance
Workflow run URL: https://github.com/xn-intenton-z2a/agentic-lib/actions/runs/15096950798
Workflow event: schedule
Workflow inputs: null
HEAD of main URL: https://github.com/xn-intenton-z2a/agentic-lib/commit/4fd4eecd61f31bcc56d436dcc11a28baf01061bd
Author:github-actions[bot], Created:2025-05-18T14:34:07Z, Comment: ## Relevant Library Document: OPENAI_NODE_SDK.md

Table of Contents
1 Installation
2 Client Initialization
3 Responses API
4 Chat Completions API
5 Streaming Responses
6 File Uploads
7 Error Handling
8 Retries and Backoff
9 Timeouts
10 Request IDs
11 Auto-Pagination
12 Realtime API Beta
13 Azure OpenAI
14 Advanced Usage
15 Browser Support
16 Requirements

1 Installation
  Commands:
    npm install openai
    deno add jsr:@openai/openai
    npx jsr add @openai/openai

2 Client Initialization
  new OpenAI({
    apiKey?:string,         default process.env.OPENAI_API_KEY
    maxRetries?:number,     default 2
    timeout?:number,        default 600000
    fetch?:(url,init)=>Promise<Response>
    httpAgent?:http.Agent|HttpsProxyAgent
    dangerouslyAllowBrowser?:boolean default false
  })
  new AzureOpenAI({
    azureADTokenProvider:()=>Promise<string>
    apiVersion:string       e.g. '2024-10-01-preview'
  })

3 Responses API
  Method: client.responses.create
  Params:{ model:string; instructions?:string; input:string; stream?:boolean }
  Options:{ maxRetries?:number; timeout?:number; httpAgent?:http.Agent }
  Returns:Promise<{ output_text:string; _request_id:string }>

4 Chat Completions API
  Method: client.chat.completions.create
  Params:{ model:string; messages:{role:'system'|'user'|'assistant'|'developer';content:string}[]; stream?:boolean }
  Returns:Promise<{ choices:{message:{role:string;content:string}}[]; _request_id:string }>

5 Streaming Responses
  Call create with stream:true
  Returns AsyncIterable<{ type:string; data:string; id?:string }>

6 File Uploads
  Method: client.files.create
  Params:{ file:ReadStream|File|Response|ReturnType<typeof toFile>; purpose:'fine-tune' }

7 Error Handling
  Throws subclasses of OpenAI.APIError:
    BadRequestError(400), AuthenticationError(401), PermissionDeniedError(403), NotFoundError(404), UnprocessableEntityError(422), RateLimitError(429), InternalServerError(>=500), APIConnectionError
  Timeout throws APIConnectionTimeoutError

8 Retries and Backoff
  Default maxRetries=2
  Retries on network errors, 408,409,429,>=500
  Override via maxRetries

9 Timeouts
  Default timeout=600000
  Override via timeout
  Throws APIConnectionTimeoutError

10 Request IDs
  Responses include _request_id from x-request-id
  .withResponse() returns { data, request_id, response }

11 Auto-Pagination
  List methods return PagedResult<T> with .data[], .hasNextPage(), .getNextPage()
  Async iteration across pages

12 Realtime API Beta
  import { OpenAIRealtimeWebSocket } from 'openai/beta/realtime/websocket'
  new OpenAIRealtimeWebSocket({ model:string })
  on('response.text.delta', callback)

13 Azure OpenAI
  import { AzureOpenAI } from 'openai'
  new AzureOpenAI({ azureADTokenProvider, apiVersion })

14 Advanced Usage
  .asResponse(), .withResponse()
  client.get/post for undocumented endpoints
  ts-ignore extra params

15 Browser Support
  dangerouslyAllowBrowser:true to enable
  exposes secret key risks

16 Requirements
  TS>=4.5, Node>=18, Deno>=1.28, Bun>=1.0, Cloudflare Workers, Vercel Edge, Jest28(node), Nitro>=2.6
Author:github-actions[bot], Created:2025-05-18T14:34:07Z, Comment: This issue has been reviewed and marked as 'ready'. The description has been updated with testable acceptance criteria, and relevant library documents ([OPENAI_NODE_SDK.md], 1 in total) have been added as comments.
Author:github-actions[bot], Created:2025-05-18T14:34:32Z, Comment: Workflow name: flow-publish-stats
Workflow run URL: https://github.com/xn-intenton-z2a/agentic-lib/actions/runs/15096985668
Workflow event: schedule
Workflow inputs: null
HEAD of main URL: https://github.com/xn-intenton-z2a/agentic-lib/commit/6b77e22cf9dcf27dd946a708179fb1b491f23bf8
Author:github-actions[bot], Created:2025-05-18T14:37:13Z, Comment: Workflow name: discussions-bot
Workflow run URL: https://github.com/xn-intenton-z2a/agentic-lib/actions/runs/15096956317
Workflow event: discussion_comment
Workflow inputs: null
HEAD of main URL: https://github.com/xn-intenton-z2a/agentic-lib/commit/547250600a7bc2e65e3c3d6e15c571732592d79f
Author:github-actions[bot], Created:2025-05-18T14:38:04Z, Comment: ## Relevant Library Document: OPENAI_NODE_SDK.md

Table of Contents
1 Installation
2 Client Initialization
3 Responses API
4 Chat Completions API
5 Streaming Responses
6 File Uploads
7 Error Handling
8 Retries and Backoff
9 Timeouts
10 Request IDs
11 Auto-Pagination
12 Realtime API Beta
13 Azure OpenAI
14 Advanced Usage
15 Browser Support
16 Requirements

1 Installation
  Commands:
    npm install openai
    deno add jsr:@openai/openai
    npx jsr add @openai/openai

2 Client Initialization
  new OpenAI({
    apiKey?:string,         default process.env.OPENAI_API_KEY
    maxRetries?:number,     default 2
    timeout?:number,        default 600000
    fetch?:(url,init)=>Promise<Response>
    httpAgent?:http.Agent|HttpsProxyAgent
    dangerouslyAllowBrowser?:boolean default false
  })
  new AzureOpenAI({
    azureADTokenProvider:()=>Promise<string>
    apiVersion:string       e.g. '2024-10-01-preview'
  })

3 Responses API
  Method: client.responses.create
  Params:{ model:string; instructions?:string; input:string; stream?:boolean }
  Options:{ maxRetries?:number; timeout?:number; httpAgent?:http.Agent }
  Returns:Promise<{ output_text:string; _request_id:string }>

4 Chat Completions API
  Method: client.chat.completions.create
  Params:{ model:string; messages:{role:'system'|'user'|'assistant'|'developer';content:string}[]; stream?:boolean }
  Returns:Promise<{ choices:{message:{role:string;content:string}}[]; _request_id:string }>

5 Streaming Responses
  Call create with stream:true
  Returns AsyncIterable<{ type:string; data:string; id?:string }>

6 File Uploads
  Method: client.files.create
  Params:{ file:ReadStream|File|Response|ReturnType<typeof toFile>; purpose:'fine-tune' }

7 Error Handling
  Throws subclasses of OpenAI.APIError:
    BadRequestError(400), AuthenticationError(401), PermissionDeniedError(403), NotFoundError(404), UnprocessableEntityError(422), RateLimitError(429), InternalServerError(>=500), APIConnectionError
  Timeout throws APIConnectionTimeoutError

8 Retries and Backoff
  Default maxRetries=2
  Retries on network errors, 408,409,429,>=500
  Override via maxRetries

9 Timeouts
  Default timeout=600000
  Override via timeout
  Throws APIConnectionTimeoutError

10 Request IDs
  Responses include _request_id from x-request-id
  .withResponse() returns { data, request_id, response }

11 Auto-Pagination
  List methods return PagedResult<T> with .data[], .hasNextPage(), .getNextPage()
  Async iteration across pages

12 Realtime API Beta
  import { OpenAIRealtimeWebSocket } from 'openai/beta/realtime/websocket'
  new OpenAIRealtimeWebSocket({ model:string })
  on('response.text.delta', callback)

13 Azure OpenAI
  import { AzureOpenAI } from 'openai'
  new AzureOpenAI({ azureADTokenProvider, apiVersion })

14 Advanced Usage
  .asResponse(), .withResponse()
  client.get/post for undocumented endpoints
  ts-ignore extra params

15 Browser Support
  dangerouslyAllowBrowser:true to enable
  exposes secret key risks

16 Requirements
  TS>=4.5, Node>=18, Deno>=1.28, Bun>=1.0, Cloudflare Workers, Vercel Edge, Jest28(node), Nitro>=2.6
Author:github-actions[bot], Created:2025-05-18T14:38:04Z, Comment: This issue has been reviewed and marked as 'ready'. The description has been updated with testable acceptance criteria, and relevant library documents ([OPENAI_NODE_SDK.md], 1 in total) have been added as comments.
Author:github-actions[bot], Created:2025-05-18T14:38:34Z, Comment: Workflow name: flow-maintenance-activity-to-issue
Workflow run URL: https://github.com/xn-intenton-z2a/agentic-lib/actions/runs/15097010409
Workflow event: schedule
Workflow inputs: null
HEAD of main URL: https://github.com/xn-intenton-z2a/agentic-lib/commit/547250600a7bc2e65e3c3d6e15c571732592d79f
Author:github-actions[bot], Created:2025-05-18T14:39:13Z, Comment: Workflow name: flow-publish-stats
Workflow run URL: https://github.com/xn-intenton-z2a/agentic-lib/actions/runs/15097024251
Workflow event: workflow_run
Workflow inputs: null
HEAD of main URL: https://github.com/xn-intenton-z2a/agentic-lib/commit/7e8190696a09d15ef30d487f5797a4161e45a616
Author:github-actions[bot], Created:2025-05-18T14:39:22Z, Comment: ## Relevant Library Document: OPENAI_NODE_SDK.md

Table of Contents
1 Installation
2 Client Initialization
3 Responses API
4 Chat Completions API
5 Streaming Responses
6 File Uploads
7 Error Handling
8 Retries and Backoff
9 Timeouts
10 Request IDs
11 Auto-Pagination
12 Realtime API Beta
13 Azure OpenAI
14 Advanced Usage
15 Browser Support
16 Requirements

1 Installation
  Commands:
    npm install openai
    deno add jsr:@openai/openai
    npx jsr add @openai/openai

2 Client Initialization
  new OpenAI({
    apiKey?:string,         default process.env.OPENAI_API_KEY
    maxRetries?:number,     default 2
    timeout?:number,        default 600000
    fetch?:(url,init)=>Promise<Response>
    httpAgent?:http.Agent|HttpsProxyAgent
    dangerouslyAllowBrowser?:boolean default false
  })
  new AzureOpenAI({
    azureADTokenProvider:()=>Promise<string>
    apiVersion:string       e.g. '2024-10-01-preview'
  })

3 Responses API
  Method: client.responses.create
  Params:{ model:string; instructions?:string; input:string; stream?:boolean }
  Options:{ maxRetries?:number; timeout?:number; httpAgent?:http.Agent }
  Returns:Promise<{ output_text:string; _request_id:string }>

4 Chat Completions API
  Method: client.chat.completions.create
  Params:{ model:string; messages:{role:'system'|'user'|'assistant'|'developer';content:string}[]; stream?:boolean }
  Returns:Promise<{ choices:{message:{role:string;content:string}}[]; _request_id:string }>

5 Streaming Responses
  Call create with stream:true
  Returns AsyncIterable<{ type:string; data:string; id?:string }>

6 File Uploads
  Method: client.files.create
  Params:{ file:ReadStream|File|Response|ReturnType<typeof toFile>; purpose:'fine-tune' }

7 Error Handling
  Throws subclasses of OpenAI.APIError:
    BadRequestError(400), AuthenticationError(401), PermissionDeniedError(403), NotFoundError(404), UnprocessableEntityError(422), RateLimitError(429), InternalServerError(>=500), APIConnectionError
  Timeout throws APIConnectionTimeoutError

8 Retries and Backoff
  Default maxRetries=2
  Retries on network errors, 408,409,429,>=500
  Override via maxRetries

9 Timeouts
  Default timeout=600000
  Override via timeout
  Throws APIConnectionTimeoutError

10 Request IDs
  Responses include _request_id from x-request-id
  .withResponse() returns { data, request_id, response }

11 Auto-Pagination
  List methods return PagedResult<T> with .data[], .hasNextPage(), .getNextPage()
  Async iteration across pages

12 Realtime API Beta
  import { OpenAIRealtimeWebSocket } from 'openai/beta/realtime/websocket'
  new OpenAIRealtimeWebSocket({ model:string })
  on('response.text.delta', callback)

13 Azure OpenAI
  import { AzureOpenAI } from 'openai'
  new AzureOpenAI({ azureADTokenProvider, apiVersion })

14 Advanced Usage
  .asResponse(), .withResponse()
  client.get/post for undocumented endpoints
  ts-ignore extra params

15 Browser Support
  dangerouslyAllowBrowser:true to enable
  exposes secret key risks

16 Requirements
  TS>=4.5, Node>=18, Deno>=1.28, Bun>=1.0, Cloudflare Workers, Vercel Edge, Jest28(node), Nitro>=2.6
Author:github-actions[bot], Created:2025-05-18T14:39:23Z, Comment: ## Relevant Library Document: OPENAI_SDK.md

Table of Contents
1. Installation and Imports
2. Client Initialization Options
3. Text Generation (Responses API)
4. Chat Completions API
5. Streaming with SSE
6. File Upload Patterns
7. Error Handling and Status Mapping
8. Retry Strategy and Configuration
9. Timeout Configuration
10. Request ID Extraction
11. Auto-pagination Patterns
12. Realtime WebSocket API
13. Azure OpenAI Integration
14. Raw Response Access
15. Custom HTTP Requests
16. Client Fetch Middleware
17. Logging and Debugging
18. HTTP/S Agent Configuration
19. Semantic Versioning Guidelines
20. Runtime Requirements

1. Installation and Imports
- npm install openai
- import OpenAI from 'openai'
- deno add jsr:@openai/openai or npx jsr add @openai/openai

2. Client Initialization Options
Constructor OpenAI({
  apiKey?: string;           // default from OPENAI_API_KEY
  maxRetries?: number;       // default 2
  timeout?: number;          // default 600000 ms
  httpAgent?: Agent;
  fetch?: (RequestInfo, RequestInit?) => Promise<Response>;
  dangerouslyAllowBrowser?: boolean;
  azureADTokenProvider?: TokenProvider;
  apiVersion?: string;
});

3. Text Generation (Responses API)
Method: client.responses.create(params, options?)
Params:
  model: string
  instructions?: string
  input?: string
  stream?: boolean
Options:
  maxRetries?: number
  timeout?: number
  httpAgent?: Agent
  fetch?: function
Returns:
  Promise<{ output_text: string; _request_id: string }>
  or AsyncIterable<ServerSentEvent> if stream=true

4. Chat Completions API
Method: client.chat.completions.create(params, options?)
Params:
  model: string
  messages: Array<{ role: 'system'|'developer'|'user'|'assistant'; content: string }>
  stream?: boolean
Returns:
  Promise<{ choices: Array<{ message: { role: string; content: string } }>; _request_id: string }>
  or AsyncIterable<ServerSentEvent>

5. Streaming with SSE
- Set stream:true in create()
- Iterate AsyncIterable<ServerSentEvent>

6. File Upload Patterns
Method: client.files.create({ file, purpose })
file can be: fs.ReadStream | File | Response | toFile(Buffer,email) helper
purpose: string (e.g., 'fine-tune')

7. Error Handling and Status Mapping
Throws APIError subclasses with: request_id, status, name, headers
Mapping: 400 BadRequestError; 401 AuthenticationError; 403 PermissionDeniedError; 404 NotFoundError; 422 UnprocessableEntityError; 429 RateLimitError; >=500 InternalServerError; connection errors APIConnectionError

8. Retry Strategy and Configuration
Default retries: 2 for connection errors, 408,409,429,>=500
Override: global maxRetries in constructor or per-request in options

9. Timeout Configuration
Default: 600000 ms; global override in constructor; per-request override in options
On timeout: throws APIConnectionTimeoutError; still retried by default

10. Request ID Extraction
_All response objects include _request_id_
Use .withResponse() to destructure { data, request_id }

11. Auto-pagination Patterns
Use for awaitof on list() methods; or manual via page.hasNextPage() and page.getNextPage()

12. Realtime WebSocket API
Import OpenAIRealtimeWebSocket from 'openai/beta/realtime/websocket'
Constructor: new OpenAIRealtimeWebSocket({ model: string })
Events: 'response.text.delta'

13. Azure OpenAI Integration
Import AzureOpenAI; require azureADTokenProvider, apiVersion
auth via DefaultAzureCredential + getBearerTokenProvider

14. Raw Response Access
Use .asResponse() or .withResponse() to access fetch Response headers and statusText

15. Custom HTTP Requests
Use client.get/post/put/delete('/path', { body, query, headers })

16. Client Fetch Middleware
Pass fetch override in constructor to log/modify requests

17. Logging and Debugging
Set DEBUG=true to log all requests and responses automatically

18. HTTP/S Agent Configuration
Pass httpAgent in constructor or per-request; supports proxies (HttpsProxyAgent)

19. Semantic Versioning Guidelines
Follows SemVer; minor may include type-only or internal changes

20. Runtime Requirements
TypeScript>=4.5; Node >=18; Deno>=1.28; Bun>=1.0; Cloudflare Workers; Vercel Edge; Jest>=28; Nitro>=2.6; Browser support opt-in
Author:github-actions[bot], Created:2025-05-18T14:39:23Z, Comment: This issue has been reviewed and marked as 'ready'. The description has been updated with testable acceptance criteria, and relevant library documents ([OPENAI_NODE_SDK.md,OPENAI_SDK.md], 2 in total) have been added as comments.
Author:github-actions[bot], Created:2025-05-18T14:39:45Z, Comment: Workflow name: flow-maintenance-activity-to-issue
Workflow run URL: https://github.com/xn-intenton-z2a/agentic-lib/actions/runs/15097010409
Workflow event: schedule
Workflow inputs: null
HEAD of main URL: https://github.com/xn-intenton-z2a/agentic-lib/commit/7e8190696a09d15ef30d487f5797a4161e45a616
Author:github-actions[bot], Created:2025-05-18T14:40:39Z, Comment: Workflow name: flow-maintenance-activity-to-issue
Workflow run URL: https://github.com/xn-intenton-z2a/agentic-lib/actions/runs/15097010409
Workflow event: schedule
Workflow inputs: null
HEAD of main URL: https://github.com/xn-intenton-z2a/agentic-lib/commit/7e8190696a09d15ef30d487f5797a4161e45a616
ISSUE_END            

Dependencies list from command: npm list
DEPENDENCIES_LIST_START
@xn-intenton-z2a/agentic-lib@6.6.5-0 /home/runner/work/agentic-lib/agentic-lib
├── @aws-sdk/client-lambda@3.812.0
├── @microsoft/eslint-formatter-sarif@3.1.0
├── @vitest/coverage-v8@3.1.3
├── @xn-intenton-z2a/s3-sqs-bridge@0.24.0
├── aws-cdk@2.1016.0
├── chalk@5.4.1
├── change-case@5.4.4
├── dayjs@1.11.13
├── dotenv@16.5.0
├── ejs@3.1.10
├── eslint-config-google@0.14.0
├── eslint-config-prettier@8.10.0
├── eslint-plugin-import@2.31.0
├── eslint-plugin-prettier@5.4.0
├── eslint-plugin-promise@7.2.1
├── eslint-plugin-react@7.37.5
├── eslint-plugin-security@3.0.1
├── eslint-plugin-sonarjs@3.0.2
├── eslint@9.27.0
├── figlet@1.8.1
├── js-yaml@4.1.0
├── lodash@4.17.21
├── markdown-it-github@0.5.0
├── markdown-it@14.1.0
├── minimatch@10.0.1
├── npm-check-updates@18.0.1
├── openai@4.100.0
├── prettier@3.5.3
├── seedrandom@3.0.5
├── vitest@3.1.3
└── zod@3.24.4
DEPENDENCIES_LIST_END    

Build output from command: npm run build
BUILD_OUTPUT_START

> @xn-intenton-z2a/agentic-lib@6.6.5-0 build
> echo "Nothing to build"

Nothing to build
BUILD_OUTPUT_END      

Test output from command: npm test
TEST_OUTPUT_START

> @xn-intenton-z2a/agentic-lib@6.6.5-0 test
> vitest tests/unit/*.test.js sandbox/tests/*.test.js


[1m[46m RUN [49m[22m [36mv3.1.3 [39m[90m/home/runner/work/agentic-lib/agentic-lib[39m

 [32m✓[39m tests/unit/module-index.test.js [2m([22m[2m1 test[22m[2m)[22m[32m 3[2mms[22m[39m
[90mstdout[2m | tests/unit/main.test.js
[22m[39m{"level":"info","timestamp":"2025-05-18T14:42:01.095Z","message":"Configuration loaded","config":{"GITHUB_API_BASE_URL":"https://api.github.com.test/","OPENAI_API_KEY":"key-test"}}

 [32m✓[39m tests/unit/main.test.js [2m([22m[2m1 test[22m[2m)[22m[32m 58[2mms[22m[39m
[90mstdout[2m | sandbox/tests/server.test.js
[22m[39mServer started on port 3000

 [32m✓[39m sandbox/tests/server.test.js [2m([22m[2m4 tests[22m[2m)[22m[32m 26[2mms[22m[39m

[2m Test Files [22m [1m[32m3 passed[39m[22m[90m (3)[39m
[2m      Tests [22m [1m[32m6 passed[39m[22m[90m (6)[39m
[2m   Start at [22m 14:42:00
[2m   Duration [22m 413ms[2m (transform 88ms, setup 0ms, collect 222ms, tests 87ms, environment 1ms, prepare 251ms)[22m
TEST_OUTPUT_END            

Main execution output from command: npm run start
MAIN_OUTPUT_START

> @xn-intenton-z2a/agentic-lib@6.6.5-0 start
> node src/lib/main.js

{"level":"info","timestamp":"2025-05-18T14:42:01.382Z","message":"Configuration loaded","config":{}}
No command argument supplied.

Usage:
  --help                     Show this help message and usage instructions.
  --digest                   Run a full bucket replay simulating an SQS event.
  --version                  Show version information with current timestamp.
MAIN_OUTPUT_END    

Please produce updated versions of the files that resolve the issue.
Note that the README.md file is provided for context only - any documentation changes should be written to the documentation files.
The source files, test files, and documentation files can be individual files or directories containing multiple files.
Never truncate the files, when returning a file, always return the entire file content.

Paths in (updatedFile01Filepath, updatedFile02Filepath, etc...) must begin with one of: sandbox/SOURCES.md;sandbox/library/;sandbox/features/;sandbox/tests/;sandbox/source/;sandbox/docs/;sandbox/README.md

Answer strictly with a JSON object following this schema:
{
  "message": "A short sentence explaining the change applied (or why no changes were applied) suitable for a commit message or PR text.",
  "updatedFile01Filepath": "sandbox/source/orderParser.js",
  "updatedFile01Contents": "The entire new content of the source file, with all necessary changes applied, if any.",
  "updatedFile02Filepath":  "sandbox/tests/orderParser.test.js",
  "updatedFile02Contents": "The entire new content of the test file, with all necessary changes applied, if any.",
  "updatedFile03Filepath": "sandbox/docs/USAGE.md",
  "updatedFile03Contents": "The entire new content of the documentation file, with all necessary changes applied, if any.",
  "updatedFile04Filepath": "sandbox/docs/A_FILE_WE_DONT_WANT.md",
  "updatedFile04Contents": "delete",
  "updatedFile05Filepath": "unused",
  "updatedFile05Contents": "unused",
  "updatedFile06Filepath": "unused",
  "updatedFile06Contents": "unused",
  "updatedFile07Filepath": "unused",
  "updatedFile07Contents": "unused",
  "updatedFile08Filepath": "unused",
  "updatedFile08Contents": "unused",
  "updatedFile09Filepath": "unused",
  "updatedFile09Contents": "unused",
  "updatedFile10Filepath": "unused",
  "updatedFile10Contents": "unused",
  "updatedFile11Filepath": "unused",
  "updatedFile11Contents": "unused",
  "updatedFile12Filepath": "unused",
  "updatedFile12Contents": "unused",
  "updatedFile13Filepath": "unused",
  "updatedFile13Contents": "unused",
  "updatedFile14Filepath": "unused",
  "updatedFile14Contents": "unused",
  "updatedFile15Filepath": "unused",
  "updatedFile15Contents": "unused",
  "updatedFile16Filepath": "unused",
  "updatedFile16Contents": "unused"
}

You can include up to 16 files using the updatedFileXXName and updatedFileXXContents pairs (where XX is a number from 01 to 16)
Where a file name and contents slot is not used, populate tha name with "unused" and the contents with "unused".
Where a file is to be deleted, set the name to the file path and the contents to "delete".
Never truncate the files, when returning a file, always return the entire file content.

Ensure valid JSON.
