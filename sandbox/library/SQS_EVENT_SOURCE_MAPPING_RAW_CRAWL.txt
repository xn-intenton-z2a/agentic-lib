Thanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventUsing Lambda with Amazon SQSNoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see 
    Amazon EventBridge Pipes.You can use a Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda
    supports both 
    standard queues and 
      first-in, first-out (FIFO) queues for event source mappings. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.TopicsUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventCreating and configuring an Amazon SQS event source mappingConfiguring scaling behavior for SQS event source mappingsHandling errors for an SQS event source in LambdaLambda parameters for Amazon SQS event source mappingsUsing event filtering with an Amazon SQS event sourceTutorial: Using Lambda with Amazon SQSTutorial: Using a cross-account Amazon SQS queue as an event
      source
    Understanding polling and batching behavior for Amazon SQS event source mappings
    With Amazon SQS event source mappings, Lambda polls the queue and invokes your function 
      synchronously with an event. Each event can contain a batch of multiple messages from the queue. Lambda receives
      these events one batch at a time, and invokes your function once for each batch. When your function successfully
      processes a batch, Lambda deletes its messages from the queue.
    When Lambda receives a batch, the messages stay in the queue but are hidden for the length of the queue's
      
      visibility timeout. If your function successfully processes all messages in the batch, Lambda deletes
      the messages from the queue. By default, if your function encounters an error while processing a batch, all
      messages in that batch become visible in the queue again after the visibility timeout expires. For this reason,
      your function code must be able to process the same message multiple times without unintended side effects.
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    To prevent Lambda from processing a message multiple times, you can either configure your event source
      mapping to include batch item failures in your
      function response, or you can use the DeleteMessage API to
      remove messages from the queue as  your Lambda function successfully processes them.
    For more information about configuration parameters that Lambda supports for SQS event source
      mappings, see Creating an SQS event source mapping.
   
    Example standard queue message event
  
    Example Amazon SQS message event (standard queue){
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {
                "myAttribute": {
                    "stringValue": "myValue", 
                    "stringListValues": [], 
                    "binaryListValues": [], 
                    "dataType": "String"
                }
            },
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        },
        {
            "messageId": "2e1424d4-f796-459a-8184-9c92662be6da",
            "receiptHandle": "AQEBzWwaftRI0KuVm4tP+/7q1rGgNqicHq...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082650636",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082650649"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        }
    ]
}
    By default, Lambda polls up to 10 messages in your queue at once and sends that batch to your function. To avoid
      invoking the function with a small number of records, you can configure the event source to buffer records for up
      to 5 minutes by configuring a batch window. Before invoking the function, Lambda continues to poll messages from the
      standard queue until the batch window expires, the invocation payload size
      quota is reached, or the configured maximum batch size is reached.
    If you're using a batch window and your SQS queue contains very low traffic, Lambda might wait for up to 20
      seconds before invoking your function. This is true even if you set a batch window lower than 20 seconds.
    
    NoteIn Java, you might experience null pointer errors when deserializing JSON. This could be due to how case of  "Records" and "eventSourceARN" is converted by the JSON object mapper.
   
     Example FIFO queue message event
    For FIFO queues, records contain additional attributes that are related to deduplication and sequencing.
    Example Amazon SQS message event (FIFO queue){
    "Records": [
        {
            "messageId": "11d6ee51-4cc7-4302-9e22-7cd8afdaadf5",
            "receiptHandle": "AQEBBX8nesZEXmkhsmZeyIE8iQAMig7qw...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1573251510774",
                "SequenceNumber": "18849496460467696128",
                "MessageGroupId": "1",
                "SenderId": "AIDAIO23YVJENQZJOL4VO",
                "MessageDeduplicationId": "1",
                "ApproximateFirstReceiveTimestamp": "1573251510774"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:fifo.fifo",
            "awsRegion": "us-east-2"
        }
    ]
}
  Document ConventionsSecrets ManagerCreate mappingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\n\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideWhen to use LambdaKey featuresWhat is AWS Lambda?You can use AWS Lambda to run code without provisioning or managing servers. Lambda runs your code
    on a high-availability compute infrastructure and performs all of the administration of the compute resources,
    including server and operating system maintenance, capacity provisioning and automatic scaling, and
    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume—there is no charge when your code is not running. For more information, see AWS Lambda Pricing.TipTo learn how to build serverless solutions, check out the Serverless Developer Guide.
    When to use Lambda
    Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to
      zero when not in demand. For example, you can use Lambda for:
  
     
     
     
     
     
  
      File processing: Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload.
    
      Stream processing: Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering.
    
      Web applications: Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers.
    
      IoT backends: Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests.
    
      Mobile backends: Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends.
    
    When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a
      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you
      cannot log in to compute instances or customize the operating system on provided
        runtimes. Lambda performs operational and administrative activities on your behalf, including managing
      capacity, monitoring, and logging your Lambda functions.
   
    Key features
    The following key features help you develop Lambda applications that are scalable, secure, and easily
      extensible:

    
    
       

       
      
       
      
       
      
       
  
       
      
       
           
       

       
      
       

       

       
      
    
        Environment variables
        
          Use environment variables to adjust your function's behavior without updating code.
        
      
        Versions
        
          Manage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.
        
      
        Container images
        
          Create a container image for a Lambda function by using an AWS provided base image or an alternative base
            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.
        
      
        Lambda layers
        
          Package libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.
        
      
        Lambda extensions
        
          Augment your Lambda functions with tools for monitoring, observability, security, and governance.
        
      
        Function URLs
        
          Add a dedicated HTTP(S) endpoint to your Lambda function.
        
      
        Response streaming
        
          Configure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.
        
      
        Concurrency and scaling controls
        
          Apply fine-grained control over the scaling and responsiveness of your production applications.
        
      
        Code signing
        
          Verify that only approved developers publish unaltered, trusted code in your Lambda functions 
        
      
        Private networking
        
          Create a private network for resources such as databases, cache instances, or internal services.
        
      
        File system
        
          Configure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.
        
      
        Lambda SnapStart
        
          Lambda SnapStart can provide as low as sub-second startup performance, typically with no changes to your function code.
        
      
  Document ConventionsCreate your first functionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventUsing Lambda with Amazon SQSNoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see 
    Amazon EventBridge Pipes.You can use a Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda
    supports both 
    standard queues and 
      first-in, first-out (FIFO) queues for event source mappings. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.TopicsUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventCreating and configuring an Amazon SQS event source mappingConfiguring scaling behavior for SQS event source mappingsHandling errors for an SQS event source in LambdaLambda parameters for Amazon SQS event source mappingsUsing event filtering with an Amazon SQS event sourceTutorial: Using Lambda with Amazon SQSTutorial: Using a cross-account Amazon SQS queue as an event
      source
    Understanding polling and batching behavior for Amazon SQS event source mappings
    With Amazon SQS event source mappings, Lambda polls the queue and invokes your function 
      synchronously with an event. Each event can contain a batch of multiple messages from the queue. Lambda receives
      these events one batch at a time, and invokes your function once for each batch. When your function successfully
      processes a batch, Lambda deletes its messages from the queue.
    When Lambda receives a batch, the messages stay in the queue but are hidden for the length of the queue's
      
      visibility timeout. If your function successfully processes all messages in the batch, Lambda deletes
      the messages from the queue. By default, if your function encounters an error while processing a batch, all
      messages in that batch become visible in the queue again after the visibility timeout expires. For this reason,
      your function code must be able to process the same message multiple times without unintended side effects.
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    To prevent Lambda from processing a message multiple times, you can either configure your event source
      mapping to include batch item failures in your
      function response, or you can use the DeleteMessage API to
      remove messages from the queue as  your Lambda function successfully processes them.
    For more information about configuration parameters that Lambda supports for SQS event source
      mappings, see Creating an SQS event source mapping.
   
    Example standard queue message event
  
    Example Amazon SQS message event (standard queue){
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {
                "myAttribute": {
                    "stringValue": "myValue", 
                    "stringListValues": [], 
                    "binaryListValues": [], 
                    "dataType": "String"
                }
            },
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        },
        {
            "messageId": "2e1424d4-f796-459a-8184-9c92662be6da",
            "receiptHandle": "AQEBzWwaftRI0KuVm4tP+/7q1rGgNqicHq...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082650636",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082650649"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        }
    ]
}
    By default, Lambda polls up to 10 messages in your queue at once and sends that batch to your function. To avoid
      invoking the function with a small number of records, you can configure the event source to buffer records for up
      to 5 minutes by configuring a batch window. Before invoking the function, Lambda continues to poll messages from the
      standard queue until the batch window expires, the invocation payload size
      quota is reached, or the configured maximum batch size is reached.
    If you're using a batch window and your SQS queue contains very low traffic, Lambda might wait for up to 20
      seconds before invoking your function. This is true even if you set a batch window lower than 20 seconds.
    
    NoteIn Java, you might experience null pointer errors when deserializing JSON. This could be due to how case of  "Records" and "eventSourceARN" is converted by the JSON object mapper.
   
     Example FIFO queue message event
    For FIFO queues, records contain additional attributes that are related to deduplication and sequencing.
    Example Amazon SQS message event (FIFO queue){
    "Records": [
        {
            "messageId": "11d6ee51-4cc7-4302-9e22-7cd8afdaadf5",
            "receiptHandle": "AQEBBX8nesZEXmkhsmZeyIE8iQAMig7qw...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1573251510774",
                "SequenceNumber": "18849496460467696128",
                "MessageGroupId": "1",
                "SenderId": "AIDAIO23YVJENQZJOL4VO",
                "MessageDeduplicationId": "1",
                "ApproximateFirstReceiveTimestamp": "1573251510774"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:fifo.fifo",
            "awsRegion": "us-east-2"
        }
    ]
}
  Document ConventionsSecrets ManagerCreate mappingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventUsing Lambda with Amazon SQSNoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see 
    Amazon EventBridge Pipes.You can use a Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda
    supports both 
    standard queues and 
      first-in, first-out (FIFO) queues for event source mappings. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.TopicsUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventCreating and configuring an Amazon SQS event source mappingConfiguring scaling behavior for SQS event source mappingsHandling errors for an SQS event source in LambdaLambda parameters for Amazon SQS event source mappingsUsing event filtering with an Amazon SQS event sourceTutorial: Using Lambda with Amazon SQSTutorial: Using a cross-account Amazon SQS queue as an event
      source
    Understanding polling and batching behavior for Amazon SQS event source mappings
    With Amazon SQS event source mappings, Lambda polls the queue and invokes your function 
      synchronously with an event. Each event can contain a batch of multiple messages from the queue. Lambda receives
      these events one batch at a time, and invokes your function once for each batch. When your function successfully
      processes a batch, Lambda deletes its messages from the queue.
    When Lambda receives a batch, the messages stay in the queue but are hidden for the length of the queue's
      
      visibility timeout. If your function successfully processes all messages in the batch, Lambda deletes
      the messages from the queue. By default, if your function encounters an error while processing a batch, all
      messages in that batch become visible in the queue again after the visibility timeout expires. For this reason,
      your function code must be able to process the same message multiple times without unintended side effects.
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    To prevent Lambda from processing a message multiple times, you can either configure your event source
      mapping to include batch item failures in your
      function response, or you can use the DeleteMessage API to
      remove messages from the queue as  your Lambda function successfully processes them.
    For more information about configuration parameters that Lambda supports for SQS event source
      mappings, see Creating an SQS event source mapping.
   
    Example standard queue message event
  
    Example Amazon SQS message event (standard queue){
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {
                "myAttribute": {
                    "stringValue": "myValue", 
                    "stringListValues": [], 
                    "binaryListValues": [], 
                    "dataType": "String"
                }
            },
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        },
        {
            "messageId": "2e1424d4-f796-459a-8184-9c92662be6da",
            "receiptHandle": "AQEBzWwaftRI0KuVm4tP+/7q1rGgNqicHq...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082650636",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082650649"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        }
    ]
}
    By default, Lambda polls up to 10 messages in your queue at once and sends that batch to your function. To avoid
      invoking the function with a small number of records, you can configure the event source to buffer records for up
      to 5 minutes by configuring a batch window. Before invoking the function, Lambda continues to poll messages from the
      standard queue until the batch window expires, the invocation payload size
      quota is reached, or the configured maximum batch size is reached.
    If you're using a batch window and your SQS queue contains very low traffic, Lambda might wait for up to 20
      seconds before invoking your function. This is true even if you set a batch window lower than 20 seconds.
    
    NoteIn Java, you might experience null pointer errors when deserializing JSON. This could be due to how case of  "Records" and "eventSourceARN" is converted by the JSON object mapper.
   
     Example FIFO queue message event
    For FIFO queues, records contain additional attributes that are related to deduplication and sequencing.
    Example Amazon SQS message event (FIFO queue){
    "Records": [
        {
            "messageId": "11d6ee51-4cc7-4302-9e22-7cd8afdaadf5",
            "receiptHandle": "AQEBBX8nesZEXmkhsmZeyIE8iQAMig7qw...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1573251510774",
                "SequenceNumber": "18849496460467696128",
                "MessageGroupId": "1",
                "SenderId": "AIDAIO23YVJENQZJOL4VO",
                "MessageDeduplicationId": "1",
                "ApproximateFirstReceiveTimestamp": "1573251510774"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:fifo.fifo",
            "awsRegion": "us-east-2"
        }
    ]
}
  Document ConventionsSecrets ManagerCreate mappingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventUsing Lambda with Amazon SQSNoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see 
    Amazon EventBridge Pipes.You can use a Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda
    supports both 
    standard queues and 
      first-in, first-out (FIFO) queues for event source mappings. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.TopicsUnderstanding polling and batching behavior for Amazon SQS event source mappingsExample standard queue message event Example FIFO queue message eventCreating and configuring an Amazon SQS event source mappingConfiguring scaling behavior for SQS event source mappingsHandling errors for an SQS event source in LambdaLambda parameters for Amazon SQS event source mappingsUsing event filtering with an Amazon SQS event sourceTutorial: Using Lambda with Amazon SQSTutorial: Using a cross-account Amazon SQS queue as an event
      source
    Understanding polling and batching behavior for Amazon SQS event source mappings
    With Amazon SQS event source mappings, Lambda polls the queue and invokes your function 
      synchronously with an event. Each event can contain a batch of multiple messages from the queue. Lambda receives
      these events one batch at a time, and invokes your function once for each batch. When your function successfully
      processes a batch, Lambda deletes its messages from the queue.
    When Lambda receives a batch, the messages stay in the queue but are hidden for the length of the queue's
      
      visibility timeout. If your function successfully processes all messages in the batch, Lambda deletes
      the messages from the queue. By default, if your function encounters an error while processing a batch, all
      messages in that batch become visible in the queue again after the visibility timeout expires. For this reason,
      your function code must be able to process the same message multiple times without unintended side effects.
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    To prevent Lambda from processing a message multiple times, you can either configure your event source
      mapping to include batch item failures in your
      function response, or you can use the DeleteMessage API to
      remove messages from the queue as  your Lambda function successfully processes them.
    For more information about configuration parameters that Lambda supports for SQS event source
      mappings, see Creating an SQS event source mapping.
   
    Example standard queue message event
  
    Example Amazon SQS message event (standard queue){
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {
                "myAttribute": {
                    "stringValue": "myValue", 
                    "stringListValues": [], 
                    "binaryListValues": [], 
                    "dataType": "String"
                }
            },
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        },
        {
            "messageId": "2e1424d4-f796-459a-8184-9c92662be6da",
            "receiptHandle": "AQEBzWwaftRI0KuVm4tP+/7q1rGgNqicHq...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082650636",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082650649"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
            "awsRegion": "us-east-2"
        }
    ]
}
    By default, Lambda polls up to 10 messages in your queue at once and sends that batch to your function. To avoid
      invoking the function with a small number of records, you can configure the event source to buffer records for up
      to 5 minutes by configuring a batch window. Before invoking the function, Lambda continues to poll messages from the
      standard queue until the batch window expires, the invocation payload size
      quota is reached, or the configured maximum batch size is reached.
    If you're using a batch window and your SQS queue contains very low traffic, Lambda might wait for up to 20
      seconds before invoking your function. This is true even if you set a batch window lower than 20 seconds.
    
    NoteIn Java, you might experience null pointer errors when deserializing JSON. This could be due to how case of  "Records" and "eventSourceARN" is converted by the JSON object mapper.
   
     Example FIFO queue message event
    For FIFO queues, records contain additional attributes that are related to deduplication and sequencing.
    Example Amazon SQS message event (FIFO queue){
    "Records": [
        {
            "messageId": "11d6ee51-4cc7-4302-9e22-7cd8afdaadf5",
            "receiptHandle": "AQEBBX8nesZEXmkhsmZeyIE8iQAMig7qw...",
            "body": "Test message.",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1573251510774",
                "SequenceNumber": "18849496460467696128",
                "MessageGroupId": "1",
                "SenderId": "AIDAIO23YVJENQZJOL4VO",
                "MessageDeduplicationId": "1",
                "ApproximateFirstReceiveTimestamp": "1573251510774"
            },
            "messageAttributes": {},
            "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:fifo.fifo",
            "awsRegion": "us-east-2"
        }
    ]
}
  Document ConventionsSecrets ManagerCreate mappingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon EventBridgeUser GuideHow Pipes workAmazon EventBridge PipesAmazon EventBridge Pipes connects sources to targets. Pipes are intended for point-to-point
    integrations between supported sources and targets, with support for advanced transformations and
      enrichment. It reduces the need for specialized
    knowledge and integration code when developing event-driven architectures, fostering consistency
    across your company’s applications. To set up a pipe, you choose the source, add optional
    filtering, define optional enrichment, and choose the target for the event data.NoteYou can also route events using event buses. Event buses are well-suited for many-to-many
      routing of events between event-driven services. For more information, see Event buses in Amazon EventBridge.
    How EventBridge Pipes work
At a high level, here's how EventBridge Pipes works:
    
       
       
    You create a pipe in your account. This includes:
      
         
         
         
         
      Specifying one of the supported event sources from which you want your pipe to receive events.Optionally, configuring a filter so that the pipe only processes a subset of the events it receives from the source.Optionally, configuring an enrichment step that enhances the event data before sending it to the target.Specifying one of the supported targets to which you want your pipe to send events.
      The event source begins sending events to the pipe, and the pipe processes the event before sending it to the target.
      
         
         
      If you have configured a filter, the pipe evaluates the event and only sends it to the target if it matches that filter.
        You are only charged for those events that match the filter.If you have configured an enrichment, the pipe performs that enrichment on the event before sending it to the target.
        If the events are batched, the enrichment maintains the ordering of the events in the batch.
    
    
       
        
       
       
    

    For example, a pipe could be used to create an e-commerce system. Suppose you have an API
      that contains customer information, such as shipping addresses. 
    
       
       
       
       
    You then create a pipe with the following: 
        
           
           
           
        An Amazon SQS
            order received message queue as the event source.An EventBridge API Destination as an enrichmentAn AWS Step Functions state machine as the target
      Then, when an Amazon SQS order received message appears in the queue, it is sent to your pipe.The pipe then sends that data to the EventBridge API Destination enrichment, which returns the customer
          information for that order. Lastly, the pipe sends the enriched data to the AWS Step Functions state machine, which processes the
          order.
  Document ConventionsBest practicesPipes conceptsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideAmazon SQS standard queuesAmazon SQS provides standard queues as the default queue type, supporting a nearly unlimited
		number of API calls per second for actions like SendMessage, ReceiveMessage, and DeleteMessage. Standard queues ensure at-least-once message
		delivery, but due to the highly distributed architecture, more than one copy of a message
		might be delivered, and messages may occasionally arrive out of order. Despite this,
		standard queues make a best-effort attempt to maintain the order in which messages are
		sent.When you send a message using SendMessage, Amazon SQS redundantly stores the
		message in multiple availability zones (AZs) before acknowledging it. This redundancy
		ensures that no single computer, network, or AZ failure can render the messages
		inaccessible.You can create and configure queues using the Amazon SQS console. For detailed instructions,
		see Creating a standard queue using the Amazon SQS
                console. For Java-specific examples, see Amazon SQS Java SDK examples.Use cases for standard queuesStandard message queues are suitable for various scenarios, as long as your application
		can handle messages that might arrive more than once or out of order. Examples
		include:
		 
		 
		 
	
			Decoupling live user requests from intensive background
					work – Users can upload media while the system resizes or
				encodes it in the background.
		
			Allocating tasks to multiple worker nodes
				– For example, handling a high volume of credit card validation
				requests.
		
			Batching messages for future processing –
				Scheduling multiple entries to be added to a database at a later time.
		For information on quotas related to standard queues, see Amazon SQS standard queue quotas.For best practices of working with standard queues, see Amazon SQS best practices.Document ConventionsPurging a queueAmazon SQS at-least-once
				deliveryDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideEvent source mappings and triggersBatching behaviorProvisioned modeEvent source mapping APIHow Lambda processes records from stream and queue-based event sourcesAn event source mapping is a Lambda resource that reads items from stream and queue-based
    services and invokes a function with batches of records. Within an event source mapping, resources called
    event pollers actively poll for new messages and invoke functions. By default, Lambda automatically
    scales event pollers, but for certain event source types, you can use 
    provisioned mode to control the minimum and maximum number of event pollers dedicated to your event source mapping.The following services use event source mappings to invoke Lambda functions:
     
     
     
     
     
     
     
  
      Amazon DocumentDB (with MongoDB compatibility) (Amazon DocumentDB)
    
      Amazon DynamoDB
    
      Amazon Kinesis
    
      Amazon MQ
    
      Amazon Managed Streaming for Apache Kafka (Amazon MSK)
    
      Self-managed Apache Kafka
    
      Amazon Simple Queue Service (Amazon SQS)
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    How event source mappings differ from direct triggers
    Some AWS services can directly invoke Lambda functions using triggers. These services push events to Lambda, and the function is invoked immediately when the specified event occurs. Triggers are suitable for discrete events and real-time processing. When you create a trigger using the Lambda console, the console interacts with the corresponding AWS service to configure the event notification on that service. The trigger is actually stored and managed by the service that generates the events, not by Lambda. Here are some examples of services that use triggers to invoke Lambda
      functions:
    
       
       
       
    
        Amazon Simple Storage Service (Amazon S3): Invokes a function when an object is created, deleted, or modified in a bucket. For more information, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.
      
        Amazon Simple Notification Service (Amazon SNS): Invokes a function when a message is published to an SNS topic. For more information, see Tutorial: Using AWS Lambda with Amazon Simple Notification Service.
      
        Amazon API Gateway: Invokes a function when an API request is made to a specific endpoint. For more information, see Invoking a Lambda function using an Amazon API Gateway endpoint.
      
    Event source mappings are Lambda resources created and managed within the Lambda service.
      Event source mappings are designed for processing high-volume streaming data or messages from
      queues. Processing records from a stream or queue in batches is more efficient than processing
      records individually. 
   
    Batching behavior
    By default, an event source mapping batches
      records together into a single payload that Lambda sends to your function. To fine-tune batching behavior, you can
      configure a batching window (MaximumBatchingWindowInSeconds) and a batch size
      (BatchSize). A batching window is the maximum amount of time to gather records into a single payload.
      A batch size is the maximum number of records in a single batch. Lambda invokes your function when one of the
      following three criteria is met:
    
       
       
       
    
        The batching window reaches its maximum value. Default batching window behavior 
          varies depending on the specific event source.
        
           
           
        For Kinesis, DynamoDB, and Amazon SQS event sources: The default batching
            window is 0 seconds. This means that Lambda invokes your function as soon as records are available. To set a batching window, configure MaximumBatchingWindowInSeconds. You can 
            set this parameter to any value from 0 to 300 seconds in increments of 1 second. If you configure a batching window, the 
            next window begins as soon as the previous function invocation completes.For Amazon MSK, self-managed Apache Kafka, Amazon MQ, and Amazon DocumentDB event sources: The default batching
            window is 500 ms. You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to
            300 seconds in increments of seconds. A batching window begins as soon as the first record arrives.
            NoteBecause you can only change MaximumBatchingWindowInSeconds in increments of seconds, you
                cannot revert to the 500 ms default batching window after you have changed it. To restore the default
                batching window, you must create a new event source mapping.
          
      
        The batch size is met. The minimum batch size is 1. The default and
          maximum batch size depend on the event source. For details about these values, see the BatchSize specification for the CreateEventSourceMapping API
          operation.
      
        The payload size reaches 6 MB. You cannot modify this limit.
      
    The following diagram illustrates these three conditions. Suppose a batching window begins at t = 7
      seconds. In the first scenario, the batching window reaches its 40 second maximum at t = 47 seconds after
      accumulating 5 records. In the second scenario, the batch size reaches 10 before the batching window expires,
      so the batching window ends early. In the third scenario, the maximum payload size is reached before the batching
      window expires, so the batching window ends early.
    
       
        
       
       
    
    We recommend that you test with different batch and record sizes so that the polling frequency
      of each event source is tuned to how quickly your function is able to complete its task. The
      CreateEventSourceMapping BatchSize parameter controls the maximum number of
      records that can be sent to your function with each invoke. A larger batch size can often more efficiently
      absorb the invoke overhead across a larger set of records, increasing your throughput.
    Lambda doesn't wait for any configured extensions to complete
      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda
      processes the next batch of records. This can cause throttling issues if you breach any of your account's 
      concurrency settings or limits. To detect whether this is a
      potential issue, monitor your functions and check whether you're seeing higher
      concurrency metrics than expected for your event
      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage
      than the number of shards. This can be true even for Lambda functions without extensions.
    By default, if your function returns an error, the event source mapping reprocesses the entire batch until the
      function succeeds, or the items in the batch expire. To ensure in-order processing, the event source mapping
      pauses processing for the affected shard until the error is resolved. For stream sources (DynamoDB and Kinesis),
      you can configure the maximum number of times that Lambda retries when your function returns an error.
      Service errors or throttles where the batch does not reach your function do not count toward retry
      attempts. You can also configure the event source mapping to send an invocation record to a
      destination when it discards an event batch.
   
    Provisioned mode
    Lambda event source mappings use event pollers to poll your event source for new messages. By default,
      Lambda manages the autoscaling of these pollers depending on message volume. When message traffic increases,
      Lambda automatically increases the number of event pollers to handle the load, and reduces them when
      traffic decreases.
    In provisioned mode, you can fine-tune the throughput of your event source mapping by defining
      minimum and maximum limits for the number of provisioned event pollers. Lambda then scales your event
      source mapping between the minimum and maximum number of event pollers in a responsive manner. These
      provisioned event pollers are dedicated to your event source mapping, enhancing your ability to handle
      unpredictable spikes in events.
    In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.
    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.
    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,
    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,
    see AWS Lambda pricing.
    Provisioned mode is supported only for Amazon MSK and self-managed Apache Kafka event sources. While concurrency settings
      give you control over the scaling of your function, provisioned mode gives you control over the
      throughput of your event source mapping. To ensure maximum performance, you may need to adjust both
      settings independently. For details about configuring provisioned mode, see the following sections:
    
       
       
    
        Configuring provisioned mode for Amazon MSK
          event source mappings
      
        Configuring provisioned mode for self-managed Apache Kafka
          event source mappings
      
    After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring
    the ProvisionedPollers metric. For more information, see Event source mapping metrics.
   
    Event source mapping API
     To manage an event source with the AWS Command Line Interface (AWS CLI) or an AWS SDK, you can use the following API operations:
    
        
       
       
       
       
    
        CreateEventSourceMapping
      
        ListEventSourceMappings
      
        GetEventSourceMapping
      
        UpdateEventSourceMapping
      
        DeleteEventSourceMapping
      
  Document ConventionsRetaining recordsEvent source mapping tagsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideConfiguring a queue to use with LambdaSetting up Lambda execution role permissionsCreating an SQS event source mappingCreating and configuring an Amazon SQS event source mappingTo process Amazon SQS messages with Lambda, configure your queue with the appropriate settings,
        then create a Lambda event source mapping.
        Configuring a queue to use with Lambda
        If you don't already have an existing Amazon SQS queue, create one
            to serve as an event source for your Lambda function. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
        To allow your function time to process each batch of records, set the source queue's
            
            visibility timeout to at least six times the configuration
            timeout on your function. The extra time allows Lambda to retry if your function is throttled
            while processing a previous batch.
        By default, if Lambda encounters an error at any point while processing a batch, all
            messages in that batch return to the queue. After the 
            visibility timeout, the messages become visible to Lambda again. You can
            configure your event source mapping to use 
            partial batch responses to return only the failed messages back to the queue. In
            addition, if your function fails to process a message multiple times, Amazon SQS can send it to a
            
            dead-letter queue. We recommend setting the maxReceiveCount on your
            source queue's 
            redrive policy to at least 5. This gives Lambda a few chances to retry before
            sending failed messages directly to the dead-letter queue.
     
        Setting up Lambda execution role permissions
        The 
            AWSLambdaSQSQueueExecutionRole AWS managed policy includes the permissions that Lambda needs to read
            from your Amazon SQS queue. You can add this managed policy to your function's
            execution role.
        Optionally, if you're using an encrypted queue, you also need to add the following permission to your
            execution role:
        
             
        
                kms:Decrypt
            
     
        Creating an SQS event source mapping
        Create an event source mapping to tell Lambda to send items from your queue to a Lambda function.
            You can create multiple event source mappings to process items from multiple queues with a single
            function. When Lambda invokes the target function, the event can contain multiple items, up to a
            configurable maximum batch size.
        To configure your function to read from Amazon SQS, attach the 
            AWSLambdaSQSQueueExecutionRole AWS managed policy to your execution role.
            Then, create an SQS event source mapping from the console using
            the following steps.
        To add permissions and create a triggerOpen the Functions page of the Lambda console.
        Choose the name of a function.
      
        Choose the Configuration tab, and then choose Permissions.
      
        Under Role name, choose the link to your execution role. This link opens the role in the IAM console.
          
             
               
             
             
                  
      
        Choose Add permissions, and then choose Attach policies.
          
             
               
             
             
                    
      
        In the search field, enter AWSLambdaSQSQueueExecutionRole.
    Add this policy to your execution role. This is an AWS managed policy that contains the permissions
    your function needs to read from an Amazon SQS queue. For more information about this policy, see
    
    AWSLambdaSQSQueueExecutionRole in the AWS Managed Policy Reference.
      
        Go back to your function in the Lambda console. Under Function overview, choose Add trigger.
          
             
               
             
             
             
      
        Choose a trigger type.
      
        Configure the required options, and then choose Add.
      
        
            Lambda supports the following configuration options for Amazon SQS event sources:
             
             
             
             
             
             
        
                SQS queue
                
                    The Amazon SQS queue to read records from. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
                
            
                Enable trigger
                
                    The status of the event source mapping. Enable trigger is selected by default.
                
            
                Batch size
                
                    The maximum number of records to send to the function in each batch. For a standard queue,
                        this can be up to 10,000 records. For a FIFO queue, the maximum is 10. For a batch size
                        over 10, you must also set the batch window (MaximumBatchingWindowInSeconds)
                        to at least 1 second.
                    Configure your 
                        function timeout to allow enough time to process an entire batch of items. If items
                        take a long time to process, choose a smaller batch size. A large batch size can improve
                        efficiency for workloads that are very fast or have a lot of overhead. If you configure
                        reserved concurrency on your function, set
                        a minimum of five concurrent executions to reduce the chance of throttling errors when Lambda
                        invokes your function. 
                    Lambda passes all of the records in the batch to the function in a single call, as long as
                        the total size of the events doesn't exceed the 
                        invocation payload size quota for synchronous invocation (6 MB). Both Lambda and Amazon SQS
                        generate metadata for each record. This additional metadata is counted towards the total
                        payload size and can cause the total number of records sent in a batch to be lower than your
                        configured batch size. The metadata fields that Amazon SQS sends can be variable in length.
                        For more information about the Amazon SQS metadata fields, see the ReceiveMessage
                        API operation documentation in the Amazon Simple Queue Service API Reference.
                
            
                Batch window
                
                    The maximum amount of time to gather records before invoking the function, in seconds.
                        This applies only to standard queues.
                    If you're using a batch window greater than 0 seconds, you must account for the increased
                        processing time in your queue's
                        
                        visibility timeout. We recommend setting your queue's visibility timeout to six times your
                        function timeout, plus the value of
                        MaximumBatchingWindowInSeconds. This allows time for your Lambda function to process each
                        batch of events and to retry in the event of a throttling error.
                    When messages become available, Lambda starts processing messages in batches. Lambda starts
                        processing five batches at a time with five concurrent invocations of your function. If messages
                        are still available, Lambda adds up to 300 more instances of your function a minute, up to a
                        maximum of 1,000 function instances. To learn more about function scaling and concurrency,
                        see Lambda function scaling.
                    To process more messages, you can optimize your Lambda function for higher throughput.
                        For more information, see 
                        Understanding how AWS Lambda scales with Amazon SQS standard queues.
                
            
                Maximum concurrency
                
                    The maximum number of concurrent functions that the event source can invoke. For more information,
                        see Configuring maximum concurrency for Amazon SQS event sources.
                
            
                Filter criteria
                
                    Add filter criteria to control which events Lambda sends to your function for processing.
                        For more information, see Control which events Lambda sends to your function.
                
            
    Document ConventionsSQSScaling behaviorDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideMaximum concurrencyConfiguring scaling behavior for SQS event source mappingsFor standard queues, Lambda uses  long polling to poll a queue until it becomes active. When messages are
        available, Lambda starts processing five batches at a time with five concurrent invocations
        of your function. If messages are still available, Lambda increases the number of processes that are reading batches by up to 300 more instances per minute. The maximum number of batches that an event source mapping can process simultaneously is 1,000. When traffic is low, Lambda scales back the processing to
        five concurrent batches, and can optimize to as few as 2 concurrent batches to reduce the
        SQS calls and corresponding costs. However, this optimization is not available when you
        enable the maximum concurrency setting.For FIFO queues, Lambda sends messages to your function in the order that it receives them. When you send a
        message to a FIFO queue, you specify a message group
            ID. Amazon SQS ensures that messages in the same group are delivered to Lambda in order. When Lambda reads 
        your messages into batches, each batch may contain messages from more than one message group, but the order 
        of the messages is maintained. If your function returns an error, the function attempts all retries on the 
        affected messages before Lambda receives additional messages from the same
        group.
        Configuring maximum concurrency for Amazon SQS event sources
        You can use the maximum concurrency setting to control scaling behavior for your SQS event sources.
            The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS
            event source can invoke. Maximum concurrency is an event source-level setting. If you have multiple Amazon SQS
            event sources mapped to one function, each event source can have a separate maximum concurrency setting.
            You can use maximum concurrency to prevent one queue from using all of the function's
            reserved concurrency or the rest of the
            account's concurrency quota. There is no charge for
            configuring maximum concurrency on an Amazon SQS event source.
        Importantly, maximum concurrency and reserved concurrency are two independent settings. Don't set
            maximum concurrency higher than the function's reserved concurrency. If you configure maximum concurrency,
            make sure that your function's reserved concurrency is greater than or equal to the total maximum
            concurrency for all Amazon SQS event sources on the function. Otherwise, Lambda may throttle your messages.
        When your account's concurrency quota is set to the default value of 1,000, an Amazon SQS event source mapping can scale 
            to invoke function instances up to this value, unless you specify a maximum concurrency.
        If you receive an increase to your account's default concurrency quota, Lambda may not be able to invoke concurrent functions 
            instances up to your new quota. By default, Lambda can scale to invoke up to 1,250 concurrent function instances 
            for an Amazon SQS event source mapping. If this is insufficient for your use case, contact AWS support to 
            discuss an increase to your account's Amazon SQS event source mapping concurrency.
        NoteFor FIFO queues, concurrent invocations are capped either by the number of
                message group IDs
                (messageGroupId) or the maximum concurrency setting—whichever is lower. For example,
                if you have six message group IDs and maximum concurrency is set to 10, your function can have a maximum
                of six concurrent invocations.
        You can configure maximum concurrency on new and existing Amazon SQS event source mappings.
        Configure maximum concurrency using the Lambda consoleOpen the Functions page of the Lambda console.
                Choose the name of a function.
            
                Under Function overview, choose SQS. This opens the Configuration tab.
            
                Select the Amazon SQS trigger and choose Edit.
            
                For Maximum concurrency, enter a number between 2 and 1,000. To turn off maximum concurrency, leave the box empty.
            
                Choose Save.
            
         
            Configure maximum concurrency using the AWS Command Line Interface (AWS CLI)
            Use the update-event-source-mapping command with the --scaling-config option. Example:
         
        aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --scaling-config '{"MaximumConcurrency":5}'
        To turn off maximum concurrency, enter an empty value for --scaling-config:
        aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --scaling-config "{}"
         
            Configure maximum concurrency using the Lambda API
            Use the CreateEventSourceMapping or UpdateEventSourceMapping action with a ScalingConfig object.
         
    Document ConventionsCreate mappingError handlingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideBackoff strategy for failed invocationsImplementing partial batch responsesHandling errors for an SQS event source in LambdaTo handle errors related to an SQS event source, Lambda automatically uses a retry strategy with a
        backoff strategy. You can also customize error handling behavior by configuring your SQS event
        source mapping to return partial batch responses.
        Backoff strategy for failed invocations
        When an invocation fails, Lambda attempts to retry the invocation while implementing a backoff strategy.
            The backoff strategy differs slightly depending on whether Lambda encountered the failure due to an error in
            your function code, or due to throttling.
        
             
             
        
                
                    If your function code caused the error, Lambda will stop processing and retrying the invocation.
                    In the meantime, Lambda gradually backs off, reducing the amount of concurrency allocated to your Amazon SQS event source mapping.
                    After your queue's visibility timeout runs out, the message will again reappear in the queue.
                
            
                If the invocation fails due to throttling, Lambda gradually backs off
                    retries by reducing the amount of concurrency allocated to your Amazon SQS event source mapping. Lambda continues
                    to retry the message until the message's timestamp exceeds your queue's visibility timeout, at which point
                    Lambda drops the message.
            
     
        Implementing partial batch responses
        When your Lambda function encounters an error while processing a batch, all messages in that batch become
            visible in the queue again by default, including messages that Lambda processed successfully. As a result, your
            function can end up processing the same message several times.
        To avoid reprocessing successfully processed messages in a failed batch, you can configure your event
            source mapping to make only the failed messages visible again. This is called a partial batch response.
            To turn on partial batch responses, specify ReportBatchItemFailures for the
            FunctionResponseTypes
            action when configuring your event source mapping. This lets your function
            return a partial success, which can help reduce the number of unnecessary retries on records.
        When ReportBatchItemFailures is activated, Lambda doesn't scale down message polling when function invocations fail. If you expect some messages to fail—and you don't want those failures to impact the message processing rate—use ReportBatchItemFailures.
        NoteKeep the following in mind when using partial batch responses:
                 
                 
            
                    If your function throws an exception, the entire batch is considered a complete failure.
                
                    If you're using this feature with a FIFO queue, your function should stop processing messages after the
                        first failure and return all failed and unprocessed messages in batchItemFailures. This helps
                        preserve the ordering of messages in your queue.
                
        To activate partial batch reporting
                Review the Best practices for implementing partial batch responses.
            
                Run the following command to activate ReportBatchItemFailures for your function. To retrieve your event source mapping's UUID, run the list-event-source-mappings AWS CLI command.
                aws lambda update-event-source-mapping \
--uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
--function-response-types "ReportBatchItemFailures"
            
                Update your function code to catch all exceptions and return failed messages in a batchItemFailures JSON response. The batchItemFailures response must include a list of message IDs, as itemIdentifier JSON values.
                For example, suppose you have a batch of five messages, with message IDs id1, id2, id3, id4, and id5. Your function successfully processes id1, id3, and id5. To make messages id2 and id4 visible again in your queue, your function should return the following response:  
                { 
  "batchItemFailures": [ 
        {
            "itemIdentifier": "id2"
        },
        {
            "itemIdentifier": "id4"
        }
    ]
}
                Here are some examples of function code that return the list of failed message IDs in the batch:
                
    .NET
            
     

        SDK for .NET
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using .NET.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
using Amazon.Lambda.Core;
using Amazon.Lambda.SQSEvents;

// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.
[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]
namespace sqsSample;

public class Function
{
    public async Task<SQSBatchResponse> FunctionHandler(SQSEvent evnt, ILambdaContext context)
    {
        List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new List<SQSBatchResponse.BatchItemFailure>();
        foreach(var message in evnt.Records)
        {
            try
            {
                //process your message
                await ProcessMessageAsync(message, context);
            }
            catch (System.Exception)
            {
                //Add failed message identifier to the batchItemFailures list
                batchItemFailures.Add(new SQSBatchResponse.BatchItemFailure{ItemIdentifier=message.MessageId}); 
            }
        }
        return new SQSBatchResponse(batchItemFailures);
    }

    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)
    {
        if (String.IsNullOrEmpty(message.Body))
        {
            throw new Exception("No Body in SQS Message.");
        }
        context.Logger.LogInformation($"Processed message {message.Body}");
        // TODO: Do interesting work based on the new message
        await Task.CompletedTask;
    }
}

             
        
    
        
    Go
            
     

        SDK for Go V2
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Go.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"github.com/aws/aws-lambda-go/events"
	"github.com/aws/aws-lambda-go/lambda"
)

func handler(ctx context.Context, sqsEvent events.SQSEvent) (map[string]interface{}, error) {
	batchItemFailures := []map[string]interface{}{}

	for _, message := range sqsEvent.Records {
		
		if /* Your message processing condition here */ {			
			batchItemFailures = append(batchItemFailures, map[string]interface{}{"itemIdentifier": message.MessageId})
		}
	}

	sqsBatchResponse := map[string]interface{}{
		"batchItemFailures": batchItemFailures,
	}
	return sqsBatchResponse, nil
}

func main() {
	lambda.Start(handler)
}


             
        
    
        
    Java
            
     

        SDK for Java 2.x
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Java.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SQSEvent;
import com.amazonaws.services.lambda.runtime.events.SQSBatchResponse;
 
import java.util.ArrayList;
import java.util.List;
 
public class ProcessSQSMessageBatch implements RequestHandler<SQSEvent, SQSBatchResponse> {
    @Override
    public SQSBatchResponse handleRequest(SQSEvent sqsEvent, Context context) {
 
         List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new ArrayList<SQSBatchResponse.BatchItemFailure>();
         String messageId = "";
         for (SQSEvent.SQSMessage message : sqsEvent.getRecords()) {
             try {
                 //process your message
             } catch (Exception e) {
                 //Add failed message identifier to the batchItemFailures list
                 batchItemFailures.add(new SQSBatchResponse.BatchItemFailure(message.getMessageId()));
             }
         }
         return new SQSBatchResponse(batchItemFailures);
     }
}

             
        
    
        
    JavaScript
            
     

        SDK for JavaScript (v3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using JavaScript.
                
                // Node.js 20.x Lambda runtime, AWS SDK for Javascript V3
export const handler = async (event, context) => {
    const batchItemFailures = [];
    for (const record of event.Records) {
        try {
            await processMessageAsync(record, context);
        } catch (error) {
            batchItemFailures.push({ itemIdentifier: record.messageId });
        }
    }
    return { batchItemFailures };
};

async function processMessageAsync(record, context) {
    if (record.body && record.body.includes("error")) {
        throw new Error("There is an error in the SQS Message.");
    }
    console.log(`Processed message: ${record.body}`);
}

             
             
                    Reporting SQS batch item failures with Lambda using TypeScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import { SQSEvent, SQSBatchResponse, Context, SQSBatchItemFailure, SQSRecord } from 'aws-lambda';

export const handler = async (event: SQSEvent, context: Context): Promise<SQSBatchResponse> => {
    const batchItemFailures: SQSBatchItemFailure[] = [];

    for (const record of event.Records) {
        try {
            await processMessageAsync(record);
        } catch (error) {
            batchItemFailures.push({ itemIdentifier: record.messageId });
        }
    }

    return {batchItemFailures: batchItemFailures};
};

async function processMessageAsync(record: SQSRecord): Promise<void> {
    if (record.body && record.body.includes("error")) {
        throw new Error('There is an error in the SQS Message.');
    }
    console.log(`Processed message ${record.body}`);
}


             
        
    
        
    PHP
            
     

        SDK for PHP
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using PHP.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
<?php

use Bref\Context\Context;
use Bref\Event\Sqs\SqsEvent;
use Bref\Event\Sqs\SqsHandler;
use Bref\Logger\StderrLogger;

require __DIR__ . '/vendor/autoload.php';

class Handler extends SqsHandler
{
    private StderrLogger $logger;
    public function __construct(StderrLogger $logger)
    {
        $this->logger = $logger;
    }

    /**
     * @throws JsonException
     * @throws \Bref\Event\InvalidLambdaEvent
     */
    public function handleSqs(SqsEvent $event, Context $context): void
    {
        $this->logger->info("Processing SQS records");
        $records = $event->getRecords();

        foreach ($records as $record) {
            try {
                // Assuming the SQS message is in JSON format
                $message = json_decode($record->getBody(), true);
                $this->logger->info(json_encode($message));
                // TODO: Implement your custom processing logic here
            } catch (Exception $e) {
                $this->logger->error($e->getMessage());
                // failed processing the record
                $this->markAsFailed($record);
            }
        }
        $totalRecords = count($records);
        $this->logger->info("Successfully processed $totalRecords SQS records");
    }
}

$logger = new StderrLogger();
return new Handler($logger);


             
        
    
        
    Python
            
     

        SDK for Python (Boto3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Python.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

def lambda_handler(event, context):
    if event:
        batch_item_failures = []
        sqs_batch_response = {}
     
        for record in event["Records"]:
            try:
                # process message
            except Exception as e:
                batch_item_failures.append({"itemIdentifier": record['messageId']})
        
        sqs_batch_response["batchItemFailures"] = batch_item_failures
        return sqs_batch_response

             
        
    
        
    Ruby
            
     

        SDK for Ruby
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Ruby.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
require 'json'

def lambda_handler(event:, context:)
  if event
    batch_item_failures = []
    sqs_batch_response = {}

    event["Records"].each do |record|
      begin
        # process message
      rescue StandardError => e
        batch_item_failures << {"itemIdentifier" => record['messageId']}
      end
    end

    sqs_batch_response["batchItemFailures"] = batch_item_failures
    return sqs_batch_response
  end
end


             
        
    
        
    Rust
            
     

        SDK for Rust
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Rust.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
use aws_lambda_events::{
    event::sqs::{SqsBatchResponse, SqsEvent},
    sqs::{BatchItemFailure, SqsMessage},
};
use lambda_runtime::{run, service_fn, Error, LambdaEvent};

async fn process_record(_: &SqsMessage) -> Result<(), Error> {
    Err(Error::from("Error processing message"))
}

async fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<SqsBatchResponse, Error> {
    let mut batch_item_failures = Vec::new();
    for record in event.payload.records {
        match process_record(&record).await {
            Ok(_) => (),
            Err(_) => batch_item_failures.push(BatchItemFailure {
                item_identifier: record.message_id.unwrap(),
            }),
        }
    }

    Ok(SqsBatchResponse {
        batch_item_failures,
    })
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    run(service_fn(function_handler)).await
}


             
        
    
        

            
        If the failed events do not return to the queue, see How do I troubleshoot Lambda function SQS ReportBatchItemFailures? in the AWS Knowledge Center.
         
            Success and failure conditions
            Lambda treats a batch as a complete success if your function returns any of the following:
            
                 
                 
                 
                 
            
                    An empty batchItemFailures list
                
                    A null batchItemFailures list
                
                    An empty EventResponse
                
                    A null EventResponse
                
            Lambda treats a batch as a complete failure if your function returns any of the following:
            
                 
                 
                 
                 
                 
            
                    An invalid JSON response
                
                    An empty string itemIdentifier
                
                    A null itemIdentifier
                
                    An itemIdentifier with a bad key name
                
                    An itemIdentifier value with a message ID that doesn't exist
                
         
         
            CloudWatch metrics
            To determine whether your function is correctly reporting batch item failures, you can monitor the
                NumberOfMessagesDeleted and ApproximateAgeOfOldestMessage Amazon SQS metrics in
                Amazon CloudWatch.
            
                 
                 
            
                    NumberOfMessagesDeleted tracks the number of messages removed from your queue. If this
                        drops to 0, this is a sign that your function response is not correctly returning failed messages.
                
                    ApproximateAgeOfOldestMessage tracks how long the oldest message has stayed in your queue.
                        A sharp increase in this metric can indicate that your function is not correctly returning failed
                        messages.
                
         
    Document ConventionsScaling behaviorParametersDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideLambda parameters for Amazon SQS event source mappingsAll Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping
        API operations. However, only some of the parameters apply to Amazon SQS.
                
                    Parameter
                    Required
                    Default
                    Notes
                
            
                
                    
                        BatchSize
                    
                    
                        N
                    
                    
                        10
                    
                    
                        For standard queues, the maximum is 10,000. For FIFO queues, the maximum is 10.
                    
                
                
                    
                        Enabled
                    
                    
                        N
                    
                    
                        true
                    
                    none 
                
                
                    
                        EventSourceArn
                    
                    
                        Y
                    
                    N/A
                    
                        The ARN of the data stream or a stream consumer
                    
                
                
                    
                        FunctionName
                    
                    
                        Y
                    
                    N/A 
                    none 
                
                
                    
                        FilterCriteria
                    
                    
                        N
                    
                    
                        N/A 
                    
                    
                        Control which events Lambda sends to your function
                    
                
                
                    
                        FunctionResponseTypes
                    
                    
                        N
                    
                    N/A 
                    
                        To let your function report specific failures in a batch, include the value
                            ReportBatchItemFailures in FunctionResponseTypes. For more information, see
                            Implementing partial batch responses.
                    
                
                
                    
                        MaximumBatchingWindowInSeconds
                    
                    
                        N
                    
                    
                        0
                    
                    Batching window is not supported for FIFO queues
                
                
                    
                        ScalingConfig
                    
                    
                        N
                    
                    
                        N/A 
                    
                    
                        Configuring maximum concurrency for Amazon SQS event sources
                    
                
            Document ConventionsError handlingEvent filteringDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideAmazon SQS event filtering basicsUsing event filtering with an Amazon SQS event sourceYou can use event filtering to control which records from a stream or queue Lambda sends to your function.
    For general information about how event filtering works, see Control which events Lambda sends to your function.This section focuses on event filtering for Amazon SQS event sources.TopicsAmazon SQS event filtering basics
        Amazon SQS event filtering basics
        Suppose your Amazon SQS queue contains messages in the following JSON format.
        {
    "RecordNumber": 1234,
    "TimeStamp": "yyyy-mm-ddThh:mm:ss",
    "RequestCode": "AAAA"
}
        An example record for this queue would look as follows.
        {
    "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
    "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
    "body": "{\n "RecordNumber": 1234,\n "TimeStamp": "yyyy-mm-ddThh:mm:ss",\n "RequestCode": "AAAA"\n}",
    "attributes": {
        "ApproximateReceiveCount": "1",
        "SentTimestamp": "1545082649183",
        "SenderId": "AIDAIENQZJOLO23YVJ4VO",
        "ApproximateFirstReceiveTimestamp": "1545082649185"
        },
    "messageAttributes": {},
    "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
    "eventSource": "aws:sqs",
    "eventSourceARN": "arn:aws:sqs:us-west-2:123456789012:my-queue",
    "awsRegion": "us-west-2"
}
        To filter based on the contents of your Amazon SQS messages, use the body key in the Amazon SQS message record. Suppose you want to process 
            only those records where the RequestCode in your Amazon SQS message is “BBBB.” The FilterCriteria object would be 
            as follows.
        {
    "Filters": [
        {
            "Pattern": "{ \"body\" : { \"RequestCode\" : [ \"BBBB\" ] } }"
        }
    ]
}
        For added clarity, here is the value of the filter's Pattern expanded in plain JSON. 
        {
    "body": {
        "RequestCode": [ "BBBB" ]
        }
}
        You can add your filter using the console, AWS CLI or an AWS SAM template.
        
            Console
                    To add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following 
                        string for the Filter criteria.
                    { "body" : { "RequestCode" : [ "BBBB" ] } }
                
            AWS CLI
                    To create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following
                        command.
                    aws lambda create-event-source-mapping \
    --function-name my-function \
    --event-source-arn arn:aws:sqs:us-east-2:123456789012:my-queue \
    --filter-criteria '{"Filters": [{"Pattern": "{ \"body\" : { \"RequestCode\" : [ \"BBBB\" ] } }"}]}'
                    To add these filter criteria to an existing event source mapping, run the following command.
                    aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --filter-criteria '{"Filters": [{"Pattern": "{ \"body\" : { \"RequestCode\" : [ \"BBBB\" ] } }"}]}'        
                
            AWS SAM
                    To add this filter using AWS SAM, add the following snippet to the YAML template for your event source.
                    FilterCriteria:
  Filters:
    - Pattern: '{ "body" : { "RequestCode" : [ "BBBB" ] } }'
                
        
        Suppose you want your function to process only those records where RecordNumber is greater than 9999. The FilterCriteria 
            object would be as follows.
        {
    "Filters": [
        {
            "Pattern": "{ \"body\" : { \"RecordNumber\" : [ { \"numeric\": [ \">\", 9999 ] } ] } }"
        }
    ]
}
        For added clarity, here is the value of the filter's Pattern expanded in plain JSON. 
        {
    "body": {
        "RecordNumber": [
            {
                "numeric": [ ">", 9999 ]
            }
        ]
    }
}
        You can add your filter using the console, AWS CLI or an AWS SAM template.
        
            Console
                    To add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following 
                        string for the Filter criteria.
                    { "body" : { "RecordNumber" : [ { "numeric": [ ">", 9999 ] } ] } }
                
            AWS CLI
                    To create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following
                        command.
                    aws lambda create-event-source-mapping \
    --function-name my-function \
    --event-source-arn arn:aws:sqs:us-east-2:123456789012:my-queue \
    --filter-criteria '{"Filters": [{"Pattern": "{ \"body\" : { \"RecordNumber\" : [ { \"numeric\": [ \">\", 9999 ] } ] } }"}]}'
                    To add these filter criteria to an existing event source mapping, run the following command.
                    aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --filter-criteria '{"Filters": [{"Pattern": "{ \"body\" : { \"RecordNumber\" : [ { \"numeric\": [ \">\", 9999 ] } ] } }"}]}'        
                
            AWS SAM
                    To add this filter using AWS SAM, add the following snippet to the YAML template for your event source.
                    FilterCriteria:
  Filters:
    - Pattern: '{ "body" : { "RecordNumber" : [ { "numeric": [ ">", 9999 ] } ] } }'
                
        
        For Amazon SQS, the message body can be any string. However, this can be problematic if your FilterCriteria expect body 
            to be in a valid JSON format. The reverse scenario is also true—if the incoming message body is in JSON format but your filter criteria 
            expects body to be a plain string, this can lead to unintended behavior.
        To avoid this issue, ensure that the format of body in your FilterCriteria matches the expected format of body in messages 
            that you receive from your queue. Before filtering your messages, Lambda automatically evaluates the format of the incoming message body and 
            of your filter pattern for body. If there is a mismatch, Lambda drops the message. The following table summarizes this evaluation:
        
                    
                        Incoming message body format
                        Filter pattern body format
                        Resulting action
                    
                
                    
                        
                            Plain string
                        
                        
                            Plain string
                        
                        
                            Lambda filters based on your filter criteria.
                        
                    
                    
                        
                            Plain string
                        
                        
                            No filter pattern for data properties
                        
                        
                            Lambda filters (on the other metadata properties only) based on your filter criteria.
                        
                    
                    
                        
                            Plain string
                        
                        
                            Valid JSON
                        
                        
                            Lambda drops the message.
                        
                    
                    
                        
                            Valid JSON
                        
                        
                            Plain string
                        
                        
                            Lambda drops the message.
                        
                    
                    
                        
                            Valid JSON
                        
                        
                            No filter pattern for data properties
                        
                        
                            Lambda filters (on the other metadata properties only) based on your filter criteria.
                        
                    
                    
                        
                            Valid JSON
                        
                        
                            Valid JSON
                        
                        
                            Lambda filters based on your filter criteria.
                        
                    
                
    Document ConventionsParametersTutorialDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution roleCreate the functionTest the functionCreate an Amazon SQS queueConfigure the event sourceSend a test messageCheck the logsClean up your resourcesTutorial: Using Lambda with Amazon SQSIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue. The Lambda function runs whenever a new message is added to the queue. The function writes the messages to an Amazon CloudWatch Logs stream. The following diagram shows the AWS 
    resources you use to complete the tutorial.
       
        
       
       
    To complete this tutorial, you carry out the following steps:
       
       
       
       
    
        Create a Lambda function that writes messages to CloudWatch Logs.
      
        Create an Amazon SQS queue.
      
        Create a Lambda event source mapping. The event source mapping reads the Amazon SQS queue and invokes your Lambda function when a new message is added.
      
        Test the setup by adding messages to your queue and monitoring the results in 
        CloudWatch Logs.
      
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role
      
       
        
       
       
    
    An execution role is an AWS Identity and Access Management (IAM) role that grants a Lambda function permission to access AWS services and resources. To allow 
      your function to read items from Amazon SQS, attach the AWSLambdaSQSQueueExecutionRole permissions policy.
    To create an execution role and attach an Amazon SQS permissions policy
        Open the Roles page of the IAM console.
      
        Choose Create role.
      
        For Trusted entity type, choose AWS service.
      
        For Use case, choose Lambda.
      
        Choose Next.
      
        In the Permissions policies search box, enter AWSLambdaSQSQueueExecutionRole.
      
        Select the AWSLambdaSQSQueueExecutionRole policy, and
          then choose Next.
      
        Under Role details, for Role name, enter
          lambda-sqs-role, then choose Create role.
      
    After role creation, note down the Amazon Resource Name (ARN) of your execution role. You'll
      need it in later steps.
   
    Create the function
    
       
        
       
       
    
    Create a Lambda function that processes your Amazon SQS messages. The function code logs the body of
      the Amazon SQS message to CloudWatch Logs.
    This tutorial uses the Node.js 18.x runtime, but we've also provided
      example code in other runtime languages. You can select the tab in the following box to see code
      for the runtime you're interested in. The JavaScript code you'll use in this step is in the first
      example shown in the JavaScript tab.
    
    .NET
            
     

        SDK for .NET
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using .NET.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
﻿using Amazon.Lambda.Core;
using Amazon.Lambda.SQSEvents;


// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.
[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]

namespace SqsIntegrationSampleCode
{
    public async Task FunctionHandler(SQSEvent evnt, ILambdaContext context)
    {
        foreach (var message in evnt.Records)
        {
            await ProcessMessageAsync(message, context);
        }

        context.Logger.LogInformation("done");
    }

    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)
    {
        try
        {
            context.Logger.LogInformation($"Processed message {message.Body}");

            // TODO: Do interesting work based on the new message
            await Task.CompletedTask;
        }
        catch (Exception e)
        {
            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.
            context.Logger.LogError($"An error occurred");
            throw;
        }

    }
}


             
        
    
        
    Go
            
     

        SDK for Go V2
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using Go.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package integration_sqs_to_lambda

import (
	"fmt"
	"github.com/aws/aws-lambda-go/events"
	"github.com/aws/aws-lambda-go/lambda"
)

func handler(event events.SQSEvent) error {
	for _, record := range event.Records {
		err := processMessage(record)
		if err != nil {
			return err
		}
	}
	fmt.Println("done")
	return nil
}

func processMessage(record events.SQSMessage) error {
	fmt.Printf("Processed message %s\n", record.Body)
	// TODO: Do interesting work based on the new message
	return nil
}

func main() {
	lambda.Start(handler)
}


             
        
    
        
    Java
            
     

        SDK for Java 2.x
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using Java.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SQSEvent;
import com.amazonaws.services.lambda.runtime.events.SQSEvent.SQSMessage;

public class Function implements RequestHandler<SQSEvent, Void> {
    @Override
    public Void handleRequest(SQSEvent sqsEvent, Context context) {
        for (SQSMessage msg : sqsEvent.getRecords()) {
            processMessage(msg, context);
        }
        context.getLogger().log("done");
        return null;
    }

    private void processMessage(SQSMessage msg, Context context) {
        try {
            context.getLogger().log("Processed message " + msg.getBody());

            // TODO: Do interesting work based on the new message

        } catch (Exception e) {
            context.getLogger().log("An error occurred");
            throw e;
        }

    }
}

             
        
    
        
    JavaScript
            
     

        SDK for JavaScript (v3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using JavaScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
exports.handler = async (event, context) => {
  for (const message of event.Records) {
    await processMessageAsync(message);
  }
  console.info("done");
};

async function processMessageAsync(message) {
  try {
    console.log(`Processed message ${message.body}`);
    // TODO: Do interesting work based on the new message
    await Promise.resolve(1); //Placeholder for actual async work
  } catch (err) {
    console.error("An error occurred");
    throw err;
  }
}


             
             
                    Consuming an SQS event with Lambda using TypeScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import { SQSEvent, Context, SQSHandler, SQSRecord } from "aws-lambda";

export const functionHandler: SQSHandler = async (
  event: SQSEvent,
  context: Context
): Promise<void> => {
  for (const message of event.Records) {
    await processMessageAsync(message);
  }
  console.info("done");
};

async function processMessageAsync(message: SQSRecord): Promise<any> {
  try {
    console.log(`Processed message ${message.body}`);
    // TODO: Do interesting work based on the new message
    await Promise.resolve(1); //Placeholder for actual async work
  } catch (err) {
    console.error("An error occurred");
    throw err;
  }
}


             
        
    
        
    PHP
            
     

        SDK for PHP
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using PHP.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
<?php

# using bref/bref and bref/logger for simplicity

use Bref\Context\Context;
use Bref\Event\InvalidLambdaEvent;
use Bref\Event\Sqs\SqsEvent;
use Bref\Event\Sqs\SqsHandler;
use Bref\Logger\StderrLogger;

require __DIR__ . '/vendor/autoload.php';

class Handler extends SqsHandler
{
    private StderrLogger $logger;
    public function __construct(StderrLogger $logger)
    {
        $this->logger = $logger;
    }

    /**
     * @throws InvalidLambdaEvent
     */
    public function handleSqs(SqsEvent $event, Context $context): void
    {
        foreach ($event->getRecords() as $record) {
            $body = $record->getBody();
            // TODO: Do interesting work based on the new message
        }
    }
}

$logger = new StderrLogger();
return new Handler($logger);


             
        
    
        
    Python
            
     

        SDK for Python (Boto3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using Python.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def lambda_handler(event, context):
    for message in event['Records']:
        process_message(message)
    print("done")

def process_message(message):
    try:
        print(f"Processed message {message['body']}")
        # TODO: Do interesting work based on the new message
    except Exception as err:
        print("An error occurred")
        raise err


             
        
    
        
    Ruby
            
     

        SDK for Ruby
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using Ruby.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def lambda_handler(event:, context:)
  event['Records'].each do |message|
    process_message(message)
  end
  puts "done"
end

def process_message(message)
  begin
    puts "Processed message #{message['body']}"
    # TODO: Do interesting work based on the new message
  rescue StandardError => err
    puts "An error occurred"
    raise err
  end
end

             
        
    
        
    Rust
            
     

        SDK for Rust
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SQS event with Lambda using Rust.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
use aws_lambda_events::event::sqs::SqsEvent;
use lambda_runtime::{run, service_fn, Error, LambdaEvent};

async fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<(), Error> {
    event.payload.records.iter().for_each(|record| {
        // process the record
        tracing::info!("Message body: {}", record.body.as_deref().unwrap_or_default())
    });

    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        // disable printing the name of the module in every log line.
        .with_target(false)
        // disabling time is handy because CloudWatch will add the ingestion time.
        .without_time()
        .init();

    run(service_fn(function_handler)).await
}

             
        
    
        

    To create a Node.js Lambda function
        Create a directory for the project, and then switch to that directory.
        mkdir sqs-tutorial
cd sqs-tutorial
      
        Copy the sample JavaScript code into a new file named index.js.
      
        Create a deployment package using the following zip command.
        zip function.zip index.js
      
        Create a Lambda function using the create-function
          AWS CLI command. For the role parameter, enter the ARN of the execution role
          that you created earlier.
        NoteThe Lambda function and the Amazon SQS queue must be in the same AWS Region.
aws lambda create-function --function-name ProcessSQSRecord \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::111122223333:role/lambda-sqs-role
      
   
    Test the function
    
       
        
       
       
    
    Invoke your Lambda function manually using the invoke AWS CLI command and a sample Amazon SQS
      event.
    To invoke the Lambda function with a sample event
        Save the following JSON as a file named input.json. This JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue. In this example, the message is "test". 
        Example  Amazon SQS eventThis is a test event—you don't need to change the message or the account number.{
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:my-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
      
        Run the following invoke AWS CLI command. This command returns CloudWatch logs in the response. For more information about retrieving logs, see Access logs with the AWS CLI.
        aws lambda invoke --function-name ProcessSQSRecord --payload file://input.json out --log-type Tail \
--query 'LogResult' --output text --cli-binary-format raw-in-base64-out | base64 --decode
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
        
        Find the INFO log in the response. This is where the Lambda function logs the message body.
          You should see logs that look like this:
        2023-09-11T22:45:04.271Z	348529ce-2211-4222-9099-59d07d837b60	INFO	Processed message test
2023-09-11T22:45:04.288Z	348529ce-2211-4222-9099-59d07d837b60	INFO	done
      
   
    Create an Amazon SQS queue
    
       
        
       
       
    
    Create an Amazon SQS queue that the Lambda function can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Enter a name for the queue. Leave all other options at the default settings.
      
        Choose Create queue.
      
    After creating the queue, note down its ARN. You need this in the next step when you
      associate the queue with your Lambda function.
   
    Configure the event source
    
       
        
       
       
    
    Connect the Amazon SQS queue to your Lambda function by creating an event source mapping. The event source mapping reads the Amazon SQS queue and invokes your Lambda function when a new message is added.
    To create a mapping between your Amazon SQS queue and your Lambda function, use the create-event-source-mapping AWS CLI command. Example:
    aws lambda create-event-source-mapping --function-name ProcessSQSRecord  --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:111122223333:my-queue
    To get a list of your event source mappings, use the list-event-source-mappings command. Example:
    aws lambda list-event-source-mappings --function-name ProcessSQSRecord
   
    Send a test message
    
       
        
       
       
    
    To send an Amazon SQS message to the Lambda function
        Open the Amazon SQS console.
      
        Choose the queue that you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message, such as "this is a test message."
      
        Choose Send message.
      
    Lambda polls the queue for updates. When there is a new message, Lambda invokes your function with this new
      event data from the queue. If the function handler returns without exceptions, Lambda considers the message successfully processed and
      begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes it
      from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
     
    Check the CloudWatch logs
    
       
        
       
       
    
    To confirm that the function processed the messageOpen the Functions page of the Lambda console.
        Choose the ProcessSQSRecord function.
      
        Choose Monitor.
      
         Choose View CloudWatch logs.
      
        In the CloudWatch console, choose the Log stream for the function.
      
        Find the INFO log. This is where the Lambda function logs the message body. You should see the message that you sent from the Amazon SQS queue. Example:
        2023-09-11T22:49:12.730Z b0c41e9c-0556-5a8b-af83-43e59efeec71 INFO Processed message this is a test message.
      
   
    Clean up your resources
     
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
     
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsEvent filteringSQS cross-account tutorialDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideInvoke a Lambda function synchronouslyWhen you invoke a function synchronously, Lambda runs the function and waits for a response. When the function
    completes, Lambda returns the response from the function's code with additional data, such as the version of the
    function that was invoked. To invoke a function synchronously with the AWS CLI, use the invoke
    command.aws lambda invoke --function-name my-function \
    --cli-binary-format raw-in-base64-out \
    --payload '{ "key": "value" }' response.jsonThe cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.You should see the following output:{
    "ExecutedVersion": "$LATEST",
    "StatusCode": 200
}The following diagram shows clients invoking a Lambda function synchronously. Lambda sends the events directly to
    the function and sends the function's response back to the invoker.
     
      
     
     
  The payload is a string that contains an event in JSON format. The name of the file where the AWS CLI
    writes the response from the function is response.json. If the function returns an object or error, the
    response body is the object or error in JSON format. If the function exits without error, the response body is
      null.NoteLambda does not wait for external extensions to complete before sending the response. External extensions run as independent processes in the execution environment and continue to run after the function invocation is complete. For more information, see Augment Lambda functions using Lambda extensions.The output from the command, which is displayed in the terminal, includes information from headers in the
    response from Lambda. This includes the version that processed the event (useful when you use aliases), and the status code returned by Lambda. If Lambda was able to run
    the function, the status code is 200, even if the function returned an error.NoteFor functions with a long timeout, your client might be disconnected during synchronous invocation while it
      waits for a response. Configure your HTTP client, SDK, firewall, proxy, or operating system to allow for long
      connections with timeout or keep-alive settings.If Lambda isn't able to run the function, the error is displayed in the output.aws lambda invoke --function-name my-function \
    --cli-binary-format raw-in-base64-out \
    --payload value response.jsonYou should see the following output:An error occurred (InvalidRequestContentException) when calling the Invoke operation: Could not parse request body into json: Unrecognized token 'value': was expecting ('true', 'false' or 'null')
 at [Source: (byte[])"value"; line: 1, column: 11]The AWS CLI is an open-source tool that enables you to interact with AWS services using commands in your command line shell. To complete the steps in this section, you must have the AWS CLI version 2.You can use the AWS CLI to retrieve logs for an invocation using the --log-type command option. The response contains a LogResult field that contains up to 4 KB of base64-encoded logs from the invocation.Example retrieve a log IDThe following example shows how to retrieve a log ID from the LogResult field for a function named my-function.aws lambda invoke --function-name my-function out --log-type TailYou should see the following output:{
    "StatusCode": 200,
    "LogResult": "U1RBUlQgUmVxdWVzdElkOiA4N2QwNDRiOC1mMTU0LTExZTgtOGNkYS0yOTc0YzVlNGZiMjEgVmVyc2lvb...",
    "ExecutedVersion": "$LATEST"
}Example decode the logsIn the same command prompt, use the base64 utility to decode the logs. The following example shows how to retrieve base64-encoded logs for my-function.aws lambda invoke --function-name my-function out --log-type Tail \
--query 'LogResult' --output text --cli-binary-format raw-in-base64-out | base64 --decodeThe cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.You should see the following output:START RequestId: 57f231fb-1730-4395-85cb-4f71bd2b87b8 Version: $LATEST
"AWS_SESSION_TOKEN": "AgoJb3JpZ2luX2VjELj...", "_X_AMZN_TRACE_ID": "Root=1-5d02e5ca-f5792818b6fe8368e5b51d50;Parent=191db58857df8395;Sampled=0"",ask/lib:/opt/lib",
END RequestId: 57f231fb-1730-4395-85cb-4f71bd2b87b8
REPORT RequestId: 57f231fb-1730-4395-85cb-4f71bd2b87b8  Duration: 79.67 ms      Billed Duration: 80 ms         Memory Size: 128 MB     Max Memory Used: 73 MBThe base64 utility is available on Linux, macOS, and Ubuntu on Windows. macOS users may need to use base64 -D.For more information about the Invoke API, including a full list of parameters, headers, and
    errors, see Invoke.When you invoke a function directly, you can check the response for errors and retry. The AWS CLI and AWS SDK also
    automatically retry on client timeouts, throttling, and service errors. For more information, see Understanding retry behavior in Lambda.Document ConventionsInvoking functionsAsynchronous invocationDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideVisibility timeout use casesSetting and adjusting the visibility
                timeoutIn flight messages and quotasUnderstanding visibility
                timeout in standard and FIFO queuesHandling failuresChanging and terminating
                visibility timeoutBest practicesAmazon SQS visibility timeoutWhen you receive a message from an Amazon SQS queue, it remains in the queue but becomes
        temporarily invisible to other consumers. This invisibility is controlled by the visibility
        timeout, which ensures that other consumers cannot process the same message while you are
        working on it. Amazon SQS offers two options for deleting messages after processing:
         
         
    
            Manual deletion – You explicitly delete
                messages using the DeleteMessage action.
        
            Automatic deletion – Supported in certain
                AWS SDKs, messages are automatically deleted upon successful processing,
                simplifying workflows.
        
         
            
         
         
    
        Visibility timeout use cases
        Manage long-running tasks – Use the visibility
            timeout to handle tasks that require extended processing times. Set an appropriate
            visibility timeout for messages that require extended processing time. This ensures that
            other consumers don't pick up the same message while it's being processed, preventing
            duplicate work and maintaining system efficiency.
        Implement retry mechanisms – Extend the
            visibility timeout programmatically for tasks that fail to complete within the initial
            timeout. If a task fails to complete within the initial visibility timeout, you can
            extend the timeout programmatically. This allows your system to retry processing the
            message without it becoming visible to other consumers, improving fault tolerance and
            reliability. Combine with Dead-Letter Queues (DLQs) to
            manage persistent failures.
        Coordinate distributed systems – Use SQS
            visibility timeout to coordinate tasks across distributed systems. Set visibility
            timeouts that align with your expected processing times for different components. This
            helps maintain consistency and prevents race conditions in complex, distributed
            architectures.
        Optimize resource utilization – Adjust SQS
            visibility timeouts to optimize resource utilization in your application. By setting
            appropriate timeouts, you can ensure that messages are processed efficiently without
            tying up resources unnecessarily. This leads to better overall system performance and
            cost-effectiveness.
     
        Setting and adjusting the visibility
                timeout
        The visibility timeout starts as soon as a message is delivered to you. During this
            period, you're expected to process and delete the message. If you don't delete it before
            the timeout expires, the message becomes visible again in the queue and can be retrieved
            by another consumer. The default visibility timeout for a queue is 30 seconds, but you
            can adjust this to match the time your application needs to process and delete a
            message. You can also set a specific visibility timeout for individual messages without
            changing the queue's overall setting. Use the ChangeMessageVisibility action to programmatically extend
            or shorten the timeout as needed.
     
        In flight messages and quotas
        In Amazon SQS, in-flight messages are messages that have been received by a consumer but
            not yet deleted. For standard queues, there's a limit of approximately 120,000 in-flight
            messages, depending on queue traffic and message backlog. If you reach this limit, Amazon SQS
            returns an OverLimit error, indicating that no additional messages can be
            received until some in-flight messages are deleted. For FIFO queues, limits depend on
            active message groups.
        
             
             
        
                When using short polling – If this
                    limit is reached while using short polling, Amazon SQS will return an
                        OverLimit error, indicating that no additional messages can be
                    received until some in-flight messages are deleted.
            
                When using long polling – If you are
                    using long polling, Amazon SQS does not return an error when the in-flight message
                    limit is reached. Instead, it will not return any new messages until the number
                    of in-flight messages drops below the limit.
            
        To manage in-flight messages effectively:
        
             
             
             
             
        
                Prompt deletion – Delete messages
                    (manually or automatically) after processing to reduce the in-flight
                    count.
            
                Monitor with CloudWatch – Set alarms for
                    high in-flight counts to prevent reaching the limit.
            
                Distribute load – If you're processing
                    a high volume of messages, use additional queues or consumers to balance load
                    and avoid bottlenecks.
            
                Request a quota increase – Submit a
                    request to AWS
                        Support if higher limits are required.
            
     
        Understanding visibility
                timeout in standard and FIFO queues
        In both standard and FIFO (First-In-First-Out) queues, the visibility timeout helps
            prevent multiple consumers from processing the same message simultaneously. However, due
            to the at-least-once delivery model of Amazon SQS, there's no absolute guarantee that a
            message won't be delivered more than once during the visibility timeout period.
        
             
             
        
                Standard queues – The visibility
                    timeout in standard queues prevents multiple consumers from processing the same
                    message at the same time. However, because of the at-least-once delivery model,
                    Amazon SQS doesn't guarantee that a message won’t be delivered more than once within
                    the visibility timeout period.
            
                FIFO queues – For FIFO queues,
                    messages with the same message group ID are processed in a strict sequence. When
                    a message with a message group ID is in-flight, subsequent messages in that
                    group are not made available until the in-flight message is either deleted or
                    the visibility timeout expires. However, this doesn’t "lock" the group
                    indefinitely– each message is processed in sequence, and only when each
                    message is deleted or becomes visible again will the next message in that group
                    be available to consumers. This approach ensures ordered processing within the
                    group without unnecessarily locking the group from delivering messages.
            
     
        Handling failures
        If you don't process and delete a message before the visibility timeout expires—due to
            application errors, crashes, or connectivity problems—the message becomes visible again
            in the queue. It can then be retrieved by the same or a different consumer for another
            processing attempt. This ensures that messages aren't lost even if the initial
            processing fails. However, setting the visibility timeout too high can delay the
            reappearance of unprocessed messages, potentially slowing down retries. It's crucial to
            set an appropriate visibility timeout based on the expected processing time for timely
            message handling.
     
        Changing and terminating
                visibility timeout
        You can change or terminate the visibility timeout using the
                ChangeMessageVisibility action:
        
             
             
        
                Changing the timeout – Adjust the
                    visibility timeout dynamically using ChangeMessageVisibility. This allows you to extend
                    or reduce timeout durations to match processing needs.
            
                Terminating the timeout – If you
                    decide not to process a received message, terminate its visibility timeout by
                    setting the VisibilityTimeout to 0 seconds through the
                        ChangeMessageVisibility action. This immediately makes the
                    message available for other consumers to process.
            
     
        Best practices
        Use the following best practices for managing visibility timeouts in Amazon SQS, including
            setting, adjusting, and extending timeouts, as well as handling unprocessed messages
            using Dead-Letter Queues (DLQs).
        
             
             
             
        
                Setting and adjusting the timeout. Start by
                    setting the visibility timeout to match the maximum time your application
                    typically needs to process and delete a message. If you're unsure about the
                    exact processing time, begin with a shorter timeout (for example, 2 minutes) and
                    extend it as necessary. Implement a heartbeat mechanism to periodically extend
                    the visibility timeout, ensuring the message remains invisible until processing
                    is complete. This minimizes delays in reprocessing unhandled messages and
                    prevents premature visibility.
            
                Extending the timeout and handling the 12-Hour
                        limit. If your processing time varies or may exceed the initially
                    set timeout, use the ChangeMessageVisibility action to extend the
                    visibility timeout while processing the message. Keep in mind that the
                    visibility timeout has a maximum limit of 12 hours from when the message is
                    first received. Extending the timeout doesn't reset this 12-hour limit. If your
                    processing requires more time than this limit, consider using AWS Step Functions or
                    breaking the task into smaller steps.
            
                Handling unprocessed messages. To manage
                    messages that fail multiple processing attempts, configure a Dead-Letter Queue
                    (DLQ). This ensures that messages that can't be processed after several retries
                    are captured separately for further analysis or handling, preventing them from
                    repeatedly circulating in the main queue. 
            
    Document ConventionsShort and long pollingDelay queuesDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideBackoff strategy for failed invocationsImplementing partial batch responsesHandling errors for an SQS event source in LambdaTo handle errors related to an SQS event source, Lambda automatically uses a retry strategy with a
        backoff strategy. You can also customize error handling behavior by configuring your SQS event
        source mapping to return partial batch responses.
        Backoff strategy for failed invocations
        When an invocation fails, Lambda attempts to retry the invocation while implementing a backoff strategy.
            The backoff strategy differs slightly depending on whether Lambda encountered the failure due to an error in
            your function code, or due to throttling.
        
             
             
        
                
                    If your function code caused the error, Lambda will stop processing and retrying the invocation.
                    In the meantime, Lambda gradually backs off, reducing the amount of concurrency allocated to your Amazon SQS event source mapping.
                    After your queue's visibility timeout runs out, the message will again reappear in the queue.
                
            
                If the invocation fails due to throttling, Lambda gradually backs off
                    retries by reducing the amount of concurrency allocated to your Amazon SQS event source mapping. Lambda continues
                    to retry the message until the message's timestamp exceeds your queue's visibility timeout, at which point
                    Lambda drops the message.
            
     
        Implementing partial batch responses
        When your Lambda function encounters an error while processing a batch, all messages in that batch become
            visible in the queue again by default, including messages that Lambda processed successfully. As a result, your
            function can end up processing the same message several times.
        To avoid reprocessing successfully processed messages in a failed batch, you can configure your event
            source mapping to make only the failed messages visible again. This is called a partial batch response.
            To turn on partial batch responses, specify ReportBatchItemFailures for the
            FunctionResponseTypes
            action when configuring your event source mapping. This lets your function
            return a partial success, which can help reduce the number of unnecessary retries on records.
        When ReportBatchItemFailures is activated, Lambda doesn't scale down message polling when function invocations fail. If you expect some messages to fail—and you don't want those failures to impact the message processing rate—use ReportBatchItemFailures.
        NoteKeep the following in mind when using partial batch responses:
                 
                 
            
                    If your function throws an exception, the entire batch is considered a complete failure.
                
                    If you're using this feature with a FIFO queue, your function should stop processing messages after the
                        first failure and return all failed and unprocessed messages in batchItemFailures. This helps
                        preserve the ordering of messages in your queue.
                
        To activate partial batch reporting
                Review the Best practices for implementing partial batch responses.
            
                Run the following command to activate ReportBatchItemFailures for your function. To retrieve your event source mapping's UUID, run the list-event-source-mappings AWS CLI command.
                aws lambda update-event-source-mapping \
--uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
--function-response-types "ReportBatchItemFailures"
            
                Update your function code to catch all exceptions and return failed messages in a batchItemFailures JSON response. The batchItemFailures response must include a list of message IDs, as itemIdentifier JSON values.
                For example, suppose you have a batch of five messages, with message IDs id1, id2, id3, id4, and id5. Your function successfully processes id1, id3, and id5. To make messages id2 and id4 visible again in your queue, your function should return the following response:  
                { 
  "batchItemFailures": [ 
        {
            "itemIdentifier": "id2"
        },
        {
            "itemIdentifier": "id4"
        }
    ]
}
                Here are some examples of function code that return the list of failed message IDs in the batch:
                
    .NET
            
     

        SDK for .NET
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using .NET.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
using Amazon.Lambda.Core;
using Amazon.Lambda.SQSEvents;

// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.
[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]
namespace sqsSample;

public class Function
{
    public async Task<SQSBatchResponse> FunctionHandler(SQSEvent evnt, ILambdaContext context)
    {
        List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new List<SQSBatchResponse.BatchItemFailure>();
        foreach(var message in evnt.Records)
        {
            try
            {
                //process your message
                await ProcessMessageAsync(message, context);
            }
            catch (System.Exception)
            {
                //Add failed message identifier to the batchItemFailures list
                batchItemFailures.Add(new SQSBatchResponse.BatchItemFailure{ItemIdentifier=message.MessageId}); 
            }
        }
        return new SQSBatchResponse(batchItemFailures);
    }

    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)
    {
        if (String.IsNullOrEmpty(message.Body))
        {
            throw new Exception("No Body in SQS Message.");
        }
        context.Logger.LogInformation($"Processed message {message.Body}");
        // TODO: Do interesting work based on the new message
        await Task.CompletedTask;
    }
}

             
        
    
        
    Go
            
     

        SDK for Go V2
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Go.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"github.com/aws/aws-lambda-go/events"
	"github.com/aws/aws-lambda-go/lambda"
)

func handler(ctx context.Context, sqsEvent events.SQSEvent) (map[string]interface{}, error) {
	batchItemFailures := []map[string]interface{}{}

	for _, message := range sqsEvent.Records {
		
		if /* Your message processing condition here */ {			
			batchItemFailures = append(batchItemFailures, map[string]interface{}{"itemIdentifier": message.MessageId})
		}
	}

	sqsBatchResponse := map[string]interface{}{
		"batchItemFailures": batchItemFailures,
	}
	return sqsBatchResponse, nil
}

func main() {
	lambda.Start(handler)
}


             
        
    
        
    Java
            
     

        SDK for Java 2.x
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Java.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SQSEvent;
import com.amazonaws.services.lambda.runtime.events.SQSBatchResponse;
 
import java.util.ArrayList;
import java.util.List;
 
public class ProcessSQSMessageBatch implements RequestHandler<SQSEvent, SQSBatchResponse> {
    @Override
    public SQSBatchResponse handleRequest(SQSEvent sqsEvent, Context context) {
 
         List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new ArrayList<SQSBatchResponse.BatchItemFailure>();
         String messageId = "";
         for (SQSEvent.SQSMessage message : sqsEvent.getRecords()) {
             try {
                 //process your message
             } catch (Exception e) {
                 //Add failed message identifier to the batchItemFailures list
                 batchItemFailures.add(new SQSBatchResponse.BatchItemFailure(message.getMessageId()));
             }
         }
         return new SQSBatchResponse(batchItemFailures);
     }
}

             
        
    
        
    JavaScript
            
     

        SDK for JavaScript (v3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using JavaScript.
                
                // Node.js 20.x Lambda runtime, AWS SDK for Javascript V3
export const handler = async (event, context) => {
    const batchItemFailures = [];
    for (const record of event.Records) {
        try {
            await processMessageAsync(record, context);
        } catch (error) {
            batchItemFailures.push({ itemIdentifier: record.messageId });
        }
    }
    return { batchItemFailures };
};

async function processMessageAsync(record, context) {
    if (record.body && record.body.includes("error")) {
        throw new Error("There is an error in the SQS Message.");
    }
    console.log(`Processed message: ${record.body}`);
}

             
             
                    Reporting SQS batch item failures with Lambda using TypeScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import { SQSEvent, SQSBatchResponse, Context, SQSBatchItemFailure, SQSRecord } from 'aws-lambda';

export const handler = async (event: SQSEvent, context: Context): Promise<SQSBatchResponse> => {
    const batchItemFailures: SQSBatchItemFailure[] = [];

    for (const record of event.Records) {
        try {
            await processMessageAsync(record);
        } catch (error) {
            batchItemFailures.push({ itemIdentifier: record.messageId });
        }
    }

    return {batchItemFailures: batchItemFailures};
};

async function processMessageAsync(record: SQSRecord): Promise<void> {
    if (record.body && record.body.includes("error")) {
        throw new Error('There is an error in the SQS Message.');
    }
    console.log(`Processed message ${record.body}`);
}


             
        
    
        
    PHP
            
     

        SDK for PHP
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using PHP.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
<?php

use Bref\Context\Context;
use Bref\Event\Sqs\SqsEvent;
use Bref\Event\Sqs\SqsHandler;
use Bref\Logger\StderrLogger;

require __DIR__ . '/vendor/autoload.php';

class Handler extends SqsHandler
{
    private StderrLogger $logger;
    public function __construct(StderrLogger $logger)
    {
        $this->logger = $logger;
    }

    /**
     * @throws JsonException
     * @throws \Bref\Event\InvalidLambdaEvent
     */
    public function handleSqs(SqsEvent $event, Context $context): void
    {
        $this->logger->info("Processing SQS records");
        $records = $event->getRecords();

        foreach ($records as $record) {
            try {
                // Assuming the SQS message is in JSON format
                $message = json_decode($record->getBody(), true);
                $this->logger->info(json_encode($message));
                // TODO: Implement your custom processing logic here
            } catch (Exception $e) {
                $this->logger->error($e->getMessage());
                // failed processing the record
                $this->markAsFailed($record);
            }
        }
        $totalRecords = count($records);
        $this->logger->info("Successfully processed $totalRecords SQS records");
    }
}

$logger = new StderrLogger();
return new Handler($logger);


             
        
    
        
    Python
            
     

        SDK for Python (Boto3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Python.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

def lambda_handler(event, context):
    if event:
        batch_item_failures = []
        sqs_batch_response = {}
     
        for record in event["Records"]:
            try:
                # process message
            except Exception as e:
                batch_item_failures.append({"itemIdentifier": record['messageId']})
        
        sqs_batch_response["batchItemFailures"] = batch_item_failures
        return sqs_batch_response

             
        
    
        
    Ruby
            
     

        SDK for Ruby
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Ruby.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
require 'json'

def lambda_handler(event:, context:)
  if event
    batch_item_failures = []
    sqs_batch_response = {}

    event["Records"].each do |record|
      begin
        # process message
      rescue StandardError => e
        batch_item_failures << {"itemIdentifier" => record['messageId']}
      end
    end

    sqs_batch_response["batchItemFailures"] = batch_item_failures
    return sqs_batch_response
  end
end


             
        
    
        
    Rust
            
     

        SDK for Rust
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Rust.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
use aws_lambda_events::{
    event::sqs::{SqsBatchResponse, SqsEvent},
    sqs::{BatchItemFailure, SqsMessage},
};
use lambda_runtime::{run, service_fn, Error, LambdaEvent};

async fn process_record(_: &SqsMessage) -> Result<(), Error> {
    Err(Error::from("Error processing message"))
}

async fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<SqsBatchResponse, Error> {
    let mut batch_item_failures = Vec::new();
    for record in event.payload.records {
        match process_record(&record).await {
            Ok(_) => (),
            Err(_) => batch_item_failures.push(BatchItemFailure {
                item_identifier: record.message_id.unwrap(),
            }),
        }
    }

    Ok(SqsBatchResponse {
        batch_item_failures,
    })
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    run(service_fn(function_handler)).await
}


             
        
    
        

            
        If the failed events do not return to the queue, see How do I troubleshoot Lambda function SQS ReportBatchItemFailures? in the AWS Knowledge Center.
         
            Success and failure conditions
            Lambda treats a batch as a complete success if your function returns any of the following:
            
                 
                 
                 
                 
            
                    An empty batchItemFailures list
                
                    A null batchItemFailures list
                
                    An empty EventResponse
                
                    A null EventResponse
                
            Lambda treats a batch as a complete failure if your function returns any of the following:
            
                 
                 
                 
                 
                 
            
                    An invalid JSON response
                
                    An empty string itemIdentifier
                
                    A null itemIdentifier
                
                    An itemIdentifier with a bad key name
                
                    An itemIdentifier value with a message ID that doesn't exist
                
         
         
            CloudWatch metrics
            To determine whether your function is correctly reporting batch item failures, you can monitor the
                NumberOfMessagesDeleted and ApproximateAgeOfOldestMessage Amazon SQS metrics in
                Amazon CloudWatch.
            
                 
                 
            
                    NumberOfMessagesDeleted tracks the number of messages removed from your queue. If this
                        drops to 0, this is a sign that your function response is not correctly returning failed messages.
                
                    ApproximateAgeOfOldestMessage tracks how long the oldest message has stayed in your queue.
                        A sharp increase in this metric can indicate that your function is not correctly returning failed
                        messages.
                
         
    Document ConventionsScaling behaviorParametersDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceAPI ReferenceRequest SyntaxRequest ParametersResponse ElementsErrorsExamplesSee AlsoDeleteMessageDeletes the specified message from the specified queue. To select the message to
            delete, use the ReceiptHandle of the message (not the
                MessageId which you receive when you send the message). Amazon SQS can
            delete a message from a queue even if a visibility timeout setting causes the message to
            be locked by another consumer. Amazon SQS automatically deletes messages left in a queue
            longer than the retention period configured for the queue. NoteEach time you receive a message, meaning when a consumer retrieves a message from
                the queue, it comes with a unique ReceiptHandle. If you receive the
                same message more than once, you will get a different ReceiptHandle
                each time. When you want to delete a message using the DeleteMessage
                action, you must use the ReceiptHandle from the most recent time you
                received the message. If you use an old ReceiptHandle, the request will
                succeed, but the message might not be deleted. For standard queues, it is possible to receive a message even after you
                delete it. This might happen on rare occasions if one of the servers which stores a
                copy of the message is unavailable when you send the request to delete the message.
                The copy remains on the server and might be returned to you during a subsequent
                receive request. You should ensure that your application is idempotent, so that
                receiving a message more than once does not cause issues.
      Request Syntax
      {
   "QueueUrl": "string",
   "ReceiptHandle": "string"
}
    
      Request Parameters
      For information about the parameters that are common to all actions, see Common Parameters.
      The request accepts the following data in JSON format.
      
          
          
      
            
               
                  QueueUrl
               
            
            
               The URL of the Amazon SQS queue from which messages are deleted.
               Queue URLs and names are case-sensitive.
               Type: String
               Required: Yes
            
          
            
               
                  ReceiptHandle
               
            
            
               The receipt handle associated with the message to delete.
               Type: String
               Required: Yes
            
         
    
      Response Elements
      If the action is successful, the service sends back an HTTP 200 response with an empty HTTP body.
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
          
          
      
            
               
                  InvalidAddress
               
            
            
               The specified ID is invalid.
               HTTP Status Code: 400
            
          
            
               
                  InvalidIdFormat
               
            
            
               
                  This error has been deprecated.
               
               The specified receipt handle isn't valid for the current version.
               HTTP Status Code: 400
            
          
            
               
                  InvalidSecurity
               
            
            
               The request was not made over HTTPS or did not use SigV4 for signing.
               HTTP Status Code: 400
            
          
            
               
                  QueueDoesNotExist
               
            
            
               Ensure that the QueueUrl is correct and that the queue has not been
            deleted.
               HTTP Status Code: 400
            
          
            
               
                  ReceiptHandleIsInvalid
               
            
            
               The specified receipt handle isn't valid.
               HTTP Status Code: 400
            
          
            
               
                  RequestThrottled
               
            
            
               The request was denied due to request throttling.
               
                   
                   
               
                     Exceeds the permitted request rate for the queue or for the recipient of the
                    request.
                  
                     Ensure that the request rate is within the Amazon SQS limits for
                    sending messages. For more information, see Amazon SQS quotas in the Amazon SQS
                        Developer Guide.
                  
               HTTP Status Code: 400
            
          
            
               
                  UnsupportedOperation
               
            
            
               Error code 400. Unsupported operation.
               HTTP Status Code: 400
            
         
    
      Examples
      The following example query request deletes a message from the queue named
                    MyQueue. The structure of AUTHPARAMS depends on the signature of the API request. 
                For more information, see 
                Examples of Signed Signature Version 4 Requests in the 
            AWS General Reference.
       
         Example
         
            Using AWS JSON protocol
                        (Default)
         
          
            Sample Request
            POST / HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Target: AmazonSQS.DeleteMessage
X-Amz-Date: <Date>
Content-Type: application/x-amz-json-1.0
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive 
{
    "QueueUrl": "https://sqs.us-east-1.amazonaws.com/177715257436/MyQueue/",
    "ReceiptHandle": "AQEB3LQoW7GQWgodQCEJXHjMvO/QkeCHiRldRfLC/E6RUggm+BjpthqxfoUOUn6Vs271qmrBaufFqEmnMKgk2n1EuUBne1pe+hZcrDE8IveUUPmqkUT54FGhAAjPX3oEIryz/XeQ/muKAuLclcZvt2Q+ZDPW8DvZqMa1RoHxOqSq+6kQ4PwgQxB+VqDYvIc/LpHOoL4PTROBXgLPjWrzz/knK6HTzKpqC4ESvFdJ/dkk2nvS0iqYOly5VQknK/lv/rTUOgEYevjJSrNLIPDgZGyvgcLwbm6+yo1cW/c9cPDiVm96gIhVkuiCZ1gtskoOtyroZVPcY71clDG2EPZJeY8akMd3u+sXEMWhiOPFs1cgWQs2ugsL+vdwMCbsZRkXbJv7"
}
          
          
            Sample Response
            HTTP/1.1 200 OK
x-amzn-RequestId: <requestId>
Content-Length: 0
Date: <Date>
Content-Type: application/x-amz-json-1.0
          
       
       
         Example
         
            Using AWS query
                    protocol
         
          
            Sample Request
            POST /177715257436/MyQueue/ HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
Content-Type: application/x-www-form-urlencoded
X-Amz-Date: <Date>
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive
Action=DeleteMessage
&ReceiptHandle=AQEBMeG2RcZZrIcgBkDFb6lHqL9B9tbbEHNh+4uxMIG+CPupPjqJtRswDlOr6hOTzgcq105i0iZNci5GS5RTnHTkD2zipM9gHfSP2tWPhY7HHsU5GCTZ+egzS5HiEvmGZ71g71Lucdk7mes1/WGXnmU27K26Koo9GGrB0AKTv16dync1ezCMNyrBHEMUyIWS2lUTbrSj7fw93dgZSg2eWTk+thSVUB/ibOwpmj+wBN99nKQQklsZHtZd4exT1V3JHwP4kqz+D3C2RGn7js3nNdFpH41lBH8rCTZDU8DQp9eQNHLIL6RUf1WrI8gv8L7NErGlIH4Y3wZbFEOMKilVHenfpP2G6ElMuxyM3y+qdlZq4m00VGIIZeMg9PPmVsLtB7u9mruLyNFraN5ihKMjzQoKgA==
          
          
            Sample Response
            HTTP/1.1 200 OK
<?xml version="1.0"?>
<DeleteMessageResponse xmlns="http://queue.amazonaws.com/doc/2012-11-05/">
    <ResponseMetadata>
        <RequestId>b5293cb5-d306-4a17-9048-b263635abe42</RequestId>
    </ResponseMetadata>
</DeleteMessageResponse>
          
       
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsCreateQueueDeleteMessageBatchDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideConfiguring a queue to use with LambdaSetting up Lambda execution role permissionsCreating an SQS event source mappingCreating and configuring an Amazon SQS event source mappingTo process Amazon SQS messages with Lambda, configure your queue with the appropriate settings,
        then create a Lambda event source mapping.
        Configuring a queue to use with Lambda
        If you don't already have an existing Amazon SQS queue, create one
            to serve as an event source for your Lambda function. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
        To allow your function time to process each batch of records, set the source queue's
            
            visibility timeout to at least six times the configuration
            timeout on your function. The extra time allows Lambda to retry if your function is throttled
            while processing a previous batch.
        By default, if Lambda encounters an error at any point while processing a batch, all
            messages in that batch return to the queue. After the 
            visibility timeout, the messages become visible to Lambda again. You can
            configure your event source mapping to use 
            partial batch responses to return only the failed messages back to the queue. In
            addition, if your function fails to process a message multiple times, Amazon SQS can send it to a
            
            dead-letter queue. We recommend setting the maxReceiveCount on your
            source queue's 
            redrive policy to at least 5. This gives Lambda a few chances to retry before
            sending failed messages directly to the dead-letter queue.
     
        Setting up Lambda execution role permissions
        The 
            AWSLambdaSQSQueueExecutionRole AWS managed policy includes the permissions that Lambda needs to read
            from your Amazon SQS queue. You can add this managed policy to your function's
            execution role.
        Optionally, if you're using an encrypted queue, you also need to add the following permission to your
            execution role:
        
             
        
                kms:Decrypt
            
     
        Creating an SQS event source mapping
        Create an event source mapping to tell Lambda to send items from your queue to a Lambda function.
            You can create multiple event source mappings to process items from multiple queues with a single
            function. When Lambda invokes the target function, the event can contain multiple items, up to a
            configurable maximum batch size.
        To configure your function to read from Amazon SQS, attach the 
            AWSLambdaSQSQueueExecutionRole AWS managed policy to your execution role.
            Then, create an SQS event source mapping from the console using
            the following steps.
        To add permissions and create a triggerOpen the Functions page of the Lambda console.
        Choose the name of a function.
      
        Choose the Configuration tab, and then choose Permissions.
      
        Under Role name, choose the link to your execution role. This link opens the role in the IAM console.
          
             
               
             
             
                  
      
        Choose Add permissions, and then choose Attach policies.
          
             
               
             
             
                    
      
        In the search field, enter AWSLambdaSQSQueueExecutionRole.
    Add this policy to your execution role. This is an AWS managed policy that contains the permissions
    your function needs to read from an Amazon SQS queue. For more information about this policy, see
    
    AWSLambdaSQSQueueExecutionRole in the AWS Managed Policy Reference.
      
        Go back to your function in the Lambda console. Under Function overview, choose Add trigger.
          
             
               
             
             
             
      
        Choose a trigger type.
      
        Configure the required options, and then choose Add.
      
        
            Lambda supports the following configuration options for Amazon SQS event sources:
             
             
             
             
             
             
        
                SQS queue
                
                    The Amazon SQS queue to read records from. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
                
            
                Enable trigger
                
                    The status of the event source mapping. Enable trigger is selected by default.
                
            
                Batch size
                
                    The maximum number of records to send to the function in each batch. For a standard queue,
                        this can be up to 10,000 records. For a FIFO queue, the maximum is 10. For a batch size
                        over 10, you must also set the batch window (MaximumBatchingWindowInSeconds)
                        to at least 1 second.
                    Configure your 
                        function timeout to allow enough time to process an entire batch of items. If items
                        take a long time to process, choose a smaller batch size. A large batch size can improve
                        efficiency for workloads that are very fast or have a lot of overhead. If you configure
                        reserved concurrency on your function, set
                        a minimum of five concurrent executions to reduce the chance of throttling errors when Lambda
                        invokes your function. 
                    Lambda passes all of the records in the batch to the function in a single call, as long as
                        the total size of the events doesn't exceed the 
                        invocation payload size quota for synchronous invocation (6 MB). Both Lambda and Amazon SQS
                        generate metadata for each record. This additional metadata is counted towards the total
                        payload size and can cause the total number of records sent in a batch to be lower than your
                        configured batch size. The metadata fields that Amazon SQS sends can be variable in length.
                        For more information about the Amazon SQS metadata fields, see the ReceiveMessage
                        API operation documentation in the Amazon Simple Queue Service API Reference.
                
            
                Batch window
                
                    The maximum amount of time to gather records before invoking the function, in seconds.
                        This applies only to standard queues.
                    If you're using a batch window greater than 0 seconds, you must account for the increased
                        processing time in your queue's
                        
                        visibility timeout. We recommend setting your queue's visibility timeout to six times your
                        function timeout, plus the value of
                        MaximumBatchingWindowInSeconds. This allows time for your Lambda function to process each
                        batch of events and to retry in the event of a throttling error.
                    When messages become available, Lambda starts processing messages in batches. Lambda starts
                        processing five batches at a time with five concurrent invocations of your function. If messages
                        are still available, Lambda adds up to 300 more instances of your function a minute, up to a
                        maximum of 1,000 function instances. To learn more about function scaling and concurrency,
                        see Lambda function scaling.
                    To process more messages, you can optimize your Lambda function for higher throughput.
                        For more information, see 
                        Understanding how AWS Lambda scales with Amazon SQS standard queues.
                
            
                Maximum concurrency
                
                    The maximum number of concurrent functions that the event source can invoke. For more information,
                        see Configuring maximum concurrency for Amazon SQS event sources.
                
            
                Filter criteria
                
                    Add filter criteria to control which events Lambda sends to your function for processing.
                        For more information, see Control which events Lambda sends to your function.
                
            
    Document ConventionsSQSScaling behaviorDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideCompute and storageFunction configuration, deployment,
        and executionLambda API requestsOther servicesLambda quotasImportantNew AWS accounts have reduced concurrency and memory quotas. AWS raises these quotas automatically based on your usage.AWS Lambda is designed to scale rapidly to meet demand, allowing your functions to scale up to serve traffic
    in your application. Lambda is designed for short-lived compute tasks that do not retain or rely upon state between
    invocations. Code can run for up to 15 minutes in a single invocation and a single function can use up to
    10,240 MB of memory.It’s important to understand the guardrails that are put in place to protect your account and the workloads of
    other customers. Service quotas exist in all AWS services and consist of hard limits, which you cannot change,
    and soft limits, which you can request increases for. By default, all new accounts are assigned a quota profile
    that allows exploration of AWS services.To see the quotas that apply to your account, navigate to the
    Service Quotas dashboard. Here, you can view
    your service quotas, request a quota increase, and view current utilization. From here, you can drill down to a
    specific AWS service, such as Lambda:
     
      
     
     
  The following sections list default quotas and limits in Lambda by category.TopicsCompute and storageFunction configuration, deployment, and
        executionLambda API requestsOther services
    Compute and storage
    Lambda sets quotas for the amount of compute and storage resources that you can use to run and store functions.
      Quotas for concurrent executions and storage apply per AWS Region. Elastic network interface (ENI) quotas apply
      per virtual private cloud (VPC), regardless of Region. The following quotas can be increased from their default
      values. For more information, see Requesting a quota increase in the
        Service Quotas User Guide.
    
          
            Resource
            Default quota
            Can be increased up to
          
        
          
            
              Concurrent executions
            
            
              1,000
            
            
              Tens of thousands
            
          
          
            
              Storage for uploaded functions (.zip file archives) and layers. Each function version and layer
                version consumes storage.
               For best practices on managing your code storage, see Monitoring Lambda code storage in Serverless Land.
            
            
              75 GB
            
            
              Terabytes
            
          
          
            
              Storage for functions defined as container images. These images are stored in Amazon ECR.
            
            
              See Amazon ECR service
                  quotas.
            
            
               
            
          
          
            
              Elastic network interfaces per virtual private cloud
                (VPC)
              NoteThis quota is shared with other services, such as Amazon Elastic File System (Amazon EFS). See Amazon VPC quotas.
            
            
              500
            
            
              Thousands
            
          
        
    For details on concurrency and how Lambda scales your function concurrency in response to traffic, see Understanding Lambda function scaling.
   
    Function configuration, deployment, and
        execution
    
    The following quotas apply to function configuration, deployment, and execution. Except as noted, they can't be changed.
    
    NoteThe Lambda documentation, log messages, and console use the abbreviation MB (rather than MiB) to refer to
        1,024 KB.
    
          
            Resource
            Quota
          
        
          
            
              Function memory allocation
            
            
              128 MB to 10,240 MB, in 1-MB increments.
              Note: Lambda allocates CPU power in proportion to the amount of memory configured. You can increase or decrease the memory and CPU power allocated to your function using the Memory (MB) setting. At 1,769 MB, a function has the equivalent of one vCPU.
            
          
          
            
              Function timeout
            
            
              900 seconds (15 minutes)
            
          
          
            
              Function environment variables
            
            
              4 KB, for all environment variables associated with the function, in aggregate
            
          
          
            
              Function resource-based policy
            
            
              20 KB
            
          
          
            
              Function layers
            
            
              five layers
            
          
          
            
              Function concurrency scaling limit
            
            
              For each function, 1,000 execution environments every 10 seconds
            
          
          
            
              Invocation payload (request and response)
            
            
              6 MB each for request and response (synchronous)
              20 MB for each streamed response (Synchronous. The payload size for streamed responses can be increased from default values. Contact Support to inquire further.)
              256 KB (asynchronous)
              1 MB for the total combined size of request line and header values
            
          
          
            
              Bandwidth for streamed responses
            
            
              Uncapped for the first 6 MB of your function's response
              For responses larger than 6 MB, 2MBps for the remainder of the response
            
          
          
            
              Deployment package (.zip file archive) size
            
            
              50 MB (zipped, when uploaded through the Lambda API or SDKs). Upload larger files with Amazon S3.
              50 MB (when uploaded through the Lambda console)
              250 MB The maximum size of the contents of a deployment package, including layers and custom runtimes. (unzipped)
              

            
          
          
            
              Container image settings size
            
            
              16 KB
            
                   
          
            
              Container image code package size
            
            
              10 GB (maximum uncompressed image size, including all layers)
            
          
          
            
              Test events (console editor)
            
            
              10
            
          
          
            
              /tmp directory storage
            
            
              Between 512 MB and 10,240 MB, in 1-MB increments
            
          
          
            
              File descriptors
            
            
              1,024
            
          
          
            
              Execution processes/threads
            
            
              1,024
            
          
        
   
    Lambda API requests
    
    The following quotas are associated with Lambda API requests.
    
          
            Resource
            Quota
          
        
          
            
              Invocation requests per function per Region (synchronous)
            
            
              Each instance of your execution environment can serve up to 10 requests per second.
                In other words, the total invocation limit is 10 times your concurrency limit. See
                Understanding Lambda function scaling.
            
          
          
            
              Invocation requests per function per Region (asynchronous)
            
            
              Each instance of your execution environment can serve an unlimited number of requests.
                In other words, the total invocation limit is based only on concurrency available to your
                function. See Understanding Lambda function scaling.
            
          
          
            
              Invocation requests per function version or alias (requests per second)
            
            
              10 x allocated provisioned concurrency
              NoteThis quota applies only to functions that use provisioned concurrency.
            
          
          
            
              GetFunction API requests
            
            
              100 requests per second. Cannot be increased.
            
          
          
            
              GetPolicy API requests
            
            
              15 requests per second. Cannot be increased.
            
          
          
            
              Remainder of the control plane API requests (excludes invocation, GetFunction, and GetPolicy
                requests)
            
            
              15 requests per second across all APIs (not 15 requests per second per API). Cannot be increased.
            
          
        
   
    Other services
    
    Quotas for other services, such as AWS Identity and Access Management (IAM), Amazon CloudFront (Lambda@Edge), and Amazon Virtual Private Cloud (Amazon VPC), can
      impact your Lambda functions. For more information, see AWS service quotas in the
        Amazon Web Services General Reference, and Invoking Lambda with events from other AWS services.
    Many applications involving Lambda use multiple AWS services. Because different services
      have different quotas for various features, it can be challenging to manage these quotas across
      your entire application. For example, API Gateway has a default throttle limit of 10,000 requests per
      second, whereas Lambda has a default concurrency limit of 1,000. Due to this mismatch, it's possible
      to have more incoming requests from API Gateway that Lambda can handle. You can resolve this by requesting
      a Lambda concurrency limit increase to match the expected level of traffic.
    Load testing your application allows you to monitor the performance of your application end-to-end
      before deploying to production. During a load test, you can identify any quotas that may act as a
      limiting factor for the traffic levels you expect and take action accordingly.
  Document ConventionsBuild and test a serverless applicationDocument historyDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationReference guideAWS security credentialsAWS IP address rangesAWS APIsAWS services endpoints and quotasAWS GlossaryAWS General ReferenceThe AWS General Reference provides AWS service endpoint and quota information for Amazon Web Services. Additionally, you can find links to other common topics.ContentsAWS security credentialsAWS IP address rangesAWS APIsAWS services endpoints and quotasAWS Glossary
    AWS security credentials
    
    When you interact with AWS, you specify your AWS security
      credentials to verify who you are and whether you have permission to access the
      resources that you are requesting. AWS uses the security credentials to authenticate and
      authorize your requests.
 
    For more information, see the following resources:
        
    
       
       
      
    
      
      AWS security credentials in the
        IAM User GuideAWS
        security audit guidelines in the
        IAM User Guide
    
   
    AWS IP address ranges
    
    AWS publishes its current IP address ranges in JSON format. You can download
      a .json file to view current ranges. 
    The IP address ranges that you bring to AWS through bring your own IP addresses (BYOIP)
      are not included in the .json file.
    For more information, see the following resources:
    
    
       
       
    AWS IP address ranges in the
        Amazon VPC User GuideAWS services that support IPv6 in the
        Amazon VPC User Guide
   
    AWS APIs
    
    The following pages provide information that is useful when using an AWS API:
    
    
       
       
    Retry behavior in the
        AWS SDKs and Tools Reference GuideSigning AWS API requests in the
        IAM User Guide
    
   
    AWS services endpoints and quotas
    
    You can learn about the endpoints and service quotas in the following pages:
    
    
       
       
       
       
    AWS service endpointsAWS service quotasService endpoints and quotasSpecifying which AWS Regions your account can use in the AWS Account Management Guide
    
    
    
    
    
    
    
   
    AWS Glossary
    
    For the latest AWS terminology, see the AWS Glossary.  
  Document ConventionsAWS service endpointsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideWhen to use LambdaKey featuresWhat is AWS Lambda?You can use AWS Lambda to run code without provisioning or managing servers. Lambda runs your code
    on a high-availability compute infrastructure and performs all of the administration of the compute resources,
    including server and operating system maintenance, capacity provisioning and automatic scaling, and
    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume—there is no charge when your code is not running. For more information, see AWS Lambda Pricing.TipTo learn how to build serverless solutions, check out the Serverless Developer Guide.
    When to use Lambda
    Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to
      zero when not in demand. For example, you can use Lambda for:
  
     
     
     
     
     
  
      File processing: Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload.
    
      Stream processing: Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering.
    
      Web applications: Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers.
    
      IoT backends: Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests.
    
      Mobile backends: Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends.
    
    When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a
      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you
      cannot log in to compute instances or customize the operating system on provided
        runtimes. Lambda performs operational and administrative activities on your behalf, including managing
      capacity, monitoring, and logging your Lambda functions.
   
    Key features
    The following key features help you develop Lambda applications that are scalable, secure, and easily
      extensible:

    
    
       

       
      
       
      
       
      
       
  
       
      
       
           
       

       
      
       

       

       
      
    
        Environment variables
        
          Use environment variables to adjust your function's behavior without updating code.
        
      
        Versions
        
          Manage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.
        
      
        Container images
        
          Create a container image for a Lambda function by using an AWS provided base image or an alternative base
            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.
        
      
        Lambda layers
        
          Package libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.
        
      
        Lambda extensions
        
          Augment your Lambda functions with tools for monitoring, observability, security, and governance.
        
      
        Function URLs
        
          Add a dedicated HTTP(S) endpoint to your Lambda function.
        
      
        Response streaming
        
          Configure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.
        
      
        Concurrency and scaling controls
        
          Apply fine-grained control over the scaling and responsiveness of your production applications.
        
      
        Code signing
        
          Verify that only approved developers publish unaltered, trusted code in your Lambda functions 
        
      
        Private networking
        
          Create a private network for resources such as databases, cache instances, or internal services.
        
      
        File system
        
          Configure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.
        
      
        Lambda SnapStart
        
          Lambda SnapStart can provide as low as sub-second startup performance, typically with no changes to your function code.
        
      
  Document ConventionsCreate your first functionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideWhen to use LambdaKey featuresWhat is AWS Lambda?You can use AWS Lambda to run code without provisioning or managing servers. Lambda runs your code
    on a high-availability compute infrastructure and performs all of the administration of the compute resources,
    including server and operating system maintenance, capacity provisioning and automatic scaling, and
    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume—there is no charge when your code is not running. For more information, see AWS Lambda Pricing.TipTo learn how to build serverless solutions, check out the Serverless Developer Guide.
    When to use Lambda
    Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to
      zero when not in demand. For example, you can use Lambda for:
  
     
     
     
     
     
  
      File processing: Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload.
    
      Stream processing: Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering.
    
      Web applications: Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers.
    
      IoT backends: Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests.
    
      Mobile backends: Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends.
    
    When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a
      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you
      cannot log in to compute instances or customize the operating system on provided
        runtimes. Lambda performs operational and administrative activities on your behalf, including managing
      capacity, monitoring, and logging your Lambda functions.
   
    Key features
    The following key features help you develop Lambda applications that are scalable, secure, and easily
      extensible:

    
    
       

       
      
       
      
       
      
       
  
       
      
       
           
       

       
      
       

       

       
      
    
        Environment variables
        
          Use environment variables to adjust your function's behavior without updating code.
        
      
        Versions
        
          Manage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.
        
      
        Container images
        
          Create a container image for a Lambda function by using an AWS provided base image or an alternative base
            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.
        
      
        Lambda layers
        
          Package libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.
        
      
        Lambda extensions
        
          Augment your Lambda functions with tools for monitoring, observability, security, and governance.
        
      
        Function URLs
        
          Add a dedicated HTTP(S) endpoint to your Lambda function.
        
      
        Response streaming
        
          Configure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.
        
      
        Concurrency and scaling controls
        
          Apply fine-grained control over the scaling and responsiveness of your production applications.
        
      
        Code signing
        
          Verify that only approved developers publish unaltered, trusted code in your Lambda functions 
        
      
        Private networking
        
          Create a private network for resources such as databases, cache instances, or internal services.
        
      
        File system
        
          Configure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.
        
      
        Lambda SnapStart
        
          Lambda SnapStart can provide as low as sub-second startup performance, typically with no changes to your function code.
        
      
  Document ConventionsCreate your first functionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideCreate environment variablesExample scenario for environment variablesRetrieve environment variablesDefined runtime environment variablesWorking with Lambda environment variablesYou can use environment variables to adjust your function's behavior without updating code. An environment
    variable is a pair of strings that is stored in a function's version-specific configuration. The Lambda runtime makes
    environment variables available to your code and sets additional environment variables that contain information
    about the function and invocation request.NoteTo increase security, we recommend that you use AWS Secrets Manager instead of environment variables to store
      database credentials and other sensitive information like API keys or authorization tokens. For more information, see Use Secrets Manager secrets in Lambda functions.Environment variables are not evaluated before the function invocation. Any value you define is considered a
    literal string and not expanded. Perform the variable evaluation in your function code.
    Creating Lambda environment variables
  You can configure environment variables in Lambda using the Lambda console, the AWS Command Line Interface (AWS CLI), AWS Serverless Application Model (AWS SAM), or using an AWS SDK.
 
   Console
       You define environment variables on the unpublished version of your function. When you publish a version, the
         environment variables are locked for that version along with other version-specific configuration settings.
       You create an environment variable for your function by defining a key and a value. Your function uses the
         name of the key to retrieve the value of the environment variable.
       To set environment variables in the Lambda consoleOpen the Functions page of the Lambda console.
           Choose a function.
         
           Choose the Configuration tab, then choose Environment variables.
         
           Under Environment variables, choose Edit.
         
           Choose Add environment variable.
         
           Enter a key and value.
           
             Requirements
              
              
              
              
           
               Keys start with a letter and are at least two characters.
             
               Keys only contain letters, numbers, and the underscore character (_).
             
               Keys aren't reserved by Lambda.
             
               The total size of all environment variables doesn't exceed 4 KB.
             
         
           Choose Save.
         
       To generate a list of environment variables in the console code editorYou can generate a list of environment variables in the Lambda code editor. This is a quick way to reference
           your environment variables while you code.
           Choose the Code tab.
         
           Scroll down to the ENVIRONMENT VARIABLES section of the code editor. Existing environment variables are listed here:
           
              
               
              
              
           
         
           To create new environment variables, choose the choose the plus sign (
                
                 
                
                
             ):
           
              
               
              
              
           
         
       Environment variables remain encrypted when listed in the console code editor. If you enabled encryption helpers for encryption in transit, then those settings remain unchanged. For more information, see Securing Lambda environment variables.
The environment variables list is read-only and is available only on the Lambda console. This file is not included when you download the function's .zip file archive, and you can't add environment variables by uploading this file.
     
   AWS CLI
       The following example sets two environment variables on a function named my-function.
       aws lambda update-function-configuration \
  --function-name my-function \
  --environment "Variables={BUCKET=amzn-s3-demo-bucket,KEY=file.txt}"
       When you apply environment variables with the update-function-configuration command, the entire
         contents of the Variables structure is replaced. To retain existing environment variables when you
         add a new one, include all existing values in your request.
       To get the current configuration, use the get-function-configuration command.
       aws lambda get-function-configuration \
  --function-name my-function
       You should see the following output:
       {
    "FunctionName": "my-function",
    "FunctionArn": "arn:aws:lambda:us-east-2:111122223333:function:my-function",
    "Runtime": "nodejs22.x",
    "Role": "arn:aws:iam::111122223333:role/lambda-role",
    "Environment": {
        "Variables": {
            "BUCKET": "amzn-s3-demo-bucket",
            "KEY": "file.txt"
        }
    },
    "RevisionId": "0894d3c1-2a3d-4d48-bf7f-abade99f3c15",
    ...
}
       You can pass the revision ID from the output of get-function-configuration as a parameter to
         update-function-configuration. This ensures that the values don't change between when you read the
         configuration and when you update it.
       
       To configure a function's encryption key, set the KMSKeyARN option.
       aws lambda update-function-configuration \
  --function-name my-function \
  --kms-key-arn arn:aws:kms:us-east-2:111122223333:key/055efbb4-xmpl-4336-ba9c-538c7d31f599
     
   AWS SAM
       You can use the  AWS Serverless Application Model to configure environment variables for your function. Update the Environment and Variables properties in your template.yaml file and then run sam deploy.
       Example template.yamlAWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: An AWS Serverless Application Model template describing your function.
Resources:
  my-function:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: .
      Description: ''
      MemorySize: 128
      Timeout: 120
      Handler: index.handler
      Runtime: nodejs22.x
      Architectures:
        - x86_64
      EphemeralStorage:
        Size: 10240
      Environment:
        Variables:
          BUCKET: amzn-s3-demo-bucket
          KEY: file.txt
      # Other function properties...   
     
   AWS SDKs
       To manage environment variables using an AWS SDK, use the following API operations.
         
         
       
       
       
    UpdateFunctionConfigurationGetFunctionConfigurationCreateFunction
       To learn more, refer to the AWS SDK documentation for your preferred programming language.
     
 
   
    Example scenario for environment variables
    
    You can use environment variables to customize function behavior in your test environment and production
      environment. For example, you can create two functions with the same code but different configurations. One
      function connects to a test database, and the other connects to a production database. In this situation, you use
      environment variables to pass the hostname and other connection details for the database to the function. 
    The following example shows how to define the database host and database name as environment variables.
    
       
        
       
       
    
    If you want your test environment to generate more debug information than the production environment, you
      could set an environment variable to configure your test environment to use more verbose logging or more detailed
      tracing.
    For example, in your test environment, you could set an environment variable with the key LOG_LEVEL and a value indicating a log level of 
      debug or trace. In your Lambda function's code, you can then use this environment variable to set the log level.
    The following code examples in Python and Node.js illustrate how you can achieve this. These examples assume your environment variable has a 
      value of DEBUG in Python or debug in Node.js.
    
      Python
          Example Python code to set log levelimport os
import logging

# Initialize the logger
logger = logging.getLogger()

# Get the log level from the environment variable and default to INFO if not set
log_level = os.environ.get('LOG_LEVEL', 'INFO')

# Set the log level
logger.setLevel(log_level)

def lambda_handler(event, context):
    # Produce some example log outputs
    logger.debug('This is a log with detailed debug information - shown only in test environment')
    logger.info('This is a log with standard information - shown in production and test environments')
    

        
      Node.js (ES module format)
          Example Node.js code to set log levelThis example uses the winston logging library. Use npm to add this library to your function's deployment package. For more information, see
              Creating a .zip deployment package with dependencies.import winston from 'winston';

// Initialize the logger using the log level from environment variables, defaulting to INFO if not set
const logger = winston.createLogger({
   level: process.env.LOG_LEVEL || 'info',
   format: winston.format.json(),
   transports: [new winston.transports.Console()]
});

export const handler = async (event) => {
   // Produce some example log outputs
   logger.debug('This is a log with detailed debug information - shown only in test environment');
   logger.info('This is a log with standard information - shown in production and test environment');
   
};
        
    
   
    Retrieving Lambda environment variables
    To retrieve environment variables in your function code, use the standard method for your programming
      language.
    
      Node.js
          let region = process.env.AWS_REGION
        
      Python
          import os
  region = os.environ['AWS_REGION']
          NoteIn some cases, you may need to use the following format:region = os.environ.get('AWS_REGION')
        
      Ruby
          region = ENV["AWS_REGION"]
        
      Java
          String region = System.getenv("AWS_REGION");
        
      Go
          var region = os.Getenv("AWS_REGION")
        
      C#
          string region = Environment.GetEnvironmentVariable("AWS_REGION");
        
      PowerShell
          $region = $env:AWS_REGION
        
    
    
    Lambda stores environment variables securely by encrypting them at rest. You can configure Lambda to use a different encryption key, encrypt
      environment variable values on the client side, or set environment variables in an AWS CloudFormation template with
      AWS Secrets Manager.
   
    Defined runtime environment variables
    Lambda runtimes set several environment variables during initialization.
      Most of the environment variables provide information about the function or runtime. The keys for these
      environment variables are reserved and cannot be set in your function configuration.
    
      Reserved environment variables
       
       
       
       
       
       
       
       
       
       
       
       
       
       
    
        _HANDLER – The handler location configured on the function.
      
        _X_AMZN_TRACE_ID – The X-Ray tracing
          header. This environment variable changes with each invocation.
        
           
           
        
            This environment variable is not defined for OS-only runtimes (the provided runtime family).
              You can set _X_AMZN_TRACE_ID for custom runtimes using the
              Lambda-Runtime-Trace-Id response header from the
              Next invocation.
          
            For Java runtime versions 17 and later, this environment variable is not used.
              Instead, Lambda stores tracing information in the com.amazonaws.xray.traceHeader
              system property.
          
      
        AWS_DEFAULT_REGION – The default AWS Region where the Lambda function is executed.
      
        AWS_REGION – The AWS Region where the Lambda function is executed. If defined, this value overrides the AWS_DEFAULT_REGION.
        
           
        
            For more information about using the AWS Region environment variables with AWS SDKs, see AWS Region 
              in the AWS SDKs and Tools Reference Guide.
          
      
        AWS_EXECUTION_ENV – The runtime identifier,
          prefixed by AWS_Lambda_ (for example, AWS_Lambda_java8). This environment variable is not defined for OS-only runtimes (the provided runtime family).
      
        AWS_LAMBDA_FUNCTION_NAME – The name of the function.
      
        AWS_LAMBDA_FUNCTION_MEMORY_SIZE – The amount of memory available to the function in
          MB.
      
        AWS_LAMBDA_FUNCTION_VERSION – The version of the function being
          executed.
      
        AWS_LAMBDA_INITIALIZATION_TYPE – The initialization type of the function, which is on-demand, provisioned-concurrency, or snap-start. For information, see  Configuring provisioned concurrency or Improving startup performance with Lambda SnapStart.
      
        AWS_LAMBDA_LOG_GROUP_NAME, AWS_LAMBDA_LOG_STREAM_NAME – The name of the
          Amazon CloudWatch Logs group and stream for the function. The AWS_LAMBDA_LOG_GROUP_NAME and AWS_LAMBDA_LOG_STREAM_NAME environment variables are not available in Lambda SnapStart functions.
      
        AWS_ACCESS_KEY, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN
          – The access keys obtained from the function's execution
            role.
      
        AWS_LAMBDA_RUNTIME_API – (Custom runtime) The
          host and port of the runtime API.
      
        LAMBDA_TASK_ROOT – The path to your Lambda function code.
      
        LAMBDA_RUNTIME_DIR – The path to runtime libraries.
      
    The following additional environment variables aren't reserved and can be extended in your function
      configuration.
    
      Unreserved environment variables
       
       
       
       
       
       
       
       
       
       
    
        LANG – The locale of the runtime (en_US.UTF-8).
      
        PATH – The execution path
          (/usr/local/bin:/usr/bin/:/bin:/opt/bin).
      
        LD_LIBRARY_PATH – The system library path
          (/var/lang/lib:/lib64:/usr/lib64:$LAMBDA_RUNTIME_DIR:$LAMBDA_RUNTIME_DIR/lib:$LAMBDA_TASK_ROOT:$LAMBDA_TASK_ROOT/lib:/opt/lib).
      
        NODE_PATH – (Node.js) The Node.js library path
          (/opt/nodejs/node12/node_modules/:/opt/nodejs/node_modules:$LAMBDA_RUNTIME_DIR/node_modules).
      
        PYTHONPATH – (Python) The Python
          library path ($LAMBDA_RUNTIME_DIR).
      
        GEM_PATH – (Ruby) The Ruby library path
          ($LAMBDA_TASK_ROOT/vendor/bundle/ruby/3.3.0:/opt/ruby/gems/3.3.0).
      
        AWS_XRAY_CONTEXT_MISSING – For X-Ray tracing, Lambda sets this to
          LOG_ERROR to avoid throwing runtime errors from the X-Ray SDK.
      
        AWS_XRAY_DAEMON_ADDRESS – For X-Ray tracing, the IP address and port of the X-Ray
          daemon.
      
        AWS_LAMBDA_DOTNET_PREJIT – (.NET) Set this variable to enable or
          disable .NET specific runtime optimizations. Values include always, never, and
          provisioned-concurrency. For more information, see Configuring provisioned concurrency for a function.
      
        TZ – The environment's time zone (:UTC). The execution environment uses
          NTP to synchronize the system clock.
      
    The sample values shown reflect the latest runtimes. The presence of specific variables or their values can
      vary on earlier runtimes.
  Document ConventionsTimeoutSecuring environment variablesDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideCreating function versionsUsing versionsGranting permissionsManage Lambda function versionsYou can use versions to manage the deployment of your functions. For example, you can publish a new version of a
    function for beta testing without affecting users of the stable production version. Lambda creates a new version of
    your function each time that you publish the function. The new version is a copy of the unpublished version of the
    function. The unpublished version is named $LATEST.Importantly, any time you deploy your function code, you overwrite the current code in $LATEST.
    To save the current iteration of $LATEST, create a new function version. If $LATEST
    is identical to a previously published version, you won't be able to create a new version until you deploy changes
    to $LATEST. These changes can include updating the code, or modifying the function configuration settings.After you publish a function version, its code, runtime, architecture, memory, layers, and
    most other configuration settings are immutable. This means that you can't change these settings
    without publishing a new version from $LATEST. You can configure the following items for a
    published function version:
     
     
     
     
     
  TriggersDestinationsProvisioned concurrencyAsynchronous invocationDatabase connections and proxiesNoteWhen using runtime management controls with Auto
      mode, the runtime version used by the function version is updated automatically. 
      When using Function update or Manual mode, the runtime version is not updated.
      For more information, see Understanding how Lambda manages runtime version updates.SectionsCreating function versionsUsing versionsGranting permissions
    Creating function versions
    You can change the function code and settings only on the unpublished version of a function. When you publish
      a version, Lambda locks the code and most of the settings to maintain a consistent experience for users of that
      version.
    You can create a function version using the Lambda console.
    To create a new function versionOpen the Functions page of the Lambda console.
        Choose a function and then choose the Versions tab.
      
        On the versions configuration page, choose Publish new
          version.
      
        (Optional) Enter a version description.
      
        Choose Publish.
      
    Alternatively, you can publish a version of a function using the PublishVersion API operation.
    The following AWS CLI command publishes a new version of a function. The response returns configuration information
      about the new version, including the version number and the function ARN with the version suffix.
    aws lambda publish-version --function-name my-function
    You should see the following output:
{
  "FunctionName": "my-function",
  "FunctionArn": "arn:aws:lambda:us-east-2:123456789012:function:my-function:1",
  "Version": "1",
  "Role": "arn:aws:iam::123456789012:role/lambda-role",
  "Handler": "function.handler",
  "Runtime": "nodejs22.x",
  ...
}
    NoteLambda assigns monotonically increasing sequence numbers for versioning. Lambda never reuses version
      numbers, even after you delete and recreate a function.
   
    Using versions
    You can reference your Lambda function using either a qualified ARN or an unqualified ARN.
    
       
       
    
        Qualified ARN – The function ARN with a version suffix. The
          following example refers to version 42 of the helloworld function.
        arn:aws:lambda:aws-region:acct-id:function:helloworld:42       
      
        Unqualified ARN – The function ARN without a version suffix.
        arn:aws:lambda:aws-region:acct-id:function:helloworld
      
    You can use a qualified or an unqualified ARN in all relevant API operations. However, you can't use an
      unqualified ARN to create an alias.
    If you decide not to publish function versions, you can invoke the function using either the qualified or
      unqualified ARN in your event source mapping. When you invoke
      a function using an unqualified ARN, Lambda implicitly invokes $LATEST.
    Lambda publishes a new function version only if the code has never been published, or if
      the code has changed from the last published version. If there is no change, the function
      version remains at the last published version.
    The qualified ARN for each Lambda function version is unique. After you publish a version, you can't change the
      ARN or the function code.
   
    Granting permissions
    You can use a resource-based policy or an identity-based policy to grant access to your function. The scope
      of the permission depends on whether you apply the policy to a function or to one version of a function. For more
      information about function resource names in policies, see Fine-tuning the Resources and Conditions sections of policies. 
    You can simplify the management of event sources and AWS Identity and Access Management (IAM) policies by using function aliases. For
      more information, see Create an alias for a Lambda function.
  Document ConventionsWeighted aliasesTagsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideRequirementsUsing an AWS base imageUsing an AWS OS-only base imageUsing a non-AWS base imageRuntime interface clientsAmazon ECR permissionsFunction lifecycleCreate a Lambda function using a container imageYour AWS Lambda function's code consists of scripts or compiled programs and their dependencies. 
You use a deployment package to deploy your function code to Lambda. Lambda supports two types of deployment packages: 
container images and .zip file archives. There are three ways to build a container image for a Lambda function:
     
	 
     
  
      Using an AWS base image for Lambda
      The AWS base images are preloaded with a language runtime, a runtime interface client to manage the interaction between Lambda and your function code, and a runtime interface emulator for local testing.
    
      Using an AWS OS-only base image
      AWS OS-only base images contain an Amazon Linux distribution and the runtime interface emulator. These images are commonly used to create container images for compiled languages, such as Go and Rust, and for a language or language version that Lambda doesn't provide a base image for, such as Node.js 19. You can also use OS-only base images to implement a custom runtime. To make the image compatible with Lambda, you must include a runtime interface client for your language in the image.
    
      Using a non-AWS base image
      You can use an alternative base image from another container registry, such as Alpine Linux or Debian. You can also use a custom image created by your organization. To make the image compatible with Lambda, you must include a runtime interface client for your language in the image.
    TipTo reduce the time it takes for Lambda container functions to become active, see Use multi-stage builds in the Docker documentation. To build efficient container images, follow the Best practices for writing Dockerfiles.To create a Lambda function from a container image, build your image locally and upload it to an Amazon Elastic Container Registry
    (Amazon ECR) repository. If you're using a container image provided by an AWS Marketplace seller, you need to clone the
    image to your private Amazon ECR repository first. Then, specify the repository URI when you create the function.
    The Amazon ECR repository must be in the same AWS Region as the Lambda function. You can create a function using an image in a different AWS account, as long as the image is in the same Region as the Lambda function. For more information, see  Amazon ECR cross-account permissions.NoteLambda does not support Amazon ECR FIPS endpoints for container images. If your repository URI includes
      ecr-fips, you are using a FIPS endpoint. Example: 111122223333.dkr.ecr-fips.us-east-1.amazonaws.com.This page explains the base image types and requirements for creating Lambda-compatible container images.NoteYou cannot change the deployment package type (.zip or container image) for an existing function. For example, you cannot convert a container image function to use a .zip file archive. You must create a new function.TopicsRequirementsUsing an AWS base image for LambdaUsing an AWS OS-only base imageUsing a non-AWS base imageRuntime interface clientsAmazon ECR permissionsFunction lifecycle
    Requirements
    Install the AWS CLI version 2 and the Docker CLI. 
      Additionally, note the following requirements:
    
       
       
       
       
       
    
        The container image must implement the Using the Lambda runtime API for custom runtimes. The AWS
          open-source runtime interface clients implement the API. You can
          add a runtime interface client to your preferred base image to make it compatible with Lambda.
      
        The container image must be able to run on a read-only file system. Your function code can access a
          writable /tmp directory with between 512 MB and 10,240 MB, in 1-MB increments, of storage.  
      
        The default Lambda user must be able to read all the files required to run your function code. Lambda
          follows security best practices by defining a default Linux user with least-privileged permissions. This means that you don't need to specify a USER  in your Dockerfile. Verify
          that your application code does not rely on files that other Linux users are restricted from running.
      
        Lambda supports only Linux-based container images.
      
        Lambda provides multi-architecture base images. However, the image you build for your function must target
          only one of the architectures. Lambda does not support functions that use multi-architecture container
          images.
      
   
    Using an AWS base image for Lambda
    You can use one of the AWS base images for Lambda to build the container image for your
      function code. The base images are preloaded with a language runtime and other components
      required to run a container image on Lambda. You add your function code and dependencies to the
      base image and then package it as a container image.
    AWS periodically provides updates to the AWS base images for Lambda. If your Dockerfile includes the
      image name in the FROM property, your Docker client pulls the latest version of the image from the Amazon ECR repository. To
      use the updated base image, you must rebuild your container image and update the function code.
      The Node.js 20, Python 3.12, Java 21, .NET 8, Ruby 3.3, and later base images are based on the Amazon Linux 2023 minimal container image. Earlier base images use Amazon Linux 2. AL2023 provides several advantages over Amazon Linux 2, including a smaller deployment footprint and updated versions of libraries such as glibc.
    AL2023-based images use microdnf (symlinked as dnf) as the package manager instead of yum, which is the default package manager in Amazon Linux 2. microdnf is a standalone implementation of dnf. For a list of packages that are included in AL2023-based images, refer to the Minimal Container columns in Comparing packages installed on Amazon Linux 2023 Container Images. For more information about the differences between AL2023 and Amazon Linux 2, see Introducing the Amazon Linux 2023 runtime for AWS Lambda on the AWS
      Compute Blog.    
    NoteTo run AL2023-based images locally, including with AWS Serverless Application Model (AWS SAM), you must use Docker version 20.10.10 or later.  
    To build a container image using an AWS base image, choose the instructions for your preferred language:
    
       
       
       
       
       
       
       
    
        Node.js
      
        TypeScript (uses a Node.js base image) 
      
        Python
      
        Java 
      
        Go
      
        .NET
      
        Ruby
      
   
    Using an AWS OS-only base image
    AWS OS-only base images contain an Amazon Linux distribution and the runtime interface emulator. These images are commonly used to create container images for compiled languages, such as Go and Rust, and for a language or language version that Lambda doesn't provide a base image for, such as Node.js 19. You can also use OS-only base images to implement a custom runtime. To make the image compatible with Lambda, you must include a runtime interface client for your language in the image.
    
          
            Tags
            Runtime
            Operating system
            Dockerfile
            Deprecation
          
        
      
            al2023
            OS-only Runtime
            Amazon Linux 2023
            Dockerfile
                for OS-only Runtime on GitHub
            
                          Jun 30, 2029
            
              
          
      
            al2
            OS-only Runtime
            Amazon Linux 2
            Dockerfile
                for OS-only Runtime on GitHub
            
                          Jun 30, 2026
            
              
          
    
    Amazon Elastic Container Registry Public Gallery: gallery.ecr.aws/lambda/provided
     
    Using a non-AWS base image
    Lambda
      supports any image that conforms to one of the following image manifest formats:
    
       
       
    Docker image manifest V2, schema 2 (used with Docker version 1.10 and newer)Open Container Initiative (OCI) Specifications (v1.0.0 and up)
    Lambda supports a maximum uncompressed image size of 10 GB, including all layers.
    NoteTo make the image compatible with Lambda, you must include a runtime interface client for your language in the image.
   
    Runtime interface clients
    If you use an OS-only base image or an alternative base image, you must include a runtime interface client in your image. The runtime interface client must extend the Using the Lambda runtime API for custom runtimes, which manages the interaction between Lambda and your function code. AWS provides open-source runtime interface clients for the following languages:
    
  
   
  
   
  
   
  
   
  
   
  
   
  
     

    
      Node.js
    
  
    
      Python
    
  
    
      Java
    
  
    
      .NET
    
  
    
      Go
    
  
    
      Ruby
    
  
    
      Rust – The Rust runtime client is an experimental package. It is subject to change and intended only for evaluation purposes.
    
  
    If you're using a language that doesn't have an AWS-provided runtime interface client, you must create your own.
   
    Amazon ECR permissions
    Before you create a Lambda function from a container image, you must build the image locally and upload it to an Amazon ECR repository. When you create the function, specify the Amazon ECR repository URI.
    Make sure that the permissions for the user or role that creates the function includes GetRepositoryPolicy and SetRepositoryPolicy.
    For example, use the IAM console to create a role with the following policy:
    {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "VisualEditor0",
      "Effect": "Allow",
      "Action": [
        "ecr:SetRepositoryPolicy",
        "ecr:GetRepositoryPolicy"
      ],
      "Resource": "arn:aws:ecr:us-east-1:111122223333:repository/hello-world"
    }
  ]
}
     
      Amazon ECR repository policies
      For a function in the same account as the container image in Amazon ECR, you can add ecr:BatchGetImage
        and ecr:GetDownloadUrlForLayer permissions to your Amazon ECR repository policy. The following example shows the
        minimum policy:
      {
        "Sid": "LambdaECRImageRetrievalPolicy",
        "Effect": "Allow",
        "Principal": {
          "Service": "lambda.amazonaws.com"
        },
        "Action": [
          "ecr:BatchGetImage",
          "ecr:GetDownloadUrlForLayer"
        ]
    }   
      For more information about Amazon ECR
        repository permissions, see Private repository policies in the
        Amazon Elastic Container Registry User Guide.
      If the Amazon ECR repository does not include these permissions, Lambda attempts to add them automatically. Lambda can add
        permissions only if the principal calling Lambda has ecr:getRepositoryPolicy and
          ecr:setRepositoryPolicy permissions. 
      To view or edit your Amazon ECR repository permissions, follow the directions in Setting a private repository policy statement in the
          Amazon Elastic Container Registry User Guide.
       
         Amazon ECR cross-account permissions
        A different account in the same region can create a function that uses a container image owned by your
          account. In the following example, your Amazon ECR repository permissions policy needs the following statements to
          grant access to account number 123456789012.
        
           
           
        CrossAccountPermission – Allows account 123456789012 to create and update Lambda
              functions that use images from this ECR repository.LambdaECRImageCrossAccountRetrievalPolicy – Lambda will eventually set a
              function's state to inactive if it is not invoked for an extended period. This statement is required so that
              Lambda can retrieve the container image for optimization and caching on behalf of the function owned by
              123456789012.  
        Example — Add cross-account permission to your repository{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "CrossAccountPermission",
      "Effect": "Allow",
      "Action": [
        "ecr:BatchGetImage",
        "ecr:GetDownloadUrlForLayer"
      ],
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:root"
      }
    },
    {
      "Sid": "LambdaECRImageCrossAccountRetrievalPolicy",
      "Effect": "Allow",
      "Action": [
        "ecr:BatchGetImage",
        "ecr:GetDownloadUrlForLayer"
      ],
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Condition": {
        "StringLike": {
          "aws:sourceARN": "arn:aws:lambda:us-east-1:123456789012:function:*"
        }
      }
    }
  ]
}
        To give access to multiple accounts, you add the account IDs to the Principal list in the
            CrossAccountPermission policy and to the Condition evaluation list in the
            LambdaECRImageCrossAccountRetrievalPolicy.
        If you are working with multiple accounts in an AWS Organization, we recommend that you enumerate each
          account ID in the ECR permissions policy. This approach aligns with the AWS security best practice of setting
          narrow permissions in IAM policies.
        In addition to Lambda permissions, the user or role that creates the function must also have BatchGetImage and GetDownloadUrlForLayer permissions.
       
     
   
    Function lifecycle
    After you upload a new or updated container image, Lambda optimizes the image before the function can process invocations. The
      optimization process can take a few seconds. The function remains in the Pending state until the
      process completes, when the state transitions to Active. You can't invoke the function until it reaches the Active state. 
    If a function is not invoked for multiple weeks, Lambda reclaims its optimized version, and the function
      transitions to the Inactive state. To reactivate the function, you must invoke it. Lambda rejects the
      first invocation and the function enters the Pending state until Lambda re-optimizes the image. The
      function then returns to the Active state.
    Lambda periodically fetches the associated container image from the Amazon ECR repository. If the
      corresponding container image no longer exists on Amazon ECR or permissions are revoked, the function enters the Failed state, and
      Lambda returns a failure for any function invocations.
    You can use the Lambda API to get information about a function's state. For more information, see Lambda function states.
  Document ConventionsEncryptionMemoryDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideHow to use layersLayers and layer versionsManaging Lambda dependencies with layersA Lambda layer is a .zip file archive that contains supplementary code or data.
  Layers usually contain library dependencies, a custom runtime,
  or configuration files. There are multiple reasons why you might consider using layers:
     
     
     
     
  
      To reduce the size of your deployment packages.
        Instead of including all of your function dependencies along with your function code
        in your deployment package, put them in a layer. This keeps deployment packages small
        and organized.
    
      To separate core function logic from dependencies.
        With layers, you can update your function dependencies independent of your function code,
        and vice versa. This promotes separation of concerns and helps you focus on your function
        logic.
    
      To share dependencies across multiple functions.
        After you create a layer, you can apply it to any number of functions in your account.
        Without layers, you need to include the same dependencies in each individual deployment
        package.
    
      To use the Lambda console code editor. The code
        editor is a useful tool for testing minor function code updates quickly. However, you
        can’t use the editor if your deployment package size is too large. Using layers reduces
        your package size and can unlock usage of the code editor.
    If you're working with Lambda functions in Go or Rust, we recommend against using layers.
    For Go and Rust functions, you provide your function code as an executable, which includes your
    compiled function code along with all of its dependencies. Putting your dependencies in a
    layer forces your function to manually load additional assemblies during the initialization
    phase, which can increase cold start times. For optimal performance for Go and Rust functions,
    include your dependencies along with your deployment package.The following diagram illustrates the high-level architectural differences between two
    functions that share dependencies. One uses Lambda layers, and the other does not.
     
      
     
     
  When you add a layer to a function, Lambda extracts the layer contents into the /opt
    directory in your function’s execution environment.
    All natively supported Lambda runtimes include paths to specific directories within the
    /opt directory. This gives your function access to your layer content. For more
    information about these specific paths and how to properly package your layers, see
    Packaging your layer content.You can include up to five layers per function. Also, you can use layers only with Lambda functions
    deployed as a .zip file archive. For functions
    defined as a container image, package your preferred runtime
    and all code dependencies when you create the container image. For more information, see
    
    Working with Lambda layers and extensions in container images on the AWS Compute Blog.TopicsHow to use layersLayers and layer versionsPackaging your layer contentCreating and deleting layers in LambdaAdding layers to functionsUsing AWS CloudFormation with layersUsing AWS SAM with layers
    How to use layers
    To create a layer, package your dependencies into a .zip file, similar to how you
      create a normal deployment package. More
      specifically, the general process of creating and using layers involves these three steps:
    
       
       
       
    
        First, package your layer content. This means creating a
          .zip file archive. For more information, see Packaging your layer content.
      
        Next, create the layer in Lambda. For more information,
          see Creating and deleting layers in Lambda.
      
        Add the layer to your function(s). For more information,
          see Adding layers to functions.
      
   
    Layers and layer versions
    A layer version is an immutable snapshot of a specific version of a layer. When you create
      a new layer, Lambda creates a new layer version with a version number of 1. Each time you publish
      an update to the layer, Lambda increments the version number and creates a new layer version.
    Every layer version is identified by a unique Amazon Resource Name (ARN). When adding a layer
      to the function, you must specify the exact layer version you want to use (for example, arn:aws:lambda:us-east-1:123456789012:layer:my-layer:1).
  Document ConventionsApplication SignalsPackaging layersDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideExecution environmentImpact on performance and resources PermissionsAugment Lambda functions using Lambda extensionsYou can use Lambda extensions to augment your Lambda functions. For example, use Lambda extensions to integrate
    functions with your preferred monitoring, observability, security, and governance tools. You can choose from a broad
    set of tools that AWS Lambda Partners provides, or you can
    create your own Lambda extensions.Lambda supports external and internal extensions. An external extension runs as an independent process in the
    execution environment and continues to run after the function invocation is fully processed. Because extensions run
    as separate processes, you can write them in a different language than the function. All Lambda runtimes support extensions.An internal extension runs as part of the runtime process. Your function accesses internal extensions by using
    wrapper scripts or in-process mechanisms such as JAVA_TOOL_OPTIONS. For more information, see Modifying the runtime environment.You can add extensions to a function using the Lambda console, the AWS Command Line Interface (AWS CLI), or infrastructure as code
    (IaC) services and tools such as AWS CloudFormation, AWS Serverless Application Model (AWS SAM), and Terraform.You are charged for the execution time that the extension consumes (in 1 ms increments). There is no cost to
    install your own extensions. For more pricing information for extensions, see
    AWS Lambda Pricing. For pricing
    information for partner extensions, see those partners' websites. See AWS Lambda extensions partners
    for a list of official partner extensions.For a tutorial on extensions and how to use them with your Lambda functions, see the
    AWS Lambda Extensions Workshop.TopicsExecution environmentImpact on performance and resources PermissionsConfiguring Lambda extensionsAWS Lambda extensions partnersUsing the Lambda Extensions API to create extensionsAccessing real-time telemetry data for extensions using the Telemetry API
    Execution environment
    Lambda invokes your function in an execution environment, which
      provides a secure and isolated runtime environment. The execution environment manages the resources required to
      run your function and provides lifecycle support for the function's runtime and extensions.
    
The lifecycle of the execution environment includes the following phases:
      
         
         
         
         
      
          Init: In this phase, Lambda creates or
            unfreezes an execution environment with the configured resources, downloads the code for the function and
            all layers, initializes any extensions, initializes the runtime, and then runs the function’s initialization
            code (the code outside the main handler). The Init phase happens either during the first
            invocation, or in advance of function invocations if you have enabled provisioned concurrency.
          The Init phase is split into three sub-phases: Extension init, 
          Runtime init, and Function init. These sub-phases ensure that all extensions and the runtime complete their setup tasks before the function code runs.
          When Lambda SnapStart is activated, the Init phase happens when you publish a function version. Lambda saves a snapshot of the memory and disk state of the initialized execution environment, persists the encrypted snapshot, and caches it for low-latency access. If you have a before-checkpoint runtime hook, then the code runs at the end of Init phase.
        
        Restore (SnapStart only): When you first invoke a SnapStart function and as the function scales up, Lambda resumes new execution environments from the persisted snapshot instead of initializing the function from scratch. If you have an after-restore runtime hook, the code runs at the end of the Restore phase. You are charged for the duration of after-restore runtime hooks. The runtime must load and after-restore runtime hooks must complete within the timeout limit (10 seconds). Otherwise, you'll get a SnapStartTimeoutException. When the Restore phase completes, Lambda invokes the function handler (the Invoke phase).
        
          Invoke: In this phase, Lambda invokes the function handler.
            After the function runs to completion, Lambda prepares to handle another function
            invocation.
        
          Shutdown: This phase is triggered if the Lambda function does not
            receive any invocations for a period of time. In the Shutdown phase, Lambda shuts down
            the runtime, alerts the extensions to let them stop cleanly, and then removes the environment. Lambda
            sends a Shutdown event to each extension, which tells the extension that the environment is about
            to be shut down.
        

    During the Init phase, Lambda extracts layers containing extensions into the /opt
      directory in the execution environment. Lambda looks for extensions in the /opt/extensions/ directory,
      interprets each file as an executable bootstrap for launching the extension, and starts all extensions in
      parallel.
   
    Impact on performance and resources 
    The size of your function's extensions counts towards the deployment package size limit. For a .zip file
      archive, the total unzipped size of the function and all extensions cannot exceed the unzipped deployment package
      size limit of 250 MB.
    Extensions can impact the performance of your function because they share function resources such as CPU,
      memory, and storage. For example, if an extension performs compute-intensive operations, you may see your
      function's execution duration increase.
    Each extension must complete its initialization before Lambda invokes the function. Therefore, an extension
      that consumes significant initialization time can increase the latency of the function invocation.
    To measure the extra time that the extension takes after the function execution, you can use the
      PostRuntimeExtensionsDuration
      function metric. To measure the increase in memory used, you can use the
      MaxMemoryUsed metric. To understand the impact of a specific extension, you can run different
      versions of your functions side by side.
    NoteMaxMemoryUsed metric is one of the Metrics collected by Lambda Insights and not a Lambda native metric.
   
    Permissions
    Extensions have access to the same resources as functions. Because extensions are executed within the same
      environment as the function, permissions are shared between the function and the extension.
    For a .zip file archive, you can create an AWS CloudFormation template to simplify the task of attaching the same extension
      configuration—including AWS Identity and Access Management (IAM) permissions—to multiple functions.
  Document ConventionsLayers with AWS SAMConfiguring extensionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideCreating a function URL (console)Creating a function URL (AWS CLI)Adding a function URL to a CloudFormation templateCross-origin resource sharing (CORS)Throttling function URLsDeactivating function URLsDeleting function URLsCreating and managing Lambda function URLsA function URL is a dedicated HTTP(S) endpoint for your Lambda function. You can create and configure a function URL through the Lambda console or the Lambda API.TipLambda offers two ways to invoke your function through an HTTP endpoint: function URLs and Amazon API Gateway. If you're not sure which is the best method for your 
   use case, see Select a method to invoke your Lambda function using an HTTP request.When you create a function URL, Lambda automatically generates a unique URL endpoint for you. Once you create a function URL, its URL endpoint never changes. Function
      URL endpoints have the following format:https://<url-id>.lambda-url.<region>.on.awsNoteFunction URLs are not supported in the following AWS Regions: Asia Pacific (Hyderabad) (ap-south-2),  Asia Pacific (Melbourne) (ap-southeast-4), Asia Pacific (Malaysia) (ap-southeast-5), Canada West (Calgary) (ca-west-1), Europe (Spain) (eu-south-2), Europe (Zurich) (eu-central-2), Israel (Tel Aviv) (il-central-1), and Middle East (UAE) (me-central-1).Function URLs are dual stack-enabled, supporting IPv4 and IPv6. After you configure a function URL for your
    function, you can invoke your function through its HTTP(S) endpoint via a web browser, curl, Postman, or any HTTP
    client.NoteYou can access your function URL through the public Internet only. While Lambda functions do support
    AWS PrivateLink, function URLs do not.Lambda function URLs use resource-based policies for
    security and access control. Function URLs also support cross-origin resource sharing (CORS) configuration
    options.You can apply function URLs to any function alias, or to the $LATEST unpublished function version.
    You can't add a function URL to any other function version.The following section show how to create and manage a function URL using the Lambda console, AWS CLI, and AWS CloudFormation templateTopicsCreating a function URL (console)Creating a function URL (AWS CLI)Adding a function URL to a CloudFormation templateCross-origin resource sharing (CORS)Throttling function URLsDeactivating function URLsDeleting function URLsControl access to Lambda function URLsInvoking Lambda function URLsMonitoring Lambda function URLsSelect a method to invoke your Lambda function using an HTTP requestTutorial: Creating a webhook endpoint using a Lambda function URL
    Creating a function URL (console)
    Follow these steps to create a function URL using the console.
    
            Open the Functions page of the Lambda console.
          
            Choose the name of the function that you want to create the function URL for.
          
            Choose the Configuration tab, and then choose Function
              URL.
          
            Choose Create function URL.
          
            For Auth type, choose AWS_IAM or
                NONE. For more information about function URL authentication, see Access control.
          
            (Optional) Select Configure cross-origin resource sharing (CORS), and then configure
              the CORS settings for your function URL. For more information about CORS, see Cross-origin resource sharing (CORS).
          
            Choose Save.
          This creates a function URL for the $LATEST unpublished version of your function. The function URL
          appears in the Function overview section of the console.
    
            Open the Functions page of the Lambda console.
          
            Choose the name of the function with the alias that you want to create the function URL for.
          
            Choose the Aliases tab, and then choose the name of the alias that you want to create
              the function URL for.
          
            Choose the Configuration tab, and then choose Function
              URL.
          
            Choose Create function URL.
          
            For Auth type, choose AWS_IAM or NONE.
              For more information about function URL authentication, see Access control.
          
            (Optional) Select Configure cross-origin resource sharing (CORS), and then configure
              the CORS settings for your function URL. For more information about CORS, see Cross-origin resource sharing (CORS).
          
            Choose Save.
          This creates a function URL for your function alias. The function URL appears in the console's Function
          overview section for your alias.
    To create a new function with a function URL (console)
            Open the Functions page of the Lambda console.
          
            Choose Create function.
          
            Under Basic information, do the following:
            
                For Function name, enter a name for your function, such as
                    my-function.
              
                For Runtime, choose the language runtime that you prefer, such as
                    Node.js 18.x.
              
                For Architecture, choose either x86_64 or
                    arm64.
              
                Expand Permissions, then choose whether to create a new execution role or use
                  an existing one.
              
          
            Expand Advanced settings, and then select Function URL.
          
            For Auth type, choose AWS_IAM or NONE.
              For more information about function URL authentication, see Access control.
          
            (Optional) Select Configure cross-origin resource sharing (CORS). By selecting this
              option during function creation, your function URL allows requests from all origins by default. You can edit
              the CORS settings for your function URL after creating the function. For more information about CORS, see
              Cross-origin resource sharing (CORS).
          
            Choose Create function.
          This creates a new function with a function URL for the $LATEST
          unpublished version of the function. The function URL appears in the Function
            overview section of the console.
   
    Creating a function URL (AWS CLI)
    To create a function URL for an existing Lambda function using the AWS Command Line Interface (AWS CLI), run the following
      command:
    aws lambda create-function-url-config \
    --function-name my-function \
    --qualifier prod \ // optional
    --auth-type AWS_IAM
    --cors-config {AllowOrigins="https://example.com"} // optional
    This adds a function URL to the prod qualifier for the function
        my-function. For more information about these configuration parameters, see
      CreateFunctionUrlConfig in the API reference.
    NoteTo create a function URL via the AWS CLI, the function must already exist.
   
    Adding a function URL to a CloudFormation template
    To add an AWS::Lambda::Url resource to your AWS CloudFormation template, use the following syntax:
     
      JSON
      {
  "Type" : "AWS::Lambda::Url",
  "Properties" : {
      "AuthType" : String,
      "Cors" : Cors,
      "Qualifier" : String,
      "TargetFunctionArn" : String
    }
}
     
     
      YAML
      Type: AWS::Lambda::Url
Properties: 
  AuthType: String
  Cors: 
    Cors
  Qualifier: String
  TargetFunctionArn: String
     
     
      Parameters
      
         
         
         
         
      (Required) AuthType – Defines the type of authentication for your function URL. Possible
            values are either AWS_IAM or NONE. To restrict access to authenticated users
            only, set to AWS_IAM. To bypass IAM authentication and allow any user to make requests to
            your function, set to NONE.
          (Optional) Cors – Defines the CORS settings for
            your function URL. To add Cors to your AWS::Lambda::Url resource in CloudFormation,
            use the following syntax.
          
          Example AWS::Lambda::Url.Cors (JSON){
  "AllowCredentials" : Boolean,
  "AllowHeaders" : [ String, ... ],
  "AllowMethods" : [ String, ... ],
  "AllowOrigins" : [ String, ... ],
  "ExposeHeaders" : [ String, ... ],
  "MaxAge" : Integer
}
          Example AWS::Lambda::Url.Cors (YAML)  AllowCredentials: Boolean
  AllowHeaders: 
    - String
  AllowMethods: 
    - String
  AllowOrigins: 
    - String
  ExposeHeaders: 
    - String
  MaxAge: Integer
        (Optional) Qualifier – The alias name.(Required) TargetFunctionArn – The name or Amazon Resource Name (ARN) of the Lambda function.
          Valid name formats include the following:
          
             
             
             
          Function name – my-functionFunction ARN –
              arn:aws:lambda:us-west-2:123456789012:function:my-functionPartial ARN – 123456789012:function:my-function
        
     
   
    Cross-origin resource sharing (CORS)
    To define how different origins can access your function URL, use cross-origin resource sharing (CORS). We
      recommend configuring CORS if you intend to call your function URL from a different domain. Lambda supports the
      following CORS headers for function URLs.
    
          
            CORS header
            CORS configuration property
            Example values
          
        
          
            
              
                Access-Control-Allow-Origin
            
            
              AllowOrigins
            
            
              * (allow all origins)
              https://www.example.com
              http://localhost:60905
            
          
          
            
              
                Access-Control-Allow-Methods
            
            
              AllowMethods
            
            
              GET, POST, DELETE, *
            
          
          
            
              
                Access-Control-Allow-Headers
            
            
              AllowHeaders
            
            
              Date, Keep-Alive, X-Custom-Header
            
          
          
            
              
                Access-Control-Expose-Headers
            
            
              ExposeHeaders
            
            
              Date, Keep-Alive, X-Custom-Header
            
          
          
            
              
                Access-Control-Allow-Credentials
            
            
              AllowCredentials
            
            
              TRUE
            
          
          
            
              
                Access-Control-Max-Age
            
            
              MaxAge
            
            
              5 (default), 300
            
          
        
    When you configure CORS for a function URL using the Lambda console or the AWS CLI, Lambda automatically adds the
      CORS headers to all responses through the function URL. Alternatively, you can manually add CORS headers to your
      function response. If there are conflicting headers, the expected behavior depends on the
      type of request:
    
       
       
    
        For preflight requests such as OPTIONS requests, the configured CORS headers on the function URL take
          precedence. Lambda returns only these CORS headers in the response.
      
        For non-preflight requests such as GET or POST requests, Lambda returns both the configured CORS headers
          on the function URL, as well as the CORS headers returned by the function. This can result in duplicate CORS
          headers in the response. You may see an error similar to the following: The 'Access-Control-Allow-Origin'
          header contains multiple values '*, *', but only one is allowed.
      
    In general, we recommend configuring all CORS settings on the function URL, rather than sending CORS
      headers manually in the function response.
   
    Throttling function URLs
    Throttling limits the rate at which your function processes requests. This is useful in many situations, such
      as preventing your function from overloading downstream resources, or handling a sudden surge in requests.
    You can throttle the rate of requests that your Lambda function processes through a function URL by configuring
      reserved concurrency. Reserved concurrency limits the number of maximum concurrent invocations
      for your function. Your function's maximum request rate per second (RPS) is equivalent to 10 times the configured
      reserved concurrency. For example, if you configure your function with a reserved concurrency of 100, then the
      maximum RPS is 1,000.
    Whenever your function concurrency exceeds the reserved concurrency, your function URL returns an HTTP
      429 status code. If your function receives a request that exceeds the 10x RPS maximum based on your
      configured reserved concurrency, you also receive an HTTP 429 error. For more information about
      reserved concurrency, see Configuring reserved concurrency for a function.
   
    Deactivating function URLs
    In an emergency, you might want to reject all traffic to your function URL. To deactivate your function URL,
      set the reserved concurrency to zero. This throttles all requests to your function URL, resulting in HTTP
      429 status responses. To reactivate your function URL, delete the reserved concurrency
      configuration, or set the configuration to an amount greater than zero.
   
    Deleting function URLs
    When you delete a function URL, you can’t recover it. Creating a new function URL will result in a different URL address.
    NoteIf you delete a function URL with auth type NONE, Lambda doesn't automatically delete the
        associated resource-based policy. If you want to delete this policy, you must manually do so.
    
        Open the Functions page of the Lambda console.
      
        Choose the name of the function.
      
        Choose the Configuration tab, and then choose Function
          URL.
      
        Choose Delete.
      
        Enter the word delete into the field to confirm the deletion.
      
        Choose Delete.
      
    NoteWhen you delete a function that has a function URL, Lambda asynchronously deletes the function URL. If you immediately create a new function with the same name in the same account, it is possible that the original function URL will be mapped to the new function instead of deleted.
  Document ConventionsRecursive loop detectionAccess controlDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideBandwidth limits for response streamingResponse streaming for Lambda functionsYou can configure your Lambda function URLs to stream response payloads back to clients. Response
    streaming can benefit latency sensitive applications by improving time to first byte (TTFB)
    performance. This is because you can send partial responses back to the client as they become
    available. Additionally, you can use response streaming to build functions that return larger
    payloads. Response stream payloads have a soft limit of 20 MB as compared to the 6 MB limit for
    buffered responses. Streaming a response also means that your function doesn’t need to fit the entire 
    response in memory. For very large responses, this can reduce the amount of memory you need to 
    configure for your function. The speed at which Lambda streams your responses depends on the response size. The streaming rate for 
    the first 6MB of your function’s response is uncapped. For responses larger than 6MB, the remainder of the response 
    is subject to a bandwidth cap. For more information on streaming bandwidth, see Bandwidth limits for response streaming.Streaming responses incurs a cost. For more information, see AWS Lambda Pricing.Lambda supports response streaming on Node.js managed runtimes. For other languages, you can use a
    custom runtime with a custom Runtime API integration to stream responses or use the Lambda Web Adapter. 
    You can stream responses through Lambda function URLs, the AWS SDK, 
    or using the Lambda InvokeWithResponseStream 
    API.NoteWhen testing your function through the Lambda console, you'll always see responses as buffered.TopicsBandwidth limits for response streamingWriting response streaming-enabled Lambda functionsInvoking a response streaming enabled function using Lambda function URLsTutorial: Creating a response streaming Lambda
      function with a function URL
    Bandwidth limits for response streaming
    The first 6MB of your function’s response payload has uncapped bandwidth. After this initial burst, Lambda streams your response at a 
      maximum rate of  2MBps. If your function responses never exceed 6MB, then this bandwidth limit never applies. 
    NoteBandwidth limits only apply to your function’s response payload, and not to network access by your function.
    The rate of uncapped bandwidth varies depending on a number of factors, including your function’s processing speed.  You can normally 
      expect a rate higher than 2MBps for the first 6MB of your function’s response. If your function is streaming a response to a destination 
      outside of AWS, the streaming rate also depends on the speed of the external internet connection.
    
  Document ConventionsTagsWriting functionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUnderstanding and visualizing concurrencyCalculating concurrency for a functionUnderstanding reserved concurrency and provisioned concurrencyUnderstanding concurrency and requests per secondConcurrency quotasUnderstanding Lambda function scalingConcurrency is the number of in-flight requests that your AWS Lambda function is
    handling at the same time. For each concurrent request, Lambda provisions a separate instance of your execution
    environment. As your functions receive more requests, Lambda automatically handles scaling the number of execution
    environments until you reach your account's concurrency limit. By default, Lambda provides your account with a total
    concurrency limit of 1,000 concurrent executions across all functions in an AWS Region. To support your specific
    account needs, you can request a quota
      increase and configure function-level concurrency controls so that your critical functions don't
    experience throttling.This topic explains concurrency concepts and function scaling in Lambda. By the end of this topic, you'll be able to
    understand how to calculate concurrency, visualize the two main concurrency control options (reserved and
    provisioned), estimate appropriate concurrency control settings, and view metrics for further optimization.SectionsUnderstanding and visualizing concurrencyCalculating concurrency for a functionUnderstanding reserved concurrency and provisioned concurrencyUnderstanding concurrency and requests per secondConcurrency quotasConfiguring reserved concurrency for a functionConfiguring provisioned concurrency for a functionLambda scaling behaviorMonitoring concurrency
    Understanding and visualizing concurrency
    Lambda invokes your function in a secure and isolated execution environment. To handle a
      request, Lambda must first initialize an execution environment (the Init phase),
      before using it to invoke your function (the Invoke phase):
    
       
        
       
       
    
    NoteActual Init and Invoke durations can vary depending on many factors, such as the runtime you choose
      and the Lambda function code. The previous diagram isn't meant to represent the exact proportions of Init and
      Invoke phase durations.
    The previous diagram uses a rectangle to represent a single execution environment. When your function
      receives its very first request (represented by the yellow circle with label 1), Lambda creates
      a new execution environment and runs the code outside your main handler during the Init phase. Then, Lambda
      runs your function's main handler code during the Invoke phase. During this entire process, this execution
      environment is busy and cannot process other requests.
    When Lambda finishes processing the first request, this execution environment can then process additional
      requests for the same function. For subsequent requests, Lambda doesn't need to re-initialize the environment.
    
       
        
       
       
    
    In the previous diagram, Lambda reuses the execution environment to handle the second request
      (represented by the yellow circle with label 2).
    So far, we've focused on just a single instance of your execution environment (that is, a concurrency of 1).
      In practice, Lambda may need to provision multiple execution environment instances in parallel to handle all
      incoming requests. When your function receives a new request, one of two things can happen:
    
       
       
    
        If a pre-initialized execution environment instance is available, Lambda uses it to process the request.
      
        Otherwise, Lambda creates a new execution environment instance to process the request.
      
    For example, let's explore what happens when your function receives 10 requests:
    
       
        
       
       
    
    In the previous diagram, each horizontal plane represents a single execution environment instance (labeled
      from A through F). Here's how Lambda handles each request:
    
          
            Request
            Lambda behavior
            Reasoning
          
        
          
            
              1
            
            
              Provisions new environment A
            
            
              This is the first request; no execution environment instances are available.
            
          
          
            
              2
            
            
              Provisions new environment B
            
            
              Existing execution environment instance A is busy.
            
          
          
            
              3
            
            
              Provisions new environment C
            
            
              Existing execution environment instances A and B are both busy.
            
          
          
            
              4
            
            
              Provisions new environment D
            
            
              Existing execution environment instances A, B, and C are all busy.
            
          
          
            
              5
            
            
              Provisions new environment E
            
            
              Existing execution environment instances A, B, C, and D are all
                busy.
            
          
          
            
              6
            
            
              Reuses environment A
            
            
              Execution environment instance A has finished processing request
                  1 and is now available.
            
          
          
            
              7
            
            
              Reuses environment B
            
            
              Execution environment instance B has finished processing request
                  2 and is now available.
            
          
          
            
              8
            
            
              Reuses environment C
            
            
              Execution environment instance C has finished processing request
                  3 and is now available.
            
          
          
            
              9
            
            
              Provisions new environment F
            
            
              Existing execution environment instances A, B, C, D, and E are all busy.
            
          
          
            
              10
            
            
              Reuses environment D
            
            
              Execution environment instance D has finished processing request
                  4 and is now available.
            
          
        
    As your function receives more concurrent requests, Lambda scales up the number of execution
      environment instances in response. The following animation tracks the number of concurrent
      requests over time:
    
       
        
       
       
    
    By freezing the previous animation at six distinct points in time, we get the following
      diagram:
    
       
        
       
       
    
    In the previous diagram, we can draw a vertical line at any point in time and count the number of environments
      that intersect this line. This gives us the number of concurrent requests at that point in time. For example, at
      time t1, there are three active environments serving three concurrent requests. The maximum number of
      concurrent requests in this simulation occurs at time t4, when there are six active environments
      serving six concurrent requests.
    To summarize, your function's concurrency is the number of concurrent requests that it's handling
      at the same time. In response to an increase in your function's concurrency, Lambda provisions more
      execution environment instances to meet request demand.
   
    Calculating concurrency for a function
    In general, concurrency of a system is the ability to process more than one task simultaneously.
      In Lambda, concurrency is the number of in-flight requests that your function is handling at the same
      time. A quick and practical way of measuring concurrency of a Lambda function is to use the following
      formula:
    Concurrency = (average requests per second) * (average request duration in seconds)
    Concurrency differs from requests per second. For example, suppose your
      function receives 100 requests per second on average. If the average request duration is one second, then it's
      true that the concurrency is also 100:
    Concurrency = (100 requests/second) * (1 second/request) = 100
    However, if the average request duration is 500 ms, then the concurrency is 50:
    Concurrency = (100 requests/second) * (0.5 second/request) = 50
    What does a concurrency of 50 mean in practice? If the average request duration is 500 ms, then you can think
      of an instance of your function as being able to handle two requests per second. Then, it takes 50 instances of
      your function to handle a load of 100 requests per second. A concurrency of 50 means that Lambda must provision 50
      execution environment instances to efficiently handle this workload without any throttling. Here's how to express
      this in equation form:
    Concurrency = (100 requests/second) / (2 requests/second) = 50
    If your function receives double the number of requests (200 requests per second), but only requires half the
      time to process each request (250 ms), then the concurrency is still 50:
    Concurrency = (200 requests/second) * (0.25 second/request) = 50
    Suppose you have a function that takes, on average, 200 ms to run. During peak load, you
          observe 5,000 requests per second. What is the concurrency of your function during
          peak load?
        The average function duration is 200 ms, or 0.2 seconds. Using the concurrency formula, you can plug
              in the numbers to get a concurrency of 1,000:Concurrency = (5,000 requests/second) * (0.2 seconds/request) = 1,000Alternatively, an average function duration of 200 ms means that your function can process
              5 requests per second. To handle the 5,000 request per second workload, you need 1,000
              execution environment instances. Thus, the concurrency is 1,000:Concurrency = (5,000 requests/second) / (5 requests/second) = 1,000
   
    Understanding reserved concurrency and provisioned concurrency
    By default, your account has a concurrency limit of 1,000 concurrent executions across all functions in a
      Region. Your functions share this pool of 1,000 concurrency on an on-demand basis. Your functions experiences
      throttling (that is, they start to drop requests) if you run out of available concurrency.
    Some of your functions might be more critical than others. As a result, you might want to configure
      concurrency settings to ensure that critical functions get the concurrency that they need. There are two types of
      concurrency controls available: reserved concurrency and provisioned concurrency.
    
       
       
    
        Use reserved concurrency to reserve a portion of your account's
          concurrency for a function. This is useful if you don't want other functions taking up all the
          available unreserved concurrency.
      
        Use provisioned concurrency to pre-initialize a number of
          environment instances for a function. This is useful for reducing cold start latencies.
      
     
      Reserved concurrency
      If you want to guarantee that a certain amount of concurrency is available for your function at any
        time, use reserved concurrency.
      Reserved concurrency is the maximum number of concurrent instances that you want to allocate to your
        function. When you dedicate reserved concurrency to a function, no other function can use that concurrency. In
        other words, setting reserved concurrency can impact the concurrency pool that's available to other functions.
        Functions that don't have reserved concurrency share the remaining pool of unreserved concurrency.
      Configuring reserved concurrency counts towards your overall account concurrency limit. There is no charge
        for configuring reserved concurrency for a function.
      To better understand reserved concurrency, consider the following diagram:
      
         
          
         
         
      
      In this diagram, your account concurrency limit for all the functions in this Region is at the default limit
        of 1,000. Suppose you have two critical functions, function-blue and function-orange,
        that routinely expect to get high invocation volumes. You decide to give 400 units of reserved concurrency to
          function-blue, and 400 units of reserved concurrency to function-orange. In this
        example, all other functions in your account must share the remaining 200 units of unreserved
        concurrency.
      The diagram has five points of interest:
      
         
         
         
         
         
      
          At t1, both function-orange and function-blue begin receiving
            requests. Each function begins to use up its allocated portion of reserved concurrency units.
        
          At t2, function-orange and function-blue steadily receive more
            requests. At the same time, you deploy some other Lambda functions, which begin receiving requests. You don't
            allocate reserved concurrency to these other functions. They begin using the remaining 200 units of
            unreserved concurrency.
        
          At t3, function-orange hits the max concurrency of 400. Although there is
            unused concurrency elsewhere in your account, function-orange cannot access it. The red line
            indicates that function-orange is experiencing throttling, and Lambda may drop requests.
        
          At t4, function-orange starts to receive fewer requests and is no longer
            throttling. However, your other functions experience a spike in traffic and begin throttling. Although
            there is unused concurrency elsewhere in your account, these other functions cannot access it. The red
            line indicates that your other functions are experiencing throttling.
        
          At t5, other functions start to receive fewer requests and are no longer throttling.
        
      From this example, notice that reserving concurrency has the following effects:
      
         
         
         
      
          Your function can scale independently of other functions in your
              account. All of your account's functions in the same Region that don't have reserved
            concurrency share the pool of unreserved concurrency. Without reserved concurrency, other functions can
            potentially use up all of your available concurrency. This prevents critical functions from scaling up if
            needed.
        
          Your function can't scale out of control. Reserved concurrency caps
            your function's maximum concurrency. This means that your function can't use concurrency reserved for other
            functions, or concurrency from the unreserved pool. You can reserve concurrency to prevent your function
            from using all the available concurrency in your account, or from overloading downstream resources.
        
          You may not be able to use all of your account's available concurrency.
            Reserving concurrency counts towards your account concurrency limit, but this also means that other
            functions cannot use that chunk of reserved concurrency. If your function doesn't use up all of the
            concurrency that you reserve for it, you're effectively wasting that concurrency. This isn't an issue unless
            other functions in your account could benefit from the wasted concurrency.
        
      To learn how to manage reserved concurrency settings for your functions, see Configuring reserved concurrency for a function.
     
     
      Provisioned concurrency
      You use reserved concurrency to define the maximum number of execution environments reserved for a Lambda function.
        However, none of these environments come pre-initialized. As a result, your function invocations may take longer
        because Lambda must first initialize the new environment before being able to use it to invoke your function.
        When Lambda has to initialize a new environment in order to carry out an invocation, this is known as a
        cold start. 
        To mitigate cold starts, you can use provisioned concurrency.
      Provisioned concurrency is the number of pre-initialized execution environments that you want to allocate to
        your function. If you set provisioned concurrency on a function, Lambda initializes that number of execution
        environments so that they are prepared to respond immediately to function requests.
      NoteUsing provisioned concurrency incurs additional charges to your account. If you're working with the Java
          11 or Java 17 runtimes, you can also use Lambda SnapStart to mitigate cold start issues at no additional cost.
          SnapStart uses cached snapshots of your execution environment to significantly improve startup
          performance. You cannot use both SnapStart and provisioned concurrency on the same function version. For more
          information about SnapStart features, limitations, and supported Regions, see Improving startup performance with Lambda SnapStart.
      When using provisioned concurrency, Lambda still recycles execution environments in the background. For example,
        this can occur after an invocation failure. However, at
        any given time, Lambda always ensures that the number of pre-initialized environments is equal to the value of your
        function's provisioned concurrency setting. Importantly, even if you're using provisioned concurrency, you can
        still experience a cold start delay if Lambda has to reset the execution environment.
      In contrast, when using reserved concurrency, Lambda may completely terminate an environment after a period of
        inactivity. The following diagram illustrates this by comparing the lifecycle of a single execution environment
        when you configure your function using reserved concurrency compared to provisioned concurrency.
      
         
          
         
         
      
      The diagram has four points of interest:
      
            
              Time
              Reserved concurrency
              Provisioned concurrency
            
          
            
              
                t1
              
              
                Nothing happens.
              
              
                Lambda pre-initializes an execution environment instance.
              
            
            
              
                t2
              
              
                Request 1 comes in. Lambda must initialize a new execution environment instance.
              
              
                Request 1 comes in. Lambda uses the pre-initialized environment instance.
              
            
            
              
                t3
              
              
                After some inactivity, Lambda terminates the active environment instance.
              
              
                Nothing happens.
              
            
            
              
                t4
              
              
                Request 2 comes in. Lambda must initialize a new execution environment instance.
              
              
                Request 2 comes in. Lambda uses the pre-initialized environment instance.
              
            
          
      To better understand provisioned concurrency, consider the following diagram:
      
         
          
         
         
      
      In this diagram, you have an account concurrency limit of 1,000. You decide to give 400 units of provisioned
        concurrency to function-orange. All functions in your account, including
        function-orange, can use the remaining 600 units of unreserved concurrency.
      The diagram has five points of interest:
      
         
         
         
         
         
      
          At t1, function-orange begins receiving requests. Since Lambda has pre-initialized
            400 execution environment instances, function-orange is ready for immediate invocation.
        
          At t2, function-orange reaches 400 concurrent requests. As a result,
            function-orange runs out of provisioned concurrency. However, since there's still unreserved
            concurrency available, Lambda can use this to handle additional requests to function-orange
            (there's no throttling). Lambda must create new instances to serve these requests, and your function may
            experience cold start latencies.
        
          At t3, function-orange returns to 400 concurrent requests after a brief spike
            in traffic. Lambda is again able to handle all requests without cold start latencies.
        
          At t4, functions in your account experience a burst in traffic. This burst can come from
            function-orange or any other function in your account. Lambda uses unreserved concurrency to
            handle these requests.
        
          At t5, functions in your account reach the maximum concurrency limit of 1,000, and
            experience throttling.
        
      The previous example considered only provisioned concurrency. In practice, you can set both provisioned
        concurrency and reserved concurrency on a function. You might do this if you had a function that handles a
        consistent load of invocations on weekdays, but routinely sees spikes of traffic on weekends. In this case, you
        could use provisioned concurrency to set a baseline amount of environments to handle request during weekdays,
        and use reserved concurrency to handle the weekend spikes. Consider the following diagram:
      
         
          
         
         
      
      In this diagram, suppose that you configure 200 units of provisioned concurrency and 400 units of
        reserved concurrency for function-orange. Because you configured reserved concurrency,
        function-orange cannot use any of the 600 units of unreserved concurrency.
      This diagram has five points of interest:
      
         
         
         
         
         
      
          At t1, function-orange begins receiving requests. Since Lambda has
            pre-initialized 200 execution environment instances, function-orange is ready for
            immediate invocation.
        
          At t2, function-orange uses up all its provisioned concurrency.
            function-orange can continue serving requests using reserved concurrency, but these
            requests may experience cold start latencies.
        
          At t3, function-orange reaches 400 concurrent requests. As a result,
            function-orange uses up all its reserved concurrency. Since function-orange
            cannot use unreserved concurrency, requests begin to throttle.
        
          At t4, function-orange starts to receive fewer requests, and no longer
            throttles.
        
          At t5, function-orange drops down to 200 concurrent requests, so all requests
            are again able to use provisioned concurrency (that is, no cold start latencies).
        
      Both reserved concurrency and provisioned concurrency count towards your account concurrency limit and Regional quotas. In other words, allocating reserved and provisioned
        concurrency can impact the concurrency pool that's available to other functions. Configuring provisioned
        concurrency incurs charges to your AWS account.
      NoteIf the amount of provisioned concurrency on a function's versions and aliases adds up to the function's reserved
          concurrency, then all invocations run on provisioned concurrency. This configuration also has the effect of
          throttling the unpublished version of the function ($LATEST), which prevents it from executing.
          You can't allocate more provisioned concurrency than reserved concurrency for a function.
      To manage provisioned concurrency settings for your functions, see Configuring provisioned concurrency for a function. To automate
        provisioned concurrency scaling based on a schedule or application utilization, see Using Application Auto Scaling to automate provisioned concurrency management.
     
     
      How Lambda allocates provisioned concurrency
      Provisioned concurrency doesn't come online immediately after you configure it. Lambda
        starts allocating provisioned concurrency after a minute or two of preparation. For each 
        function, Lambda can provision up to 6,000 execution environments every minute, regardless 
        of AWS Region. This is exactly the same as the concurrency scaling rate for functions.
      When you submit a request to allocate provisioned concurrency, you can't access
        any of those environments until Lambda completely finishes allocating them. For example,
        if you request 5,000 provisioned concurrency, none of your requests can use
        provisioned concurrency until Lambda completely finishes allocating the 5,000 execution
        environments.
     
     
      Comparing reserved concurrency and provisioned concurrency
      The following table summarizes and compares reserved and provisioned concurrency.
      
            
              Topic
              Reserved concurrency
              Provisioned concurrency
            
          
            
              
                Definition
              
              
                Maximum number of execution environment instances for your function.
              
              
                Set number of pre-provisioned execution environment instances for your function.
              
            
            
              
                Provisioning behavior
              
              
                Lambda provisions new instances on an on-demand basis.
              
              
                Lambda pre-provisions instances (that is, before your function starts receiving requests).
              
            
            
              
                Cold start behavior
              
              
                Cold start latency possible, since Lambda must create new instances on-demand.
              
              
                Cold start latency not possible, since Lambda doesn't have to create instances on-demand.
              
            
            
              
                Throttling behavior
              
              
                Function throttled when reserved concurrency limit reached.
              
              
                If reserved concurrency not set: function uses unreserved concurrency when provisioned concurrency
                  limit reached.
                If reserved concurrency set: function throttled when reserved concurrency limit reached.
              
            
            
              
                Default behavior if not set
              
              
                Function uses unreserved concurrency available in your account.
              
              
                Lambda doesn't pre-provision any instances. Instead, if reserved concurrency not set: function uses
                  unreserved concurrency available in your account.
                If reserved concurrency set: function uses reserved concurrency.
              
            
            
              
                Pricing
              
              
                No additional charge.
              
              
                Incurs additional charges.
              
            
          
     
   
    Understanding concurrency and requests per second
    As mentioned in the previous section, concurrency differs from requests per second. This is an
      especially important distinction to make when working with functions that have an average request
      duration of less than 100 ms.
    Across all functions in your account, Lambda enforces a requests per second limit that's equal to 10
      times your account concurrency. For example, since the default account concurrency limit is 1,000,
      functions in your account can handle a maximum of 10,000 requests per second.
    For example, consider a function with an average request duration of 50 ms. At 20,000 requests
      per second, here's the concurrency of this function:
    Concurrency = (20,000 requests/second) * (0.05 second/request) = 1,000
    Based on this result, you might expect that the account concurrency limit of 1,000 is sufficient
      to handle this load. However, because of the 10,000 requests per second limit, your function can only
      handle 10,000 requests per second out of the 20,000 total requests. This function experiences throttling.
    The lesson is that you must consider both concurrency and requests per second when configuring concurrency
      settings for your functions. In this case, you need to request an account concurrency limit increase to
      2,000, since this would increase your total requests per second limit to 20,000.
    NoteBased on this request per second limit, it's incorrect to say that each Lambda execution
        environment can handle only a maximum of 10 requests per second. Instead of observing the
        load on any individual execution environment, Lambda only considers overall concurrency and
        overall requests per second when calculating your quotas.
    Suppose that you have a function that takes, on average, 20 ms to run. During peak load, you observe 30,000
          requests per second. What is the concurrency of your function during peak load?The average function duration is 20 ms, or 0.02 seconds. Using the concurrency formula,
              you can plug in the numbers to get a concurrency of 600:Concurrency = (30,000 requests/second) * (0.02 seconds/request) = 600By default, the account concurrency limit of 1,000 seems sufficient to handle this load.
              However, the requests per second limit of 10,000 isn't enough to handle the incoming 30,000
              requests per second. To fully accommodate the 30,000 requests, you need to request an account
              concurrency limit increase to 3,000 or higher.
    The requests per second limit applies to all quotas in Lambda that involve concurrency. In other
      words, it applies to synchronous on-demand functions, functions that use provisioned concurrency,
      and concurrency scaling behavior. For example, here
      are a few scenarios where you must carefully consider both your concurrency and request per second limits:
    
       
       
    
        A function using on-demand concurrency can experience a burst increase of 500 concurrency
          every 10 seconds, or by 5,000 requests per second every 10 seconds, whichever happens first.
      
        Suppose you have a function that has a provisioned concurrency allocation of 10. This
          function spills over into on-demand concurrency after 10 concurrency or 100 requests per
          second, whichever happens first.
      
   
    Concurrency quotas
    Lambda sets quotas for the total amount of concurrency that you can use across all functions in a Region. These
      quotas exist on two levels:
    
       
       
       
    
        At the account level, your functions can have up to 1,000 units of
          concurrency by default. To increase this limit, see Requesting a quota increase in the
            Service Quotas User Guide.
      
        At the function level, you can reserve up to 900 units of concurrency
          across all your functions by default. Regardless of your total account concurrency limit, Lambda always
          reserves 100 units of concurrency for your functions that don't explicitly reserve concurrency. For example,
          if you increased your account concurrency limit to 2,000, then you can reserve up to 1,900 units of
          concurrency at the function level.
      
        At both the account level and the function level, Lambda also enforces a requests per second limit of
          equal to 10 times the corresponding concurrency quota. For instance, this applies to account-level
          concurrency, functions using on-demand concurrency, functions using provisoned concurrency, and
          concurrency scaling behavior. For more information, see
          Understanding concurrency and requests per second.
      
    To check your current account level concurrency quota, use the AWS Command Line Interface (AWS CLI) to run the following
      command:
    aws lambda get-account-settings
    You should see output that looks like the following:
    {
    "AccountLimit": {
        "TotalCodeSize": 80530636800,
        "CodeSizeUnzipped": 262144000,
        "CodeSizeZipped": 52428800,
        "ConcurrentExecutions": 1000,
        "UnreservedConcurrentExecutions": 900
    },
    "AccountUsage": {
        "TotalCodeSize": 410759889,
        "FunctionCount": 8
    }
}

    ConcurrentExecutions is your total account-level concurrency quota.
        UnreservedConcurrentExecutions is the amount of reserved concurrency that you can still allocate to
      your functions.
    As your function receives more requests, Lambda automatically scales up the number of execution environments to
      handle these requests until your account reaches its concurrency quota. However, to protect against over-scaling in
      response to sudden bursts of traffic, Lambda limits how fast your functions can scale. This 
      concurrency scaling rate is the maximum rate at which functions in your account can scale in response
      to increased requests. (That is, how quickly Lambda can create new execution environments.) The concurrency scaling
      rate differs from the account-level concurrency limit, which is the total amount of concurrency available to your
      functions.
    In each AWS Region, and for each function,
      your concurrency scaling rate is 1,000 execution environment instances every 10 seconds (or 10,000 requests per
      second every 10 seconds). In other words, every 10 seconds, Lambda can allocate
      at most 1,000 additional execution environment instances, or accommodate 10,000 additional
      requests per second, to each of your functions.
    Usually, you don't need to worry about this limitation. Lambda's scaling rate is sufficient for most
      use cases.
    Importantly, the concurrency scaling rate is a function-level limit. This means that each function in your
      account can scale independently of other functions.
    For more information about scaling behavior, see Lambda scaling behavior.
  Document ConventionsTutorial: Creating a webhook endpointConfiguring reserved concurrencyDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideSignature validationUsing code signing to verify code integrity with LambdaCode signing helps ensure that only trusted code is deployed to your Lambda functions. Using AWS Signer, you can create digitally signed code packages for your functions. When you add a code signing configuration to a function, Lambda verifies that all new code deployments are signed by a trusted source. Because code signing validation checks run at deployment time, there is no impact on function execution.ImportantCode signing configurations only prevent new deployments of unsigned code. If you add a code signing configuration to an existing function that has unsigned code, that code keeps running until you deploy a new code package.When you enable code signing for a function, any layers that you add to the function must also be signed by an allowed signing profile.There is no additional charge for using AWS Signer or code signing for AWS Lambda.
    Signature validation
    Lambda performs the following validation checks when you deploy a signed code package to your function:
    
       
       
       
       
    
        Integrity: Validates that the code package has not been modified since it was signed. Lambda
          compares the hash of the package with the hash from the signature.
      
        Expiry: Validates that the signature of the code package has not expired.
      
        Mismatch: Validates that the code package is signed with an allowed signing profile
      
        Revocation: Validates that the signature of the code package has not been revoked.
      
    When you create a code signing configuration, you can use the UntrustedArtifactOnDeployment parameter to specify how Lambda should respond if the expiry, mismatch, or revocation checks fail. You can choose one of these actions:
    
       
       
    
        Warn: This is the default setting. Lambda allows the deployment of the code package, but issues a warning. Lambda issues a new
          Amazon CloudWatch metric and also stores the warning in the CloudTrail log.
      
        Enforce Lambda issues a warning (the same as for the Warn action) and blocks the deployment of the
          code package.
      
  TopicsCreating code signing configurations for LambdaConfiguring IAM policies for Lambda code signing configurationsUsing tags on code signing configurationsDocument ConventionsSecuring workloads with public endpointsCreate configurationDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideRequired IAM permissionsAttaching Lambda functions to an Amazon VPC in your AWS accountInternet access when attached to a VPCIPv6 supportBest practices for using Lambda with Amazon VPCsUnderstanding Hyperplane Elastic Network Interfaces (ENIs)Using IAM condition keys for VPC settingsVPC tutorialsGiving Lambda functions access to resources in an Amazon VPCWith Amazon Virtual Private Cloud (Amazon VPC), you can create private networks in your AWS account to host resources such as Amazon Elastic Compute Cloud (Amazon EC2) 
    instances, Amazon Relational Database Service (Amazon RDS) instances, and Amazon ElastiCache instances. You can give your Lambda function access to resources hosted 
    in an Amazon VPC by attaching your function to the VPC through the private subnets that contain the resources. Follow the instructions 
    in the following sections to attach a Lambda function to an Amazon VPC using the Lambda console, the AWS Command Line Interface (AWS CLI), or AWS SAM.NoteEvery Lambda function runs inside a VPC that is owned and managed by the Lambda service. These VPCs are maintained automatically 
      by Lambda and are not visible to customers. Configuring your function to access other AWS resources in an Amazon VPC has no effect on the 
      Lambda-managed VPC your function runs inside.SectionsRequired IAM permissionsAttaching Lambda functions to an Amazon VPC in your AWS accountInternet access when attached to a VPCIPv6 supportBest practices for using Lambda with Amazon VPCsUnderstanding Hyperplane Elastic Network Interfaces (ENIs)Using IAM condition keys for VPC settingsVPC tutorials
      Required IAM permissions
    To attach a Lambda function to an Amazon VPC in your AWS account, Lambda needs permissions to create and manage the network 
      interfaces it uses to give your function access to the resources in the VPC.
    The network interfaces that Lambda creates are known as Hyperplane Elastic Network Interfaces, or Hyperplane ENIs. To learn more about 
      these network interfaces, see Understanding Hyperplane Elastic Network Interfaces (ENIs).
      You can give your function the permissions it needs by attaching the AWS managed policy 
        AWSLambdaVPCAccessExecutionRole to your function's execution role. When you create a new function in the Lambda console 
        and attach it to a VPC, Lambda automatically adds this permissions policy for you. 
      If you prefer to create your own IAM permissions policy, make sure to add all of the following permissions:
    
       
       
       
       
       
       
    
        ec2:CreateNetworkInterface
      
        ec2:DescribeNetworkInterfaces – This action only works if it's allowed on all resources 
          ("Resource": "*").
      
        ec2:DescribeSubnets
      
        ec2:DeleteNetworkInterface – If you don't specify a resource ID for 
          DeleteNetworkInterface in the execution role, your function may not be able to access the 
          VPC. Either specify a unique resource ID, or include all resource IDs, for example, "Resource": "arn:aws:ec2:us-west-2:123456789012:*/*".
      
        ec2:AssignPrivateIpAddresses
      
        ec2:UnassignPrivateIpAddresses
      
    Note that your function's role only needs these permissions to create the network interfaces, not to invoke your function. You can still 
      invoke your function successfully when it’s attached to an Amazon VPC, even if you remove these permissions from your function’s execution role. 
    To attach your function to a VPC, Lambda also needs to verify network resources using your IAM user role. Ensure that your user role 
      has the following IAM permissions:
    
       
       
       
       
    
        ec2:DescribeSecurityGroups
      
        ec2:DescribeSubnets
      
        ec2:DescribeVpcs
      
        ec2:getSecurityGroupsForVpc
      
    NoteThe Amazon EC2 permissions that you grant to your function's execution role are used by the Lambda service to attach your function 
        to a VPC. However, you're also implicitly granting these permissions to your function's code. This means that your function code is 
        able to make these Amazon EC2 API calls. For advice on following security best practices, see Security best practices.
     
    Attaching Lambda functions to an Amazon VPC in your AWS account
    Attach your function to an Amazon VPC in your AWS account by using the Lambda console, the AWS CLI or AWS SAM. If you're using the AWS CLI or AWS SAM, or attaching 
    an existing function to a VPC using the Lambda console, make sure that your function's execution role has the necessary permissions listed in the previous section.
      Lambda functions can't connect directly to a VPC with  dedicated instance tenancy. To connect to resources
        in a dedicated VPC, peer it to a
          second VPC with default tenancy.
    
    
      Lambda console
          To attach a function to an Amazon VPC when you create it
              Open the Functions page of the Lambda console and choose Create function.
            
              Under Basic information, for Function name, enter a name for your function.
            
              Configure VPC settings for the function by doing the following:
              
                  Expand Advanced settings.
                
                  Select Enable VPC, and then select the VPC you want to attach the function to.
                
                  (Optional) To allow outbound IPv6 traffic, select Allow IPv6 traffic for dual-stack subnets.
                
                  Choose the subnets and security groups to create the network interface for. If you selected Allow IPv6 traffic for dual-stack subnets, 
                    all selected subnets must have an IPv4 CIDR block and an IPv6 CIDR block.
                  NoteTo access private resources, connect your function to private subnets. If your function needs internet access, see 
                      Enable internet access for VPC-connected Lambda functions. Connecting a function to a public subnet doesn't give it internet access or a public IP address. 
                
            
              Choose Create function.
            
          To attach an existing function to an Amazon VPC
              Open the Functions page of the Lambda console and select your function.
            
              Choose the Configuration tab, then choose VPC.
            
              Choose Edit.
            
              Under VPC, select the Amazon VPC you want to attach your function to.
            
              (Optional) To allow outbound IPv6 traffic, select Allow IPv6 traffic for dual-stack subnets. 
            
              Choose the subnets and security groups to create the network interface for. If you selected Allow IPv6 traffic for dual-stack subnets, 
                all selected subnets must have an IPv4 CIDR block and an IPv6 CIDR block.
              NoteTo access private resources, connect your function to private subnets. If your function needs internet access, see 
                  Enable internet access for VPC-connected Lambda functions. Connecting a function to a public subnet doesn't give it internet access or a public IP address. 
            
              Choose Save.
            
        
      AWS CLI
          To attach a function to an Amazon VPC when you create it
              To create a Lambda function and attach it to a VPC, run the following CLI create-function command.
              aws lambda create-function --function-name my-function \
--runtime nodejs22.x --handler index.js --zip-file fileb://function.zip \
--role arn:aws:iam::123456789012:role/lambda-role \
--vpc-config Ipv6AllowedForDualStack=true,SubnetIds=subnet-071f712345678e7c8,subnet-07fd123456788a036,SecurityGroupIds=sg-085912345678492fb
              Specify your own subnets and security groups and set Ipv6AllowedForDualStack to true or false according to your use case.
            
          To attach an existing function to an Amazon VPC
              To attach an existing function to a VPC, run the following CLI update-function-configuration command.
              aws lambda update-function-configuration --function-name my-function \
--vpc-config Ipv6AllowedForDualStack=true, SubnetIds=subnet-071f712345678e7c8,subnet-07fd123456788a036,SecurityGroupIds=sg-085912345678492fb
            
           To unattach your function from a VPC
              To unattach your function from a VPC, run the following update-function-configurationCLI command with an empty list of VPC subnets and security groups.
              aws lambda update-function-configuration --function-name my-function \
--vpc-config SubnetIds=[],SecurityGroupIds=[]
            
        
      AWS SAM
          To attach your function to a VPC
              To attach a Lambda function to an Amazon VPC, add the VpcConfig property to your function definition as shown in 
                the following example template. For more information about this property, see AWS::Lambda::Function VpcConfig 
                in the AWS CloudFormation User Guide (the AWS SAM VpcConfig property is passed directly to the VpcConfig 
                property of an AWS CloudFormation AWS::Lambda::Function resource).
              AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31

Resources:
  MyFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: ./lambda_function/
      Handler: lambda_function.handler
      Runtime: python3.12
      VpcConfig:
        SecurityGroupIds:
          - !Ref MySecurityGroup
        SubnetIds:
          - !Ref MySubnet1
          - !Ref MySubnet2
      Policies:
        - AWSLambdaVPCAccessExecutionRole

  MySecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for Lambda function
      VpcId: !Ref MyVPC

  MySubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MyVPC
      CidrBlock: 10.0.1.0/24

  MySubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MyVPC
      CidrBlock: 10.0.2.0/24

  MyVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
              For more information about configuring your VPC in AWS SAM, see AWS::EC2::VPC 
                in the AWS CloudFormation User Guide.
            
        
    
   
    Internet access when attached to a VPC
    By default, Lambda functions have access to the public internet. When you attach your function to a VPC, it can only access resources 
      available within that VPC. To give your function access to the internet, you also need to configure the VPC to have internet access. To 
      learn more, see Enable internet access for VPC-connected Lambda functions.
   
  IPv6 support
  Your function can connect to resources in dual-stack VPC subnets over IPv6. This option is turned off by default. To allow outbound IPv6 traffic, use the console or the --vpc-config Ipv6AllowedForDualStack=true option with the create-function or update-function-configuration command.
  NoteTo allow outbound IPv6 traffic in a VPC, all of the subnets that are connected to the function must be dual-stack subnets. Lambda doesn't support outbound IPv6 connections for IPv6-only subnets in a VPC or outbound IPv6 connections for functions that are not connected to a VPC.
  You can update your function code to explicitly connect to subnet resources over IPv6. The following Python example opens a socket and connects to an IPv6 server.
  Example  — Connect to IPv6 serverdef connect_to_server(event, context):
    server_address = event['host']
    server_port = event['port']
    message = event['message']
    run_connect_to_server(server_address, server_port, message)

def run_connect_to_server(server_address, server_port, message):
    sock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM, 0)
    try:
        # Send data
        sock.connect((server_address, int(server_port), 0, 0))
        sock.sendall(message.encode())
        BUFF_SIZE = 4096
        data = b''
        while True:
            segment = sock.recv(BUFF_SIZE)
            data += segment
            # Either 0 or end of data
            if len(segment) < BUFF_SIZE:
                break
        return data
    finally:
        sock.close()

 
    Best practices for using Lambda with Amazon VPCs
    To ensure that your Lambda VPC configuration meets best practice guidelines, follow the advice in the following sections.
     
      Security best practices
      To attach your Lambda function to a VPC, you need to give your function’s execution role a number of Amazon EC2 permissions. These 
        permissions are required to create the network interfaces your function uses to access the resources in the VPC. However, these 
        permissions are also implicitly granted to your function’s code. This means that your function code has permission to make these Amazon EC2 API calls.
      To follow the principle of least-privilege access, add a deny policy like the following example to your function’s execution role. 
        This policy prevents your function from making calls to the Amazon EC2 APIs that the Lambda service uses to attach your function to a VPC.
      {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Action": [ 
                 "ec2:CreateNetworkInterface",
                 "ec2:DeleteNetworkInterface",
                 "ec2:DescribeNetworkInterfaces",
                 "ec2:DescribeSubnets",
                 "ec2:DetachNetworkInterface",
                 "ec2:AssignPrivateIpAddresses",
                 "ec2:UnassignPrivateIpAddresses"
            ],
            "Resource": [ "*" ],
            "Condition": {
                "ArnEquals": {
                    "lambda:SourceFunctionArn": [
                        "arn:aws:lambda:us-west-2:123456789012:function:my_function"
                    ]
                }
            }
        }
    ]
}
      AWS provides security groups and 
        network Access Control Lists (ACLs) to increase security 
        in your VPC. Security groups control inbound and outbound traffic for your resources, and network ACLs control inbound and outbound traffic 
        for your subnets. Security groups provide enough access control for most subnets. You can use network ACLs if you want an additional layer 
        of security for your VPC. For general guidelines on security best practices when using Amazon VPCs, see Security best practices for your VPC 
        in the Amazon Virtual Private Cloud User Guide.
     
     
      Performance best practices
      When you attach your function to a VPC, Lambda checks to see if there is an available network resource (Hyperplane ENI) it can use to 
        connect to. Hyperplane ENIs are associated with a particular combination of security groups and VPC subnets. If you’ve already attached 
        one function to a VPC, specifying the same subnets and security groups when you attach another function means that Lambda can share the 
        network resources and avoid the need to create a new Hyperplane ENI. For more information about Hyperplane ENIs and their lifecycle, 
        see Understanding Hyperplane Elastic Network Interfaces (ENIs).
     
   
    Understanding Hyperplane Elastic Network Interfaces (ENIs)
    A Hyperplane ENI is a managed resource that acts as a network interface between your Lambda function and the resources you want your function 
      to connect to. The Lambda service creates and manages these ENIs automatically when you attach your function to a VPC.
    Hyperplane ENIs are not directly visible to you, and you don’t need to configure or manage them. However, knowing how they work can help 
      you to understand your function’s behavior when you attach it to a VPC.
    The first time you attach a function to a VPC using a particular subnet and security group combination, Lambda creates a Hyperplane ENI. Other 
      functions in your account that use the same subnet and security group combination can also use this ENI. Wherever possible, Lambda reuses existing 
      ENIs to optimize resource utilization and minimize the creation of new ENIs. Each Hyperplane ENI supports up to 65,000 connections/ports. If the 
      number of connections exceeds this limit, Lambda scales the number of ENIs automatically based on network traffic and concurrency requirements.
    For new functions, while Lambda is creating a Hyperplane ENI, your function remains in the Pending state and you can’t invoke it. Your function 
      transitions to the Active state only when the Hyperplane ENI is ready, which can take several minutes. For existing functions, you can’t perform 
      additional operations that target the function, such as creating versions or updating the function’s code, but you can continue to invoke previous 
      versions of the function.
      As part of managing the ENI lifecycle, Lambda may delete and recreate ENIs to load balance network traffic across ENIs or to address issues found 
        in ENI health-checks. Additionally, if a Lambda function remains idle for 30 days, Lambda reclaims any unused Hyperplane ENIs and sets the function state to idle. The next invocation attempt will fail, and the function re-enters the Pending state until Lambda completes the creation or allocation of a Hyperplane ENI. We recommend that your design doesn't rely on the persistence of ENIs.
    When you update a function to remove its VPC configuration, Lambda requires up to 20 minutes to delete the
      attached Hyperplane ENI. Lambda only deletes the ENI if no other function (or published function version) is
      using that Hyperplane ENI. 
    Lambda relies on permissions in the function  execution
      role to delete the Hyperplane ENI. If you delete the execution role before Lambda deletes the Hyperplane
      ENI, Lambda won't be able to delete the Hyperplane ENI. You can manually perform the deletion.
   
    Using IAM condition keys for VPC settings
    You can use Lambda-specific condition keys for VPC settings to provide additional permission controls for your
      Lambda functions. For example, you can require that all functions in your organization are connected to a VPC. You
      can also specify the subnets and security groups that the function's users can and can't use.
    Lambda supports the following condition keys in IAM policies:
    
       
       
       
    
        lambda:VpcIds – Allow or deny one or more VPCs.
      
        lambda:SubnetIds – Allow or deny one or more subnets.
      
        lambda:SecurityGroupIds – Allow or deny one or more security
          groups.
      
    The Lambda API operations CreateFunction and UpdateFunctionConfiguration support these condition keys. For
      more information about using condition keys in IAM policies, see IAM JSON Policy Elements:
        Condition in the IAM User Guide.
    TipIf your function already includes a VPC configuration from a previous API request, you can send an
        UpdateFunctionConfiguration request without the VPC configuration.
     
      Example policies with condition keys for VPC settings
      The following examples demonstrate how to use condition keys for VPC settings. After you create a policy
        statement with the desired restrictions, append the policy statement for the target user or role.
       
        Ensure that users deploy only VPC-connected functions
        To ensure that all users deploy only VPC-connected functions, you can deny function create and update
          operations that don't include a valid VPC ID. 
        Note that VPC ID is not an input parameter to the CreateFunction or
          UpdateFunctionConfiguration request. Lambda retrieves the VPC ID value based on the subnet and
          security group parameters.
        {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "EnforceVPCFunction",
      "Action": [
          "lambda:CreateFunction",
          "lambda:UpdateFunctionConfiguration"
       ],
      "Effect": "Deny",
      "Resource": "*",
      "Condition": {
        "Null": {
           "lambda:VpcIds": "true"
        }
      }
    }
  ]
}
        
       
       
        Deny users access to specific VPCs, subnets, or security
            groups
        To deny users access to specific VPCs, use StringEquals to check the value of the
          lambda:VpcIds condition. The following example denies users access to vpc-1 and
          vpc-2.
        {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "EnforceOutOfVPC",
      "Action": [
          "lambda:CreateFunction",
          "lambda:UpdateFunctionConfiguration"
       ],
      "Effect": "Deny",
      "Resource": "*",
      "Condition": {
        "StringEquals": {
            "lambda:VpcIds": ["vpc-1", "vpc-2"]
        }
      }
    } 
        
        To deny users access to specific subnets, use StringEquals to check the value of the
          lambda:SubnetIds condition. The following example denies users access to subnet-1
          and subnet-2.
        {
      "Sid": "EnforceOutOfSubnet",
      "Action": [
          "lambda:CreateFunction",
          "lambda:UpdateFunctionConfiguration"
       ],
      "Effect": "Deny",
      "Resource": "*",
      "Condition": {
        "ForAnyValue:StringEquals": {
            "lambda:SubnetIds": ["subnet-1", "subnet-2"]
        }
      }
    }
       
        To deny users access to specific security groups, use StringEquals to check the value of the
          lambda:SecurityGroupIds condition. The following example denies users access to
          sg-1 and sg-2.
        {
      "Sid": "EnforceOutOfSecurityGroups",
      "Action": [
          "lambda:CreateFunction",
          "lambda:UpdateFunctionConfiguration"
       ],
      "Effect": "Deny",
      "Resource": "*",
      "Condition": {
        "ForAnyValue:StringEquals": {
            "lambda:SecurityGroupIds": ["sg-1", "sg-2"]
        }
      }
    }
  ]
}
        
       
       
        Allow users to create and update functions with specific VPC
            settings
        To allow users to access specific VPCs, use StringEquals to check the value of the
          lambda:VpcIds condition. The following example allows users to access vpc-1 and
          vpc-2.
        {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "EnforceStayInSpecificVpc",
      "Action": [
          "lambda:CreateFunction",
          "lambda:UpdateFunctionConfiguration"
       ],
      "Effect": "Allow",
      "Resource": "*",
      "Condition": {
        "StringEquals": {
            "lambda:VpcIds": ["vpc-1", "vpc-2"]
        }
      }
    }
        
        To allow users to access specific subnets, use StringEquals to check the value of the
          lambda:SubnetIds condition. The following example allows users to access subnet-1
          and subnet-2.
        {
      "Sid": "EnforceStayInSpecificSubnets",
      "Action": [
          "lambda:CreateFunction",
          "lambda:UpdateFunctionConfiguration"
       ],
      "Effect": "Allow",
      "Resource": "*",
      "Condition": {
        "ForAllValues:StringEquals": {
            "lambda:SubnetIds": ["subnet-1", "subnet-2"]
        }
      }
    }
      
        To allow users to access specific security groups, use StringEquals to check the value of the
          lambda:SecurityGroupIds condition. The following example allows users to access
          sg-1 and sg-2.
        {
      "Sid": "EnforceStayInSpecificSecurityGroup",
      "Action": [
          "lambda:CreateFunction",
          "lambda:UpdateFunctionConfiguration"
       ],
      "Effect": "Allow",
      "Resource": "*",
      "Condition": {
        "ForAllValues:StringEquals": {
            "lambda:SecurityGroupIds": ["sg-1", "sg-2"]
        }
      }
    }
  ]
} 
        
       
     
   
    VPC tutorials
    In the following tutorials, you connect a Lambda function to resources in your VPC.
    
       
       
    
        Tutorial: Using a Lambda function to access Amazon RDS in an Amazon VPC
      
        Tutorial: Configuring a Lambda function to access Amazon ElastiCache in an Amazon VPC
      
  Document ConventionsSecuring environment variablesAttaching functions to resources in another accountDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideExecution role and user permissionsConfiguring a file system and access pointConnecting to a file system (console)Configuring file system access for Lambda functionsYou can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your
    function code can access and modify shared resources safely and at high concurrency.SectionsExecution role and user permissionsConfiguring a file system and access pointConnecting to a file system (console)
    Execution role and user permissions
    
    If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. 
      If the file system has a user-configured IAM policy, your function's
      execution role must have the correct elasticfilesystem permissions.
    
      Execution role permissions
       
       
    
        elasticfilesystem:ClientMount
      
        elasticfilesystem:ClientWrite (not required for read-only
          connections)
      
    These permissions are included in the AmazonElasticFileSystemClientReadWriteAccess
      managed policy. Additionally, your execution role must have the permissions
      required to connect to the file system's VPC.
    When you configure a file system, Lambda uses your permissions to verify mount targets. To configure a function
      to connect to a file system, your user needs the following permissions:
    
      User permissions
       
    
        elasticfilesystem:DescribeMountTargets
      
   
    Configuring a file system and access point
    
    Create a file system in Amazon EFS with a mount target in every Availability Zone that your function connects to.
      For performance and resilience, use at least two Availability Zones. For example, in a simple configuration you
      could have a VPC with two private subnets in separate Availability Zones. The function connects to both subnets
      and a mount target is available in each. Ensure that NFS traffic (port 2049) is allowed by the security groups
      used by the function and mount targets.
    
    NoteWhen you create a file system, you choose a performance mode that can't be changed later. General
        purpose mode has lower latency, and Max I/O mode supports a higher maximum
        throughput and IOPS. For help choosing, see Amazon EFS
          performance in the Amazon Elastic File System User Guide.
    
    An access point connects each instance of the function to the right mount target for the Availability Zone it
      connects to. For best performance, create an access point with a non-root path, and limit the number of files that
      you create in each directory. The
      following example creates a directory named my-function on the file system and sets the owner ID to
      1001 with standard directory permissions (755).
    Example access point configuration
         
         
         
         
         
         
         
      
          Name – files
        
          User ID – 1001
        
          Group ID – 1001
        
          Path – /my-function
        
          Permissions – 755
        
          Owner user ID – 1001
        
          Group user ID – 1001
        
    When a function uses the access point, it is given user ID 1001 and has full access to the directory.
    For more information, see the following topics in the Amazon Elastic File System User Guide:
    
       
       
    
        Creating resources for Amazon EFS
      
        Working with users, groups, and
          permissions
      
   
    Connecting to a file system (console)
    
    A function connects to a file system over the local network in a VPC. The subnets that your function connects to
      can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone
      that can route NFS traffic (port 2049) to the file system.
    NoteIf your function is not already connected to a VPC, see Giving Lambda functions access to resources in an Amazon VPC.
    To configure file system accessOpen the Functions page of the Lambda console.
        Choose a function.
      
        Choose Configuration and then choose File systems.
      
        Under File system, choose Add file system.
      
        Configure the following properties:
        
           
           
        
            EFS file system – The access point for a file system in the same VPC.
          
            Local mount path – The location where the file system is mounted on the
              Lambda function, starting with /mnt/.
          
      
    
    PricingAmazon EFS charges for storage and throughput, with rates that vary by storage class. For details, see Amazon EFS pricing.Lambda charges for data transfer between VPCs. This only applies if your function's VPC is peered to another
        VPC with a file system. The rates are the same as for Amazon EC2 data transfer between VPCs in the same Region. For
        details, see Lambda pricing.
  Document ConventionsInbound networkingAliasesDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUse casesSupported features and limitationsSupported RegionsCompatibility considerationsPricingImproving startup performance with Lambda SnapStartLambda SnapStart can provide as low as sub-second startup performance, typically with no changes to your function code. SnapStart makes it easier to build highly responsive and scalable applications without provisioning resources or implementing complex performance optimizations.The largest contributor to startup latency (often referred to as cold start time) is the time that Lambda spends initializing the function, which includes loading the function's code, starting the runtime, and initializing the function code. With SnapStart, Lambda initializes your function when you publish a function version. Lambda takes a Firecracker
      microVM snapshot of the memory and disk state of the initialized execution environment, encrypts the snapshot, and intelligently caches it to optimize retrieval latency.To ensure resiliency, Lambda maintains several copies of each snapshot. Lambda automatically patches snapshots and their copies with the latest runtime and security updates. When you invoke the function version for the first time, and as the invocations scale up,
    Lambda resumes new execution environments from the cached snapshot instead of initializing them from scratch, improving
    startup latency.ImportantIf your applications depend on uniqueness of state, you must evaluate your function code and verify that it is
      resilient to snapshot operations. For more information, see Handling uniqueness with Lambda SnapStart.TopicsWhen to use SnapStartSupported features and limitationsSupported RegionsCompatibility considerationsSnapStart pricingActivating and managing Lambda SnapStartHandling uniqueness with Lambda SnapStartImplement code before or after Lambda function snapshotsMonitoring for Lambda SnapStartSecurity model for Lambda SnapStartMaximize Lambda SnapStart performanceTroubleshooting SnapStart errors for Lambda functions
    When to use SnapStart
    Lambda SnapStart is designed to address the latency variability introduced by one-time initialization code, such as loading module dependencies or frameworks. These operations can sometimes take several seconds to complete during the initial invocation. Use SnapStart to reduce this latency from several seconds to as low as sub-second, in optimal scenarios. SnapStart works best when used with function invocations at scale. Functions that are invoked infrequently might not experience the same performance improvements.
      SnapStart is particularly beneficial for two main types of applications:
    
       
       
    
        Latency-sensitive APIs and user flows: Functions that are part of critical API endpoints or user-facing flows can benefit from SnapStart's reduced latency and improved response times.
      
        Latency-sensitive data processing workflows: Time-bound data processing workflows that use Lambda functions can achieve better throughput by reducing outlier function initialization latency.
      
    Provisioned concurrency keeps functions initialized and ready to respond in double-digit milliseconds. Use provisioned concurrency if your application has strict cold start latency requirements that can't be adequately addressed by SnapStart.
   
    Supported features and limitations
    SnapStart is available for the following Lambda managed runtimes:
    
       
       
       
    
        Java 11 and later
      
        Python 3.12 and later
      
        .NET 8 and later. If you're using the Lambda Annotations framework for .NET, upgrade to Amazon.Lambda.Annotations version 1.6.0 or later to ensure compatibility with SnapStart.
      
    Other managed runtimes (such as nodejs22.x and ruby3.4), OS-only runtimes, and container images are not supported.
    SnapStart does not support provisioned concurrency, Amazon Elastic File System (Amazon EFS), or ephemeral
      storage greater than 512 MB.
    NoteYou can use SnapStart only on published function
          versions and aliases that point to versions. You can't use
        SnapStart on a function's unpublished version ($LATEST).
   
    Supported Regions
    For Java runtimes, Lambda SnapStart is available in all commercial Regions except Asia Pacific (Malaysia).For Python and .NET runtimes, Lambda SnapStart is available in the following AWS Regions:
           
           
           
           
           
           
           
           
           
        US East (N. Virginia)US East (Ohio)US West (Oregon)Asia Pacific (Singapore)Asia Pacific (Sydney)Asia Pacific (Tokyo)Europe (Frankfurt)Europe (Ireland)Europe (Stockholm)
   
    Compatibility considerations
    
      With SnapStart, Lambda uses a single snapshot as the initial state for multiple execution environments. If
        your function uses any of the following during the initialization
          phase, then you might need to make some changes before using SnapStart:
       
       
       
    
        Uniqueness
        
          If your initialization code generates unique content that is included in the snapshot, then the content might not be unique when it is reused across execution environments. To maintain uniqueness when using SnapStart, you must generate unique content after initialization. This includes unique IDs, unique secrets, and entropy that's used to generate pseudorandomness. To learn how to restore uniqueness, see Handling uniqueness with Lambda SnapStart.
        
      
        Network connections
        
          The state of connections that your function establishes during the initialization phase isn't guaranteed when Lambda resumes your function from a snapshot. Validate the state of your network connections and re-establish them as necessary.
            In most cases, network connections that an AWS SDK establishes automatically resume. For other connections, review the best
              practices. 
        
      
        Temporary data
        
          Some functions download or initialize ephemeral data, such as temporary credentials or cached
            timestamps, during the initialization phase. Refresh ephemeral data in the function handler before using it, even when not using SnapStart.
        
      
   
    SnapStart pricing
    NoteFor Java managed runtimes, there's no additional cost for SnapStart. You're charged based on the number of requests for your functions,
        the time that it takes your code to run, and the memory configured for your function.
    The cost of using SnapStart includes the following:    
    
       
       
    
        Caching: For every function version that you publish with SnapStart enabled,  you pay for the cost of caching and maintaining the snapshot. The price depends on the amount of memory that you allocate to your function. You're charged for a minimum of 3 hours. You will continue to be charged as long as your function remains active. Use the ListVersionsByFunction API action to identify function versions, and then use DeleteFunction to delete unused versions. To automatically delete unused function versions, see the Lambda Version Cleanup pattern on Serverless Land.
      
        Restoration: Each time a function instance is restored from a snapshot, you pay a restoration charge. The price depends on the amount of memory you allocate to your function.
      
    As with all Lambda functions, duration charges apply to code that runs in the function handler. For SnapStart functions, duration charges also apply to initialization code that's declared outside of the handler, the time it takes for the runtime to load, and any code that runs in a runtime hook. Duration is calculated from the time that your code begins running until it returns or otherwise ends, rounded up to the nearest 1 ms. Lambda maintains cached copies of your snapshot for resiliency and automatically applies software updates, such as runtime upgrades and security patches to them. Charges apply each time that Lambda re-runs your initialization code to apply software updates.
    For more information about the cost of using SnapStart, see AWS Lambda Pricing.
  Document ConventionsTesting serverless functionsActivating SnapStartDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon EventBridgeUser GuideWhat Is Amazon EventBridge?EventBridge is a serverless service that uses events to connect application components together,
    making it easier for you to build scalable event-driven applications. Event-driven architecture
    is a style of building loosely-coupled software systems that work together by emitting and
    responding to events. Event-driven architecture can help you boost agility and build reliable,
    scalable applications. Use EventBridge to route events from sources such as home-grown applications, AWS services, and
    third-party software to consumer applications across your organization. EventBridge provides simple and
    consistent ways to ingest, filter, transform, and deliver events so you can build applications
    quickly.The following video provides a brief introduction to the features of Amazon EventBridge: 
      
      
      
  EventBridge includes two ways to process events: event buses and
    pipes.
     
     
  Event buses are routers that receive events and
      delivers them to zero or more targets. 
      Event buses are well-suited for routing events from many sources to many targets, with optional transformation of events prior to delivery to a target. 
      The following video provides a high-level overview of event buses: 
      
          
          
          
       
      
    Pipes EventBridge Pipes is intended for point-to-point integrations;
      each pipe receives events from a single source for processing and delivery to a single
      target. Pipes also include support for advanced transformations and enrichment of events
      prior to delivery to a target.
    Pipes and event buses are often used together. A common use case is to create a pipe with an event bus as its target; 
    the pipe sends events to the event bus, which then
    sends those events on to multiple targets. For example, you could create a pipe with a DynamoDB stream for a source, and an event bus as the target. 
    The pipe receives events from the DynamoDB stream and sends them to the event bus, which then sends them on to multiple targets according to the rules you've specified on the event bus.Document ConventionsCloudWatch EventsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon EventBridgeUser GuideHow Pipes workAmazon EventBridge PipesAmazon EventBridge Pipes connects sources to targets. Pipes are intended for point-to-point
    integrations between supported sources and targets, with support for advanced transformations and
      enrichment. It reduces the need for specialized
    knowledge and integration code when developing event-driven architectures, fostering consistency
    across your company’s applications. To set up a pipe, you choose the source, add optional
    filtering, define optional enrichment, and choose the target for the event data.NoteYou can also route events using event buses. Event buses are well-suited for many-to-many
      routing of events between event-driven services. For more information, see Event buses in Amazon EventBridge.
    How EventBridge Pipes work
At a high level, here's how EventBridge Pipes works:
    
       
       
    You create a pipe in your account. This includes:
      
         
         
         
         
      Specifying one of the supported event sources from which you want your pipe to receive events.Optionally, configuring a filter so that the pipe only processes a subset of the events it receives from the source.Optionally, configuring an enrichment step that enhances the event data before sending it to the target.Specifying one of the supported targets to which you want your pipe to send events.
      The event source begins sending events to the pipe, and the pipe processes the event before sending it to the target.
      
         
         
      If you have configured a filter, the pipe evaluates the event and only sends it to the target if it matches that filter.
        You are only charged for those events that match the filter.If you have configured an enrichment, the pipe performs that enrichment on the event before sending it to the target.
        If the events are batched, the enrichment maintains the ordering of the events in the batch.
    
    
       
        
       
       
    

    For example, a pipe could be used to create an e-commerce system. Suppose you have an API
      that contains customer information, such as shipping addresses. 
    
       
       
       
       
    You then create a pipe with the following: 
        
           
           
           
        An Amazon SQS
            order received message queue as the event source.An EventBridge API Destination as an enrichmentAn AWS Step Functions state machine as the target
      Then, when an Amazon SQS order received message appears in the queue, it is sent to your pipe.The pipe then sends that data to the EventBridge API Destination enrichment, which returns the customer
          information for that order. Lastly, the pipe sends the enriched data to the AWS Step Functions state machine, which processes the
          order.
  Document ConventionsBest practicesPipes conceptsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon EventBridgeUser GuideAmazon EventBridge Pipes sourcesEventBridge Pipes receives event data from a variety of sources, applies optional filters and
    enrichments to that data, and sends it to a destination.If a source enforces order to the events sent to EventBridge Pipes, that order is maintained throughout the entire process to the destination.The following AWS services can be specified as sources for EventBridge Pipes:
     
     
     
     
     
     
  Amazon DynamoDB streamAmazon Kinesis streamAmazon MQ brokerAmazon MSK stream Amazon SQS queueApache Kafka stream
      When you specify an Apache Kafka stream as a pipe source, you can specify an Apache Kafka
        stream that you manage yourself, or one managed by a third-party provider such as:
      
         
         
         
      Confluent CloudCloudKarafkaRedpanda
    Document ConventionsStarting or stopping a pipeDynamoDB streamDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon EventBridgeUser GuideTarget parametersPermissionsInvoking targetsAWS Batch job queuesCloudWatch Logs groupAmazon ECS taskLambda functions and Step Functions
          workflowsTimestream for LiveAnalytics tableAmazon EventBridge Pipes targetsYou can send data in your pipe to a specific target. You can configure the following targets
    when setting up a pipe in EventBridge:
     
     

     

     

     

     

     

     

     

     

     

     

     

     

     
     
  
      API destination
    
      API Gateway
    
      Batch job queue
    
      CloudWatch log group
    
      ECS task
    
      Event bus in the same account and Region
    
      Firehose delivery stream
    
      Inspector assessment template
    
      Kinesis stream
    
      Lambda function (SYNC or
          ASYNC)
    
      Redshift cluster data API queries
    
      SageMaker AI Pipeline
    
      Amazon SNS topic (SNS FIFO topics not supported)
    
      Amazon SQS queue
    
      Step Functions state
        machine
      
         
         
      
          Express workflows (SYNC or ASYNC)
        
          Standard workflows (ASYNC)
        
    Timestream for LiveAnalytics table
    Target parameters
    Some target services don't send the event payload to the target, instead, they treat the
      event as a trigger for invoking a specific API. EventBridge uses the PipeTargetParameters to specify what information gets sent to that
      API. These include the following:
    
       
       
       
       
       
    
        API destinations (The data sent to an API destination must match the structure of the
          API. You must use the InputTemplate object to make sure the data is structured
          correctly. If you want to include the original event payload, reference it in the InputTemplate.)
      
        API Gateway (The data sent to API Gateway must match the structure of the API. You must use
          the InputTemplate object to make sure the data is structured
          correctly. If you want to include the original event payload, reference it in the InputTemplate.)
      
        PipeTargetRedshiftDataParameters (Amazon Redshift Data API clusters)
      
        PipeTargetSageMakerPipelineParameters (Amazon SageMaker Runtime Model
          Building Pipelines)
      
        PipeTargetBatchJobParameters (AWS Batch)
      
    NoteEventBridge does not support all JSON Path syntax and evaluate it at runtime. 
        Supported syntax includes: 
         
         
         
         
         
         
    dot notation (for example,$.detail)dashesunderscoresalphanumeric charactersarray indiceswildcards (*)
     
      Dynamic path parameters
      EventBridge Pipes target parameters support optional dynamic JSON path syntax. You can use this
        syntax to specify JSON paths instead of static values (for example
          $.detail.state). The entire value has to be a JSON path, not only part of it.
        For example, RedshiftParameters.Sql can be $.detail.state but it
        can't be "SELECT * FROM $.detail.state". These paths are replaced dynamically
        at runtime with data from the event payload itself at the specified path. Dynamic path
        parameters can't reference new or transformed values resulting from input transformation.
        The supported syntax for dynamic parameter JSON paths is the same as when transforming
        input. For more information, see Amazon EventBridge Pipes input transformation.
      Dynamic syntax can be used on all string, non-enum fields of all EventBridge Pipes enrichment
        and target parameters except:
      
         
         
         
         
      
          PipeTargetCloudWatchLogsParameters.LogStreamName
        
          PipeTargetEventBridgeEventBusParameters.EndpointId
        
          PipeEnrichmentHttpParameters.HeaderParameters
        
          PipeTargetHttpParameters.HeaderParameters
        
      For example, to set the PartitionKey of a pipe Kinesis target to a custom key
        from your source event, set the KinesisTargetParameter.PartitionKey to: 
      
         
         
      
          "$.data.someKey" for a Kinesis source
        
          "$.body.someKey" for an Amazon SQS source
        
      Then, if the event payload is a valid JSON string, such as
            {"someKey":"someValue"},
        EventBridge extracts the value from the JSON path and uses it as the target parameter. In this
        example, EventBridge would set the Kinesis PartitionKey to
          "someValue".
      
     
   
    Permissions
    To make API calls on the resources that you own, EventBridge Pipes needs appropriate permission.
      EventBridge PIpes uses the IAM role that you specify on the pipe for enrichment and target calls
      using the IAM principal pipes.amazonaws.com.
   
    Invoking targets
    EventBridge has the following ways to invoke a target:
    
       
       
    
        Synchronously (invocation type set to
            REQUEST_RESPONSE) – EventBridge waits for a response from the target before
          proceeding.
      
        Asynchronously (invocation type set to
            FIRE_AND_FORGET) – EventBridge doesn't wait for a response before
          proceeding.
      
    By default, for pipes with ordered sources, EventBridge invokes targets synchronously because a
      response from the target is needed before proceeding to the next event. 
    If an source doesn't enforce order, such as a standard Amazon SQS queue, EventBridge can invoke a
      supported target synchronously or asynchronously. 
    With Lambda functions and Step Functions state machines, you can configure the invocation
      type.
    NoteFor Step Functions state machines, Standard
          workflows must be invoked asynchronously.
   
      AWS Batch job queues target specifics
      All AWS Batch submitJob parameters are configured explicitly with
          BatchParameters, and as with all Pipe parameters, these can be dynamic using
        a JSON path to your incoming event payload.
     
      CloudWatch Logs group target specifics
      Whether you use an input transformer or not, the event payload is used as the log
        message. You can set the Timestamp (or the explicit LogStreamName
        of your destination) through CloudWatchLogsParameters in
          PipeTarget. As with all pipe parameters, these parameters can be dynamic when
        using a JSON path to your incoming event payload.
     
      Amazon ECS task target specifics
      All Amazon ECS runTask parameters are configured explicitly through
          EcsParameters. As with all pipe parameters, these parameters can be dynamic
        when using a JSON path to your incoming event payload.
     
      Lambda functions and Step Functions
          workflow target specifics
      Lambda and Step Functions do not have a batch API. To process batches of events from a pipe
        source, the batch is converted to a JSON array and passed to as input to the Lambda or Step Functions
        target. For more information, see Amazon EventBridge Pipes batching and concurrency. 
     
      Timestream for LiveAnalytics table target specifics
      Considerations when specifying a Timestream for LiveAnalytics table as a pipe target include:
      
         
         
      
          Apache Kafka streams (including fromAmazon MSK or third-party providers) are not
            currently supported as a pipe source.
          If you have specified a Kinesis or DynamoDB stream as the pipe
            source, you must  specify the number of retry attempts.
          For more information, see Configuring the pipe settings.
        
      
    Document ConventionsEnrichmentBatching and concurrencyDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon EventBridgeUser GuideFiltering events using enrichmentInvoking enrichmentsEvent enrichment in Amazon EventBridge PipesWith the enrichment step of EventBridge Pipes, you can enhance the data from the source before
        sending it to the target. For example, you might receive Ticket
            created events that don’t include the full ticket data. Using enrichment, you
        can have a Lambda function call the get-ticket API for the full ticket details.
        Pipes can then send that information to a target.You can configure the following enrichments when setting up a pipe in EventBridge:
         
         
         
         
    API destinationAmazon API GatewayLambda functionStep Functions state machine
            NoteEventBridge Pipes only supports Express workflows as enrichments.
        EventBridge invokes enrichments synchronously because it must wait for a response from the
        enrichment before invoking the target.Enrichment responses are limited to a maximum size of 6MB.You can also transform the data you receive from the source before sending it for
        enhancement. For more information, see Amazon EventBridge Pipes input transformation.
        Filtering events using enrichment
        EventBridge Pipes passes the enrichment responses directly to the configured target. This
            includes array responses for targets that support batches. For more information about
            batch behavior, see Amazon EventBridge Pipes batching and concurrency. You can also use your
            enrichment as a filter and pass fewer events than were received from the source. If you
            don’t want to invoke the target, return an empty response, such as "",
            {}, or [].
        NoteIf you want to invoke the target with an empty payload, return an array with empty JSON [{}].
     
        Invoking enrichments
        EventBridge invokes enrichments synchronously (invocation type set to REQUEST_RESPONSE) because it must wait for a response from the
            enrichment before invoking the target.
        NoteFor Step Functions state machines, EventBridge only supports Express
                    workflows as enrichments, because they can be invoked
                synchronously.
    Document ConventionsFilteringTargetsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon EventBridgeUser GuideHow event buses workEvent buses in Amazon EventBridgeAn event bus is a router that receives events and
        delivers them to zero or more destinations, or targets. Event buses are
        well-suited for routing events from many sources to many targets, with optional
        transformation of events prior to delivery to a target. 
         
            
         
         
    Rules associated with the event bus evaluate events as
        they arrive. Each rule checks whether an event matches the rule's pattern. If the event does
        match, EventBridge sends the event You associate a rule with a specific event bus, so the rule only applies to events
        received by that event bus. NoteYou can also process events using EventBridge Pipes. EventBridge Pipes is intended for
            point-to-point integrations; each pipe receives events from a single source for
            processing and delivery to a single target. Pipes also include support for advanced
            transformations and enrichment of events prior to delivery to a target. For more
            information, see Amazon EventBridge Pipes.
        How event buses work in EventBridge
        Event buses enable you to route events from multiple sources to multiple destinations,
            or targets. 
        At a high level, here's how it works:
        
             
             
        
                An event source, which can be an AWS service, your own custom application,
                    or a SaaS provider, sends an event to an event bus.
            
                EventBridge then evaluates the event against each rule defined
                    for that event bus. 
                For each event that matches a rule, EventBridge then sends the event to the targets
                    specified for that rule. Optionally, as part of the rule, you can also specify
                    how EventBridge should transform the event prior to sending it to the target(s).
                An event might match multiple rules, and each rule can specify up to five
                    targets. (An event may not match any rules, in which case EventBridge takes no
                    action.)
            

        
             
                
             
             
        
        
        Consider an example using the EventBridge default event bus, which automatically receives
            events from AWS services:
        
             
             
             
        
                You create a rule on the default event bus for the EC2 Instance
                        State-change Notification event: 
                
                     
                     
                
                        You specify that the rule matches events where an Amazon EC2 instance has
                            changed its state to running.
                        You do this by specifying JSON that defines the attributes and values
                            an event must match to trigger the rule. This is called an
                                event pattern. 
                        {
  "source": ["aws.ec2"],
  "detail-type": ["EC2 Instance State-change Notification"],
    "detail": {
      "state": ["running"]
  }
}
                    
                        You specify the target of the rule to be a given Lambda
                            function.
                    

            
                Whenever an Amazon EC2 instance changes state, Amazon EC2 (the event source)
                    automatically sends that event to the default event bus.
            
                EventBridge evaluates all events sent to the default event bus against the rule
                    you've created.
                If the event matches your rule (that is, if the event was an Amazon EC2 instance
                    changing state to running), EventBridge sends the event to the specified
                    target. In this case, that's the Lambda function.
            

         The following video describes what event buses are and
            explains some of the basics of them:
        
        
              
                
              
         
        
        

         The following video covers the different event buses and
            when to use them:
        
              
                
              
         
        
        
    Document ConventionsGetting startedEvent bus
                conceptsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideBenefits of using Amazon SQSBasic architectureDifferences between Amazon SQS, Amazon MQ,
				and Amazon SNSWhat is Amazon Simple Queue Service?Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you
		integrate and decouple distributed software systems and components. Amazon SQS offers common
		constructs such as dead-letter queues and
			cost allocation tags. It provides a generic web
		services API that you can access using any programming language that the AWS SDK
		supports.
		Benefits of using Amazon SQS
		
			 
			 
			 
			 
			 
			 
		
				Security – You control who can
					send messages to and receive messages from an Amazon SQS queue. You can choose to
					transmit sensitive data by protecting the contents of messages in queues by
					using default Amazon SQS managed server-side encryption (SSE), or by using custom
						SSE keys managed in
					AWS Key Management Service (AWS KMS).
			
				Durability – For the safety of your
					messages, Amazon SQS stores them on multiple servers. Standard queues support at-least-once message
						delivery, and FIFO queues support exactly-once message
						processing and high-throughput mode.
			
				Availability – Amazon SQS uses redundant infrastructure to provide
					highly-concurrent access to messages and high availability for producing and
					consuming messages. 
			
				Scalability – Amazon SQS can process each
						buffered
						request independently, scaling transparently to handle any load
					increases or spikes without any provisioning instructions.
			
				Reliability – Amazon SQS locks your messages
					during processing, so that multiple producers can send and multiple consumers
					can receive messages at the same time. 
			
				Customization – Your queues don't
					have to be exactly alike—for example, you can set a default delay on a queue. You can
					store the contents of messages larger than 256 KB using Amazon Simple Storage Service (Amazon S3) or Amazon DynamoDB, with
					Amazon SQS holding a pointer to the Amazon S3 object, or you can split a large message
					into smaller messages.
			
	 
    Basic Amazon SQS architecture
    
         
    
    This section describes the components of a distributed messaging system and explains the
        lifecycle of an Amazon SQS message.
     
        Distributed queues
        There are three main parts in a distributed messaging system: the components of
        your distributed system, your queue (distributed on Amazon SQS servers), and the messages
        in the queue.
    In the following scenario, your system has several producers (components that send messages
        to the queue) and consumers (components that receive messages from the queue). The queue (which
        holds messages A through E) redundantly stores the messages across multiple Amazon SQS servers.
    
         
            
         
             
    
     
     
        Message lifecycle
        The following scenario describes the lifecycle of an Amazon SQS message in a queue, from
            creation to deletion.
        
             
                
             
             
        
        
                 
                    
                 
                 
             A producer (Component 1) sends message A to a queue, and the
            message is distributed across the Amazon SQS servers redundantly.
        
                 
                    
                 
                 
             When a consumer (Component 2) is ready to process messages, it
            consumes messages from the queue, and message A is returned. While message A is being
            processed, it remains in the queue and isn't returned to subsequent receive requests for
            the duration of the visibility
            timeout.
        
                 
                    
                 
                 
             The consumer (Component 2) deletes message A from the queue to
            prevent the message from being received and processed again when the visibility timeout
            expires.
        NoteAmazon SQS automatically deletes messages that have been in a queue for more than the
                maximum message retention period. The default message retention period is 4 days.
                However, you can set the message retention period to a value from 60 seconds to
                1,209,600 seconds (14 days) using the SetQueueAttributes action.
     
 
		Differences between Amazon SQS, Amazon MQ,
				and Amazon SNS
		 Amazon SQS, Amazon SNS, and Amazon MQ offer highly scalable and easy-to-use
			managed messaging services, each designed for specific roles within distributed systems.
			Here's an enhanced overview of the differences between these services:
		
			Amazon SQS decouples and scales distributed software systems
			and components as a queue service. It processes messages through a single subscriber
			typically, ideal for workflows where order and loss prevention are critical. For wider
			distribution, integrating Amazon SQS with Amazon SNS enables a fanout messaging pattern, effectively pushing messages to multiple
			subscribers at once.
		
			Amazon SNS allows publishers to send messages to multiple
			subscribers through topics, which serve as communication channels. Subscribers receive
			published messages using a supported endpoint type, such as Amazon Data Firehose, Amazon SQS,
				Lambda, HTTP,
			email, mobile push notifications, and mobile text messages (SMS). This service is ideal
			for scenarios requiring immediate notifications, such as real-time user engagement or
			alarm systems. To prevent message loss when subscribers are offline, integrating Amazon SNS
			with Amazon SQS queue messages ensures consistent delivery.
		
			Amazon MQ fits best with enterprises looking to migrate
			from traditional message brokers, supporting standard messaging protocols like AMQP and
			MQTT, along with Apache ActiveMQ and
				RabbitMQ. It offers compatibility
			with legacy systems needing stable, reliable messaging without significant
			reconfiguration.
		 The following chart provides an overview of each services' resource type: 
		
					
						Resource type
						Amazon SNS
						Amazon SQS
						Amazon MQ
					
				
					
						Synchronous
						No
						No
						Yes
					
					
						Asynchronous
						Yes
						Yes
						Yes
					
					
						Queues
						No
						Yes
						Yes
					
					
						Publisher-subscriber messaging
						Yes
						No
						Yes
					
					
						Message brokers
						No
						No
						Yes
					
				
		Both Amazon SQS and Amazon SNS are recommended for new applications that can benefit from nearly
			unlimited scalability and simple APIs. They generally offer more cost-effective
			solutions for high-volume applications with their pay-as-you-go pricing. We recommend
			Amazon MQ for migrating applications from existing message brokers that rely on
			compatibility with APIs such as JMS or protocols such as Advanced Message Queuing
			Protocol (AMQP), MQTT, OpenWire, and Simple Text Oriented Message Protocol
			(STOMP).
	Document ConventionsGetting startedDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceAPI ReferenceRequest SyntaxRequest ParametersResponse SyntaxResponse ElementsErrorsExamplesSee AlsoSendMessageDelivers a message to the specified queue.ImportantA message can include only XML, JSON, and unformatted text. The following Unicode characters are allowed. For more information, see the W3C specification for characters.
         #x9 | #xA | #xD | #x20 to #xD7FF | #xE000 to #xFFFD | #x10000 to #x10FFFF
      Amazon SQS does not throw an exception or completely reject the message if it contains invalid characters. Instead, it replaces those invalid characters with U+FFFD before storing the message in the queue, as long as the message body contains at least one valid character.
      Request Syntax
      {
   "DelaySeconds": number,
   "MessageAttributes": { 
      "string" : { 
         "BinaryListValues": [ blob ],
         "BinaryValue": blob,
         "DataType": "string",
         "StringListValues": [ "string" ],
         "StringValue": "string"
      }
   },
   "MessageBody": "string",
   "MessageDeduplicationId": "string",
   "MessageGroupId": "string",
   "MessageSystemAttributes": { 
      "string" : { 
         "BinaryListValues": [ blob ],
         "BinaryValue": blob,
         "DataType": "string",
         "StringListValues": [ "string" ],
         "StringValue": "string"
      }
   },
   "QueueUrl": "string"
}
    
      Request Parameters
      For information about the parameters that are common to all actions, see Common Parameters.
      The request accepts the following data in JSON format.
      
          
          
          
          
          
          
          
      
            
               
                  DelaySeconds
               
            
            
                The length of time, in seconds, for which to delay a specific message. Valid values:
            0 to 900. Maximum: 15 minutes. Messages with a positive DelaySeconds value
            become available for processing after the delay period is finished. If you don't specify
            a value, the default value for the queue applies. 
               NoteWhen you set FifoQueue, you can't set DelaySeconds per message. You can set this parameter only on a queue level.
               Type: Integer
               Required: No
            
          
            
               
                  MessageAttributes
               
            
            
               Each message attribute consists of a Name, Type, 
and Value. For more information, see 
Amazon SQS 
message attributes in the Amazon SQS Developer Guide.
               Type: String to MessageAttributeValue object map
               Required: No
            
          
            
               
                  MessageBody
               
            
            
               The message to send. The minimum size is one character. The maximum size is 256
            KiB.
               ImportantA message can include only XML, JSON, and unformatted text. The following Unicode characters are allowed. For more information, see the W3C specification for characters.
                     #x9 | #xA | #xD | #x20 to #xD7FF | #xE000 to #xFFFD | #x10000 to #x10FFFF
                  Amazon SQS does not throw an exception or completely reject the message if it contains invalid characters. Instead, it replaces those invalid characters with U+FFFD before storing the message in the queue, as long as the message body contains at least one valid character.
               Type: String
               Required: Yes
            
          
            
               
                  MessageDeduplicationId
               
            
            
               This parameter applies only to FIFO (first-in-first-out) queues.
               The token used for deduplication of sent messages. If a message with a particular
                MessageDeduplicationId is sent successfully, any messages sent with the
            same MessageDeduplicationId are accepted successfully but aren't delivered
            during the 5-minute deduplication interval. For more information, see  Exactly-once processing in the Amazon SQS Developer
            Guide.
               
                   
                   
                   
               
                     Every message must have a unique MessageDeduplicationId,
                     
                         
                         
                         
                         
                     
                           You may provide a MessageDeduplicationId
                            explicitly.
                        
                           If you aren't able to provide a MessageDeduplicationId
                            and you enable ContentBasedDeduplication for your queue,
                            Amazon SQS uses a SHA-256 hash to generate the
                                MessageDeduplicationId using the body of the message
                            (but not the attributes of the message). 
                        
                           If you don't provide a MessageDeduplicationId and the
                            queue doesn't have ContentBasedDeduplication set, the
                            action fails with an error.
                        
                           If the queue has ContentBasedDeduplication set, your
                                MessageDeduplicationId overrides the generated
                            one.
                        
                  
                     When ContentBasedDeduplication is in effect, messages with
                    identical content sent within the deduplication interval are treated as
                    duplicates and only one copy of the message is delivered.
                  
                     If you send one message with ContentBasedDeduplication enabled
                    and then another message with a MessageDeduplicationId that is the
                    same as the one generated for the first MessageDeduplicationId, the
                    two messages are treated as duplicates and only one copy of the message is
                    delivered. 
                  
               NoteThe MessageDeduplicationId is available to the consumer of the
                message (this can be useful for troubleshooting delivery issues).If a message is sent successfully but the acknowledgement is lost and the message
                is resent with the same MessageDeduplicationId after the deduplication
                interval, Amazon SQS can't detect duplicate messages.Amazon SQS continues to keep track of the message deduplication ID even after the message is received and deleted.
               The maximum length of MessageDeduplicationId is 128 characters.
                MessageDeduplicationId can contain alphanumeric characters
                (a-z, A-Z, 0-9) and punctuation
                (!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~).
               For best practices of using MessageDeduplicationId, see Using the MessageDeduplicationId Property in the Amazon SQS Developer
                Guide.
               Type: String
               Required: No
            
          
            
               
                  MessageGroupId
               
            
            
               This parameter applies only to FIFO (first-in-first-out) queues.
               The tag that specifies that a message belongs to a specific message group. Messages
            that belong to the same message group are processed in a FIFO manner (however,
            messages in different message groups might be processed out of order). To interleave
            multiple ordered streams within a single queue, use MessageGroupId values
            (for example, session data for multiple users). In this scenario, multiple consumers can
            process the queue, but the session data of each user is processed in a FIFO
            fashion.
               
                   
                   
               
                     You must associate a non-empty MessageGroupId with a message. If
                    you don't provide a MessageGroupId, the action fails.
                  
                     
                        ReceiveMessage might return messages with multiple
                        MessageGroupId values. For each MessageGroupId,
                    the messages are sorted by time sent. The caller can't specify a
                        MessageGroupId.
                  
               The maximum length of MessageGroupId is 128 characters. Valid values:
            alphanumeric characters and punctuation
                (!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~).
               For best practices of using MessageGroupId, see Using the MessageGroupId Property in the Amazon SQS Developer
                Guide.
               Important
                     MessageGroupId is required for FIFO queues. You can't use it for
                Standard queues.
               Type: String
               Required: No
            
          
            
               
                  MessageSystemAttributes
               
            
            
               The message system attribute to send. Each message system attribute consists of a Name, Type, and Value.
               Important
                      
                      
                  
                        Currently, the only supported message system attribute is AWSTraceHeader.
                    Its type must be String and its value must be a correctly formatted
                    AWS X-Ray trace header string.
                     
                        The size of a message system attribute doesn't count towards the total size of a message.
                     
               Type: String to MessageSystemAttributeValue object map
               Valid Keys: AWSTraceHeader
               
               Required: No
            
          
            
               
                  QueueUrl
               
            
            
               The URL of the Amazon SQS queue to which a message is sent.
               Queue URLs and names are case-sensitive.
               Type: String
               Required: Yes
            
         
    
      Response Syntax
      {
   "MD5OfMessageAttributes": "string",
   "MD5OfMessageBody": "string",
   "MD5OfMessageSystemAttributes": "string",
   "MessageId": "string",
   "SequenceNumber": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 200 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
      
            
               
                  MD5OfMessageAttributes
               
            
            
               An MD5 digest of the non-URL-encoded message attribute string. You can use this attribute to verify that Amazon SQS received the message correctly. Amazon SQS URL-decodes the message before creating the MD5 digest. For information about MD5, see RFC1321.
               Type: String
            
          
            
               
                  MD5OfMessageBody
               
            
            
               An MD5 digest of the non-URL-encoded message body string. You can use this attribute to verify that Amazon SQS received the message correctly. Amazon SQS URL-decodes the message before creating the MD5 digest. For information about MD5, see RFC1321.
               Type: String
            
          
            
               
                  MD5OfMessageSystemAttributes
               
            
            
               An MD5 digest of the non-URL-encoded message system attribute string. You can use this 
attribute to verify that Amazon SQS received the message correctly. Amazon SQS URL-decodes the message before creating the MD5 digest.
               Type: String
            
          
            
               
                  MessageId
               
            
            
               An attribute containing the MessageId of the message sent to the queue.
            For more information, see Queue and Message Identifiers in the Amazon SQS Developer
                Guide. 
               Type: String
            
          
            
               
                  SequenceNumber
               
            
            
               This parameter applies only to FIFO (first-in-first-out) queues.
               The large, non-consecutive number that Amazon SQS assigns to each message.
               The length of SequenceNumber is 128 bits. SequenceNumber
            continues to increase for a particular MessageGroupId.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  InvalidAddress
               
            
            
               The specified ID is invalid.
               HTTP Status Code: 400
            
          
            
               
                  InvalidMessageContents
               
            
            
               The message contains characters outside the allowed set.
               HTTP Status Code: 400
            
          
            
               
                  InvalidSecurity
               
            
            
               The request was not made over HTTPS or did not use SigV4 for signing.
               HTTP Status Code: 400
            
          
            
               
                  KmsAccessDenied
               
            
            
               The caller doesn't have the required KMS access.
               HTTP Status Code: 400
            
          
            
               
                  KmsDisabled
               
            
            
               The request was denied due to request throttling.
               HTTP Status Code: 400
            
          
            
               
                  KmsInvalidKeyUsage
               
            
            
               The request was rejected for one of the following reasons:
               
                   
                   
               
                     The KeyUsage value of the KMS key is incompatible with the API
                    operation.
                  
                     The encryption algorithm or signing algorithm specified for the operation is
                    incompatible with the type of key material in the KMS key (KeySpec).
                  
               HTTP Status Code: 400
            
          
            
               
                  KmsInvalidState
               
            
            
               The request was rejected because the state of the specified resource is not valid for
            this request.
               HTTP Status Code: 400
            
          
            
               
                  KmsNotFound
               
            
            
               The request was rejected because the specified entity or resource could not be found.
        
               HTTP Status Code: 400
            
          
            
               
                  KmsOptInRequired
               
            
            
               The request was rejected because the specified key policy isn't syntactically or
            semantically correct.
               HTTP Status Code: 400
            
          
            
               
                  KmsThrottled
               
            
            
               
                  AWS KMS throttles requests for the following conditions.
               HTTP Status Code: 400
            
          
            
               
                  QueueDoesNotExist
               
            
            
               Ensure that the QueueUrl is correct and that the queue has not been
            deleted.
               HTTP Status Code: 400
            
          
            
               
                  RequestThrottled
               
            
            
               The request was denied due to request throttling.
               
                   
                   
               
                     Exceeds the permitted request rate for the queue or for the recipient of the
                    request.
                  
                     Ensure that the request rate is within the Amazon SQS limits for
                    sending messages. For more information, see Amazon SQS quotas in the Amazon SQS
                        Developer Guide.
                  
               HTTP Status Code: 400
            
          
            
               
                  UnsupportedOperation
               
            
            
               Error code 400. Unsupported operation.
               HTTP Status Code: 400
            
         
    
      Examples
      The following example SendMessage request sends a message containing
                    This is a test message to the queue. You must URL-encode the entire URL. However, in this example only the message body is URL-encoded to make the example easier to read.
                The structure of AUTHPARAMS depends on the signature of the API request. 
                For more information, see 
                Examples of Signed Signature Version 4 Requests in the 
            AWS General Reference.
       
         Example
         
            Using AWS JSON protocol
                        (Default)
         
          
            Sample Request
            POST / HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Target: AmazonSQS.SendMessage
X-Amz-Date: <Date>
Content-Type: application/x-amz-json-1.0
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive 
{
    "QueueUrl": "https://sqs.us-east-1.amazonaws.com/177715257436/MyQueue/",
    "MessageBody": "This is a test message",
    "MessageAttributes": {
        "my_attribute_name_1": {
            "DataType": "String",
            "StringValue": "my_attribute_value_1"
        },
        "my_attribute_name_2": {
            "DataType": "String",
            "StringValue": "my_attribute_value_2"
        }
    }
}
          
          
            Sample Response
            HTTP/1.1 200 OK
x-amzn-RequestId: <requestId>
Content-Length: <PayloadSizeBytes>
Date: <Date>
Content-Type: application/x-amz-json-1.0
{
    "MD5OfMessageAttributes": "c48838208d2b4e14e3ca0093a8443f09",
    "MD5OfMessageBody": "fafb00f5732ab283681e124bf8747ed1",
    "MessageId": "219f8380-5770-4cc2-8c3e-5c715e145f5e"
}
          
       
       
         Example
         
            Using AWS query
                    protocol
         
          
            Sample Request
            POST /177715257436/MyQueue/ HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Date: <Date>
Content-Type: application/x-www-form-urlencoded
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive
Action=SendMessage
&MessageBody=This+is+a+test+message
&MessageAttribute.1.Name=my_attribute_name_1
&MessageAttribute.1.Value.StringValue=my_attribute_value_1
&MessageAttribute.1.Value.DataType=String
&MessageAttribute.2.Name=my_attribute_name_2
&MessageAttribute.2.Value.StringValue=my_attribute_value_2
&MessageAttribute.2.Value.DataType=String
          
          
            Sample Response
            HTTP/1.1 200 OK
<?xml version="1.0"?>
<SendMessageResponse xmlns="http://queue.amazonaws.com/doc/2012-11-05/">
    <SendMessageResult>
        <MessageId>374cec7b-d0c8-4a2e-ad0b-67be763cf97e</MessageId>
        <MD5OfMessageBody>fafb00f5732ab283681e124bf8747ed1</MD5OfMessageBody>
        <MD5OfMessageAttributes>c48838208d2b4e14e3ca0093a8443f09</MD5OfMessageAttributes>
    </SendMessageResult>
    <ResponseMetadata>
        <RequestId>7fe4446e-b452-53f7-8f85-181e06f2dd99</RequestId>
    </ResponseMetadata>
</SendMessageResponse>
          
       
       
         Example
         The following example creates a message
                    timer—applying a 45-second initial visibility delay to a
                    single message— by calling the SendMessage action with the
                        DelaySeconds parameter set to 45 seconds.
         NoteQueue URLs and names are case-sensitive.
         
            Using AWS JSON protocol
                        (Default)
         
          
            Sample Request
            POST / HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Target: AmazonSQS.SendMessage
X-Amz-Date: <Date>
Content-Type: application/x-amz-json-1.0
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive 
{
    "QueueUrl": "https://sqs.us-east-1.amazonaws.com/177715257436/MyQueue/",
    "MessageBody": "This is a test message",
    "DelaySeconds": 45
}
          
       
       
         Example
         
            Using AWS query
                    protocol
         
          
            Sample Request
            POST /177715257436/MyQueue/ HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Date: <Date>
Content-Type: application/x-www-form-urlencoded
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive
Action=SendMessage
&MessageBody=This+is+a+test+message
&DelaySeconds=45
          
       
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsRemovePermissionSendMessageBatchDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceAPI ReferenceRequest SyntaxRequest ParametersResponse SyntaxResponse ElementsErrorsExamplesSee AlsoReceiveMessageRetrieves one or more messages (up to 10), from the specified queue. Using the
                WaitTimeSeconds parameter enables long-poll support. For more
            information, see Amazon SQS
                Long Polling in the Amazon SQS Developer Guide. Short poll is the default behavior where a weighted random set of machines is sampled
            on a ReceiveMessage call. Therefore, only the messages on the sampled
            machines are returned. If the number of messages in the queue is small (fewer than
            1,000), you most likely get fewer messages than you requested per
                ReceiveMessage call. If the number of messages in the queue is
            extremely small, you might not receive any messages in a particular
                ReceiveMessage response. If this happens, repeat the request.For each message returned, the response includes the following:
       
       
       
       
       
       
   
         The message body.
      
         An MD5 digest of the message body. For information about MD5, see RFC1321.
      
         The MessageId you received when you sent the message to the
                    queue.
      
         The receipt handle.
      
         The message attributes.
      
         An MD5 digest of the message attributes.
      The receipt handle is the identifier you must provide when deleting the message. For
            more information, see Queue and Message Identifiers in the Amazon SQS Developer
                Guide.You can provide the VisibilityTimeout parameter in your request. The
            parameter is applied to the messages that Amazon SQS returns in the response. If you don't
            include the parameter, the overall visibility timeout for the queue is used for the
            returned messages. The default visibility timeout for a queue is 30 seconds. NoteIn the future, new attributes might be added. If you write code that calls this action, we recommend that you structure your code so that it can handle new attributes gracefully.
      Request Syntax
      {
   "AttributeNames": [ "string" ],
   "MaxNumberOfMessages": number,
   "MessageAttributeNames": [ "string" ],
   "MessageSystemAttributeNames": [ "string" ],
   "QueueUrl": "string",
   "ReceiveRequestAttemptId": "string",
   "VisibilityTimeout": number,
   "WaitTimeSeconds": number
}
    
      Request Parameters
      For information about the parameters that are common to all actions, see Common Parameters.
      The request accepts the following data in JSON format.
      
          
          
          
          
          
          
          
          
      
            
               
                  AttributeNames
               
            
            
               
                  This parameter has been deprecated.
               
               ImportantThis parameter has been discontinued but will be supported for backward
                compatibility. To provide attribute names, you are encouraged to use
                    MessageSystemAttributeNames. 
               A list of attributes that need to be returned along with each message. These
            attributes include:
               
                   
                   
                   
                   
                   
                   
                   
                   
                   
                   
               
                     
                        All – Returns all values.
                  
                     
                        ApproximateFirstReceiveTimestamp – Returns the time the
                    message was first received from the queue (epoch time in
                    milliseconds).
                  
                     
                        ApproximateReceiveCount – Returns the number of times a
                    message has been received across all queues but not deleted.
                  
                     
                        AWSTraceHeader – Returns the AWS X-Ray trace
                    header string. 
                  
                     
                        SenderId
                     
                     
                         
                         
                     
                           For a user, returns the user ID, for example
                                ABCDEFGHI1JKLMNOPQ23R.
                        
                           For an IAM role, returns the IAM role ID, for example
                                ABCDE1F2GH3I4JK5LMNOP:i-a123b456.
                        
                  
                     
                        SentTimestamp – Returns the time the message was sent to the
                    queue (epoch time in
                    milliseconds).
                  
                     
                        SqsManagedSseEnabled – Enables server-side queue encryption
                    using SQS owned encryption keys. Only one server-side encryption option is
                    supported per queue (for example, SSE-KMS or SSE-SQS).
                  
                     
                        MessageDeduplicationId – Returns the value provided by the
                    producer that calls the 
                           SendMessage
                        
                    action.
                  
                     
                        MessageGroupId – Returns the value provided by the
                    producer that calls the 
                           SendMessage
                         action.
                    Messages with the same MessageGroupId are returned in
                    sequence.
                  
                     
                        SequenceNumber – Returns the value provided by
                    Amazon SQS.
                  
               Type: Array of strings
               Valid Values: All | Policy | VisibilityTimeout | MaximumMessageSize | MessageRetentionPeriod | ApproximateNumberOfMessages | ApproximateNumberOfMessagesNotVisible | CreatedTimestamp | LastModifiedTimestamp | QueueArn | ApproximateNumberOfMessagesDelayed | DelaySeconds | ReceiveMessageWaitTimeSeconds | RedrivePolicy | FifoQueue | ContentBasedDeduplication | KmsMasterKeyId | KmsDataKeyReusePeriodSeconds | DeduplicationScope | FifoThroughputLimit | RedriveAllowPolicy | SqsManagedSseEnabled
               
               Required: No
            
          
            
               
                  MaxNumberOfMessages
               
            
            
               The maximum number of messages to return. Amazon SQS never returns more messages than this
            value (however, fewer messages might be returned). Valid values: 1 to 10. Default:
            1.
               Type: Integer
               Required: No
            
          
            
               
                  MessageAttributeNames
               
            
            
               The name of the message attribute, where N is the index.
               
                   
                   
                   
                   
                   
               
                     The name can contain alphanumeric characters and the underscore
                        (_), hyphen (-), and period
                    (.).
                  
                     The name is case-sensitive and must be unique among all attribute names for
                    the message.
                  
                     The name must not start with AWS-reserved prefixes such as AWS.
                    or Amazon. (or any casing variants).
                  
                     The name must not start or end with a period (.), and it should
                    not have periods in succession (..).
                  
                     The name can be up to 256 characters long.
                  
               When using ReceiveMessage, you can send a list of attribute names to
            receive, or you can return all of the attributes by specifying All or
                .* in your request. You can also use all message attributes starting
            with a prefix, for example bar.*.
               Type: Array of strings
               Required: No
            
          
            
               
                  MessageSystemAttributeNames
               
            
            
               A list of attributes that need to be returned along with each message. These
            attributes include:
               
                   
                   
                   
                   
                   
                   
                   
                   
                   
                   
               
                     
                        All – Returns all values.
                  
                     
                        ApproximateFirstReceiveTimestamp – Returns the time the
                    message was first received from the queue (epoch time in
                    milliseconds).
                  
                     
                        ApproximateReceiveCount – Returns the number of times a
                    message has been received across all queues but not deleted.
                  
                     
                        AWSTraceHeader – Returns the AWS X-Ray trace
                    header string. 
                  
                     
                        SenderId
                     
                     
                         
                         
                     
                           For a user, returns the user ID, for example
                                ABCDEFGHI1JKLMNOPQ23R.
                        
                           For an IAM role, returns the IAM role ID, for example
                                ABCDE1F2GH3I4JK5LMNOP:i-a123b456.
                        
                  
                     
                        SentTimestamp – Returns the time the message was sent to the
                    queue (epoch time in
                    milliseconds).
                  
                     
                        SqsManagedSseEnabled – Enables server-side queue encryption
                    using SQS owned encryption keys. Only one server-side encryption option is
                    supported per queue (for example, SSE-KMS or SSE-SQS).
                  
                     
                        MessageDeduplicationId – Returns the value provided by the
                    producer that calls the 
                           SendMessage
                        
                    action.
                  
                     
                        MessageGroupId – Returns the value provided by the
                    producer that calls the 
                           SendMessage
                         action.
                    Messages with the same MessageGroupId are returned in
                    sequence.
                  
                     
                        SequenceNumber – Returns the value provided by
                    Amazon SQS.
                  
               Type: Array of strings
               Valid Values: All | SenderId | SentTimestamp | ApproximateReceiveCount | ApproximateFirstReceiveTimestamp | SequenceNumber | MessageDeduplicationId | MessageGroupId | AWSTraceHeader | DeadLetterQueueSourceArn
               
               Required: No
            
          
            
               
                  QueueUrl
               
            
            
               The URL of the Amazon SQS queue from which messages are received.
               Queue URLs and names are case-sensitive.
               Type: String
               Required: Yes
            
          
            
               
                  ReceiveRequestAttemptId
               
            
            
               This parameter applies only to FIFO (first-in-first-out) queues.
               The token used for deduplication of ReceiveMessage calls. If a networking
            issue occurs after a ReceiveMessage action, and instead of a response you
            receive a generic error, it is possible to retry the same action with an identical
                ReceiveRequestAttemptId to retrieve the same set of messages, even if
            their visibility timeout has not yet expired.
               
                   
                   
                   
                   
                   
                   
               
                     You can use ReceiveRequestAttemptId only for 5 minutes after a
                        ReceiveMessage action.
                  
                     When you set FifoQueue, a caller of the
                        ReceiveMessage action can provide a
                        ReceiveRequestAttemptId explicitly.
                  
                     It is possible to retry the ReceiveMessage action with the same
                        ReceiveRequestAttemptId if none of the messages have been
                    modified (deleted or had their visibility changes).
                  
                     During a visibility timeout, subsequent calls with the same
                        ReceiveRequestAttemptId return the same messages and receipt
                    handles. If a retry occurs within the deduplication interval, it resets the
                    visibility timeout. For more information, see Visibility Timeout in the Amazon SQS Developer
                        Guide.
                     ImportantIf a caller of the ReceiveMessage action still processes
                        messages when the visibility timeout expires and messages become visible,
                        another worker consuming from the same queue can receive the same messages
                        and therefore process duplicates. Also, if a consumer whose message
                        processing time is longer than the visibility timeout tries to delete the
                        processed messages, the action fails with an error.To mitigate this effect, ensure that your application observes a safe
                        threshold before the visibility timeout expires and extend the visibility
                        timeout as necessary.
                  
                     While messages with a particular MessageGroupId are invisible, no
                    more messages belonging to the same MessageGroupId are returned
                    until the visibility timeout expires. You can still receive messages with
                    another MessageGroupId as long as it is also visible.
                  
                     If a caller of ReceiveMessage can't track the
                        ReceiveRequestAttemptId, no retries work until the original
                    visibility timeout expires. As a result, delays might occur but the messages in
                    the queue remain in a strict order.
                  
               The maximum length of ReceiveRequestAttemptId is 128 characters.
                ReceiveRequestAttemptId can contain alphanumeric characters
                (a-z, A-Z, 0-9) and punctuation
                (!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~).
               For best practices of using ReceiveRequestAttemptId, see Using the ReceiveRequestAttemptId Request Parameter in the Amazon SQS
                Developer Guide.
               Type: String
               Required: No
            
          
            
               
                  VisibilityTimeout
               
            
            
               The duration (in seconds) that the received messages are hidden from subsequent
            retrieve requests after being retrieved by a ReceiveMessage request. If not
            specified, the default visibility timeout for the queue is used, which is 30
            seconds.
               Understanding VisibilityTimeout:
               
                   
                   
                   
                   
               
                     When a message is received from a queue, it becomes temporarily invisible to
                    other consumers for the duration of the visibility timeout. This prevents
                    multiple consumers from processing the same message simultaneously. If the
                    message is not deleted or its visibility timeout is not extended before the
                    timeout expires, it becomes visible again and can be retrieved by other
                    consumers.
                  
                     Setting an appropriate visibility timeout is crucial. If it's too short, the
                    message might become visible again before processing is complete, leading to
                    duplicate processing. If it's too long, it delays the reprocessing of messages
                    if the initial processing fails.
                  
                     You can adjust the visibility timeout using the
                        --visibility-timeout parameter in the
                        receive-message command to match the processing time required
                    by your application.
                  
                     A message that isn't deleted or a message whose visibility isn't extended
                    before the visibility timeout expires counts as a failed receive. Depending on
                    the configuration of the queue, the message might be sent to the dead-letter
                    queue.
                  
               For more information, see Visibility Timeout in the Amazon SQS Developer
            Guide.
               Type: Integer
               Required: No
            
          
            
               
                  WaitTimeSeconds
               
            
            
               The duration (in seconds) for which the call waits for a message to arrive in the
            queue before returning. If a message is available, the call returns sooner than
                WaitTimeSeconds. If no messages are available and the wait time
            expires, the call does not return a message list. If you are using the Java SDK, it
            returns a ReceiveMessageResponse object, which has a empty list instead of
            a Null object.
               ImportantTo avoid HTTP errors, ensure that the HTTP response timeout for
                    ReceiveMessage requests is longer than the
                    WaitTimeSeconds parameter. For example, with the Java SDK, you can
                set HTTP transport settings using the  NettyNioAsyncHttpClient for asynchronous clients, or the  ApacheHttpClient for synchronous clients. 
               Type: Integer
               Required: No
            
         
    
      Response Syntax
      {
   "Messages": [ 
      { 
         "Attributes": { 
            "string" : "string" 
         },
         "Body": "string",
         "MD5OfBody": "string",
         "MD5OfMessageAttributes": "string",
         "MessageAttributes": { 
            "string" : { 
               "BinaryListValues": [ blob ],
               "BinaryValue": blob,
               "DataType": "string",
               "StringListValues": [ "string" ],
               "StringValue": "string"
            }
         },
         "MessageId": "string",
         "ReceiptHandle": "string"
      }
   ]
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 200 response.
      The following data is returned in JSON format by the service.
      
          
      
            
               
                  Messages
               
            
            
               A list of messages.
               Type: Array of Message objects
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  InvalidAddress
               
            
            
               The specified ID is invalid.
               HTTP Status Code: 400
            
          
            
               
                  InvalidSecurity
               
            
            
               The request was not made over HTTPS or did not use SigV4 for signing.
               HTTP Status Code: 400
            
          
            
               
                  KmsAccessDenied
               
            
            
               The caller doesn't have the required KMS access.
               HTTP Status Code: 400
            
          
            
               
                  KmsDisabled
               
            
            
               The request was denied due to request throttling.
               HTTP Status Code: 400
            
          
            
               
                  KmsInvalidKeyUsage
               
            
            
               The request was rejected for one of the following reasons:
               
                   
                   
               
                     The KeyUsage value of the KMS key is incompatible with the API
                    operation.
                  
                     The encryption algorithm or signing algorithm specified for the operation is
                    incompatible with the type of key material in the KMS key (KeySpec).
                  
               HTTP Status Code: 400
            
          
            
               
                  KmsInvalidState
               
            
            
               The request was rejected because the state of the specified resource is not valid for
            this request.
               HTTP Status Code: 400
            
          
            
               
                  KmsNotFound
               
            
            
               The request was rejected because the specified entity or resource could not be found.
        
               HTTP Status Code: 400
            
          
            
               
                  KmsOptInRequired
               
            
            
               The request was rejected because the specified key policy isn't syntactically or
            semantically correct.
               HTTP Status Code: 400
            
          
            
               
                  KmsThrottled
               
            
            
               
                  AWS KMS throttles requests for the following conditions.
               HTTP Status Code: 400
            
          
            
               
                  OverLimit
               
            
            
               The specified action violates a limit. For example, ReceiveMessage
            returns this error if the maximum number of in flight messages is reached and
                AddPermission returns this error if the maximum number of permissions
            for the queue is reached.
               HTTP Status Code: 400
            
          
            
               
                  QueueDoesNotExist
               
            
            
               Ensure that the QueueUrl is correct and that the queue has not been
            deleted.
               HTTP Status Code: 400
            
          
            
               
                  RequestThrottled
               
            
            
               The request was denied due to request throttling.
               
                   
                   
               
                     Exceeds the permitted request rate for the queue or for the recipient of the
                    request.
                  
                     Ensure that the request rate is within the Amazon SQS limits for
                    sending messages. For more information, see Amazon SQS quotas in the Amazon SQS
                        Developer Guide.
                  
               HTTP Status Code: 400
            
          
            
               
                  UnsupportedOperation
               
            
            
               Error code 400. Unsupported operation.
               HTTP Status Code: 400
            
         
    
      Examples
      The following example query request receives messages from the specified queue.
                The structure of AUTHPARAMS depends on the signature of the API request. 
                For more information, see 
                Examples of Signed Signature Version 4 Requests in the 
            AWS General Reference.
       
         Example
         
            Using AWS JSON protocol
                        (Default)
         
          
            Sample Request
            POST / HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Target: AmazonSQS.ReceiveMessage
X-Amz-Date: <Date>
Content-Type: application/x-amz-json-1.0
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive 
{
    "QueueUrl": "https://sqs.us-east-1.amazonaws.com/177715257436/MyQueue/",
    "MaxNumberOfMessages": 5,
    "VisibilityTimeout": 15,
    "AttributeNames": ["All"]
}
          
          
            Sample Response
            HTTP/1.1 200 OK
x-amzn-RequestId: <requestId>
Content-Length: <PayloadSizeBytes>
Date: <Date>
Content-Type: application/x-amz-json-1.0
{
    "Messages": [
        {
            "Attributes": {
                "SenderId": "AIDASSYFHUBOBT7F4XT75",
                "ApproximateFirstReceiveTimestamp": "1677112433437",
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1677112427387"
            },
            "Body": "This is a test message",
            "MD5OfBody": "fafb00f5732ab283681e124bf8747ed1",
            "MessageId": "219f8380-5770-4cc2-8c3e-5c715e145f5e",
            "ReceiptHandle": "AQEBaZ+j5qUoOAoxlmrCQPkBm9njMWXqemmIG6shMHCO6fV20JrQYg/AiZ8JELwLwOu5U61W+aIX5Qzu7GGofxJuvzymr4Ph53RiR0mudj4InLSgpSspYeTRDteBye5tV/txbZDdNZxsi+qqZA9xPnmMscKQqF6pGhnGIKrnkYGl45Nl6GPIZv62LrIRb6mSqOn1fn0yqrvmWuuY3w2UzQbaYunJWGxpzZze21EOBtywknU3Je/g7G9is+c6K9hGniddzhLkK1tHzZKjejOU4jokaiB4nmi0dF3JqLzDsQuPF0Gi8qffhEvw56nl8QCbluSJScFhJYvoagGnDbwOnd9z50L239qtFIgETdpKyirlWwl/NGjWJ45dqWpiW3d2Ws7q"
        }
    ]
}
          
       
       
         Example
         
            Using AWS query
                    protocol
         
          
            Sample Request
            POST /177715257436/MyQueue/ HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Date: <Date>
Content-Type: application/x-www-form-urlencoded
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive
Action=ReceiveMessage
&MaxNumberOfMessages=5
&VisibilityTimeout=15
&AttributeName=All
          
          
            Sample Response
            HTTP/1.1 200 OK
<ReceiveMessageResponse xmlns="http://queue.amazonaws.com/doc/2012-11-05/">
    <ReceiveMessageResult>
        <Message>
            <MessageId>60e827c3-c8a5-410a-af0e-fb43746e70b1</MessageId>
            <ReceiptHandle>AQEBwPTK2fT2gy97H1iyU5in9umgT+Y4IOxyKGOzpZa8iemEqoR5/aPn0xAodmiVTzyrW7S4e8XwcWbB04XK92jIQzUpiGwRFA4Dl7r3GOw84Qzq/0OBQe/JaKxJw6iilafYA5fo1SJQo5Wg8xXbJHTVlJqgvTXd/UtlByLMhWMi0JMra1UUjYiPsGtYUpLVnOaRkYSPvzRnFFYUbcqCW9lm2Bi/jQKK6KNOZyCCfIh8TooE5i4P2L9N3o9yUHwMdv6p0nb5lKaGurQ2sJwwsyhXf38ZHnVN6pWwsqQnWKYuEXpxPofxd2lcLdgUurMpydS22DzCrkAaf6gmrdxbmCAoeQxE0sFf8alwX9yQmcOjny9aLGe7ro4Vl5o5KMr5hHM4vHEyhwi4wHeKM6MGX0vATA==</ReceiptHandle>
            <MD5OfBody>0e024d309850c78cba5eabbeff7cae71</MD5OfBody>
            <Body>test message body 1</Body>
            <Attribute>
                <Name>SenderId</Name>
                <Value>AIDASSYFHUBOBT7F4XT75</Value>
            </Attribute>
            <Attribute>
                <Name>ApproximateFirstReceiveTimestamp</Name>
                <Value>1677112300463</Value>
            </Attribute>
            <Attribute>
                <Name>ApproximateReceiveCount</Name>
                <Value>1</Value>
            </Attribute>
            <Attribute>
                <Name>SentTimestamp</Name>
                <Value>1677111805489</Value>
            </Attribute>
        </Message>
    </ReceiveMessageResult>
    <ResponseMetadata>
        <RequestId>5ba605cc-1e4b-58ba-93db-59bca8677ec9</RequestId>
    </ResponseMetadata>
</ReceiveMessageResponse>
          
       
       
         Example
         The following example enables long polling by calling the
                        ReceiveMessage action with the WaitTimeSeconds
                    parameter set to 10 seconds.
         
            Using AWS JSON protocol
                        (Default)
         
          
            Sample Request
            POST / HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Target: AmazonSQS.ReceiveMessage
X-Amz-Date: <Date>
Content-Type: application/x-amz-json-1.0
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive 
{
    "QueueUrl": "https://sqs.us-east-1.amazonaws.com/177715257436/MyQueue/",
    "WaitTimeSeconds": 10,
    "MaxNumberOfMessages": 5,
    "VisibilityTimeout": 15,
    "AttributeNames": ["All"]
}
            
          
       
       
         Example
         The following example shows the request and response when using the parameter
                        MessageSystemAttributeNames.
          
            Sample Request
            aws sqs receive-message \
  --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/MyQueue \
  --message-system-attribute-names SentTimestamp SenderId
          
          
            Sample Response
            {
  "Messages": [
    {
      "MessageId": "abc1234d-5678-90ab-cdef-EXAMPLE11111",
      "ReceiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
      "MD5OfBody": "e99a18c428cb38d5f260853678922e03",
      "Body": "Example message",
      "Attributes": {
        "SenderId": "AIDAEXAMPLE123ABC",
        "SentTimestamp": "1638368280000"
      }
    }
  ]
}
          
       
       
         Example
         
            Using AWS query
                    protocol
         
          
            Sample Request
            POST /177715257436/MyQueue/ HTTP/1.1
Host: sqs.us-east-1.amazonaws.com
X-Amz-Date: <Date>
Content-Type: application/x-www-form-urlencoded
Authorization: <AuthParams>
Content-Length: <PayloadSizeBytes>
Connection: Keep-Alive
Action=ReceiveMessage
&WaitTimeSeconds=10
&MaxNumberOfMessages=5
&VisibilityTimeout=15
&AttributeName=All
          
       
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsPurgeQueueRemovePermissionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideCreating a queueSending a message using a standard queueCreating an Amazon SQS standard queue and sending
            a messageYou can create a standard queue and send messages
        using the Amazon SQS console. This topic also emphasizes best practices, including avoiding
        sensitive information in queue names and utilizing managed server-side encryption.
        Creating a standard queue using the Amazon SQS
                console
        
        ImportantOn August 17, 2022, default server-side encryption (SSE) was applied to all Amazon SQS
                queues.Do not add personally identifiable information (PII) or other confidential or
                sensitive information in queue names. Queue names are accessible to many Amazon Web Services,
                including billing and CloudWatch logs. Queue names are not intended to be used for private
                or sensitive data.
        To create an Amazon SQS standard queueOpen the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
                Choose Create queue.
            
                For Type, the Standard queue type is
                    set by default.
                NoteYou can't change the queue type after you create the queue.
            
                 Enter a Name for your queue.
            
                (Optional) The console sets default values for the queue configuration parameters.
                    Under Configuration, you can set new values for the
                    following parameters:
                
                        For Visibility timeout , enter the duration and
                            units. The range is from 0 seconds to 12 hours. The default value is 30
                            seconds.
                    
                        For Message retention period, enter the duration
                            and units. The range is from 1 minute to 14 days. The default value is 4
                            days.
                    
                        For Delivery delay, enter the duration and units.
                            The range is from 0 seconds to 15 minutes. The default value is 0
                            seconds.
                    
                        For Maximum message size, enter a value. The
                            range is from 1 KB to 256 KB. The default value is 256 KB. 
                    
                        For Receive message wait time, enter a value. The
                            range is from 0 to 20 seconds. The default value is 0 seconds, which
                            sets short polling.
                            Any non-zero value sets long polling.
                    
            
                (Optional) Define an Access policy. The access
                        policy defines the accounts, users, and roles that can access the
                    queue. The access policy also defines the actions (such as SendMessage, ReceiveMessage, or DeleteMessage) that the users can access. The
                    default policy allows only the queue owner to send and receive messages.
                To define the access policy, do one of the following:
                
                     
                     
                
                        Choose Basic to configure who can send messages
                            to the queue and who can receive messages from the queue. The console
                            creates the policy based on your choices and displays the resulting
                            access policy in the read-only JSON panel.
                    
                        Choose Advanced to modify the JSON access policy
                            directly. This allows you to specify a custom set of actions that each
                            principal (account, user, or role) can perform.
                    
            
                For Redrive allow policy, choose
                        Enabled. Select one of the following: Allow
                        all, By queue, or Deny
                        all. When choosing By queue, specify a list
                    of up to 10 source queues by the Amazon Resource Name (ARN).
            
                Amazon SQS provides managed server-side encryption by default. To choose an
                    encryption key type, or to disable Amazon SQS managed server-side encryption, expand
                        Encryption. For more on encryption key types, see Configuring server-side encryption for a
			queue using SQS-managed encryption keys and Configuring server-side encryption for a queue
			using the Amazon SQS console.
                NoteWith SSE enabled, anonymous SendMessage and
                            ReceiveMessage requests to the encrypted queue will be
                        rejected. Amazon SQS security best practises recommend against using anonymous
                        requests. If you wish to send anonymous requests to an Amazon SQS queue, make
                        sure to disable SSE.

            
                (Optional) To configure a dead-letter queue to receive undeliverable messages, expand
                        Dead-letter queue.
            
                (Optional) To add tags to the
                    queue, expand Tags.
            
                Choose Create queue. Amazon SQS creates the queue and displays
                    the queue's Details page.
            
        Amazon SQS propagates information about the new queue across the system. Because Amazon SQS is a
            distributed system, you might experience a slight delay before the console displays the
            queue on the Queues page.
     
        Sending a message using a standard queue
        After your queue has been created, you can send a message to it.
        
                From the left navigation pane, choose Queues. From the
                    queue list, select the queue that you created.
            
                From Actions, choose Send and receive
                        messages.
                The console displays the Send and receive messages
                    page.
            
                In the Message body, enter the message
                    text.
            
                For a standard queue, you can enter a value for Delivery
                        delay and choose the units. For example, enter 60
                    and choose seconds. For more information, see Amazon SQS message timers.
            
                Choose Send message.
                When your message is sent, the console displays a success message. Choose
                        View details to display information about the sent
                    message.
            
    Document ConventionsImplementing request-response
                systems in Amazon SQSCreating a FIFO queueDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideAmazon SQS Java SDK examplesThe AWS SDK for Java allows you build Java applications that interact with Amazon SQS and other
		AWS services.
		 
		 
		 
	
			To install and set up the SDK, see Getting started in the
					AWS SDK for Java 2.x Developer Guide.
		
			For basic queue operations—such as creating a queue or sending a message—see
					 Working with Amazon SQS Message
					Queues in the AWS SDK for Java 2.x Developer Guide.
		
			This guide also includes examples of additional Amazon SQS features, such as:
			
				 
				 
				 
			
					Using server-side encryption with Amazon SQS queues
				
					Configuring tags for an Amazon SQS queue 
				
					Sending message attributes to an
			Amazon SQS queue
				
		Document ConventionsUsing the appropriate
				polling mode in Amazon SQSUsing server-side encryptionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideAmazon SQS standard queue quotasThe following table lists quotas related to standard queues.
					
						Quota
						Description
					
				
					
						Delay queue
						The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.
					
					
						Listed queues
						1,000 queues per ListQueues request.
					
					
						Long polling wait time
						The maximum long polling wait time is 20 seconds.
					
					
						Messages per queue (backlog)
						The number of messages that an Amazon SQS queue can store is unlimited.
					
					
						Messages per queue (in flight)
						
							For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 in flight messages (received from a queue by a consumer, but not yet deleted from the queue).
    If you reach this quota while using short polling, Amazon SQS returns the OverLimit error message. If you use long polling, Amazon SQS returns no error messages.
    To avoid reaching the quota, you should delete messages from the queue after they're processed. You can also increase the number of queues you use to process your messages.
    To request a quota increase, submit a support request.
						
					
					
						Queue name
						
							A queue name can have up to 80 characters. The following characters are accepted: alphanumeric characters, hyphens (-), and underscores (_).
							NoteQueue names are case-sensitive (for example,
										Test-queue and test-queue are
									different queues).
						
					
					
						Queue tag
						We don't recommend adding more than 50 tags to a queue. Tagging supports Unicode characters in UTF-8.
					
					
						The tag Key is required, but the tag Value
							is optional.
					
					
						The tag Key and tag Value are
							case-sensitive.
					
					
						The tag Key and tag Value can include
							Unicode alphanumeric characters in UTF-8 and whitespaces. The following
							special characters are allowed: _ . : / = + - @
					
					
						The tag Key or Value must not include the
							reserved prefix aws: (you can't delete tag keys or values
							with this prefix).
					
					
						The maximum tag Key length is 128 Unicode characters in
							UTF-8. The tag Key must not be empty or null.
					
					
						The maximum tag Value length is 256 Unicode characters
							in UTF-8. The tag Value may be empty or null.
					
					
						Tagging actions are limited to 30 TPS per AWS account. If your application requires a higher throughput, submit a request.
					
				Document ConventionsFIFO queue quotasMessage quotasDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideAmazon SQS best practicesAmazon SQS manages and processes message queues, enabling different parts of an
		application to exchange messages reliably and at scale. This topic covers key operational
		best practices, including using long polling to reduce empty responses, implementing
		dead-letter queues to handle processing errors, and optimizing queue permissions for
		security.TopicsError handling and problematic messagesMessage deduplication and groupingMessage processing and timingDocument ConventionsMessage attributesError handling and problematic messagesDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideEvent source mappings and triggersBatching behaviorProvisioned modeEvent source mapping APIHow Lambda processes records from stream and queue-based event sourcesAn event source mapping is a Lambda resource that reads items from stream and queue-based
    services and invokes a function with batches of records. Within an event source mapping, resources called
    event pollers actively poll for new messages and invoke functions. By default, Lambda automatically
    scales event pollers, but for certain event source types, you can use 
    provisioned mode to control the minimum and maximum number of event pollers dedicated to your event source mapping.The following services use event source mappings to invoke Lambda functions:
     
     
     
     
     
     
     
  
      Amazon DocumentDB (with MongoDB compatibility) (Amazon DocumentDB)
    
      Amazon DynamoDB
    
      Amazon Kinesis
    
      Amazon MQ
    
      Amazon Managed Streaming for Apache Kafka (Amazon MSK)
    
      Self-managed Apache Kafka
    
      Amazon Simple Queue Service (Amazon SQS)
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    How event source mappings differ from direct triggers
    Some AWS services can directly invoke Lambda functions using triggers. These services push events to Lambda, and the function is invoked immediately when the specified event occurs. Triggers are suitable for discrete events and real-time processing. When you create a trigger using the Lambda console, the console interacts with the corresponding AWS service to configure the event notification on that service. The trigger is actually stored and managed by the service that generates the events, not by Lambda. Here are some examples of services that use triggers to invoke Lambda
      functions:
    
       
       
       
    
        Amazon Simple Storage Service (Amazon S3): Invokes a function when an object is created, deleted, or modified in a bucket. For more information, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.
      
        Amazon Simple Notification Service (Amazon SNS): Invokes a function when a message is published to an SNS topic. For more information, see Tutorial: Using AWS Lambda with Amazon Simple Notification Service.
      
        Amazon API Gateway: Invokes a function when an API request is made to a specific endpoint. For more information, see Invoking a Lambda function using an Amazon API Gateway endpoint.
      
    Event source mappings are Lambda resources created and managed within the Lambda service.
      Event source mappings are designed for processing high-volume streaming data or messages from
      queues. Processing records from a stream or queue in batches is more efficient than processing
      records individually. 
   
    Batching behavior
    By default, an event source mapping batches
      records together into a single payload that Lambda sends to your function. To fine-tune batching behavior, you can
      configure a batching window (MaximumBatchingWindowInSeconds) and a batch size
      (BatchSize). A batching window is the maximum amount of time to gather records into a single payload.
      A batch size is the maximum number of records in a single batch. Lambda invokes your function when one of the
      following three criteria is met:
    
       
       
       
    
        The batching window reaches its maximum value. Default batching window behavior 
          varies depending on the specific event source.
        
           
           
        For Kinesis, DynamoDB, and Amazon SQS event sources: The default batching
            window is 0 seconds. This means that Lambda invokes your function as soon as records are available. To set a batching window, configure MaximumBatchingWindowInSeconds. You can 
            set this parameter to any value from 0 to 300 seconds in increments of 1 second. If you configure a batching window, the 
            next window begins as soon as the previous function invocation completes.For Amazon MSK, self-managed Apache Kafka, Amazon MQ, and Amazon DocumentDB event sources: The default batching
            window is 500 ms. You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to
            300 seconds in increments of seconds. A batching window begins as soon as the first record arrives.
            NoteBecause you can only change MaximumBatchingWindowInSeconds in increments of seconds, you
                cannot revert to the 500 ms default batching window after you have changed it. To restore the default
                batching window, you must create a new event source mapping.
          
      
        The batch size is met. The minimum batch size is 1. The default and
          maximum batch size depend on the event source. For details about these values, see the BatchSize specification for the CreateEventSourceMapping API
          operation.
      
        The payload size reaches 6 MB. You cannot modify this limit.
      
    The following diagram illustrates these three conditions. Suppose a batching window begins at t = 7
      seconds. In the first scenario, the batching window reaches its 40 second maximum at t = 47 seconds after
      accumulating 5 records. In the second scenario, the batch size reaches 10 before the batching window expires,
      so the batching window ends early. In the third scenario, the maximum payload size is reached before the batching
      window expires, so the batching window ends early.
    
       
        
       
       
    
    We recommend that you test with different batch and record sizes so that the polling frequency
      of each event source is tuned to how quickly your function is able to complete its task. The
      CreateEventSourceMapping BatchSize parameter controls the maximum number of
      records that can be sent to your function with each invoke. A larger batch size can often more efficiently
      absorb the invoke overhead across a larger set of records, increasing your throughput.
    Lambda doesn't wait for any configured extensions to complete
      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda
      processes the next batch of records. This can cause throttling issues if you breach any of your account's 
      concurrency settings or limits. To detect whether this is a
      potential issue, monitor your functions and check whether you're seeing higher
      concurrency metrics than expected for your event
      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage
      than the number of shards. This can be true even for Lambda functions without extensions.
    By default, if your function returns an error, the event source mapping reprocesses the entire batch until the
      function succeeds, or the items in the batch expire. To ensure in-order processing, the event source mapping
      pauses processing for the affected shard until the error is resolved. For stream sources (DynamoDB and Kinesis),
      you can configure the maximum number of times that Lambda retries when your function returns an error.
      Service errors or throttles where the batch does not reach your function do not count toward retry
      attempts. You can also configure the event source mapping to send an invocation record to a
      destination when it discards an event batch.
   
    Provisioned mode
    Lambda event source mappings use event pollers to poll your event source for new messages. By default,
      Lambda manages the autoscaling of these pollers depending on message volume. When message traffic increases,
      Lambda automatically increases the number of event pollers to handle the load, and reduces them when
      traffic decreases.
    In provisioned mode, you can fine-tune the throughput of your event source mapping by defining
      minimum and maximum limits for the number of provisioned event pollers. Lambda then scales your event
      source mapping between the minimum and maximum number of event pollers in a responsive manner. These
      provisioned event pollers are dedicated to your event source mapping, enhancing your ability to handle
      unpredictable spikes in events.
    In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.
    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.
    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,
    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,
    see AWS Lambda pricing.
    Provisioned mode is supported only for Amazon MSK and self-managed Apache Kafka event sources. While concurrency settings
      give you control over the scaling of your function, provisioned mode gives you control over the
      throughput of your event source mapping. To ensure maximum performance, you may need to adjust both
      settings independently. For details about configuring provisioned mode, see the following sections:
    
       
       
    
        Configuring provisioned mode for Amazon MSK
          event source mappings
      
        Configuring provisioned mode for self-managed Apache Kafka
          event source mappings
      
    After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring
    the ProvisionedPollers metric. For more information, see Event source mapping metrics.
   
    Event source mapping API
     To manage an event source with the AWS Command Line Interface (AWS CLI) or an AWS SDK, you can use the following API operations:
    
        
       
       
       
       
    
        CreateEventSourceMapping
      
        ListEventSourceMappings
      
        GetEventSourceMapping
      
        UpdateEventSourceMapping
      
        DeleteEventSourceMapping
      
  Document ConventionsRetaining recordsEvent source mapping tagsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideEvent source mappings and triggersBatching behaviorProvisioned modeEvent source mapping APIHow Lambda processes records from stream and queue-based event sourcesAn event source mapping is a Lambda resource that reads items from stream and queue-based
    services and invokes a function with batches of records. Within an event source mapping, resources called
    event pollers actively poll for new messages and invoke functions. By default, Lambda automatically
    scales event pollers, but for certain event source types, you can use 
    provisioned mode to control the minimum and maximum number of event pollers dedicated to your event source mapping.The following services use event source mappings to invoke Lambda functions:
     
     
     
     
     
     
     
  
      Amazon DocumentDB (with MongoDB compatibility) (Amazon DocumentDB)
    
      Amazon DynamoDB
    
      Amazon Kinesis
    
      Amazon MQ
    
      Amazon Managed Streaming for Apache Kafka (Amazon MSK)
    
      Self-managed Apache Kafka
    
      Amazon Simple Queue Service (Amazon SQS)
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    How event source mappings differ from direct triggers
    Some AWS services can directly invoke Lambda functions using triggers. These services push events to Lambda, and the function is invoked immediately when the specified event occurs. Triggers are suitable for discrete events and real-time processing. When you create a trigger using the Lambda console, the console interacts with the corresponding AWS service to configure the event notification on that service. The trigger is actually stored and managed by the service that generates the events, not by Lambda. Here are some examples of services that use triggers to invoke Lambda
      functions:
    
       
       
       
    
        Amazon Simple Storage Service (Amazon S3): Invokes a function when an object is created, deleted, or modified in a bucket. For more information, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.
      
        Amazon Simple Notification Service (Amazon SNS): Invokes a function when a message is published to an SNS topic. For more information, see Tutorial: Using AWS Lambda with Amazon Simple Notification Service.
      
        Amazon API Gateway: Invokes a function when an API request is made to a specific endpoint. For more information, see Invoking a Lambda function using an Amazon API Gateway endpoint.
      
    Event source mappings are Lambda resources created and managed within the Lambda service.
      Event source mappings are designed for processing high-volume streaming data or messages from
      queues. Processing records from a stream or queue in batches is more efficient than processing
      records individually. 
   
    Batching behavior
    By default, an event source mapping batches
      records together into a single payload that Lambda sends to your function. To fine-tune batching behavior, you can
      configure a batching window (MaximumBatchingWindowInSeconds) and a batch size
      (BatchSize). A batching window is the maximum amount of time to gather records into a single payload.
      A batch size is the maximum number of records in a single batch. Lambda invokes your function when one of the
      following three criteria is met:
    
       
       
       
    
        The batching window reaches its maximum value. Default batching window behavior 
          varies depending on the specific event source.
        
           
           
        For Kinesis, DynamoDB, and Amazon SQS event sources: The default batching
            window is 0 seconds. This means that Lambda invokes your function as soon as records are available. To set a batching window, configure MaximumBatchingWindowInSeconds. You can 
            set this parameter to any value from 0 to 300 seconds in increments of 1 second. If you configure a batching window, the 
            next window begins as soon as the previous function invocation completes.For Amazon MSK, self-managed Apache Kafka, Amazon MQ, and Amazon DocumentDB event sources: The default batching
            window is 500 ms. You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to
            300 seconds in increments of seconds. A batching window begins as soon as the first record arrives.
            NoteBecause you can only change MaximumBatchingWindowInSeconds in increments of seconds, you
                cannot revert to the 500 ms default batching window after you have changed it. To restore the default
                batching window, you must create a new event source mapping.
          
      
        The batch size is met. The minimum batch size is 1. The default and
          maximum batch size depend on the event source. For details about these values, see the BatchSize specification for the CreateEventSourceMapping API
          operation.
      
        The payload size reaches 6 MB. You cannot modify this limit.
      
    The following diagram illustrates these three conditions. Suppose a batching window begins at t = 7
      seconds. In the first scenario, the batching window reaches its 40 second maximum at t = 47 seconds after
      accumulating 5 records. In the second scenario, the batch size reaches 10 before the batching window expires,
      so the batching window ends early. In the third scenario, the maximum payload size is reached before the batching
      window expires, so the batching window ends early.
    
       
        
       
       
    
    We recommend that you test with different batch and record sizes so that the polling frequency
      of each event source is tuned to how quickly your function is able to complete its task. The
      CreateEventSourceMapping BatchSize parameter controls the maximum number of
      records that can be sent to your function with each invoke. A larger batch size can often more efficiently
      absorb the invoke overhead across a larger set of records, increasing your throughput.
    Lambda doesn't wait for any configured extensions to complete
      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda
      processes the next batch of records. This can cause throttling issues if you breach any of your account's 
      concurrency settings or limits. To detect whether this is a
      potential issue, monitor your functions and check whether you're seeing higher
      concurrency metrics than expected for your event
      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage
      than the number of shards. This can be true even for Lambda functions without extensions.
    By default, if your function returns an error, the event source mapping reprocesses the entire batch until the
      function succeeds, or the items in the batch expire. To ensure in-order processing, the event source mapping
      pauses processing for the affected shard until the error is resolved. For stream sources (DynamoDB and Kinesis),
      you can configure the maximum number of times that Lambda retries when your function returns an error.
      Service errors or throttles where the batch does not reach your function do not count toward retry
      attempts. You can also configure the event source mapping to send an invocation record to a
      destination when it discards an event batch.
   
    Provisioned mode
    Lambda event source mappings use event pollers to poll your event source for new messages. By default,
      Lambda manages the autoscaling of these pollers depending on message volume. When message traffic increases,
      Lambda automatically increases the number of event pollers to handle the load, and reduces them when
      traffic decreases.
    In provisioned mode, you can fine-tune the throughput of your event source mapping by defining
      minimum and maximum limits for the number of provisioned event pollers. Lambda then scales your event
      source mapping between the minimum and maximum number of event pollers in a responsive manner. These
      provisioned event pollers are dedicated to your event source mapping, enhancing your ability to handle
      unpredictable spikes in events.
    In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.
    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.
    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,
    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,
    see AWS Lambda pricing.
    Provisioned mode is supported only for Amazon MSK and self-managed Apache Kafka event sources. While concurrency settings
      give you control over the scaling of your function, provisioned mode gives you control over the
      throughput of your event source mapping. To ensure maximum performance, you may need to adjust both
      settings independently. For details about configuring provisioned mode, see the following sections:
    
       
       
    
        Configuring provisioned mode for Amazon MSK
          event source mappings
      
        Configuring provisioned mode for self-managed Apache Kafka
          event source mappings
      
    After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring
    the ProvisionedPollers metric. For more information, see Event source mapping metrics.
   
    Event source mapping API
     To manage an event source with the AWS Command Line Interface (AWS CLI) or an AWS SDK, you can use the following API operations:
    
        
       
       
       
       
    
        CreateEventSourceMapping
      
        ListEventSourceMappings
      
        GetEventSourceMapping
      
        UpdateEventSourceMapping
      
        DeleteEventSourceMapping
      
  Document ConventionsRetaining recordsEvent source mapping tagsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideEvent source mappings and triggersBatching behaviorProvisioned modeEvent source mapping APIHow Lambda processes records from stream and queue-based event sourcesAn event source mapping is a Lambda resource that reads items from stream and queue-based
    services and invokes a function with batches of records. Within an event source mapping, resources called
    event pollers actively poll for new messages and invoke functions. By default, Lambda automatically
    scales event pollers, but for certain event source types, you can use 
    provisioned mode to control the minimum and maximum number of event pollers dedicated to your event source mapping.The following services use event source mappings to invoke Lambda functions:
     
     
     
     
     
     
     
  
      Amazon DocumentDB (with MongoDB compatibility) (Amazon DocumentDB)
    
      Amazon DynamoDB
    
      Amazon Kinesis
    
      Amazon MQ
    
      Amazon Managed Streaming for Apache Kafka (Amazon MSK)
    
      Self-managed Apache Kafka
    
      Amazon Simple Queue Service (Amazon SQS)
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    How event source mappings differ from direct triggers
    Some AWS services can directly invoke Lambda functions using triggers. These services push events to Lambda, and the function is invoked immediately when the specified event occurs. Triggers are suitable for discrete events and real-time processing. When you create a trigger using the Lambda console, the console interacts with the corresponding AWS service to configure the event notification on that service. The trigger is actually stored and managed by the service that generates the events, not by Lambda. Here are some examples of services that use triggers to invoke Lambda
      functions:
    
       
       
       
    
        Amazon Simple Storage Service (Amazon S3): Invokes a function when an object is created, deleted, or modified in a bucket. For more information, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.
      
        Amazon Simple Notification Service (Amazon SNS): Invokes a function when a message is published to an SNS topic. For more information, see Tutorial: Using AWS Lambda with Amazon Simple Notification Service.
      
        Amazon API Gateway: Invokes a function when an API request is made to a specific endpoint. For more information, see Invoking a Lambda function using an Amazon API Gateway endpoint.
      
    Event source mappings are Lambda resources created and managed within the Lambda service.
      Event source mappings are designed for processing high-volume streaming data or messages from
      queues. Processing records from a stream or queue in batches is more efficient than processing
      records individually. 
   
    Batching behavior
    By default, an event source mapping batches
      records together into a single payload that Lambda sends to your function. To fine-tune batching behavior, you can
      configure a batching window (MaximumBatchingWindowInSeconds) and a batch size
      (BatchSize). A batching window is the maximum amount of time to gather records into a single payload.
      A batch size is the maximum number of records in a single batch. Lambda invokes your function when one of the
      following three criteria is met:
    
       
       
       
    
        The batching window reaches its maximum value. Default batching window behavior 
          varies depending on the specific event source.
        
           
           
        For Kinesis, DynamoDB, and Amazon SQS event sources: The default batching
            window is 0 seconds. This means that Lambda invokes your function as soon as records are available. To set a batching window, configure MaximumBatchingWindowInSeconds. You can 
            set this parameter to any value from 0 to 300 seconds in increments of 1 second. If you configure a batching window, the 
            next window begins as soon as the previous function invocation completes.For Amazon MSK, self-managed Apache Kafka, Amazon MQ, and Amazon DocumentDB event sources: The default batching
            window is 500 ms. You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to
            300 seconds in increments of seconds. A batching window begins as soon as the first record arrives.
            NoteBecause you can only change MaximumBatchingWindowInSeconds in increments of seconds, you
                cannot revert to the 500 ms default batching window after you have changed it. To restore the default
                batching window, you must create a new event source mapping.
          
      
        The batch size is met. The minimum batch size is 1. The default and
          maximum batch size depend on the event source. For details about these values, see the BatchSize specification for the CreateEventSourceMapping API
          operation.
      
        The payload size reaches 6 MB. You cannot modify this limit.
      
    The following diagram illustrates these three conditions. Suppose a batching window begins at t = 7
      seconds. In the first scenario, the batching window reaches its 40 second maximum at t = 47 seconds after
      accumulating 5 records. In the second scenario, the batch size reaches 10 before the batching window expires,
      so the batching window ends early. In the third scenario, the maximum payload size is reached before the batching
      window expires, so the batching window ends early.
    
       
        
       
       
    
    We recommend that you test with different batch and record sizes so that the polling frequency
      of each event source is tuned to how quickly your function is able to complete its task. The
      CreateEventSourceMapping BatchSize parameter controls the maximum number of
      records that can be sent to your function with each invoke. A larger batch size can often more efficiently
      absorb the invoke overhead across a larger set of records, increasing your throughput.
    Lambda doesn't wait for any configured extensions to complete
      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda
      processes the next batch of records. This can cause throttling issues if you breach any of your account's 
      concurrency settings or limits. To detect whether this is a
      potential issue, monitor your functions and check whether you're seeing higher
      concurrency metrics than expected for your event
      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage
      than the number of shards. This can be true even for Lambda functions without extensions.
    By default, if your function returns an error, the event source mapping reprocesses the entire batch until the
      function succeeds, or the items in the batch expire. To ensure in-order processing, the event source mapping
      pauses processing for the affected shard until the error is resolved. For stream sources (DynamoDB and Kinesis),
      you can configure the maximum number of times that Lambda retries when your function returns an error.
      Service errors or throttles where the batch does not reach your function do not count toward retry
      attempts. You can also configure the event source mapping to send an invocation record to a
      destination when it discards an event batch.
   
    Provisioned mode
    Lambda event source mappings use event pollers to poll your event source for new messages. By default,
      Lambda manages the autoscaling of these pollers depending on message volume. When message traffic increases,
      Lambda automatically increases the number of event pollers to handle the load, and reduces them when
      traffic decreases.
    In provisioned mode, you can fine-tune the throughput of your event source mapping by defining
      minimum and maximum limits for the number of provisioned event pollers. Lambda then scales your event
      source mapping between the minimum and maximum number of event pollers in a responsive manner. These
      provisioned event pollers are dedicated to your event source mapping, enhancing your ability to handle
      unpredictable spikes in events.
    In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.
    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.
    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,
    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,
    see AWS Lambda pricing.
    Provisioned mode is supported only for Amazon MSK and self-managed Apache Kafka event sources. While concurrency settings
      give you control over the scaling of your function, provisioned mode gives you control over the
      throughput of your event source mapping. To ensure maximum performance, you may need to adjust both
      settings independently. For details about configuring provisioned mode, see the following sections:
    
       
       
    
        Configuring provisioned mode for Amazon MSK
          event source mappings
      
        Configuring provisioned mode for self-managed Apache Kafka
          event source mappings
      
    After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring
    the ProvisionedPollers metric. For more information, see Event source mapping metrics.
   
    Event source mapping API
     To manage an event source with the AWS Command Line Interface (AWS CLI) or an AWS SDK, you can use the following API operations:
    
        
       
       
       
       
    
        CreateEventSourceMapping
      
        ListEventSourceMappings
      
        GetEventSourceMapping
      
        UpdateEventSourceMapping
      
        DeleteEventSourceMapping
      
  Document ConventionsRetaining recordsEvent source mapping tagsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideEvent source mappings and triggersBatching behaviorProvisioned modeEvent source mapping APIHow Lambda processes records from stream and queue-based event sourcesAn event source mapping is a Lambda resource that reads items from stream and queue-based
    services and invokes a function with batches of records. Within an event source mapping, resources called
    event pollers actively poll for new messages and invoke functions. By default, Lambda automatically
    scales event pollers, but for certain event source types, you can use 
    provisioned mode to control the minimum and maximum number of event pollers dedicated to your event source mapping.The following services use event source mappings to invoke Lambda functions:
     
     
     
     
     
     
     
  
      Amazon DocumentDB (with MongoDB compatibility) (Amazon DocumentDB)
    
      Amazon DynamoDB
    
      Amazon Kinesis
    
      Amazon MQ
    
      Amazon Managed Streaming for Apache Kafka (Amazon MSK)
    
      Self-managed Apache Kafka
    
      Amazon Simple Queue Service (Amazon SQS)
    WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    How event source mappings differ from direct triggers
    Some AWS services can directly invoke Lambda functions using triggers. These services push events to Lambda, and the function is invoked immediately when the specified event occurs. Triggers are suitable for discrete events and real-time processing. When you create a trigger using the Lambda console, the console interacts with the corresponding AWS service to configure the event notification on that service. The trigger is actually stored and managed by the service that generates the events, not by Lambda. Here are some examples of services that use triggers to invoke Lambda
      functions:
    
       
       
       
    
        Amazon Simple Storage Service (Amazon S3): Invokes a function when an object is created, deleted, or modified in a bucket. For more information, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.
      
        Amazon Simple Notification Service (Amazon SNS): Invokes a function when a message is published to an SNS topic. For more information, see Tutorial: Using AWS Lambda with Amazon Simple Notification Service.
      
        Amazon API Gateway: Invokes a function when an API request is made to a specific endpoint. For more information, see Invoking a Lambda function using an Amazon API Gateway endpoint.
      
    Event source mappings are Lambda resources created and managed within the Lambda service.
      Event source mappings are designed for processing high-volume streaming data or messages from
      queues. Processing records from a stream or queue in batches is more efficient than processing
      records individually. 
   
    Batching behavior
    By default, an event source mapping batches
      records together into a single payload that Lambda sends to your function. To fine-tune batching behavior, you can
      configure a batching window (MaximumBatchingWindowInSeconds) and a batch size
      (BatchSize). A batching window is the maximum amount of time to gather records into a single payload.
      A batch size is the maximum number of records in a single batch. Lambda invokes your function when one of the
      following three criteria is met:
    
       
       
       
    
        The batching window reaches its maximum value. Default batching window behavior 
          varies depending on the specific event source.
        
           
           
        For Kinesis, DynamoDB, and Amazon SQS event sources: The default batching
            window is 0 seconds. This means that Lambda invokes your function as soon as records are available. To set a batching window, configure MaximumBatchingWindowInSeconds. You can 
            set this parameter to any value from 0 to 300 seconds in increments of 1 second. If you configure a batching window, the 
            next window begins as soon as the previous function invocation completes.For Amazon MSK, self-managed Apache Kafka, Amazon MQ, and Amazon DocumentDB event sources: The default batching
            window is 500 ms. You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to
            300 seconds in increments of seconds. A batching window begins as soon as the first record arrives.
            NoteBecause you can only change MaximumBatchingWindowInSeconds in increments of seconds, you
                cannot revert to the 500 ms default batching window after you have changed it. To restore the default
                batching window, you must create a new event source mapping.
          
      
        The batch size is met. The minimum batch size is 1. The default and
          maximum batch size depend on the event source. For details about these values, see the BatchSize specification for the CreateEventSourceMapping API
          operation.
      
        The payload size reaches 6 MB. You cannot modify this limit.
      
    The following diagram illustrates these three conditions. Suppose a batching window begins at t = 7
      seconds. In the first scenario, the batching window reaches its 40 second maximum at t = 47 seconds after
      accumulating 5 records. In the second scenario, the batch size reaches 10 before the batching window expires,
      so the batching window ends early. In the third scenario, the maximum payload size is reached before the batching
      window expires, so the batching window ends early.
    
       
        
       
       
    
    We recommend that you test with different batch and record sizes so that the polling frequency
      of each event source is tuned to how quickly your function is able to complete its task. The
      CreateEventSourceMapping BatchSize parameter controls the maximum number of
      records that can be sent to your function with each invoke. A larger batch size can often more efficiently
      absorb the invoke overhead across a larger set of records, increasing your throughput.
    Lambda doesn't wait for any configured extensions to complete
      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda
      processes the next batch of records. This can cause throttling issues if you breach any of your account's 
      concurrency settings or limits. To detect whether this is a
      potential issue, monitor your functions and check whether you're seeing higher
      concurrency metrics than expected for your event
      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage
      than the number of shards. This can be true even for Lambda functions without extensions.
    By default, if your function returns an error, the event source mapping reprocesses the entire batch until the
      function succeeds, or the items in the batch expire. To ensure in-order processing, the event source mapping
      pauses processing for the affected shard until the error is resolved. For stream sources (DynamoDB and Kinesis),
      you can configure the maximum number of times that Lambda retries when your function returns an error.
      Service errors or throttles where the batch does not reach your function do not count toward retry
      attempts. You can also configure the event source mapping to send an invocation record to a
      destination when it discards an event batch.
   
    Provisioned mode
    Lambda event source mappings use event pollers to poll your event source for new messages. By default,
      Lambda manages the autoscaling of these pollers depending on message volume. When message traffic increases,
      Lambda automatically increases the number of event pollers to handle the load, and reduces them when
      traffic decreases.
    In provisioned mode, you can fine-tune the throughput of your event source mapping by defining
      minimum and maximum limits for the number of provisioned event pollers. Lambda then scales your event
      source mapping between the minimum and maximum number of event pollers in a responsive manner. These
      provisioned event pollers are dedicated to your event source mapping, enhancing your ability to handle
      unpredictable spikes in events.
    In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.
    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.
    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,
    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,
    see AWS Lambda pricing.
    Provisioned mode is supported only for Amazon MSK and self-managed Apache Kafka event sources. While concurrency settings
      give you control over the scaling of your function, provisioned mode gives you control over the
      throughput of your event source mapping. To ensure maximum performance, you may need to adjust both
      settings independently. For details about configuring provisioned mode, see the following sections:
    
       
       
    
        Configuring provisioned mode for Amazon MSK
          event source mappings
      
        Configuring provisioned mode for self-managed Apache Kafka
          event source mappings
      
    After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring
    the ProvisionedPollers metric. For more information, see Event source mapping metrics.
   
    Event source mapping API
     To manage an event source with the AWS Command Line Interface (AWS CLI) or an AWS SDK, you can use the following API operations:
    
        
       
       
       
       
    
        CreateEventSourceMapping
      
        ListEventSourceMappings
      
        GetEventSourceMapping
      
        UpdateEventSourceMapping
      
        DeleteEventSourceMapping
      
  Document ConventionsRetaining recordsEvent source mapping tagsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideExample Amazon DocumentDB eventPrerequisites and permissionsConfigure network securityCreating an Amazon DocumentDB event source mapping (console)Creating an Amazon DocumentDB event source mapping (SDK or CLI)Polling and stream starting positionsMonitoring your Amazon DocumentDB event sourceProcess Amazon DocumentDB events with LambdaYou can use a Lambda function to process events in an Amazon DocumentDB (with MongoDB compatibility) change stream by configuring an
    Amazon DocumentDB cluster as an event source. Then, you can automate event-driven workloads by invoking your Lambda function
    each time that data changes with your Amazon DocumentDB cluster.NoteLambda supports version 4.0 and 5.0 of Amazon DocumentDB only. Lambda doesn't support version 3.6.Also, for event source mappings, Lambda supports instance-based clusters and
      regional clusters only. Lambda doesn't support
      
        elastic clusters or
      
        global clusters. This limitation doesn't apply when using Lambda as a client to connect to Amazon DocumentDB. Lambda can connect to 
    all cluster types to perform CRUD operations.Lambda processes events from Amazon DocumentDB change streams sequentially in the order in which they arrive.
    Because of this, your function can handle only one concurrent invocation from Amazon DocumentDB at a time.
    To monitor your function, you can track its concurrency metrics.WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.TopicsExample Amazon DocumentDB eventPrerequisites and permissionsConfigure network securityCreating an Amazon DocumentDB event source mapping (console)Creating an Amazon DocumentDB event source mapping (SDK or CLI)Polling and stream starting positionsMonitoring your Amazon DocumentDB event sourceTutorial: Using AWS Lambda with Amazon DocumentDB Streams
    Example Amazon DocumentDB event
    {
    "eventSourceArn": "arn:aws:rds:us-east-1:123456789012:cluster:canaryclusterb2a659a2-qo5tcmqkcl03",
    "events": [
        {
            "event": {
                "_id": {
                    "_data": "0163eeb6e7000000090100000009000041e1"
                },
                "clusterTime": {
                    "$timestamp": {
                        "t": 1676588775,
                        "i": 9
                    }
                },
                "documentKey": {
                    "_id": {
                        "$oid": "63eeb6e7d418cd98afb1c1d7"
                    }
                },
                "fullDocument": {
                    "_id": {
                        "$oid": "63eeb6e7d418cd98afb1c1d7"
                    },
                    "anyField": "sampleValue"
                },
                "ns": {
                    "db": "test_database",
                    "coll": "test_collection"
                },
                "operationType": "insert"
            }
        }
    ],
    "eventSource": "aws:docdb"
}
    For more information about the events in this example and their shapes, see Change Events on the MongoDB
      Documentation website.
   
    Prerequisites and permissions
    Before you can use Amazon DocumentDB as an event source for your Lambda function, note the following prerequisites. You
      must:
    
       
       
       
       
       
       
       
    
        Have an existing Amazon DocumentDB cluster in the same AWS account and AWS Region as your
            function. If you don't have an existing cluster, you can create one by following the steps in
            Get Started with
              Amazon DocumentDB in the Amazon DocumentDB Developer Guide. Alternatively, the first set of steps in
            Tutorial: Using AWS Lambda with Amazon DocumentDB Streams guide you
            through creating an Amazon DocumentDB cluster with all the necessary prerequisites.
      
        Allow Lambda to access the Amazon Virtual Private Cloud (Amazon VPC) resources associated with your Amazon DocumentDB
            cluster. For more information, see Configure network security.
      
        Enable TLS on your Amazon DocumentDB cluster. This is the default setting. If you
          disable TLS, then Lambda cannot communicate with your cluster.
      
        Activate change streams on your Amazon DocumentDB cluster. For more information,
          see Using Change
            Streams with Amazon DocumentDB in the Amazon DocumentDB Developer Guide.
      
        Provide Lambda with credentials to access your Amazon DocumentDB cluster. When
          setting up the event source, provide the AWS Secrets Manager key that contains the authentication
          details (username and password) required to access your cluster. To provide this key during setup, do either
          of the following:
        
           
           
        
            If you're using the Lambda console for setup, then provide the key in the Secrets manager
                key field.
          
            If you're using the AWS Command Line Interface (AWS CLI) for setup, then provide this key in the
                source-access-configurations option. You can include this option with either the create-event-source-mapping command or the update-event-source-mapping command. For example:
            aws lambda create-event-source-mapping \
    ...
    --source-access-configurations  '[{"Type":"BASIC_AUTH","URI":"arn:aws:secretsmanager:us-west-2:123456789012:secret:DocDBSecret-AbC4E6"}]' \
    ...
          
      
        Grant Lambda permissions to manage resources related to your Amazon DocumentDB
            stream. Manually add the following permissions to your function's execution role:
        
           
           
           
           
           
           
           
           
           
           
           
        
            rds:DescribeDBClusters
          
            rds:DescribeDBClusterParameters
          
            rds:DescribeDBSubnetGroups
          
            ec2:CreateNetworkInterface
          
            ec2:DescribeNetworkInterfaces
          
            ec2:DescribeVpcs
          
            ec2:DeleteNetworkInterface
          
            ec2:DescribeSubnets
          
            ec2:DescribeSecurityGroups
          
            kms:Decrypt
          
            secretsmanager:GetSecretValue
          
      
        Keep the size of Amazon DocumentDB change stream events that you send to Lambda under 6
            MB. Lambda supports payload sizes of up to 6 MB. If your change stream tries to send Lambda an
          event larger than 6 MB, then Lambda drops the message and emits the OversizedRecordCount metric.
          Lambda emits all metrics on a best-effort basis.
      
    NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes,
      event source mappings for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with
      maximum timeout limits of 14 minutes. This constraint ensures that the event source mapping can properly
      handle function errors and retries.
   
    Configure network security
    
    

     
        To give Lambda full access to Amazon DocumentDB through your event source mapping, either your cluster must use a public endpoint 
            (public IP address), or you must provide access to the Amazon VPC you created the cluster in.
            
        When you use Amazon DocumentDB with Lambda, create AWS PrivateLink VPC endpoints that provide your function
            access to the resources in your Amazon VPC.
        
        NoteAWS PrivateLink VPC endpoints are required for functions with event source mappings that use the default (on-demand) mode
                for event pollers. If your event source mapping uses 
                provisioned mode, you don't need to configure AWS PrivateLink VPC endpoints.
        
        Create an endpoint to provide access to the following resources:
        
             
             
             
        
                
                    Lambda — Create an endpoint for the Lambda service principal.
                
            
                
                    AWS STS — Create an endpoint for the AWS STS in order for a service principal to assume a role on your behalf.
                
            
                
                    Secrets Manager — If your cluster uses Secrets Manager to store credentials, create an endpoint for Secrets Manager.
                
            
        
        
        Alternatively, configure a NAT gateway on each public subnet in the Amazon VPC. For more information, 
            see Enable internet access for VPC-connected Lambda functions.
        
        When you create an event source mapping for Amazon DocumentDB, Lambda checks whether Elastic Network Interfaces (ENIs) 
            are already present for the subnets and security groups configured for your Amazon VPC. If Lambda finds existing ENIs, it 
            attempts to re-use them. Otherwise, Lambda creates new ENIs to connect to the event source and invoke your function.
        NoteLambda functions always run inside VPCs owned by the Lambda service. Your function's VPC configuration
                does not affect the event source mapping. Only the networking configuration of the event source's determines 
                how Lambda connects to your event source.
     
    

     
        
        Configure the security groups for the Amazon VPC containing your cluster. By default,
            Amazon DocumentDB uses the following ports: 27017.
        
        
             
             
             
        
                Inbound rules – Allow all traffic on the default broker port for the security group associated with your event source.
                    Alternatively, you can use a self-referencing security group rule to allow access from instances within the same security group.
            
                Outbound rules – Allow all traffic on port 443 for external destinations if your function needs to communicate with AWS services.
                    Alternatively, you can also use a self-referencing security group rule to limit access to the broker if you don't need to communicate with other AWS services.
            
                Amazon VPC endpoint inbound rules — If you are using an Amazon VPC endpoint, the security group associated with your Amazon VPC endpoint must allow inbound traffic
                    on port 443 from the cluster security group.
            
     
    

     
        If your cluster uses authentication, you can also restrict the endpoint policy for the Secrets Manager endpoint. 
            To call the Secrets Manager API, Lambda uses your function role, not the Lambda service principal.
        Example VPC endpoint policy — Secrets Manager endpoint{
      "Statement": [
          {
              "Action": "secretsmanager:GetSecretValue",
              "Effect": "Allow",
              "Principal": {
                  "AWS": [
                      "arn:aws::iam::123456789012:role/my-role"
                  ]
              },
              "Resource": "arn:aws::secretsmanager:us-west-2:123456789012:secret:my-secret"
          }
      ]
  }
     

     
        When you use Amazon VPC endpoints, AWS routes your API calls to invoke your function using the endpoint's Elastic Network Interface (ENI).
            The Lambda service principal needs to call lambda:InvokeFunction on any roles and functions that use those ENIs.
        By default, Amazon VPC endpoints have open IAM policies that allow broad access to resources. Best practice is to restrict these
            policies to perform the needed actions using that endpoint. To ensure that your event source mapping is able to invoke your Lambda
            function, the VPC endpoint policy must allow the Lambda service principal to call sts:AssumeRole and
            lambda:InvokeFunction. Restricting your VPC endpoint policies to allow only API calls originating within your organization
            prevents the event source mapping from functioning properly, so "Resource": "*" is required in these policies.
        The following example VPC endpoint policies show how to grant the required access to the Lambda service principal for the
            AWS STS and Lambda endpoints.
        Example VPC Endpoint policy — AWS STS endpoint{
      "Statement": [
          {
              "Action": "sts:AssumeRole",
              "Effect": "Allow",
              "Principal": {
                  "Service": [
                      "lambda.amazonaws.com"
                  ]
              },
              "Resource": "*"
          }
      ]
    }
        Example VPC Endpoint policy — Lambda endpoint{
      "Statement": [
          {
              "Action": "lambda:InvokeFunction",
              "Effect": "Allow",
              "Principal": {
                  "Service": [
                      "lambda.amazonaws.com"
                  ]
              },
              "Resource": "*"
          }
      ]
  }
     
    
 
    Creating an Amazon DocumentDB event source mapping (console)
    For a Lambda function to read from an Amazon DocumentDB cluster's change stream, create an event source mapping. This section describes how to do this from
      the Lambda console. For AWS SDK and AWS CLI instructions, see Creating an Amazon DocumentDB event source mapping (SDK or CLI).
    To create an Amazon DocumentDB event source mapping (console)Open the Functions page of the Lambda console.
        Choose the name of a function.
      
        Under Function overview, choose Add
          trigger.
      
        Under Trigger configuration, in the dropdown list, choose
          DocumentDB.
      
        Configure the required options, and then choose Add.
      
    Lambda supports the following options for Amazon DocumentDB event sources:
    
       
       
       
       
       
       
       
       
       
       
    
        DocumentDB cluster – Select an Amazon DocumentDB cluster.
      
        Activate trigger – Choose whether you want to activate the trigger
          immediately. If you select this check box, then your function immediately starts receiving traffic from the
          specified Amazon DocumentDB change stream upon creation of the event source mapping. We recommend that you clear the
          check box to create the event source mapping in a deactivated state for testing. After creation, you can
          activate the event source mapping at any time.
      
        Database name – Enter the name of a database within the cluster to
          consume.
      
        (Optional) Collection name – Enter the name of a collection within the
          database to consume. If you don't specify a collection, then Lambda listens to all events from each collection
          in the database.
      
        Batch size – Set the maximum number of messages to retrieve in a single batch,
          up to 10,000. The default batch size is 100.
      
        Starting position – Choose the position in the stream to start reading records
          from.
        
           
           
           
        
            Latest – Process only new records that are added to the stream. Your
              function starts processing records only after Lambda finishes creating your event source. This means that
              some records may be dropped until your event source is created successfully.
          
            Trim horizon – Process all records in the stream. Lambda uses the log
              retention duration of your cluster to determine where to start reading events from. Specifically, Lambda
              starts reading from current_time - log_retention_duration. Your change stream must already be
              active before this timestamp for Lambda to read all events properly.
          
            At timestamp – Process records starting from a specific time. Your change
              stream must already be active before the specified timestamp for Lambda to read all events properly.
          
      
        Authentication – Choose the authentication method for accessing the brokers in
          your cluster.
        
           
        
            BASIC_AUTH – With basic authentication, you must provide the Secrets Manager key
              that contains the credentials to access your cluster.
          
      
        Secrets Manager key – Choose the Secrets Manager key that contains the authentication details
          (username and password) required to access your Amazon DocumentDB cluster.
      
        (Optional) Batch window – Set the maximum amount of time in seconds to gather
          records before invoking your function, up to 300.
      
        (Optional) Full document configuration – For document update operations,
          choose what you want to send to the stream. The default value is Default, which means that for
          each change stream event, Amazon DocumentDB sends only a delta describing the changes made. For more information about
          this field, see FullDocument in the MongoDB Javadoc API documentation.
        
           
           
        
            Default – Lambda sends only a partial document describing the changes
              made.
          
            UpdateLookup – Lambda sends a delta describing the changes, along with a
              copy of the entire document.
          
      
   
    Creating an Amazon DocumentDB event source mapping (SDK or CLI)
    To create or manage an Amazon DocumentDB event source mapping with an AWS SDK, you can use the following API
      operations:
    
       
       
       
       
       
    
        CreateEventSourceMapping
      
        ListEventSourceMappings
      
        GetEventSourceMapping
      
        UpdateEventSourceMapping
      
        DeleteEventSourceMapping
      
    To create the event source mapping with the AWS CLI, use the create-event-source-mapping command. The following example uses this command to map a
      function named my-function to an Amazon DocumentDB change stream. The event source is specified by an Amazon
      Resource Name (ARN), with a batch size of 500, starting from the timestamp in Unix time. The command also
      specifies the Secrets Manager key that Lambda uses to connect to Amazon DocumentDB. Additionally, it includes
        document-db-event-source-config parameters that specify the database and the collection to read
      from.
    aws lambda create-event-source-mapping --function-name my-function \
    --event-source-arn arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy
    --batch-size 500 \
    --starting-position AT_TIMESTAMP \
    --starting-position-timestamp 1541139109 \
    --source-access-configurations '[{"Type":"BASIC_AUTH","URI":"arn:aws:secretsmanager:us-east-1:123456789012:secret:DocDBSecret-BAtjxi"}]' \
    --document-db-event-source-config '{"DatabaseName":"test_database", "CollectionName": "test_collection"}' \
    You should see output that looks like this:
    {
    "UUID": "2b733gdc-8ac3-cdf5-af3a-1827b3b11284",
    "BatchSize": 500,
    "DocumentDBEventSourceConfig": {
        "CollectionName": "test_collection",
        "DatabaseName": "test_database",
        "FullDocument": "Default"
    },
    "MaximumBatchingWindowInSeconds": 0,
    "EventSourceArn": "arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy",
    "FunctionArn": "arn:aws:lambda:us-west-2:123456789012:function:my-function",
    "LastModified": 1541348195.412,
    "LastProcessingResult": "No records processed",
    "State": "Creating",
    "StateTransitionReason": "User action"
}
    After creation, you can use the update-event-source-mapping command to update the settings for your Amazon DocumentDB event
      source. The following example updates the batch size to 1,000 and the batch window to 10 seconds. For this
      command, you need the UUID of your event source mapping, which you can retrieve using the
        list-event-source-mapping command or the Lambda console.
    aws lambda update-event-source-mapping --function-name my-function \
    --uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \
    --batch-size 1000 \
    --batch-window 10
    You should see this output that looks like this:
    {
    "UUID": "2b733gdc-8ac3-cdf5-af3a-1827b3b11284",
    "BatchSize": 500,
    "DocumentDBEventSourceConfig": {
        "CollectionName": "test_collection",
        "DatabaseName": "test_database",
        "FullDocument": "Default"
    },
    "MaximumBatchingWindowInSeconds": 0,
    "EventSourceArn": "arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy",
    "FunctionArn": "arn:aws:lambda:us-west-2:123456789012:function:my-function",
    "LastModified": 1541359182.919,
    "LastProcessingResult": "OK",
    "State": "Updating",
    "StateTransitionReason": "User action"
}
    Lambda updates settings asynchronously, so you may not see these changes in the output until the process
      completes. To view the current settings of your event source mapping, use the get-event-source-mapping command.
    aws lambda get-event-source-mapping --uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b
    You should see this output that looks like this:
    {
    "UUID": "2b733gdc-8ac3-cdf5-af3a-1827b3b11284",
    "DocumentDBEventSourceConfig": {
        "CollectionName": "test_collection",
        "DatabaseName": "test_database",
        "FullDocument": "Default"
    },
    "BatchSize": 1000,
    "MaximumBatchingWindowInSeconds": 10,
    "EventSourceArn": "arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy",
    "FunctionArn": "arn:aws:lambda:us-west-2:123456789012:function:my-function",
    "LastModified": 1541359182.919,
    "LastProcessingResult": "OK",
    "State": "Enabled",
    "StateTransitionReason": "User action"
}
    To delete your Amazon DocumentDB event source mapping, use the delete-event-source-mapping command.
    aws lambda delete-event-source-mapping \
    --uuid 2b733gdc-8ac3-cdf5-af3a-1827b3b11284
   
    Polling and stream starting positions
    Be aware that stream polling during event source mapping creation and updates is eventually consistent.
    
       
       
    
        During event source mapping creation, it may take several minutes to start polling events from the stream.
      
        During event source mapping updates, it may take several minutes to stop and restart polling events from the stream.
      
    This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could 
    miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON 
    or AT_TIMESTAMP.
   
    Monitoring your Amazon DocumentDB event source
    To help you monitor your Amazon DocumentDB event source, Lambda emits the IteratorAge metric when your
      function finishes processing a batch of records. Iterator age is the difference between the
      timestamp of the most recent event and the current timestamp. Essentially, the IteratorAge metric
      indicates how old the last processed record in the batch is. If your function is currently processing new events,
      then you can use the iterator age to estimate the latency between when a record is added and when your function
      processes it. An increasing trend in IteratorAge can indicate issues with your function.
      For more information, see Using CloudWatch metrics with Lambda.
    Amazon DocumentDB change streams aren't optimized to handle large time gaps between events. If your Amazon DocumentDB event source doesn't
      receive any events for an extended period of time, Lambda may disable the event source mapping. The length of
      this time period can vary from a few weeks to a few months depending on cluster size and other workloads.
    Lambda supports payloads of up to 6 MB. However, Amazon DocumentDB change stream events can be up to 16 MB in size. If
      your change stream tries to send Lambda a change stream event larger than 6 MB, then Lambda drops the message and
      emits the OversizedRecordCount metric. Lambda emits all metrics on a best-effort basis.
  Document ConventionsCloudFormationTutorialDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePolling and batching streamsPolling and stream starting positionsSimultaneous readersExample eventUsing AWS Lambda with Amazon DynamoDBNoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see 
    Amazon EventBridge Pipes.You can use an AWS Lambda function to process records in an Amazon DynamoDB
      stream. With DynamoDB Streams, you can trigger a Lambda function to perform additional work each time a DynamoDB table is
    updated.TopicsPolling and batching streamsPolling and stream starting positionsSimultaneous readers of a shard in DynamoDB StreamsExample eventProcess DynamoDB records with LambdaConfiguring partial batch response with DynamoDB and LambdaRetain discarded records for a DynamoDB event source in LambdaImplementing stateful DynamoDB stream processing in LambdaLambda parameters for Amazon DynamoDB event source mappingsUsing event filtering with a DynamoDB event sourceTutorial: Using AWS Lambda with Amazon DynamoDB streams
    Polling and batching streams
    
    Lambda polls shards in your DynamoDB stream for records at a base rate of 4 times per second. When records are
      available, Lambda invokes your function and waits for the result. If processing succeeds, Lambda resumes polling until
      it receives more records.
     By default, Lambda invokes your function as soon as records are available. If the batch
      that Lambda reads from the event source has only one record in it, Lambda sends only one record to the function. To avoid invoking the function
      with a small number of records, you can tell the event source to buffer records for up to 5 minutes by configuring a
        batching window. Before invoking the function, Lambda continues to read records from the event source
      until it has gathered a full batch, the batching window expires, or the batch reaches the payload limit of 6 MB. For more information,
      see Batching behavior.
      WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    Lambda doesn't wait for any configured extensions to complete
      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda
      processes the next batch of records. This can cause throttling issues if you breach any of your account's 
      concurrency settings or limits. To detect whether this is a
      potential issue, monitor your functions and check whether you're seeing higher
      concurrency metrics than expected for your event
      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage
      than the number of shards. This can be true even for Lambda functions without extensions.
    Configure the 
      ParallelizationFactor setting to process one shard of a DynamoDB stream with more than one Lambda invocation simultaneously. 
      You can specify the number of concurrent batches that Lambda polls from a shard via a parallelization factor from 1
      (default) to 10. For example, when you set ParallelizationFactor to 2, you can have 200 concurrent
      Lambda invocations at maximum to process 100 DynamoDB stream shards (though in practice, you may see different values
      for the ConcurrentExecutions metric). This helps scale up the processing throughput when the data volume
      is volatile and the IteratorAge is high. When you increase the number of concurrent batches per shard,
      Lambda still ensures in-order processing at the item (partition and sort key) level.
   
    Polling and stream starting positions
    Be aware that stream polling during event source mapping creation and updates is eventually consistent.
    
       
       
    
        During event source mapping creation, it may take several minutes to start polling events from the stream.
      
        During event source mapping updates, it may take several minutes to stop and restart polling events from the stream.
      
    This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could 
    miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON.
   
    Simultaneous readers of a shard in DynamoDB Streams
    For single-Region tables that are not global tables, you can design for up to two Lambda functions to read from the same DynamoDB Streams shard at the same time. Exceeding this limit can result in request throttling.
      For global tables, we recommend you limit the number of simultaneous functions to one to avoid request throttling.
   
    Example event
    {
  "Records": [
    {
      "eventID": "1",
      "eventVersion": "1.0",
      "dynamodb": {
        "Keys": {
          "Id": {
            "N": "101"
          }
        },
        "NewImage": {
          "Message": {
            "S": "New item!"
          },
          "Id": {
            "N": "101"
          }
        },
        "StreamViewType": "NEW_AND_OLD_IMAGES",
        "SequenceNumber": "111",
        "SizeBytes": 26
      },
      "awsRegion": "us-west-2",
      "eventName": "INSERT",
      "eventSourceARN": "arn:aws:dynamodb:us-east-2:123456789012:table/my-table/stream/2024-06-10T19:26:16.525",
      "eventSource": "aws:dynamodb"
    },
    {
      "eventID": "2",
      "eventVersion": "1.0",
      "dynamodb": {
        "OldImage": {
          "Message": {
            "S": "New item!"
          },
          "Id": {
            "N": "101"
          }
        },
        "SequenceNumber": "222",
        "Keys": {
          "Id": {
            "N": "101"
          }
        },
        "SizeBytes": 59,
        "NewImage": {
          "Message": {
            "S": "This item has changed"
          },
          "Id": {
            "N": "101"
          }
        },
        "StreamViewType": "NEW_AND_OLD_IMAGES"
      },
      "awsRegion": "us-west-2",
      "eventName": "MODIFY",
      "eventSourceARN": "arn:aws:dynamodb:us-east-2:123456789012:table/my-table/stream/2024-06-10T19:26:16.525",
      "eventSource": "aws:dynamodb"
    }
  ]}
  Document ConventionsTutorialCreate mappingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper Guide Polling and batching streams Example eventHow Lambda processes records from Amazon Kinesis Data StreamsYou can use a Lambda function to process records in an Amazon Kinesis data stream. You can map a Lambda function to a Kinesis Data Streams shared-throughput consumer (standard iterator), or to a
    dedicated-throughput consumer with enhanced fan-out. For standard iterators, Lambda polls each shard in your Kinesis stream for records using HTTP protocol. The event source mapping shares read throughput with other consumers of the shard. For details about Kinesis data streams, see Reading Data from
      Amazon Kinesis Data Streams.NoteKinesis charges for each shard and, for enhanced fan-out, data read from the stream. For pricing details, see
      Amazon Kinesis pricing.Topics Polling and batching streams Example eventProcess Amazon Kinesis Data Streams records with LambdaConfiguring partial batch response with Kinesis Data Streams and LambdaRetain discarded batch records for a Kinesis Data Streams event source in LambdaImplementing stateful Kinesis Data Streams processing in LambdaLambda parameters for Amazon Kinesis Data Streams event source mappingsUsing event filtering with a Kinesis event sourceTutorial: Using Lambda with Kinesis Data Streams
     Polling and batching streams
  
  Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your
    function to process records from the batch. Each batch contains records from a single shard/data stream.
    For standard Kinesis data streams, Lambda polls shards in your stream for records at a rate of once per second for each shard. 
      For Kinesis enhanced fan-out, 
    Lambda uses an HTTP/2 connection to listen for records being pushed from Kinesis. When records are available, Lambda invokes your 
    function and waits for the result.
   By default, Lambda invokes your function as soon as records are available. If the batch
      that Lambda reads from the event source has only one record in it, Lambda sends only one record to the function. To avoid invoking the function
      with a small number of records, you can tell the event source to buffer records for up to 5 minutes by configuring a
        batching window. Before invoking the function, Lambda continues to read records from the event source
      until it has gathered a full batch, the batching window expires, or the batch reaches the payload limit of 6 MB. For more information,
      see Batching behavior.
      WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.
    Lambda doesn't wait for any configured extensions to complete
      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda
      processes the next batch of records. This can cause throttling issues if you breach any of your account's 
      concurrency settings or limits. To detect whether this is a
      potential issue, monitor your functions and check whether you're seeing higher
      concurrency metrics than expected for your event
      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage
      than the number of shards. This can be true even for Lambda functions without extensions.
    Configure the ParallelizationFactor setting to process one shard of a Kinesis data stream with more than one Lambda invocation simultaneously. 
      You can specify the number of concurrent batches that Lambda polls from a shard via a parallelization factor from 1 (default) to 10. For example, when you set ParallelizationFactor 
      to 2, you can have 200 concurrent Lambda invocations at maximum to process 100 Kinesis data shards (though in practice, you may see different values for the ConcurrentExecutions metric).
      This helps scale up the processing throughput when the data volume is volatile and 
      the IteratorAge is high. When you increase the number of concurrent batches per shard, Lambda still ensures in-order processing at the partition-key level.
    You can also use ParallelizationFactor with Kinesis aggregation. The behavior of the event source mapping
      depends on whether you're using enhanced fan-out:
    
       
       
    
        Without enhanced fan-out: All of the events inside an aggregated event must have the same
          partition key. The partition key must also match that of the aggregated event. If the events inside the aggregated event have
          different partition keys, Lambda cannot guarantee in-order processing of the events by partition key.
      
        With enhanced fan-out: First, Lambda decodes the aggregated event into its individual events.
          The aggregated event can have a different partition key than events it contains. However, events that don't correspond to
          the partition key are dropped and lost.
          Lambda doesn't process these events, and doesn't send them to a configured failure destination.
        
   
     Example event
    {
    "Records": [
        {
            "kinesis": {
                "kinesisSchemaVersion": "1.0",
                "partitionKey": "1",
                "sequenceNumber": "49590338271490256608559692538361571095921575989136588898",
                "data": "SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==",
                "approximateArrivalTimestamp": 1545084650.987
            },
            "eventSource": "aws:kinesis",
            "eventVersion": "1.0",
            "eventID": "shardId-000000000006:49590338271490256608559692538361571095921575989136588898",
            "eventName": "aws:kinesis:record",
            "invokeIdentityArn": "arn:aws:iam::123456789012:role/lambda-role",
            "awsRegion": "us-east-2",
            "eventSourceARN": "arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream"
        },
        {
            "kinesis": {
                "kinesisSchemaVersion": "1.0",
                "partitionKey": "1",
                "sequenceNumber": "49590338271490256608559692540925702759324208523137515618",
                "data": "VGhpcyBpcyBvbmx5IGEgdGVzdC4=",
                "approximateArrivalTimestamp": 1545084711.166
            },
            "eventSource": "aws:kinesis",
            "eventVersion": "1.0",
            "eventID": "shardId-000000000006:49590338271490256608559692540925702759324208523137515618",
            "eventName": "aws:kinesis:record",
            "invokeIdentityArn": "arn:aws:iam::123456789012:role/lambda-role",
            "awsRegion": "us-east-2",
            "eventSourceARN": "arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream"
        }
    ]
}
  Document ConventionsIoTCreate mappingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUnderstanding the Lambda consumer group for Amazon MQUsing Lambda with Amazon MQNoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see 
    Amazon EventBridge Pipes.Amazon MQ is a managed message broker service for Apache ActiveMQ
    and RabbitMQ. A message broker enables software
    applications and components to communicate using various programming languages, operating systems, and formal
    messaging protocols through either topic or queue event destinations.Amazon MQ can also manage Amazon Elastic Compute Cloud (Amazon EC2) instances on your behalf by installing ActiveMQ or RabbitMQ brokers and by providing
    different network topologies and other infrastructure needs.You can use a Lambda function to process records from your Amazon MQ message broker. Lambda invokes your function
    through an event source mapping, a Lambda resource that reads
    messages from your broker and invokes the function synchronously.WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.The Amazon MQ event source mapping has the following configuration restrictions:
   
	 
     
     
     
     
     
     
  
    Concurrency – Lambda functions that use an Amazon MQ event source mapping have a default maximum concurrency 
      setting. For ActiveMQ, the Lambda service limits the number of concurrent execution environments to five per Amazon MQ
      event source mapping. For RabbitMQ, the number of concurrent execution environments is limited to 1 per Amazon MQ
      event source mapping. Even if you change your function's reserved or provisioned concurrency settings, the Lambda 
      service won't make more execution environments available. To request an increase in the default maximum concurrency
      for a single Amazon MQ event source mapping, contact Support with the event source mapping UUID, as well as the region.
      Because increases are applied at the specific event source mapping level, not the account or region level,
      you need to manually request a scaling increase for each event source mapping.
  Cross account – Lambda does not support cross-account processing. You cannot use Lambda to process records
        from an Amazon MQ message broker that is in a different AWS account.
      Authentication – For ActiveMQ, only the ActiveMQ SimpleAuthenticationPlugin is
        supported. For RabbitMQ, only the PLAIN authentication mechanism is supported. Users must use AWS Secrets Manager to manage their credentials.
        For more information about ActiveMQ authentication, see Integrating ActiveMQ brokers
          with LDAP in the Amazon MQ Developer Guide.
    
      Connection quota – Brokers have a maximum number of allowed connections per wire-level protocol. This
        quota is based on the broker instance type. For more information, see the Brokers
        section of Quotas in Amazon MQ in the
          Amazon MQ Developer Guide.
    
      Connectivity – You can create brokers in a public or private virtual private cloud (VPC). For private VPCs, your Lambda function needs access to the VPC to receive messages. For more information, see Configure network security later in this
        section.
    
      Event destinations – Only queue destinations are supported. However, you can use a virtual topic,
        which behaves  as a topic internally while interacting with Lambda as a queue. For more information, see Virtual Destinations
        on the Apache ActiveMQ website, and Virtual Hosts
        on the RabbitMQ website.
    
      Network topology – For ActiveMQ, only one single-instance or standby broker is supported per event source mapping.
        For RabbitMQ, only one single-instance broker or cluster deployment is supported per event source mapping.
        Single-instance brokers require a failover endpoint. For more information about these broker deployment modes, see
        Active MQ Broker Architecture and
        Rabbit MQ Broker Architecturein the Amazon MQ Developer Guide.
    
      Protocols – Supported protocols depend on the type of Amazon MQ integration.
          
             
             
          For ActiveMQ integrations, Lambda consumes messages using the OpenWire/Java Message Service (JMS) protocol. No other protocols are supported for consuming messages. Within the JMS protocol, only TextMessage and BytesMessage are supported. Lambda also supports JMS custom properties. For more information about the OpenWire protocol, see
              OpenWire on the Apache ActiveMQ website.For RabbitMQ integrations, Lambda consumes messages using the AMQP 0-9-1 protocol. No other protocols are supported
            for consuming messages. For more information about RabbitMQ's implementation of the AMQP 0-9-1 protocol, see
              AMQP 0-9-1 Complete Reference
              Guide on the RabbitMQ website.
    Lambda automatically supports the latest versions of ActiveMQ and RabbitMQ that Amazon MQ supports. For the latest
    supported versions, see Amazon MQ release notes in the
      Amazon MQ Developer Guide.NoteBy default, Amazon MQ has a weekly maintenance window for brokers. During that
      window of time, brokers are unavailable. For brokers without standby, Lambda cannot process any messages during that window.TopicsUnderstanding the Lambda consumer group for Amazon MQConfiguring Amazon MQ event source for LambdaEvent source mapping parametersFilter events from an Amazon MQ event sourceTroubleshoot Amazon MQ event source mapping errors
    Understanding the Lambda consumer group for Amazon MQ
    
    To interact with Amazon MQ, Lambda creates a consumer group which can read from your Amazon MQ brokers. The consumer
      group is created with the same ID as the event source mapping UUID.
    For Amazon MQ event sources, Lambda batches records together and sends them to your function in a single payload.
      To control behavior, you can configure the batching window and batch size. Lambda pulls messages until it processes
      the payload size maximum of 6 MB, the batching window expires, or the number of records reaches the full batch
      size. For more information, see Batching behavior.
    The consumer group retrieves the messages as a BLOB of bytes, base64-encodes them into a single JSON payload, and then invokes your function. If your function returns an error for any of the messages in a batch, Lambda retries the
      whole batch of messages until processing succeeds or the messages expire.
    NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes,
      event source mappings for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with
      maximum timeout limits of 14 minutes. This constraint ensures that the event source mapping can properly
      handle function errors and retries.
    You can monitor a given function's concurrency usage using the ConcurrentExecutions metric in
      Amazon CloudWatch. For more information about concurrency, see Configuring reserved concurrency for a function.
    
    Example Amazon MQ record events
        ActiveMQ
            {
   "eventSource": "aws:mq",
   "eventSourceArn": "arn:aws:mq:us-east-2:111122223333:broker:test:b-9bcfa592-423a-4942-879d-eb284b418fc8",
   "messages": [
      { 
        "messageID": "ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-east-2.amazonaws.com-37557-1234520418293-4:1:1:1:1", 
        "messageType": "jms/text-message",
        "deliveryMode": 1,
        "replyTo": null,
        "type": null,
        "expiration": "60000",
        "priority": 1,
        "correlationId": "myJMSCoID",
        "redelivered": false,
        "destination": { 
          "physicalName": "testQueue" 
        },
        "data":"QUJDOkFBQUE=",
        "timestamp": 1598827811958,
        "brokerInTime": 1598827811958, 
        "brokerOutTime": 1598827811959, 
        "properties": {
          "index": "1",
          "doAlarm": "false",
          "myCustomProperty": "value"
        }
      },
      { 
        "messageID": "ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-east-2.amazonaws.com-37557-1234520418293-4:1:1:1:1",
        "messageType": "jms/bytes-message",
        "deliveryMode": 1,
        "replyTo": null,
        "type": null,
        "expiration": "60000",
        "priority": 2,
        "correlationId": "myJMSCoID1",
        "redelivered": false,
        "destination": { 
          "physicalName": "testQueue" 
        },
        "data":"LQaGQ82S48k=",
        "timestamp": 1598827811958,
        "brokerInTime": 1598827811958, 
        "brokerOutTime": 1598827811959, 
        "properties": {
          "index": "1",
          "doAlarm": "false",
          "myCustomProperty": "value"
        }
      }
   ]
}
                        
          
        RabbitMQ
            
{
  "eventSource": "aws:rmq",
  "eventSourceArn": "arn:aws:mq:us-east-2:111122223333:broker:pizzaBroker:b-9bcfa592-423a-4942-879d-eb284b418fc8",
  "rmqMessagesByQueue": {
    "pizzaQueue::/": [
      {
        "basicProperties": {
          "contentType": "text/plain",
          "contentEncoding": null,
          "headers": {
            "header1": {
              "bytes": [
                118,
                97,
                108,
                117,
                101,
                49
              ]
            },
            "header2": {
              "bytes": [
                118,
                97,
                108,
                117,
                101,
                50
              ]
            },
            "numberInHeader": 10
          },
          "deliveryMode": 1,
          "priority": 34,
          "correlationId": null,
          "replyTo": null,
          "expiration": "60000",
          "messageId": null,
          "timestamp": "Jan 1, 1970, 12:33:41 AM",
          "type": null,
          "userId": "AIDACKCEVSQ6C2EXAMPLE",
          "appId": null,
          "clusterId": null,
          "bodySize": 80
        },
        "redelivered": false,
        "data": "eyJ0aW1lb3V0IjowLCJkYXRhIjoiQ1pybWYwR3c4T3Y0YnFMUXhENEUifQ=="
      }
    ]
  }
}
                        
          
      NoteIn the RabbitMQ example, pizzaQueue is the name of the RabbitMQ queue, and / is the
          name of the virtual host. When receiving messages, the event source lists messages under
          pizzaQueue::/.
  Document ConventionsKubernetesConfigure event sourceDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper Guide Example eventUsing Lambda with Amazon MSKAmazon Managed Streaming for Apache Kafka (Amazon MSK) is a
    fully-managed service that you can use to build and run applications that use Apache Kafka to process streaming data.
    Amazon MSK simplifies the setup, scaling, and management of Kafka clusters. Amazon MSK also makes it easier to
    configure your application for multiple Availability Zones and for security with AWS Identity and Access Management (IAM).This chapter explains how to use an Amazon MSK cluster as an event source for your Lambda function. The general
    process for integrating Amazon MSK with Lambda involves the following steps:
     
     
     
  
      Cluster and network setup
        – First, set up your Amazon MSK
        cluster. This includes the correct networking configuration to allow Lambda to access your cluster.
    
      Event source mapping setup
        – Then, create the event source mapping
        resource that Lambda needs to securely connect your Amazon MSK cluster to your function.
    
      Function and permissions setup
        – Finally, ensure that your function is correctly set up, and has the necessary permissions in its
        execution role.
    For examples on how to set up a Lambda integration with an Amazon MSK cluster, see Tutorial: Using an Amazon MSK event source mapping to invoke a Lambda function,
    Using Amazon MSK as an event source for
      AWS Lambda on the AWS Compute Blog, and 
      Amazon MSK Lambda Integration in the Amazon MSK Labs.Topics Example eventConfiguring your Amazon MSK cluster and Amazon VPC network for LambdaConfiguring Amazon MSK event sources for LambdaConfiguring Lambda execution role permissionsUsing event filtering with an Amazon MSK event sourceCapturing discarded batches for an Amazon MSK event sourceTutorial: Using an Amazon MSK event source mapping to invoke a Lambda function
     Example event
    Lambda sends the batch of messages in the event parameter when it invokes your function. The event payload
      contains an array of messages. Each array item contains details of the Amazon MSK topic and partition identifier,
      together with a timestamp and a base64-encoded message.
    {
   "eventSource":"aws:kafka",
   "eventSourceArn":"arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2",
   "bootstrapServers":"b-2.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092,b-1.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092",
   "records":{
      "mytopic-0":[
         {
            "topic":"mytopic",
            "partition":0,
            "offset":15,
            "timestamp":1545084650987,
            "timestampType":"CREATE_TIME",
            "key":"abcDEFghiJKLmnoPQRstuVWXyz1234==",
            "value":"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==",
            "headers":[
               {
                  "headerKey":[
                     104,
                     101,
                     97,
                     100,
                     101,
                     114,
                     86,
                     97,
                     108,
                     117,
                     101
                  ]
               }
            ]
         }
      ]
   }
}
        
  Document ConventionsApache KafkaCluster and network setupDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideExample eventUsing Lambda with self-managed Apache KafkaThis topic describes how to use Lambda with a self-managed Kafka cluster. In AWS terminology, a self-managed
    cluster includes non-AWS hosted Kafka clusters. For example, you can host your Kafka cluster with a cloud provider
    such as Confluent Cloud.Apache Kafka as an event source operates similarly to using Amazon Simple Queue Service (Amazon SQS) or Amazon Kinesis. Lambda internally polls for
    new messages from the event source and then synchronously invokes the target Lambda function. Lambda reads the
    messages in batches and provides these to your function as an event payload. The maximum batch size is configurable
    (the default is 100 messages). For more information, see Batching behavior.To optimize the throughput of your self-managed Apache Kafka event source mapping, configure provisioned mode. In provisioned
    mode, you can define the minimum and maximum number of event pollers allocated to your event source mapping.
    This can improve the ability of your event source mapping to handle unexpected message spikes. For more
    information, see provisioned mode.WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues 
related to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent 
in the AWS Knowledge Center.For Kafka-based event sources, Lambda supports processing control parameters, such as batching windows
      and batch size. For more information, see Batching behavior.For an example of how to use self-managed Kafka as an event source, see Using self-hosted Apache Kafka as an
      event source for AWS Lambda on the AWS Compute Blog.TopicsExample eventConfiguring self-managed Apache Kafka event sources for LambdaProcessing self-managed Apache Kafka messages with LambdaUsing event filtering with a self-managed Apache Kafka event sourceCapturing discarded batches for a self-managed Apache Kafka event sourceTroubleshooting self-managed Apache Kafka event source mapping errors
    Example event
    Lambda sends the batch of messages in the event parameter when it invokes your Lambda function. The event payload
    contains an array of messages. Each array item contains details of the Kafka topic and Kafka partition identifier,
    together with a timestamp and a base64-encoded message.
    {
   "eventSource": "SelfManagedKafka",
   "bootstrapServers":"b-2.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092,b-1.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092",
   "records":{
      "mytopic-0":[
         {
            "topic":"mytopic",
            "partition":0,
            "offset":15,
            "timestamp":1545084650987,
            "timestampType":"CREATE_TIME",
            "key":"abcDEFghiJKLmnoPQRstuVWXyz1234==",
            "value":"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==",
            "headers":[
               {
                  "headerKey":[
                     104,
                     101,
                     97,
                     100,
                     101,
                     114,
                     86,
                     97,
                     108,
                     117,
                     101
                  ]
               }
            ]
         }
      ]
   }
}
        
  Document ConventionsTutorialConfigure event sourceDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideCreating a triggerServices listInvoking Lambda with events from other AWS servicesSome AWS services can directly invoke Lambda functions using triggers. These services push events to Lambda, and the function is invoked immediately when the specified event occurs. Triggers are suitable for discrete events and real-time processing. When you create a trigger using the Lambda console, the console interacts with the corresponding AWS service to configure the event notification on that service. The trigger is actually stored and managed by the service that generates the events, not by Lambda.The events are data structured in JSON format. The JSON structure varies depending on the service that
    generates it and the event type, but they all contain the data that the function needs to process the
    event.A function can have multiple triggers. Each trigger acts as a client invoking your function independently, and each event that
    Lambda passes to your function has data from only one trigger. Lambda converts the event document into an object and passes it to your function handler.Depending on the service, the event-driven invocation can be synchronous or asynchronous.
     
     
  
      For synchronous invocation, the service that generates the event waits for the response from your
        function. That service defines the data that the function needs to return in the response. The service
        controls the error strategy, such as whether to retry on errors.
    
      For asynchronous invocation, Lambda queues the event before passing it to your function. When Lambda
        queues the event, it immediately sends a success response to the service that generated the event. After the
        function processes the event, Lambda doesn’t return a response to the event-generating service.
    
    Creating a trigger
    The easiest way to create a trigger is to use the Lambda console. When you create a trigger using the console, Lambda automatically adds the required permissions to the function's resource-based policy.
    To create a trigger using the Lambda console
        Open the Functions page of the Lambda console.
      
        Select the function you want to create a trigger for.
      
        In the Function overview pane, choose
          Add trigger.
      
        Select the AWS service you want to invoke your function.
      
        Fill out the options in the Trigger configuration pane
          and choose Add. Depending on the AWS service you choose to
          invoke your function, the trigger configuration options will be different.
      
   
    Services that can invoke Lambda functions
    The following table lists services that can invoke Lambda functions.
    
          
            Service
            Method of invocation
          
        

          
            
              Amazon Managed Streaming for Apache Kafka
            
            
              Event source mapping
            
          
          
            
              Self-managed Apache Kafka
            
            
              Event source mapping
            
          
          
            
              Amazon API Gateway
            
            
              Event-driven; synchronous invocation
            
          
          
            
              AWS CloudFormation
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              Amazon CloudWatch Logs
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              AWS CodeCommit
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              AWS CodePipeline
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              Amazon Cognito
            
            
              Event-driven; synchronous invocation
            
          
          
            
              AWS Config
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              Amazon Connect
            
            
              Event-driven; synchronous invocation
            
          
          
            
              Amazon DocumentDB
            
            
              Event source mapping
            
          
          
            
              Amazon DynamoDB
            
            
              Event source mapping
            
          
          
            
              Elastic Load Balancing (Application Load Balancer)
            
            
              Event-driven; synchronous invocation
            
          
          
            
              Amazon EventBridge (CloudWatch Events)
            
            
              Event-driven; asynchronous invocation (event buses), synchronous or asynchronous invocation (pipes and schedules)
            
          
          
            
              AWS IoT
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              Amazon Kinesis
            
            
              Event source mapping
            
          
          
            
              Amazon Data Firehose
            
            
              Event-driven; synchronous invocation
            
          
          
            
              Amazon Lex
            
            
              Event-driven; synchronous invocation
            
          
          
            
              Amazon MQ
            
            
              Event source mapping
            
          
          
            
              Amazon Simple Email Service
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              Amazon Simple Notification Service
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              Amazon Simple Queue Service
            
            
              Event source mapping
            
          
          
            
              Amazon Simple Storage Service (Amazon S3)
            
            
              Event-driven; asynchronous invocation
            
          
          
            
              Amazon Simple Storage Service Batch
            
            
              Event-driven; synchronous invocation
            
          
          
            
              Secrets Manager
            
            
              Secret rotation
            
          
          
            
              AWS Step Functions
            
            
              Event-driven; synchronous or asynchronous invocation
            
          
          
            
              Amazon VPC Lattice
            
            
              Event-driven; synchronous invocation
            
          
        
  Document ConventionsTroubleshootingApache KafkaDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideCreate an Amazon S3 bucketUpload a test object to your bucketCreate a permissions policyCreate an execution roleCreate the Lambda functionDeploy the function codeCreate the Amazon S3 triggerTest the Lambda functionClean up your resourcesNext stepsTutorial: Using an Amazon S3 trigger to invoke a Lambda functionIn this tutorial, you use the console to create a Lambda function and configure a trigger for an Amazon Simple Storage Service (Amazon S3) bucket. Every time that you 
    add an object to your Amazon S3 bucket, your function runs and outputs the object type to Amazon CloudWatch Logs.
     
       
     
     
  This tutorial demonstrates how to:
     
     
     
     
  
      Create an Amazon S3 bucket.
    
      Create a Lambda function that returns the object type of objects in an Amazon S3 bucket.
    
      Configure a Lambda trigger that invokes your function when objects are uploaded to your bucket.
    
      Test your function, first with a dummy event, and then using the trigger.
    By completing these steps, you’ll learn how to configure a Lambda function to run whenever objects are added to or deleted from an 
    Amazon S3 bucket. You can complete this tutorial using only the AWS Management Console.
    Create an Amazon S3 bucket
    
       
        
       
       
    
    To create an Amazon S3 bucket
        Open the Amazon S3 console and select the General purpose buckets page.
      
        Select the AWS Region closest to your geographical location. You can change your region using the drop-down list at the top of the screen. 
          Later in the tutorial, you must create your Lambda function in the same Region.
        
           
            
           
           
        
      
        Choose Create bucket.
      
        Under General configuration, do the following:
        
            For Bucket type, ensure General purpose is selected.
          
            For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules. 
              Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-).
          
      
        Leave all other options set to their default values and choose Create bucket.
      
   
    Upload a test object to your bucket
    
       
        
       
       
    
    To upload a test object
        Open the Buckets page of the Amazon S3 console and choose the bucket you created during the 
          previous step.
      
        Choose Upload.
      
        Choose Add files and select the object that you want to upload. You can select any file (for example, HappyFace.jpg).
      
        Choose Open, then choose Upload.
      
    Later in the tutorial, you’ll test your Lambda function using this object.
   
    Create a permissions policy
    
       
        
       
       
    
    Create a permissions policy that allows Lambda to get objects from an Amazon S3 bucket and to write to Amazon CloudWatch Logs. 
    To create the policy
        Open the Policies page of the IAM console.
      
        Choose Create Policy.
      
        Choose the JSON tab, and then paste the following custom policy into the JSON
          editor.
        {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "logs:PutLogEvents",
                "logs:CreateLogGroup",
                "logs:CreateLogStream"
            ],
            "Resource": "arn:aws:logs:*:*:*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::*/*"
        }
    ]
}
      
        Choose Next: Tags.
      
        Choose Next: Review.
      
        Under Review policy, for the policy Name, enter
          s3-trigger-tutorial.
      
        Choose Create policy.
      
   
    Create an execution role
    
       
        
       
       
    
    An execution role is an AWS Identity and Access Management (IAM) role that grants a Lambda function permission to access AWS services and resources. In this step, create an execution role using the permissions policy that you created in the previous step.
    To create an execution role and attach your custom permissions policy
        Open the Roles page of the IAM console.
      
        Choose Create role.
      
        For the type of trusted entity, choose AWS service, then for the use case, choose Lambda.
      
        Choose Next.
      
        In the policy search box, enter s3-trigger-tutorial.
      
        In the search results, select the policy that you created (s3-trigger-tutorial), and
          then choose Next.
      
        Under Role details, for the Role name, enter
          lambda-s3-trigger-role, then choose Create role.
      
   
    Create the Lambda function
    
       
        
       
       
    
    Create a Lambda function in the console using the Python 3.13 runtime.
    To create the Lambda function
        Open the Functions page of the Lambda console.
      
        Make sure you're working in the same AWS Region you created your Amazon S3 bucket in. You can change your Region using the drop-down list 
        at the top of the screen.
        
           
            
           
           
        
      
        Choose Create function.
      
        Choose Author from scratch
      
        Under Basic information, do the following:
        
            For Function name, enter s3-trigger-tutorial
          
            For Runtime, choose Python 3.13.
          
            For Architecture, choose x86_64.
          
      
        In the Change default execution role tab, do the following:
        
            Expand the tab, then choose Use an existing role.
          
            Select the lambda-s3-trigger-role you created earlier.
          
      
        Choose Create function.
      
   
    Deploy the function code
    
       
        
       
       
    
    This tutorial uses the Python 3.13 runtime, but we’ve also provided example code files for other runtimes. You can select the 
      tab in the following box to see the code for the runtime you’re interested in.
    The Lambda function retrieves the key name of the uploaded object and the name of the bucket from the event parameter it receives 
      from Amazon S3. The function then uses the get_object  method from the AWS SDK for Python (Boto3) to retrieve the object's metadata, including the content type (MIME type) of the uploaded object.
    To deploy the function code
        Choose the Python tab in the following box and copy the code.
        
    .NET
            
     

        SDK for .NET
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using .NET.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
﻿using System.Threading.Tasks;
using Amazon.Lambda.Core;
using Amazon.S3;
using System;
using Amazon.Lambda.S3Events;
using System.Web;

// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.
[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]

namespace S3Integration
{
    public class Function
    {
        private static AmazonS3Client _s3Client;
        public Function() : this(null)
        {
        }

        internal Function(AmazonS3Client s3Client)
        {
            _s3Client = s3Client ?? new AmazonS3Client();
        }

        public async Task<string> Handler(S3Event evt, ILambdaContext context)
        {
            try
            {
                if (evt.Records.Count <= 0)
                {
                    context.Logger.LogLine("Empty S3 Event received");
                    return string.Empty;
                }

                var bucket = evt.Records[0].S3.Bucket.Name;
                var key = HttpUtility.UrlDecode(evt.Records[0].S3.Object.Key);

                context.Logger.LogLine($"Request is for {bucket} and {key}");

                var objectResult = await _s3Client.GetObjectAsync(bucket, key);

                context.Logger.LogLine($"Returning {objectResult.Key}");

                return objectResult.Key;
            }
            catch (Exception e)
            {
                context.Logger.LogLine($"Error processing request - {e.Message}");

                return string.Empty;
            }
        }
    }
}

             
        
    
        
    Go
            
     

        SDK for Go V2
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using Go.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package main

import (
	"context"
	"log"

	"github.com/aws/aws-lambda-go/events"
	"github.com/aws/aws-lambda-go/lambda"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/s3"
)

func handler(ctx context.Context, s3Event events.S3Event) error {
	sdkConfig, err := config.LoadDefaultConfig(ctx)
	if err != nil {
		log.Printf("failed to load default config: %s", err)
		return err
	}
	s3Client := s3.NewFromConfig(sdkConfig)

	for _, record := range s3Event.Records {
		bucket := record.S3.Bucket.Name
		key := record.S3.Object.URLDecodedKey
		headOutput, err := s3Client.HeadObject(ctx, &s3.HeadObjectInput{
			Bucket: &bucket,
			Key:    &key,
		})
		if err != nil {
			log.Printf("error getting head of object %s/%s: %s", bucket, key, err)
			return err
		}
		log.Printf("successfully retrieved %s/%s of type %s", bucket, key, *headOutput.ContentType)
	}

	return nil
}

func main() {
	lambda.Start(handler)
}


             
        
    
        
    Java
            
     

        SDK for Java 2.x
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using Java.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package example;

import software.amazon.awssdk.services.s3.model.HeadObjectRequest;
import software.amazon.awssdk.services.s3.model.HeadObjectResponse;
import software.amazon.awssdk.services.s3.S3Client;

import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.S3Event;
import com.amazonaws.services.lambda.runtime.events.models.s3.S3EventNotification.S3EventNotificationRecord;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class Handler implements RequestHandler<S3Event, String> {
    private static final Logger logger = LoggerFactory.getLogger(Handler.class);
    @Override
    public String handleRequest(S3Event s3event, Context context) {
        try {
          S3EventNotificationRecord record = s3event.getRecords().get(0);
          String srcBucket = record.getS3().getBucket().getName();
          String srcKey = record.getS3().getObject().getUrlDecodedKey();

          S3Client s3Client = S3Client.builder().build();
          HeadObjectResponse headObject = getHeadObject(s3Client, srcBucket, srcKey);

          logger.info("Successfully retrieved " + srcBucket + "/" + srcKey + " of type " + headObject.contentType());

          return "Ok";
        } catch (Exception e) {
          throw new RuntimeException(e);
        }
    }

    private HeadObjectResponse getHeadObject(S3Client s3Client, String bucket, String key) {
        HeadObjectRequest headObjectRequest = HeadObjectRequest.builder()
                .bucket(bucket)
                .key(key)
                .build();
        return s3Client.headObject(headObjectRequest);
    }
}

             
        
    
        
    JavaScript
            
     

        SDK for JavaScript (v3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using JavaScript.
                
                import { S3Client, HeadObjectCommand } from "@aws-sdk/client-s3";

const client = new S3Client();

export const handler = async (event, context) => {

    // Get the object from the event and show its content type
    const bucket = event.Records[0].s3.bucket.name;
    const key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\+/g, ' '));

    try {
        const { ContentType } = await client.send(new HeadObjectCommand({
            Bucket: bucket,
            Key: key,
        }));

        console.log('CONTENT TYPE:', ContentType);
        return ContentType;

    } catch (err) {
        console.log(err);
        const message = `Error getting object ${key} from bucket ${bucket}. Make sure they exist and your bucket is in the same region as this function.`;
        console.log(message);
        throw new Error(message);
    }
};


             
             
                    Consuming an S3 event with Lambda using TypeScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import { S3Event } from 'aws-lambda';
import { S3Client, HeadObjectCommand } from '@aws-sdk/client-s3';

const s3 = new S3Client({ region: process.env.AWS_REGION });

export const handler = async (event: S3Event): Promise<string | undefined> => {
  // Get the object from the event and show its content type
  const bucket = event.Records[0].s3.bucket.name;
  const key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\+/g, ' '));
  const params = {
    Bucket: bucket,
    Key: key,
  };
  try {
    const { ContentType } = await s3.send(new HeadObjectCommand(params));
    console.log('CONTENT TYPE:', ContentType);
    return ContentType;
  } catch (err) {
    console.log(err);
    const message = `Error getting object ${key} from bucket ${bucket}. Make sure they exist and your bucket is in the same region as this function.`;
    console.log(message);
    throw new Error(message);
  }
};

             
        
    
        
    PHP
            
     

        SDK for PHP
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using PHP.
                
                <?php

use Bref\Context\Context;
use Bref\Event\S3\S3Event;
use Bref\Event\S3\S3Handler;
use Bref\Logger\StderrLogger;

require __DIR__ . '/vendor/autoload.php';


class Handler extends S3Handler 
{
    private StderrLogger $logger;
    public function __construct(StderrLogger $logger)
    {
        $this->logger = $logger;
    }
    
    public function handleS3(S3Event $event, Context $context) : void
    {
        $this->logger->info("Processing S3 records");

        // Get the object from the event and show its content type
        $records = $event->getRecords();
        
        foreach ($records as $record) 
        {
            $bucket = $record->getBucket()->getName();
            $key = urldecode($record->getObject()->getKey());

            try {
                $fileSize = urldecode($record->getObject()->getSize());
                echo "File Size: " . $fileSize . "\n";
                // TODO: Implement your custom processing logic here
            } catch (Exception $e) {
                echo $e->getMessage() . "\n";
                echo 'Error getting object ' . $key . ' from bucket ' . $bucket . '. Make sure they exist and your bucket is in the same region as this function.' . "\n";
                throw $e;
            }
        }
    }
}

$logger = new StderrLogger();
return new Handler($logger);


             
        
    
        
    Python
            
     

        SDK for Python (Boto3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using Python.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json
import urllib.parse
import boto3

print('Loading function')

s3 = boto3.client('s3')


def lambda_handler(event, context):
    #print("Received event: " + json.dumps(event, indent=2))

    # Get the object from the event and show its content type
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        print("CONTENT TYPE: " + response['ContentType'])
        return response['ContentType']
    except Exception as e:
        print(e)
        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
        raise e
              

             
        
    
        
    Ruby
            
     

        SDK for Ruby
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using Ruby.
                
                require 'json'
require 'uri'
require 'aws-sdk'

puts 'Loading function'

def lambda_handler(event:, context:)
  s3 = Aws::S3::Client.new(region: 'region') # Your AWS region
  # puts "Received event: #{JSON.dump(event)}"

  # Get the object from the event and show its content type
  bucket = event['Records'][0]['s3']['bucket']['name']
  key = URI.decode_www_form_component(event['Records'][0]['s3']['object']['key'], Encoding::UTF_8)
  begin
    response = s3.get_object(bucket: bucket, key: key)
    puts "CONTENT TYPE: #{response.content_type}"
    return response.content_type
  rescue StandardError => e
    puts e.message
    puts "Error getting object #{key} from bucket #{bucket}. Make sure they exist and your bucket is in the same region as this function."
    raise e
  end
end


             
        
    
        
    Rust
            
     

        SDK for Rust
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an S3 event with Lambda using Rust.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
use aws_lambda_events::event::s3::S3Event;
use aws_sdk_s3::{Client};
use lambda_runtime::{run, service_fn, Error, LambdaEvent};


/// Main function
#[tokio::main]
async fn main() -> Result<(), Error> {
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .without_time()
        .init();

    // Initialize the AWS SDK for Rust
    let config = aws_config::load_from_env().await;
    let s3_client = Client::new(&config);

    let res = run(service_fn(|request: LambdaEvent<S3Event>| {
        function_handler(&s3_client, request)
    })).await;

    res
}

async fn function_handler(
    s3_client: &Client,
    evt: LambdaEvent<S3Event>
) -> Result<(), Error> {
    tracing::info!(records = ?evt.payload.records.len(), "Received request from SQS");

    if evt.payload.records.len() == 0 {
        tracing::info!("Empty S3 event received");
    }

    let bucket = evt.payload.records[0].s3.bucket.name.as_ref().expect("Bucket name to exist");
    let key = evt.payload.records[0].s3.object.key.as_ref().expect("Object key to exist");

    tracing::info!("Request is for {} and object {}", bucket, key);

    let s3_get_object_result = s3_client
        .get_object()
        .bucket(bucket)
        .key(key)
        .send()
        .await;

    match s3_get_object_result {
        Ok(_) => tracing::info!("S3 Get Object success, the s3GetObjectResult contains a 'body' property of type ByteStream"),
        Err(_) => tracing::info!("Failure with S3 Get Object request")
    }

    Ok(())
}

             
        
    
        

      
        In the Code source pane on the Lambda console, paste the code into the code editor, replacing the code that 
          Lambda created.
      
        In the DEPLOY section, choose Deploy to update your function's code:

   
    
   
   

      
   
    Create the Amazon S3 trigger
    
       
        
       
       
    
    To create the Amazon S3 trigger
        In the Function overview pane, choose Add trigger.
        
           
            
           
           
        
      
        Select S3.
      
        Under Bucket, select the bucket you created earlier in the tutorial.
      
        Under Event types, be sure that All object create events is selected.
      
        Under Recursive invocation, select the check box to acknowledge that using the same Amazon S3 bucket for input and 
          output is not recommended.
      
        Choose Add.
      
    NoteWhen you create an Amazon S3 trigger for a Lambda function using the Lambda console, Amazon S3 configures an event notification 
        on the bucket you specify. Before configuring this event notification, Amazon S3 performs a series of checks to confirm that the event destination exists 
        and has the required IAM policies. Amazon S3 also performs these tests on any other event notifications configured for that bucket.Because of this check, if the bucket has previously configured event destinations for resources that no longer exist, or for resources that don't have 
        the required permissions policies, Amazon S3 won't be able to create the new event notification. You'll see the following error message indicating that your trigger 
        couldn't be created:An error occurred when creating the trigger: Unable to validate the following destination configurations.You can see this error if you previously configured a trigger for another Lambda function using the same bucket, and you have since 
          deleted the function or modified its permissions policies.
   
    Test your Lambda function with a dummy event
    
       
        
       
       
    
    To test the Lambda function with a dummy event
        In the Lambda console page for your function, choose the Test tab.
        
           
            
           
           
        
      
         For Event name, enter MyTestEvent.
      
        In the Event JSON, paste the following test event. Be sure to replace these values:
        
           
           
           
        
            Replace us-east-1 with the region you created your Amazon S3 bucket in.
          
            Replace both instances of amzn-s3-demo-bucket with the name of your own Amazon S3 bucket.
          
            Replace test%2FKey with the name of the test object you uploaded to your bucket earlier (for example, 
              HappyFace.jpg).
          
            {
  "Records": [
    {
      "eventVersion": "2.0",
      "eventSource": "aws:s3",
      "awsRegion": "us-east-1",
      "eventTime": "1970-01-01T00:00:00.000Z",
      "eventName": "ObjectCreated:Put",
      "userIdentity": {
        "principalId": "EXAMPLE"
      },
      "requestParameters": {
        "sourceIPAddress": "127.0.0.1"
      },
      "responseElements": {
        "x-amz-request-id": "EXAMPLE123456789",
        "x-amz-id-2": "EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH"
      },
      "s3": {
        "s3SchemaVersion": "1.0",
        "configurationId": "testConfigRule",
        "bucket": {
          "name": "amzn-s3-demo-bucket",
          "ownerIdentity": {
            "principalId": "EXAMPLE"
          },
          "arn": "arn:aws:s3:::amzn-s3-demo-bucket"
        },
        "object": {
          "key": "test%2Fkey",
          "size": 1024,
          "eTag": "0123456789abcdef0123456789abcdef",
          "sequencer": "0A1B2C3D4E5F678901"
        }
      }
    }
  ]
}
      
        Choose Save.
      
        Choose Test.
      
        If your function runs successfully, you’ll see output similar to the following in the Execution results tab.
        Response
"image/jpeg"

Function Logs
START RequestId: 12b3cae7-5f4e-415e-93e6-416b8f8b66e6 Version: $LATEST
2021-02-18T21:40:59.280Z    12b3cae7-5f4e-415e-93e6-416b8f8b66e6    INFO    INPUT BUCKET AND KEY:  { Bucket: 'amzn-s3-demo-bucket', Key: 'HappyFace.jpg' }
2021-02-18T21:41:00.215Z    12b3cae7-5f4e-415e-93e6-416b8f8b66e6    INFO    CONTENT TYPE: image/jpeg
END RequestId: 12b3cae7-5f4e-415e-93e6-416b8f8b66e6
REPORT RequestId: 12b3cae7-5f4e-415e-93e6-416b8f8b66e6    Duration: 976.25 ms    Billed Duration: 977 ms    Memory Size: 128 MB    Max Memory Used: 90 MB    Init Duration: 430.47 ms        

Request ID
12b3cae7-5f4e-415e-93e6-416b8f8b66e6
      
     
      Test the Lambda function with the Amazon S3 trigger
      
         
          
         
         
      
      To test your function with the configured trigger, upload an object to your Amazon S3 bucket using the console. To verify that your Lambda 
        function ran as expected, use CloudWatch Logs to view your function’s output.
      To upload an object to your Amazon S3 bucket
          Open the Buckets page of the Amazon S3 console and choose the bucket that you created earlier.
        
          Choose Upload.
        
          Choose Add files and use the file selector to choose an object you want to upload. This object can be any file 
            you choose.
        
          Choose Open, then choose Upload.
        
      To verify the function invocation using CloudWatch Logs
          Open the CloudWatch console.
        
          Make sure you're working in the same AWS Region you created your Lambda function in. You can change your Region using the drop-down 
            list at the top of the screen.
          
             
              
             
             
          
        
          Choose Logs, then choose Log groups.
        
          Choose the log group for your function (/aws/lambda/s3-trigger-tutorial).
        
          Under Log streams, choose the most recent log stream.
        
          If your function was invoked correctly in response to your Amazon S3 trigger, you’ll see output similar to the following. The 
          CONTENT TYPE you see depends on the type of file you uploaded to your bucket.
          2022-05-09T23:17:28.702Z	0cae7f5a-b0af-4c73-8563-a3430333cc10	INFO	CONTENT TYPE: image/jpeg

        
     
     
   
    Clean up your resources
     
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the S3 bucket
    Open the Amazon S3 console.
  
    Select the bucket you created.
  
    Choose Delete.
  
    Enter the name of the bucket in the text input field.
  
    Choose Delete bucket.
  
   
    Next steps
    In Tutorial: Using an Amazon S3 trigger to create thumbnail images, the Amazon S3 trigger invokes a function that creates a thumbnail image for each image file that is uploaded to a
      bucket. This tutorial requires a moderate level of AWS and Lambda domain knowledge. It demonstrates how to create resources using the AWS Command Line Interface (AWS CLI) and how to create a .zip file archive deployment package for the function and its dependencies.
  Document ConventionsS3Tutorial: Use an Amazon S3 trigger to create thumbnailsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate an Amazon SNS topic (account A)Create a function execution role (account B)Create a Lambda function (account B)Add permissions to function (account B)Grant cross-account permission for Amazon SNS subscription (account A)Create a subscription (account B)Publish messages to topic (account A and account B)Clean up your resourcesTutorial: Using AWS Lambda with Amazon Simple Notification ServiceIn this tutorial, you use a Lambda function in one AWS account to subscribe to an Amazon Simple Notification Service (Amazon SNS) topic in a separate AWS account. When 
    you publish messages to your Amazon SNS topic, your Lambda function reads the contents of the message and outputs it to Amazon CloudWatch Logs. To complete this 
    tutorial, you use the AWS Command Line Interface (AWS CLI).
     
       
     
     
  To complete this tutorial, you perform the following steps:
     
     
     
     
  
      In account A, create an Amazon SNS topic.
    
      In account B, create a Lambda function that will read messages from the topic.
    
      In account B, create a subscription to the topic.
    
      Publish messages to the Amazon SNS topic in account A and confirm that the Lambda function in 
        account B outputs them to CloudWatch Logs.
    By completing these steps, you will learn how to configure an Amazon SNS topic to invoke a Lambda function. You will also learn how to create an 
   AWS Identity and Access Management (IAM) policy that gives permission for a resource in another AWS account to invoke Lambda.In the tutorial, you use two separate AWS accounts. The AWS CLI commands illustrate this by using two named profiles called accountA 
    and accountB, each configured for use with a different AWS account. To learn how to configure the AWS CLI to use different profiles, 
    see Configuration and credential file settings in the 
    AWS Command Line Interface User Guide for Version 2. Be sure to configure the same default AWS Region for both profiles.If the AWS CLI profiles you create for the two AWS accounts use different names, or if you use the default profile and one named profile, 
    modify the AWS CLI commands in the following steps as needed.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create an Amazon SNS topic (account A)
    
       
        
       
       
    
    To create the topic
        In account A, create an Amazon SNS standard topic using the following AWS CLI command.
        aws sns create-topic --name sns-topic-for-lambda --profile accountA
        You should see output similar to the following.
        {
    "TopicArn": "arn:aws:sns:us-west-2:123456789012:sns-topic-for-lambda"
}
        Make a note of the Amazon Resource Name (ARN) of your topic. You’ll need it later in the tutorial when you add permissions to your 
          Lambda function to subscribe to the topic.
      
   
    Create a function execution role (account B)
    
       
        
       
       
    
    An execution role is an IAM role that grants a Lambda function permission to access AWS services and resources. Before you create your 
      function in account B, you create a role that gives the function basic permissions to write logs to 
      CloudWatch Logs. We’ll add the permissions to read from your Amazon SNS topic in a later step.
    To create an execution role
        In account B open the roles page in the 
          IAM console.
      
        Choose Create role.
      
        For Trusted entity type, choose AWS service.
      
        For Use case, choose Lambda.
      
        Choose Next.
      
        Add a basic permissions policy to the role by doing the following:
        
            In the Permissions policies search box, enter AWSLambdaBasicExecutionRole.
          
            Choose Next.
          
      
        Finalize the role creation by doing the following:
        
            Under Role details, enter lambda-sns-role for Role name.
          
            Choose Create role.
          
      
   
    Create a Lambda function (account B)
    
       
        
       
       
    
    Create a Lambda function that processes your Amazon SNS messages. The function code logs the message
      contents of each record to Amazon CloudWatch Logs.
    This tutorial uses the Node.js 18.x runtime, but we've also provided example code in other
      runtime languages. You can select the tab in the following box to see code for the runtime
      you're interested in. The JavaScript code you'll use in this step is in the first example
      shown in the JavaScript tab.
    
    .NET
            
     

        SDK for .NET
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using .NET.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
using Amazon.Lambda.Core;
using Amazon.Lambda.SNSEvents;


// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.
[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]

namespace SnsIntegration;

public class Function
{
    public async Task FunctionHandler(SNSEvent evnt, ILambdaContext context)
    {
        foreach (var record in evnt.Records)
        {
            await ProcessRecordAsync(record, context);
        }
        context.Logger.LogInformation("done");
    }

    private async Task ProcessRecordAsync(SNSEvent.SNSRecord record, ILambdaContext context)
    {
        try
        {
            context.Logger.LogInformation($"Processed record {record.Sns.Message}");

            // TODO: Do interesting work based on the new message
            await Task.CompletedTask;
        }
        catch (Exception e)
        {
            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.
            context.Logger.LogError($"An error occurred");
            throw;
        }
    }
}

             
        
    
        
    Go
            
     

        SDK for Go V2
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using Go.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package main

import (
	"context"
	"fmt"

	"github.com/aws/aws-lambda-go/events"
	"github.com/aws/aws-lambda-go/lambda"
)

func handler(ctx context.Context, snsEvent events.SNSEvent) {
	for _, record := range snsEvent.Records {
		processMessage(record)
	}
	fmt.Println("done")
}

func processMessage(record events.SNSEventRecord) {
	message := record.SNS.Message
	fmt.Printf("Processed message: %s\n", message)
	// TODO: Process your record here
}

func main() {
	lambda.Start(handler)
}


             
        
    
        
    Java
            
     

        SDK for Java 2.x
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using Java.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package example;

import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.LambdaLogger;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SNSEvent;
import com.amazonaws.services.lambda.runtime.events.SNSEvent.SNSRecord;


import java.util.Iterator;
import java.util.List;

public class SNSEventHandler implements RequestHandler<SNSEvent, Boolean> {
    LambdaLogger logger;

    @Override
    public Boolean handleRequest(SNSEvent event, Context context) {
        logger = context.getLogger();
        List<SNSRecord> records = event.getRecords();
        if (!records.isEmpty()) {
            Iterator<SNSRecord> recordsIter = records.iterator();
            while (recordsIter.hasNext()) {
                processRecord(recordsIter.next());
            }
        }
        return Boolean.TRUE;
    }

    public void processRecord(SNSRecord record) {
        try {
            String message = record.getSNS().getMessage();
            logger.log("message: " + message);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

}





             
        
    
        
    JavaScript
            
     

        SDK for JavaScript (v3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using JavaScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
exports.handler = async (event, context) => {
  for (const record of event.Records) {
    await processMessageAsync(record);
  }
  console.info("done");
};

async function processMessageAsync(record) {
  try {
    const message = JSON.stringify(record.Sns.Message);
    console.log(`Processed message ${message}`);
    await Promise.resolve(1); //Placeholder for actual async work
  } catch (err) {
    console.error("An error occurred");
    throw err;
  }
}


             
             
                    Consuming an SNS event with Lambda using TypeScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import { SNSEvent, Context, SNSHandler, SNSEventRecord } from "aws-lambda";

export const functionHandler: SNSHandler = async (
  event: SNSEvent,
  context: Context
): Promise<void> => {
  for (const record of event.Records) {
    await processMessageAsync(record);
  }
  console.info("done");
};

async function processMessageAsync(record: SNSEventRecord): Promise<any> {
  try {
    const message: string = JSON.stringify(record.Sns.Message);
    console.log(`Processed message ${message}`);
    await Promise.resolve(1); //Placeholder for actual async work
  } catch (err) {
    console.error("An error occurred");
    throw err;
  }
}


             
        
    
        
    PHP
            
     

        SDK for PHP
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using PHP.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
<?php

/* 
Since native PHP support for AWS Lambda is not available, we are utilizing Bref's PHP functions runtime for AWS Lambda.
For more information on Bref's PHP runtime for Lambda, refer to: https://bref.sh/docs/runtimes/function

Another approach would be to create a custom runtime. 
A practical example can be found here: https://aws.amazon.com/blogs/apn/aws-lambda-custom-runtime-for-php-a-practical-example/
*/

// Additional composer packages may be required when using Bref or any other PHP functions runtime.
// require __DIR__ . '/vendor/autoload.php';

use Bref\Context\Context;
use Bref\Event\Sns\SnsEvent;
use Bref\Event\Sns\SnsHandler;

class Handler extends SnsHandler
{
    public function handleSns(SnsEvent $event, Context $context): void
    {
        foreach ($event->getRecords() as $record) {
            $message = $record->getMessage();

            // TODO: Implement your custom processing logic here
            // Any exception thrown will be logged and the invocation will be marked as failed

            echo "Processed Message: $message" . PHP_EOL;
        }
    }
}

return new Handler();


             
        
    
        
    Python
            
     

        SDK for Python (Boto3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using Python.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def lambda_handler(event, context):
    for record in event['Records']:
        process_message(record)
    print("done")

def process_message(record):
    try:
        message = record['Sns']['Message']
        print(f"Processed message {message}")
        # TODO; Process your record here
        
    except Exception as e:
        print("An error occurred")
        raise e


             
        
    
        
    Ruby
            
     

        SDK for Ruby
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using Ruby.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def lambda_handler(event:, context:)
  event['Records'].map { |record| process_message(record) }
end

def process_message(record)
  message = record['Sns']['Message']
  puts("Processing message: #{message}")
rescue StandardError => e
  puts("Error processing message: #{e}")
  raise
end


             
        
    
        
    Rust
            
     

        SDK for Rust
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Consuming an SNS event with Lambda using Rust.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
use aws_lambda_events::event::sns::SnsEvent;
use aws_lambda_events::sns::SnsRecord;
use lambda_runtime::{run, service_fn, Error, LambdaEvent};
use tracing::info;

// Built with the following dependencies:
//  aws_lambda_events = { version = "0.10.0", default-features = false, features = ["sns"] }
//  lambda_runtime = "0.8.1"
//  tokio = { version = "1", features = ["macros"] }
//  tracing = { version = "0.1", features = ["log"] }
//  tracing-subscriber = { version = "0.3", default-features = false, features = ["fmt"] }

async fn function_handler(event: LambdaEvent<SnsEvent>) -> Result<(), Error> {
    for event in event.payload.records {
        process_record(&event)?;
    }
    
    Ok(())
}

fn process_record(record: &SnsRecord) -> Result<(), Error> {
    info!("Processing SNS Message: {}", record.sns.message);

    // Implement your record handling code here.

    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .without_time()
        .init();

    run(service_fn(function_handler)).await
}

             
        
    
        

    To create the function
        Create a directory for the project, and then switch to that directory.
        mkdir sns-tutorial
cd sns-tutorial
      
        Copy the sample JavaScript code into a new file named index.js.
      
        Create a deployment package using the following zip command.
        zip function.zip index.js
      
        Run the following AWS CLI command to create your Lambda function in account B.
        aws lambda create-function --function-name Function-With-SNS \
    --zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
    --role arn:aws:iam::<AccountB_ID>:role/lambda-sns-role  \
    --timeout 60 --profile accountB
        You should see output similar to the following.
        {
    "FunctionName": "Function-With-SNS",
    "FunctionArn": "arn:aws:lambda:us-west-2:123456789012:function:Function-With-SNS",
    "Runtime": "nodejs18.x",
    "Role": "arn:aws:iam::123456789012:role/lambda_basic_role",
    "Handler": "index.handler",
    ...
    "RuntimeVersionConfig": {
        "RuntimeVersionArn": "arn:aws:lambda:us-west-2::runtime:7d5f06b69c951da8a48b926ce280a9daf2e8bb1a74fc4a2672580c787d608206"
    }
}
      
        Record the Amazon Resource Name (ARN) of your function. You’ll need it later in the tutorial
          when you add permissions to allow Amazon SNS to invoke your function.
      
   
    Add permissions to function (account B)
    
       
        
       
       
    
    For Amazon SNS to invoke your function, you need to grant it permission in a statement on a resource-based policy. 
      You add this statement using the AWS CLI add-permission command.
    To grant Amazon SNS permission to invoke your function
        In account B, run the following AWS CLI command using the ARN for your Amazon SNS topic you recorded earlier.
        aws lambda add-permission --function-name Function-With-SNS \
    --source-arn arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda \
    --statement-id function-with-sns --action "lambda:InvokeFunction" \
    --principal sns.amazonaws.com --profile accountB
        You should see output similar to the following.
        {
    "Statement": "{\"Condition\":{\"ArnLike\":{\"AWS:SourceArn\":
      \"arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda\"}},
      \"Action\":[\"lambda:InvokeFunction\"],
      \"Resource\":\"arn:aws:lambda:us-east-1:<AccountB_ID>:function:Function-With-SNS\",
      \"Effect\":\"Allow\",\"Principal\":{\"Service\":\"sns.amazonaws.com\"},
      \"Sid\":\"function-with-sns\"}"
}
      
    NoteIf the account with the Amazon SNS topic is hosted in an opt-in AWS Region, 
        you need to specify the region in the principal. For example, if you're working with an Amazon SNS topic in the Asia Pacific (Hong Kong) region, 
        you need to specify sns.ap-east-1.amazonaws.com instead of sns.amazonaws.com for the principal. 
   
    Grant cross-account permission for Amazon SNS subscription (account A)
    
       
        
       
       
    
    For your Lambda function in account B to subscribe to the Amazon SNS topic you created in account A, 
      you need to grant permission for account B to subscribe to your topic. You grant this permission using the 
      AWS CLI add-permission command. 
    To grant permission for account B to subscribe to the topic
        In account A, run the following AWS CLI command. Use the ARN for the Amazon SNS topic you recorded earlier.
        aws sns add-permission --label lambda-access --aws-account-id <AccountB_ID> \
    --topic-arn arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda \  
    --action-name Subscribe ListSubscriptionsByTopic --profile accountA
      
   
  Create a subscription (account B)
  
     
      
     
     
  
  In account B, you now subscribe your Lambda function to the Amazon SNS topic you created at the beginning of the 
    tutorial in account A. When a message is sent to this topic (sns-topic-for-lambda), Amazon SNS invokes 
    your Lambda function Function-With-SNS in account B. 
 To create a subscription
     In account B, run the following AWS CLI command. Use your default region you created your topic in and the 
     ARNs for your topic and Lambda function.
     aws sns subscribe --protocol lambda \
    --region us-east-1 \
    --topic-arn arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda \
    --notification-endpoint arn:aws:lambda:us-east-1:<AccountB_ID>:function:Function-With-SNS \
    --profile accountB
     You should see output similar to the following.
     {
    "SubscriptionArn": "arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda:5d906xxxx-7c8x-45dx-a9dx-0484e31c98xx"
}
   
 
    Publish messages to topic (account A and account B)
    
       
        
       
       
    
    Now that your Lambda function in account B is subscribed to your Amazon SNS topic in account A, 
      it’s time to test your setup by publishing messages to your topic. To confirm that Amazon SNS has invoked your Lambda function, you use CloudWatch Logs to view 
      your function’s output.
    To publish a message to your topic and view your function's output
        Enter Hello World into a text file and save it as message.txt.
      
        From the same directory you saved your text file in, run the following AWS CLI command in account A. 
        Use the ARN for your own topic.
        aws sns publish --message file://message.txt --subject Test \
    --topic-arn arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda \
    --profile accountA
        This will return a message ID with a unique identifier, indicating that Amazon SNS has accepted the message. Amazon SNS then attempts to deliver 
          the message to the topic’s subscribers. To confirm that Amazon SNS has invoked your Lambda function, use CloudWatch Logs to view your function’s output:
      
        In account B, open the Log groups page of the Amazon CloudWatch console.
      
        Choose the log group for your function (/aws/lambda/Function-With-SNS).
      
        Choose the most recent log stream.
      
        If your function was correctly invoked, you’ll see output similar to the following showing the contents of the message you published to 
          your topic.
        2023-07-31T21:42:51.250Z c1cba6b8-ade9-4380-aa32-d1a225da0e48 INFO Processed message Hello World
2023-07-31T21:42:51.250Z c1cba6b8-ade9-4380-aa32-d1a225da0e48 INFO done
      
   
    Clean up your resources
     
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    
    In Account A, clean up your Amazon SNS topic.
    To delete the Amazon SNS topic
    Open the Topics page of the Amazon SNS console.
  
    Select the topic you created.
  
    Choose Delete.
  
    Enter delete me in the text input field.
  
    Choose Delete.
   
    
    In Account B, clean up your execution role, Lambda function, and Amazon SNS subscription.
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    
    To delete the Amazon SNS subscription
    Open the Subscriptions page of the Amazon SNS console.
  
    Select the subscription you created.
  
    Choose Delete, Delete.
  
  Document ConventionsSNSLambda permissionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideChoosing an API typeAdding an endpoint to your Lambda functionProxy integrationEvent formatResponse formatPermissionsSample applicationInvoking a Lambda function using an Amazon API Gateway endpointYou can create a web API with an HTTP endpoint for your Lambda function by using Amazon API Gateway. API Gateway provides tools
    for creating and documenting web APIs that route HTTP requests to Lambda functions. You can secure access to your API
    with authentication and authorization controls. Your APIs can serve traffic over the internet or can be accessible
    only within your VPC.TipLambda offers two ways to invoke your function through an HTTP endpoint: API Gateway and Lambda function URLs. If you're not sure which is the best method for your 
      use case, see Select a method to invoke your Lambda function using an HTTP request.Resources in your API define one or more methods, such as GET or POST. Methods have an integration that routes
    requests to a Lambda function or another integration type. You can define each resource and method individually, or
    use special resource and method types to match all requests that fit a pattern. A proxy
      resource catches all paths beneath a resource. The ANY method catches all HTTP
    methods.SectionsChoosing an API typeAdding an endpoint to your Lambda functionProxy integrationEvent formatResponse formatPermissionsSample applicationTutorial: Using Lambda with API GatewayHandling Lambda errors with an API Gateway APISelect a method to invoke your Lambda function using an HTTP request
    Choosing an API type
    
    API Gateway supports three types of APIs that invoke Lambda functions:
    
       
       
       
    
        HTTP API: A lightweight, low-latency RESTful API.
      
        REST API: A customizable, feature-rich RESTful API.
      
        WebSocket API: A web API that maintains persistent connections
          with clients for full-duplex communication.
      
    
    HTTP APIs and REST APIs are both RESTful APIs that process HTTP requests and return responses. HTTP APIs are
      newer and are built with the API Gateway version 2 API. The following features are new for HTTP APIs:
    
      HTTP API features
       
       
       
    
        Automatic deployments – When you modify routes or integrations,
          changes deploy automatically to stages that have automatic deployment enabled.
      
        Default stage – You can create a default stage
          ($default) to serve requests at the root path of your API's URL. For named stages, you must
          include the stage name at the beginning of the path.
      
        CORS configuration – You can configure your API to add CORS
          headers to outgoing responses, instead of adding them manually in your function code.
      
    
    REST APIs are the classic RESTful APIs that API Gateway has supported since launch. REST APIs currently have more
      customization, integration, and management features.
    
      REST API features
       
       
       
    
        Integration types – REST APIs support custom Lambda integrations.
          With a custom integration, you can send just the body of the request to the function, or apply a transform
          template to the request body before sending it to the function.
      
        Access control – REST APIs support more options for authentication
          and authorization.
      
        Monitoring and tracing – REST APIs support AWS X-Ray tracing and
          additional logging options.
      
    
    For a detailed comparison, see Choose between HTTP APIs and REST APIs in the API Gateway Developer Guide.
    
    WebSocket APIs also use the API Gateway version 2 API and support a similar feature set. Use a WebSocket API for
      applications that benefit from a persistent connection between the client and API. WebSocket APIs provide
      full-duplex communication, which means that both the client and the API can send messages continuously without
      waiting for a response.
    
    HTTP APIs support a simplified event format (version 2.0). For an example of an event from an HTTP
      API, see Create AWS Lambda proxy integrations for HTTP APIs in API Gateway.
    For more information, see Create AWS Lambda proxy integrations for HTTP APIs in API Gateway.
   
    Adding an endpoint to your Lambda function
    To add a public endpoint to your Lambda functionOpen the Functions page of the Lambda console.
        Choose a function.
      
        Under Function overview, choose Add trigger.
      
        Select API Gateway.
      
        Choose Create an API or Use an existing API.
        
            New API: For API type, choose HTTP API. For more information, see Choosing an API type.
            
            Existing API: Select the API from the dropdown list or enter the API ID (for example, r3pmxmplak).
          
      
        For Security, choose Open.
      
        Choose Add.
      
   
    Proxy integration
    
    API Gateway APIs are comprised of stages, resources, methods, and integrations. The stage and resource determine the
      path of the endpoint:
    
      API path format
       
       
       
       
    
        /prod/ – The prod stage and root resource.
      
        /prod/user – The prod stage and user resource.
      
        /dev/{proxy+} – Any route in the dev stage.
      
        / – (HTTP APIs) The default stage and root resource.
      
    A Lambda integration maps a path and HTTP method combination to a Lambda function. You can configure API Gateway to pass
      the body of the HTTP request as-is (custom integration), or to encapsulate the request body in a document that
      includes all of the request information including headers, resource, path, and method.
    For more information, see Lambda proxy integrations in API Gateway.
   
    Event format
      
    Amazon API Gateway invokes your function synchronously with an event that contains
      a JSON representation of the HTTP request. For a custom integration, the event is the body of the request. For a
      proxy integration, the event has a defined structure. For an example of a proxy event from an API Gateway REST
      API, see Input format of a Lambda function for proxy integration in the API Gateway Developer Guide.
   
    Response format
    
    API Gateway waits for a response from your function and relays the result to the caller. For a custom integration, you
      define an integration response and a method response to convert the output from the function to an HTTP response.
      For a proxy integration, the function must respond with a representation of the response in a specific
      format.
    The following example shows a response object from a Node.js function. The response object represents a
      successful HTTP response that contains a JSON document.
    Example index.mjs – Proxy integration response object (Node.js)var response = {
      "statusCode": 200,
      "headers": {
        "Content-Type": "application/json"
      },
      "isBase64Encoded": false,
      "multiValueHeaders": { 
        "X-Custom-Header": ["My value", "My other value"],
      },
      "body": "{\n  \"TotalCodeSize\": 104330022,\n  \"FunctionCount\": 26\n}"
    }
    The Lambda runtime serializes the response object into JSON and sends it to the API. The API parses the response
      and uses it to create an HTTP response, which it then sends to the client that made the original request.
    Example HTTP response< HTTP/1.1 200 OK
  < Content-Type: application/json
  < Content-Length: 55
  < Connection: keep-alive
  < x-amzn-RequestId: 32998fea-xmpl-4268-8c72-16138d629356
  < X-Custom-Header: My value
  < X-Custom-Header: My other value
  < X-Amzn-Trace-Id: Root=1-5e6aa925-ccecxmplbae116148e52f036
  <
  {
    "TotalCodeSize": 104330022,
    "FunctionCount": 26
  }
   
    Permissions
    
    Amazon API Gateway gets permission to invoke your function from the function's resource-based policy. You can grant invoke permission to an
      entire API, or grant limited access to a stage, resource, or method.
    
    When you add an API to your function by using the Lambda console, using the API Gateway console, or in an AWS SAM
      template, the function's resource-based policy is updated automatically. The following is an example function policy.
    Example function policy{
  "Version": "2012-10-17",
  "Id": "default",
  "Statement": [
    {
      "Sid": "nodejs-apig-functiongetEndpointPermissionProd-BWDBXMPLXE2F",
      "Effect": "Allow",
      "Principal": {
        "Service": "apigateway.amazonaws.com"
      },
      "Action": "lambda:InvokeFunction",
      "Resource": "arn:aws:lambda:us-east-2:111122223333:function:nodejs-apig-function-1G3MXMPLXVXYI",
      "Condition": {
        "StringEquals": {
          "aws:SourceAccount": "111122223333"
        },
        "ArnLike": {
          "aws:SourceArn": "arn:aws:execute-api:us-east-2:111122223333:ktyvxmpls1/*/GET/"
        }
      }
    }
  ]
}
    
    You can manage function policy permissions manually with the following API operations:
    
       
       
       
    
        AddPermission
      
        RemovePermission
      
        GetPolicy
      
    To grant invocation permission to an existing API, use the add-permission command. Example:
    aws lambda add-permission \
  --function-name my-function \
  --statement-id apigateway-get --action lambda:InvokeFunction \
  --principal apigateway.amazonaws.com \
  --source-arn "arn:aws:execute-api:us-east-2:123456789012:mnh1xmpli7/default/GET/"
    
    You should see the following output:
    {
    "Statement": "{\"Sid\":\"apigateway-test-2\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"apigateway.amazonaws.com\"},\"Action\":\"lambda:InvokeFunction\",\"Resource\":\"arn:aws:lambda:us-east-2:123456789012:function:my-function\",\"Condition\":{\"ArnLike\":{\"AWS:SourceArn\":\"arn:aws:execute-api:us-east-2:123456789012:mnh1xmpli7/default/GET\"}}}"
}
    
    NoteIf your function and API are in different AWS Regions, the Region identifier in the source ARN must match the
        Region of the function, not the Region of the API. When API Gateway invokes a function, it uses a resource ARN that is
        based on the ARN of the API, but modified to match the function's Region.
    The source ARN in this example grants permission to an integration on the GET method of the root resource in
      the default stage of an API, with ID mnh1xmpli7. You can use an asterisk in the source ARN to grant
      permissions to multiple stages, methods, or resources.
    
      Resource patterns
       
       
       
    
        mnh1xmpli7/*/GET/* – GET method on all resources in all stages.
      
        mnh1xmpli7/prod/ANY/user – ANY method on the user resource in the
            prod stage.
      
        mnh1xmpli7/*/*/* – Any method on all resources in all stages.
      
    For details on viewing the policy and removing statements, see Viewing resource-based IAM policies in Lambda.
   
    Sample application
    
    The API Gateway with Node.js sample app includes a function with an AWS SAM
      template that creates a REST API that has AWS X-Ray tracing enabled. It also includes scripts for deploying,
      invoking the function, testing the API, and cleanup.
    
  Document ConventionsTroubleshootingTutorialDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoCreateEventSourceMappingCreates a mapping between an event source and an AWS Lambda function. Lambda reads items from the event source and invokes the function.For details about how to configure different event sources, see the following topics. 
       
       
       
       
       
       
       
   
         
            
            Amazon DynamoDB Streams
         
      
         
            
            Amazon Kinesis
         
      
         
            
            Amazon SQS
         
      
         
            
            Amazon MQ and RabbitMQ
         
      
         
            
            Amazon MSK
         
      
         
            
            Apache Kafka
         
      
         
            
            Amazon DocumentDB
         
      The following error handling options are available only for DynamoDB and Kinesis event sources:
       
       
       
       
   
         
            BisectBatchOnFunctionError – If the function returns an error, split the batch in two and retry.
      
         
            MaximumRecordAgeInSeconds – Discard records older than the specified age. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires
      
         
            MaximumRetryAttempts – Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
      
         
            ParallelizationFactor – Process multiple batches from each shard concurrently.
      For stream sources (DynamoDB, Kinesis, Amazon MSK, and self-managed Apache Kafka), the following option is also available:
       
   
         
            DestinationConfig – Send discarded records to an Amazon SQS queue, Amazon SNS topic, or 
            Amazon S3 bucket.
      For information about which configuration parameters apply to each event source, see the following topics.
       
       
       
       
       
       
       
   
         
            
          Amazon DynamoDB Streams
         
      
         
            
          Amazon Kinesis
         
      
         
            
          Amazon SQS
         
      
         
            
          Amazon MQ and RabbitMQ
         
      
         
            
          Amazon MSK
         
      
         
            
          Apache Kafka
         
      
         
            
          Amazon DocumentDB
         
      
      Request Syntax
      POST /2015-03-31/event-source-mappings/ HTTP/1.1
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "Enabled": boolean,
   "EventSourceArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FunctionName": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "Tags": { 
      "string" : "string" 
   },
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number
}
    
      URI Request Parameters
      The request does not use any URI parameters.
    
      Request Body
      The request accepts the following data in JSON format.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation
  (6 MB).
               
                   
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – Default 100. Max 10,000.
                  
                     
                        Amazon DynamoDB Streams – Default 100. Max 10,000.
                  
                     
                        Amazon Simple Queue Service – Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Self-managed Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Amazon MQ (ActiveMQ and RabbitMQ) – Default 100. Max 10,000.
                  
                     
                        DocumentDB – Default 100. Max 10,000.
                  
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
               Required: No
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry.
               Type: Boolean
               Required: No
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Kafka only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
               Required: No
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
               Required: No
            
          
            
               
                  Enabled
               
            
            
               When true, the event source mapping is active. When false, Lambda pauses polling and invocation.
               Default: True
               Type: Boolean
               Required: No
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – The ARN of the data stream or a stream consumer.
                  
                     
                        Amazon DynamoDB Streams – The ARN of the stream.
                  
                     
                        Amazon Simple Queue Service – The ARN of the queue.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – The ARN of the cluster or the ARN of the VPC connection (for cross-account event source mappings).
                  
                     
                        Amazon MQ – The ARN of the broker.
                  
                     
                        Amazon DocumentDB – The ARN of the DocumentDB change stream.
                  
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
               Required: No
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               Type: FilterCriteria object
               Required: No
            
          
            
               
                  FunctionName
               
            
            
               The name or ARN of the Lambda function.
               
                  Name formats
                   
                   
                   
                   
               
                     
                        Function name – MyFunction.
                  
                     
                        Function ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction.
                  
                     
                        Version or Alias ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD.
                  
                     
                        Partial ARN – 123456789012:function:MyFunction.
                  
               The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64
      characters in length.
               Type: String
               Length Constraints: Minimum length of 1. Maximum length of 140.
               Pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\d{1}:)?(\d{12}:)?(function:)?([a-zA-Z0-9-_]+)(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
               Required: Yes
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
               Required: No
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
      By default, Lambda does not encrypt your filter criteria object. Specify this
      property to encrypt data using your own customer managed key.
    
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
               Required: No
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For Kinesis, DynamoDB, and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For Kinesis, DynamoDB, and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
               Required: No
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is infinite (-1).
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
               Required: No
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
               Required: No
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
               Required: No
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process from each shard concurrently.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
               Required: No
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
               Required: No
            
          
            
               
                  Queues
               
            
            
                (MQ) The name of the Amazon MQ broker destination queue to consume. 
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
               Required: No
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
               Required: No
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster to receive records from.
               Type: SelfManagedEventSource object
               Required: No
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of authentication protocols or VPC components required to secure your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
               Required: No
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
               Required: No
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
               Required: No
            
          
            
               
                  Tags
               
            
            
               A list of tags to apply to the event source mapping.
               Type: String to string map
               Required: No
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
               Required: No
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
               Required: No
            
         
    
      Response Syntax
      HTTP/1.1 202
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "EventSourceArn": "string",
   "EventSourceMappingArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FilterCriteriaError": { 
      "ErrorCode": "string",
      "Message": "string"
   },
   "FunctionArn": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "LastModified": number,
   "LastProcessingResult": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "State": "string",
   "StateTransitionReason": "string",
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number,
   "UUID": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 202 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
               Default value: Varies by service. For Amazon SQS, the default is 10. For all other services, the default is 100.
               Related setting: When you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.
               Type: Boolean
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  EventSourceMappingArn
               
            
            
               The Amazon Resource Name (ARN) of the event source mapping.
               Type: String
               Length Constraints: Minimum length of 85. Maximum length of 120.
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso([a-z]?)))?-[a-z]+-\d{1}:\d{12}:event-source-mapping:[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}
               
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               If filter criteria is encrypted, this field shows up as null in the response
      of ListEventSourceMapping API calls. You can view this field in plaintext in the response of
      GetEventSourceMapping and DeleteEventSourceMapping calls if you have
      kms:Decrypt permissions for the correct AWS KMS key.
               Type: FilterCriteria object
            
          
            
               
                  FilterCriteriaError
               
            
            
               An object that contains details about an error related to filter criteria encryption.
               Type: FilterCriteriaError object
            
          
            
               
                  FunctionArn
               
            
            
               The ARN of the Lambda function.
               Type: String
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\d{1}:\d{12}:function:[a-zA-Z0-9-_]+(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
            
          
            
               
                  LastModified
               
            
            
               The date that the event source mapping was last updated or that its state changed, in Unix time seconds.
               Type: Timestamp
            
          
            
               
                  LastProcessingResult
               
            
            
               The result of the last Lambda invocation of your function.
               Type: String
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For streams and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1,
which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records.
               NoteThe minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1,
which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
            
          
            
               
                  Queues
               
            
            
                (Amazon MQ) The name of the Amazon MQ broker destination queue to consume.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster for your event source.
               Type: SelfManagedEventSource object
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
            
          
            
               
                  State
               
            
            
               The state of the event source mapping. It can be one of the following: Creating,
        Enabling, Enabled, Disabling, Disabled,
        Updating, or Deleting.
               Type: String
            
          
            
               
                  StateTransitionReason
               
            
            
               Indicates whether a user or Lambda made the last change to the event source mapping.
               Type: String
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
            
          
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceConflictException
               
            
            
               The resource already exists, or another operation is in progress.
               HTTP Status Code: 409
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsCreateCodeSigningConfigCreateFunctionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoCreateEventSourceMappingCreates a mapping between an event source and an AWS Lambda function. Lambda reads items from the event source and invokes the function.For details about how to configure different event sources, see the following topics. 
       
       
       
       
       
       
       
   
         
            
            Amazon DynamoDB Streams
         
      
         
            
            Amazon Kinesis
         
      
         
            
            Amazon SQS
         
      
         
            
            Amazon MQ and RabbitMQ
         
      
         
            
            Amazon MSK
         
      
         
            
            Apache Kafka
         
      
         
            
            Amazon DocumentDB
         
      The following error handling options are available only for DynamoDB and Kinesis event sources:
       
       
       
       
   
         
            BisectBatchOnFunctionError – If the function returns an error, split the batch in two and retry.
      
         
            MaximumRecordAgeInSeconds – Discard records older than the specified age. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires
      
         
            MaximumRetryAttempts – Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
      
         
            ParallelizationFactor – Process multiple batches from each shard concurrently.
      For stream sources (DynamoDB, Kinesis, Amazon MSK, and self-managed Apache Kafka), the following option is also available:
       
   
         
            DestinationConfig – Send discarded records to an Amazon SQS queue, Amazon SNS topic, or 
            Amazon S3 bucket.
      For information about which configuration parameters apply to each event source, see the following topics.
       
       
       
       
       
       
       
   
         
            
          Amazon DynamoDB Streams
         
      
         
            
          Amazon Kinesis
         
      
         
            
          Amazon SQS
         
      
         
            
          Amazon MQ and RabbitMQ
         
      
         
            
          Amazon MSK
         
      
         
            
          Apache Kafka
         
      
         
            
          Amazon DocumentDB
         
      
      Request Syntax
      POST /2015-03-31/event-source-mappings/ HTTP/1.1
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "Enabled": boolean,
   "EventSourceArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FunctionName": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "Tags": { 
      "string" : "string" 
   },
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number
}
    
      URI Request Parameters
      The request does not use any URI parameters.
    
      Request Body
      The request accepts the following data in JSON format.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation
  (6 MB).
               
                   
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – Default 100. Max 10,000.
                  
                     
                        Amazon DynamoDB Streams – Default 100. Max 10,000.
                  
                     
                        Amazon Simple Queue Service – Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Self-managed Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Amazon MQ (ActiveMQ and RabbitMQ) – Default 100. Max 10,000.
                  
                     
                        DocumentDB – Default 100. Max 10,000.
                  
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
               Required: No
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry.
               Type: Boolean
               Required: No
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Kafka only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
               Required: No
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
               Required: No
            
          
            
               
                  Enabled
               
            
            
               When true, the event source mapping is active. When false, Lambda pauses polling and invocation.
               Default: True
               Type: Boolean
               Required: No
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – The ARN of the data stream or a stream consumer.
                  
                     
                        Amazon DynamoDB Streams – The ARN of the stream.
                  
                     
                        Amazon Simple Queue Service – The ARN of the queue.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – The ARN of the cluster or the ARN of the VPC connection (for cross-account event source mappings).
                  
                     
                        Amazon MQ – The ARN of the broker.
                  
                     
                        Amazon DocumentDB – The ARN of the DocumentDB change stream.
                  
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
               Required: No
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               Type: FilterCriteria object
               Required: No
            
          
            
               
                  FunctionName
               
            
            
               The name or ARN of the Lambda function.
               
                  Name formats
                   
                   
                   
                   
               
                     
                        Function name – MyFunction.
                  
                     
                        Function ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction.
                  
                     
                        Version or Alias ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD.
                  
                     
                        Partial ARN – 123456789012:function:MyFunction.
                  
               The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64
      characters in length.
               Type: String
               Length Constraints: Minimum length of 1. Maximum length of 140.
               Pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\d{1}:)?(\d{12}:)?(function:)?([a-zA-Z0-9-_]+)(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
               Required: Yes
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
               Required: No
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
      By default, Lambda does not encrypt your filter criteria object. Specify this
      property to encrypt data using your own customer managed key.
    
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
               Required: No
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For Kinesis, DynamoDB, and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For Kinesis, DynamoDB, and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
               Required: No
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is infinite (-1).
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
               Required: No
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
               Required: No
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
               Required: No
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process from each shard concurrently.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
               Required: No
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
               Required: No
            
          
            
               
                  Queues
               
            
            
                (MQ) The name of the Amazon MQ broker destination queue to consume. 
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
               Required: No
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
               Required: No
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster to receive records from.
               Type: SelfManagedEventSource object
               Required: No
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of authentication protocols or VPC components required to secure your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
               Required: No
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
               Required: No
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
               Required: No
            
          
            
               
                  Tags
               
            
            
               A list of tags to apply to the event source mapping.
               Type: String to string map
               Required: No
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
               Required: No
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
               Required: No
            
         
    
      Response Syntax
      HTTP/1.1 202
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "EventSourceArn": "string",
   "EventSourceMappingArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FilterCriteriaError": { 
      "ErrorCode": "string",
      "Message": "string"
   },
   "FunctionArn": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "LastModified": number,
   "LastProcessingResult": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "State": "string",
   "StateTransitionReason": "string",
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number,
   "UUID": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 202 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
               Default value: Varies by service. For Amazon SQS, the default is 10. For all other services, the default is 100.
               Related setting: When you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.
               Type: Boolean
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  EventSourceMappingArn
               
            
            
               The Amazon Resource Name (ARN) of the event source mapping.
               Type: String
               Length Constraints: Minimum length of 85. Maximum length of 120.
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso([a-z]?)))?-[a-z]+-\d{1}:\d{12}:event-source-mapping:[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}
               
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               If filter criteria is encrypted, this field shows up as null in the response
      of ListEventSourceMapping API calls. You can view this field in plaintext in the response of
      GetEventSourceMapping and DeleteEventSourceMapping calls if you have
      kms:Decrypt permissions for the correct AWS KMS key.
               Type: FilterCriteria object
            
          
            
               
                  FilterCriteriaError
               
            
            
               An object that contains details about an error related to filter criteria encryption.
               Type: FilterCriteriaError object
            
          
            
               
                  FunctionArn
               
            
            
               The ARN of the Lambda function.
               Type: String
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\d{1}:\d{12}:function:[a-zA-Z0-9-_]+(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
            
          
            
               
                  LastModified
               
            
            
               The date that the event source mapping was last updated or that its state changed, in Unix time seconds.
               Type: Timestamp
            
          
            
               
                  LastProcessingResult
               
            
            
               The result of the last Lambda invocation of your function.
               Type: String
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For streams and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1,
which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records.
               NoteThe minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1,
which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
            
          
            
               
                  Queues
               
            
            
                (Amazon MQ) The name of the Amazon MQ broker destination queue to consume.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster for your event source.
               Type: SelfManagedEventSource object
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
            
          
            
               
                  State
               
            
            
               The state of the event source mapping. It can be one of the following: Creating,
        Enabling, Enabled, Disabling, Disabled,
        Updating, or Deleting.
               Type: String
            
          
            
               
                  StateTransitionReason
               
            
            
               Indicates whether a user or Lambda made the last change to the event source mapping.
               Type: String
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
            
          
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceConflictException
               
            
            
               The resource already exists, or another operation is in progress.
               HTTP Status Code: 409
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsCreateCodeSigningConfigCreateFunctionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoCreateEventSourceMappingCreates a mapping between an event source and an AWS Lambda function. Lambda reads items from the event source and invokes the function.For details about how to configure different event sources, see the following topics. 
       
       
       
       
       
       
       
   
         
            
            Amazon DynamoDB Streams
         
      
         
            
            Amazon Kinesis
         
      
         
            
            Amazon SQS
         
      
         
            
            Amazon MQ and RabbitMQ
         
      
         
            
            Amazon MSK
         
      
         
            
            Apache Kafka
         
      
         
            
            Amazon DocumentDB
         
      The following error handling options are available only for DynamoDB and Kinesis event sources:
       
       
       
       
   
         
            BisectBatchOnFunctionError – If the function returns an error, split the batch in two and retry.
      
         
            MaximumRecordAgeInSeconds – Discard records older than the specified age. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires
      
         
            MaximumRetryAttempts – Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
      
         
            ParallelizationFactor – Process multiple batches from each shard concurrently.
      For stream sources (DynamoDB, Kinesis, Amazon MSK, and self-managed Apache Kafka), the following option is also available:
       
   
         
            DestinationConfig – Send discarded records to an Amazon SQS queue, Amazon SNS topic, or 
            Amazon S3 bucket.
      For information about which configuration parameters apply to each event source, see the following topics.
       
       
       
       
       
       
       
   
         
            
          Amazon DynamoDB Streams
         
      
         
            
          Amazon Kinesis
         
      
         
            
          Amazon SQS
         
      
         
            
          Amazon MQ and RabbitMQ
         
      
         
            
          Amazon MSK
         
      
         
            
          Apache Kafka
         
      
         
            
          Amazon DocumentDB
         
      
      Request Syntax
      POST /2015-03-31/event-source-mappings/ HTTP/1.1
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "Enabled": boolean,
   "EventSourceArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FunctionName": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "Tags": { 
      "string" : "string" 
   },
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number
}
    
      URI Request Parameters
      The request does not use any URI parameters.
    
      Request Body
      The request accepts the following data in JSON format.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation
  (6 MB).
               
                   
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – Default 100. Max 10,000.
                  
                     
                        Amazon DynamoDB Streams – Default 100. Max 10,000.
                  
                     
                        Amazon Simple Queue Service – Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Self-managed Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Amazon MQ (ActiveMQ and RabbitMQ) – Default 100. Max 10,000.
                  
                     
                        DocumentDB – Default 100. Max 10,000.
                  
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
               Required: No
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry.
               Type: Boolean
               Required: No
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Kafka only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
               Required: No
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
               Required: No
            
          
            
               
                  Enabled
               
            
            
               When true, the event source mapping is active. When false, Lambda pauses polling and invocation.
               Default: True
               Type: Boolean
               Required: No
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – The ARN of the data stream or a stream consumer.
                  
                     
                        Amazon DynamoDB Streams – The ARN of the stream.
                  
                     
                        Amazon Simple Queue Service – The ARN of the queue.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – The ARN of the cluster or the ARN of the VPC connection (for cross-account event source mappings).
                  
                     
                        Amazon MQ – The ARN of the broker.
                  
                     
                        Amazon DocumentDB – The ARN of the DocumentDB change stream.
                  
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
               Required: No
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               Type: FilterCriteria object
               Required: No
            
          
            
               
                  FunctionName
               
            
            
               The name or ARN of the Lambda function.
               
                  Name formats
                   
                   
                   
                   
               
                     
                        Function name – MyFunction.
                  
                     
                        Function ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction.
                  
                     
                        Version or Alias ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD.
                  
                     
                        Partial ARN – 123456789012:function:MyFunction.
                  
               The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64
      characters in length.
               Type: String
               Length Constraints: Minimum length of 1. Maximum length of 140.
               Pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\d{1}:)?(\d{12}:)?(function:)?([a-zA-Z0-9-_]+)(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
               Required: Yes
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
               Required: No
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
      By default, Lambda does not encrypt your filter criteria object. Specify this
      property to encrypt data using your own customer managed key.
    
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
               Required: No
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For Kinesis, DynamoDB, and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For Kinesis, DynamoDB, and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
               Required: No
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is infinite (-1).
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
               Required: No
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
               Required: No
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
               Required: No
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process from each shard concurrently.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
               Required: No
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
               Required: No
            
          
            
               
                  Queues
               
            
            
                (MQ) The name of the Amazon MQ broker destination queue to consume. 
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
               Required: No
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
               Required: No
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster to receive records from.
               Type: SelfManagedEventSource object
               Required: No
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of authentication protocols or VPC components required to secure your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
               Required: No
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
               Required: No
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
               Required: No
            
          
            
               
                  Tags
               
            
            
               A list of tags to apply to the event source mapping.
               Type: String to string map
               Required: No
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
               Required: No
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
               Required: No
            
         
    
      Response Syntax
      HTTP/1.1 202
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "EventSourceArn": "string",
   "EventSourceMappingArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FilterCriteriaError": { 
      "ErrorCode": "string",
      "Message": "string"
   },
   "FunctionArn": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "LastModified": number,
   "LastProcessingResult": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "State": "string",
   "StateTransitionReason": "string",
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number,
   "UUID": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 202 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
               Default value: Varies by service. For Amazon SQS, the default is 10. For all other services, the default is 100.
               Related setting: When you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.
               Type: Boolean
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  EventSourceMappingArn
               
            
            
               The Amazon Resource Name (ARN) of the event source mapping.
               Type: String
               Length Constraints: Minimum length of 85. Maximum length of 120.
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso([a-z]?)))?-[a-z]+-\d{1}:\d{12}:event-source-mapping:[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}
               
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               If filter criteria is encrypted, this field shows up as null in the response
      of ListEventSourceMapping API calls. You can view this field in plaintext in the response of
      GetEventSourceMapping and DeleteEventSourceMapping calls if you have
      kms:Decrypt permissions for the correct AWS KMS key.
               Type: FilterCriteria object
            
          
            
               
                  FilterCriteriaError
               
            
            
               An object that contains details about an error related to filter criteria encryption.
               Type: FilterCriteriaError object
            
          
            
               
                  FunctionArn
               
            
            
               The ARN of the Lambda function.
               Type: String
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\d{1}:\d{12}:function:[a-zA-Z0-9-_]+(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
            
          
            
               
                  LastModified
               
            
            
               The date that the event source mapping was last updated or that its state changed, in Unix time seconds.
               Type: Timestamp
            
          
            
               
                  LastProcessingResult
               
            
            
               The result of the last Lambda invocation of your function.
               Type: String
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For streams and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1,
which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records.
               NoteThe minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1,
which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
            
          
            
               
                  Queues
               
            
            
                (Amazon MQ) The name of the Amazon MQ broker destination queue to consume.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster for your event source.
               Type: SelfManagedEventSource object
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
            
          
            
               
                  State
               
            
            
               The state of the event source mapping. It can be one of the following: Creating,
        Enabling, Enabled, Disabling, Disabled,
        Updating, or Deleting.
               Type: String
            
          
            
               
                  StateTransitionReason
               
            
            
               Indicates whether a user or Lambda made the last change to the event source mapping.
               Type: String
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
            
          
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceConflictException
               
            
            
               The resource already exists, or another operation is in progress.
               HTTP Status Code: 409
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsCreateCodeSigningConfigCreateFunctionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoCreateEventSourceMappingCreates a mapping between an event source and an AWS Lambda function. Lambda reads items from the event source and invokes the function.For details about how to configure different event sources, see the following topics. 
       
       
       
       
       
       
       
   
         
            
            Amazon DynamoDB Streams
         
      
         
            
            Amazon Kinesis
         
      
         
            
            Amazon SQS
         
      
         
            
            Amazon MQ and RabbitMQ
         
      
         
            
            Amazon MSK
         
      
         
            
            Apache Kafka
         
      
         
            
            Amazon DocumentDB
         
      The following error handling options are available only for DynamoDB and Kinesis event sources:
       
       
       
       
   
         
            BisectBatchOnFunctionError – If the function returns an error, split the batch in two and retry.
      
         
            MaximumRecordAgeInSeconds – Discard records older than the specified age. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires
      
         
            MaximumRetryAttempts – Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
      
         
            ParallelizationFactor – Process multiple batches from each shard concurrently.
      For stream sources (DynamoDB, Kinesis, Amazon MSK, and self-managed Apache Kafka), the following option is also available:
       
   
         
            DestinationConfig – Send discarded records to an Amazon SQS queue, Amazon SNS topic, or 
            Amazon S3 bucket.
      For information about which configuration parameters apply to each event source, see the following topics.
       
       
       
       
       
       
       
   
         
            
          Amazon DynamoDB Streams
         
      
         
            
          Amazon Kinesis
         
      
         
            
          Amazon SQS
         
      
         
            
          Amazon MQ and RabbitMQ
         
      
         
            
          Amazon MSK
         
      
         
            
          Apache Kafka
         
      
         
            
          Amazon DocumentDB
         
      
      Request Syntax
      POST /2015-03-31/event-source-mappings/ HTTP/1.1
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "Enabled": boolean,
   "EventSourceArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FunctionName": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "Tags": { 
      "string" : "string" 
   },
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number
}
    
      URI Request Parameters
      The request does not use any URI parameters.
    
      Request Body
      The request accepts the following data in JSON format.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation
  (6 MB).
               
                   
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – Default 100. Max 10,000.
                  
                     
                        Amazon DynamoDB Streams – Default 100. Max 10,000.
                  
                     
                        Amazon Simple Queue Service – Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Self-managed Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Amazon MQ (ActiveMQ and RabbitMQ) – Default 100. Max 10,000.
                  
                     
                        DocumentDB – Default 100. Max 10,000.
                  
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
               Required: No
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry.
               Type: Boolean
               Required: No
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Kafka only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
               Required: No
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
               Required: No
            
          
            
               
                  Enabled
               
            
            
               When true, the event source mapping is active. When false, Lambda pauses polling and invocation.
               Default: True
               Type: Boolean
               Required: No
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – The ARN of the data stream or a stream consumer.
                  
                     
                        Amazon DynamoDB Streams – The ARN of the stream.
                  
                     
                        Amazon Simple Queue Service – The ARN of the queue.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – The ARN of the cluster or the ARN of the VPC connection (for cross-account event source mappings).
                  
                     
                        Amazon MQ – The ARN of the broker.
                  
                     
                        Amazon DocumentDB – The ARN of the DocumentDB change stream.
                  
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
               Required: No
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               Type: FilterCriteria object
               Required: No
            
          
            
               
                  FunctionName
               
            
            
               The name or ARN of the Lambda function.
               
                  Name formats
                   
                   
                   
                   
               
                     
                        Function name – MyFunction.
                  
                     
                        Function ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction.
                  
                     
                        Version or Alias ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD.
                  
                     
                        Partial ARN – 123456789012:function:MyFunction.
                  
               The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64
      characters in length.
               Type: String
               Length Constraints: Minimum length of 1. Maximum length of 140.
               Pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\d{1}:)?(\d{12}:)?(function:)?([a-zA-Z0-9-_]+)(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
               Required: Yes
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
               Required: No
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
      By default, Lambda does not encrypt your filter criteria object. Specify this
      property to encrypt data using your own customer managed key.
    
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
               Required: No
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For Kinesis, DynamoDB, and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For Kinesis, DynamoDB, and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
               Required: No
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is infinite (-1).
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
               Required: No
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
               Required: No
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
               Required: No
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process from each shard concurrently.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
               Required: No
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
               Required: No
            
          
            
               
                  Queues
               
            
            
                (MQ) The name of the Amazon MQ broker destination queue to consume. 
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
               Required: No
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
               Required: No
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster to receive records from.
               Type: SelfManagedEventSource object
               Required: No
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
               Required: No
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of authentication protocols or VPC components required to secure your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
               Required: No
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
               Required: No
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
               Required: No
            
          
            
               
                  Tags
               
            
            
               A list of tags to apply to the event source mapping.
               Type: String to string map
               Required: No
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
               Required: No
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
               Required: No
            
         
    
      Response Syntax
      HTTP/1.1 202
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "EventSourceArn": "string",
   "EventSourceMappingArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FilterCriteriaError": { 
      "ErrorCode": "string",
      "Message": "string"
   },
   "FunctionArn": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "LastModified": number,
   "LastProcessingResult": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "State": "string",
   "StateTransitionReason": "string",
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number,
   "UUID": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 202 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
               Default value: Varies by service. For Amazon SQS, the default is 10. For all other services, the default is 100.
               Related setting: When you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.
               Type: Boolean
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  EventSourceMappingArn
               
            
            
               The Amazon Resource Name (ARN) of the event source mapping.
               Type: String
               Length Constraints: Minimum length of 85. Maximum length of 120.
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso([a-z]?)))?-[a-z]+-\d{1}:\d{12}:event-source-mapping:[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}
               
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               If filter criteria is encrypted, this field shows up as null in the response
      of ListEventSourceMapping API calls. You can view this field in plaintext in the response of
      GetEventSourceMapping and DeleteEventSourceMapping calls if you have
      kms:Decrypt permissions for the correct AWS KMS key.
               Type: FilterCriteria object
            
          
            
               
                  FilterCriteriaError
               
            
            
               An object that contains details about an error related to filter criteria encryption.
               Type: FilterCriteriaError object
            
          
            
               
                  FunctionArn
               
            
            
               The ARN of the Lambda function.
               Type: String
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\d{1}:\d{12}:function:[a-zA-Z0-9-_]+(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
            
          
            
               
                  LastModified
               
            
            
               The date that the event source mapping was last updated or that its state changed, in Unix time seconds.
               Type: Timestamp
            
          
            
               
                  LastProcessingResult
               
            
            
               The result of the last Lambda invocation of your function.
               Type: String
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For streams and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1,
which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records.
               NoteThe minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1,
which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
            
          
            
               
                  Queues
               
            
            
                (Amazon MQ) The name of the Amazon MQ broker destination queue to consume.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster for your event source.
               Type: SelfManagedEventSource object
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
            
          
            
               
                  State
               
            
            
               The state of the event source mapping. It can be one of the following: Creating,
        Enabling, Enabled, Disabling, Disabled,
        Updating, or Deleting.
               Type: String
            
          
            
               
                  StateTransitionReason
               
            
            
               Indicates whether a user or Lambda made the last change to the event source mapping.
               Type: String
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
            
          
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceConflictException
               
            
            
               The resource already exists, or another operation is in progress.
               HTTP Status Code: 409
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsCreateCodeSigningConfigCreateFunctionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideGeneral concurrency metricsProvisioned concurrency metricsWorking with the
        ClaimedAccountConcurrency metricMonitoring concurrencyLambda emits Amazon CloudWatch metrics to help you monitor concurrency for your functions. This topic
    explains these metrics and how to interpret them.SectionsGeneral concurrency metricsProvisioned concurrency metricsWorking with the
        ClaimedAccountConcurrency metric
    General concurrency metrics
    Use the following metrics to monitor concurrency for your Lambda functions. The granularity
      for each metric is 1 minute.
    
       
       
       
    
        ConcurrentExecutions – The number of active concurrent invocations
          at a given point in time. Lambda emits this metric for all functions, versions, and aliases.
          For any function in the Lambda console, Lambda displays the graph for
          ConcurrentExecutions natively in the Monitoring tab,
          under Metrics. View this metric using
          MAX.
      
        UnreservedConcurrentExecutions – The number of active concurrent
          invocations that are using unreserved concurrency. Lambda emits this metric across all
          functions in a region. View this metric using MAX.
      
        ClaimedAccountConcurrency – The amount of concurrency that is
          unavailable for on-demand invocations. ClaimedAccountConcurrency is equal to
          UnreservedConcurrentExecutions plus the amount of allocated concurrency
          (i.e. the total reserved concurrency plus total provisioned concurrency). If
          ClaimedAccountConcurrency exceeds your account concurrency limit, you can
          
          request a higher account concurrency limit. View this metric using
          MAX. For more information, see
          Working with the
        ClaimedAccountConcurrency metric.
      
   
    Provisioned concurrency metrics
    Use the following metrics to monitor Lambda functions using provisioned concurrency. The
      granularity for each metric is 1 minute.
    
       
    
        ProvisionedConcurrentExecutions – The number of execution
          environment instances that are actively processing an invocation on provisioned
          concurrency. Lambda emits this metric for each function version and alias with
          provisioned concurrency configured. View this metric using
          MAX.
      
    ProvisionedConcurrentExecutions is not the same as the total number of
      provisioned concurrency that you allocate. For example, suppose you allocate 100 units of
      provisioned concurrency to a function version. During any given minute, if at most 50 out
      of those 100 execution environments were handling invocations simultaneously, then the
      value of MAX(ProvisionedConcurrentExecutions)
      is 50.
    
       
    
        ProvisionedConcurrencyInvocations – The number of times Lambda
          invokes your function code using provisioned concurrency. Lambda emits this metric
          for each function version and alias with provisioned concurrency configured. View
          this metric using SUM.
      
    ProvisionedConcurrencyInvocations differs from
      ProvisionedConcurrentExecutions in that
      ProvisionedConcurrencyInvocations counts total number of invocations, while
      ProvisionedConcurrentExecutions counts number of active environments. To
      understand this distinction, consider the following scenario:
    
       
        
       
       
    
    In this example, suppose that you receive 1 invocation per minute, and each
      invocation takes 2 minutes to complete. Each orange horizontal bar represents a
      single request. Suppose that you allocate 10 units of provisioned concurrency to
      this function, such that each request runs on provisioned concurrency.
    In between minutes 0 and 1, Request 1 comes in.
      At minute 1, the value for
      MAX(ProvisionedConcurrentExecutions)
      is 1, since at most 1 execution environment was active during the past minute.
      The value for
      SUM(ProvisionedConcurrencyInvocations)
      is also 1, since 1 new request came in during the past minute.
    In between minutes 1 and 2, Request 2 comes in, and
      Request 1 continues to run. At minute 2,
      the value for
      MAX(ProvisionedConcurrentExecutions)
      is 2, since at most 2 execution environments were active during the past minute.
      However, the value for
      SUM(ProvisionedConcurrencyInvocations)
      is 1, since only 1 new request came in during the past minute. This metric behavior
      continues until the end of the example.
    
       
    
        ProvisionedConcurrencySpilloverInvocations – The number
          of times Lambda invokes your function on standard (reserved or unreserved)
          concurrency when all provisioned concurrency is in use. Lambda emits this metric
          for each function version and alias with provisioned concurrency configured.
          View this metric using SUM. The value of
          ProvisionedConcurrencyInvocations +
          ProvisionedConcurrencySpilloverInvocations should be equal to the
          total number of function invocations (i.e. the Invocations
          metric).
        ProvisionedConcurrencyUtilization – The percentage of
          provisioned concurrency in use (i.e. the value of
          ProvisionedConcurrentExecutions divided by the total amount of
          provisioned concurrency allocated). Lambda emits this metric for each function
          version and alias with provisioned concurrency configured. View this metric
          using MAX.
      
    For example, suppose you provision 100 units of provisioned concurrency to a
      function version. During any given minute, if at most 60 out of those 100
      execution environments were handling invocations simultaneously, then the value of
      MAX(ProvisionedConcurrentExecutions)
      is 60, and the value of
      MAX(ProvisionedConcurrencyUtilization)
      is 0.6.
    A high value for ProvisionedConcurrencySpilloverInvocations
      may indicate that you need to allocate additional provisioned concurrency for your
      function. Alternatively, you can 
      configure Application Auto Scaling to handle automatic scaling of provisioned concurrency
      based on pre-defined thresholds.
    Conversely, consistently low values for ProvisionedConcurrencyUtilization
      may indicate that you over-allocated provisioned concurrency for your function.
   
    Working with the
        ClaimedAccountConcurrency metric
    Lambda uses the ClaimedAccountConcurrency metric to determine how much
      concurrency your account is available for on-demand invocations. Lambda calculates
      ClaimedAccountConcurrency using the following formula:
    ClaimedAccountConcurrency = UnreservedConcurrentExecutions + (allocated concurrency)
    UnreservedConcurrentExecutions is the number of active concurrent
      invocations that are using unreserved concurrency. Allocated concurrency
      is the sum of the following two parts (substituting RC as "reserved
      concurrency" and PC as "provisioned concurrency"):
    
       
       
    
        The total RC across all functions in a Region.
      
        The total PC across all functions in a Region that use
          PC, excluding functions that use RC.
      
    NoteYou can’t allocate more PC than RC for a function.
        Thus, a function’s RC is always greater than or equal to its
        PC. To calculate the contribution to allocated concurrency
        for such functions with both PC and RC, Lambda
        considers only RC, which is the maximum of the two.
    Lambda uses the ClaimedAccountConcurrency metric, rather than
      ConcurrentExecutions, to determine how much concurrency is available
      for on-demand invocations. While the ConcurrentExecutions metric is
      useful for tracking the number of active concurrent invocations, it doesn't always
      reflect your true concurrency availability. This is because Lambda also considers
      reserved concurrency and provisioned concurrency to determine availability.
    To illustrate ClaimedAccountConcurrency, consider a scenario where
      you configure a lot of reserved concurrency and provisioned concurrency across your
      functions that go largely unused. In the following example, assume that your
      account concurrency limit is 1,000, and you have two main functions in your account:
      function-orange and function-blue. You allocate 600 units
      of reserved concurrency for function-orange. You allocate 200 units
      of provisioned concurrency for function-blue. Suppose that over time,
      you deploy additional functions and observe the following traffic pattern:
    
       
        
       
       
    
    In the previous diagram, the black lines indicate the actual concurrency use
      over time, and the red line indicates the value of
      ClaimedAccountConcurrency over time. Throughout this scenario,
      ClaimedAccountConcurrency is 800 at minimum, despite low actual
      concurrency utilization across your functions. This is because you
      allocated 800 total units of concurrency for function-orange and
      function-blue. From Lambda's perspective, you have "claimed" this
      concurrency for use, so you effectively have only 200 units of concurrency
      remaining for other functions.
    For this scenario, allocated concurrency is 800 in the
      ClaimedAccountConcurrency formula. We can then derive the value of
      ClaimedAccountConcurrency at various points in the diagram:
    
       
       
       
    
        At t1, ClaimedAccountConcurrency is 800
          (800 + 0 UnreservedConcurrentExecutions).
      
        At t2, ClaimedAccountConcurrency is 900
          (800 + 100 UnreservedConcurrentExecutions).
      
        At t3, ClaimedAccountConcurrency is again 900
          (800 + 100 UnreservedConcurrentExecutions).
      
     
      Setting up the ClaimedAccountConcurrency
          metric in CloudWatch
      Lambda emits the ClaimedAccountConcurrency metric in CloudWatch.
        Use this metric along with the value of SERVICE_QUOTA(ConcurrentExecutions)
        to get the percent utilization of concurrency in your account, as shown in the
        following formula:
      Utilization = (ClaimedAccountConcurrency/SERVICE_QUOTA(ConcurrentExecutions)) * 100%
      The following screenshot illustrates how you can graph this formula in CloudWatch.
        The green claim_utilization line represents the concurrency
        utilization in this account, which is at around 40%:
      
         
          
         
         
      
      The previous screenshot also includes a CloudWatch alarm that goes into
        ALARM state when the concurrency utilization exceeds 70%. You can
        use the ClaimedAccountConcurrency metric along with similar alarms
        to proactively determine when you might need to request a higher account
        concurrency limit.
     
  Document ConventionsScaling behaviorBuilding with Node.jsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideDestinationsDead-letter queuesCapturing records of Lambda asynchronous invocationsLambda can send records of asynchronous invocations to one of the following AWS services.
     
     
     
     
     
  
      Amazon SQS – A standard SQS queue
    
      Amazon SNS – A standard SNS topic
    
      Amazon S3 – An Amazon S3 bucket (on failure only)
    
      AWS Lambda – A Lambda function
    
      Amazon EventBridge – An EventBridge event bus
    The invocation record contains details about the request and response in JSON format. You can configure
    separate destinations for events that are processed successfully, and events that fail all processing attempts.
    Alternatively, you can configure a standard Amazon SQS queue or standard Amazon SNS topic as a dead-letter
      queue for discarded events. For dead-letter queues, Lambda only sends the content of the event, without
    details about the response.If Lambda can't send a record to a destination you have configured, it sends a DestinationDeliveryFailures metric to 
    Amazon CloudWatch. This can happen if your configuration includes an unsupported destination type, such as an Amazon SQS FIFO queue or an Amazon SNS FIFO 
    topic. Delivery errors can also occur due to permissions errors and size limits. For more information on Lambda invocation metrics, 
    see Invocation metrics.NoteTo prevent a function from triggering, you can set the function's reserved concurrency to zero. When you set
      reserved concurrency to zero for an asynchronously invoked function, Lambda begins sending new events to the
      configured dead-letter queue or the on-failure event destination, without any retries. To process events that
      were sent while reserved concurrency was set to zero, you must consume the events from the dead-letter queue or
      the on-failure event destination.
    Adding a destination
    To retain records of asynchronous invocations, add a destination to your function. You can choose to send
      either successful or failed invocations to a destination. Each function can have multiple destinations, so
      you can configure separate destinations for successful and failed events. Each record sent to the
      destination is a JSON document with details about the invocation. Like error handling settings, you can
      configure destinations on a function, function version, or alias.
    TipYou can also retain records of failed invocations for the following event source mapping types:
        Amazon Kinesis,
        Amazon DynamoDB,
        self-managed Apache Kafka,
        and Amazon MSK.
    
    The following table lists supported destinations for asynchronous invocation records. For Lambda to
      successfully send records to your chosen destination, ensure that your function's
      execution role also contains the relevant permissions.
      The table also describes how each destination type receives the JSON invocation record.
    
          
            Destination type
            Required permission
            Destination-specific JSON format
          
        
          
            
              Amazon SQS queue
            
            
              sqs:SendMessage
            
            
              Lambda passes the invocation record as the Message to the destination.
            
          
          
            
              Amazon SNS topic
            
            
              sns:Publish
            
            
              Lambda passes the invocation record as the Message to the destination.
            
          
          
            
              Amazon S3 bucket (on failure only)
            
            
              s3:PutObject
              s3:ListBucket
            
            
              
                 
                 
              
                  Lambda stores the invocation record as a JSON object in the destination bucket.
                
                  The S3 object name uses the following naming convention:
                  aws/lambda/async/<function-name>/YYYY/MM/DD/YYYY-MM-DDTHH.MM.SS-<Random UUID>
                
            
          
          
            
              Lambda function
            
            
              lambda:InvokeFunction
            
            
              Lambda passes the invocation record as the payload to the function.
            
          
          
            
              EventBridge
            
            
              events:PutEvents
            
            
              
                 
                 
                 
                 
                 
              
                  Lambda passes the invocation record as the detail in the PutEvents call.
                
                  The value for the source event field is lambda.
                
                  The value for the detail-type event field is either "Lambda Function
                    Invocation Result - Success" or "Lambda Function Invocation Result - Failure".
                
                  The resource event field contains the function and destination
                    Amazon Resource Names (ARNs).
                
                  For other event fields, see Amazon EventBridge events.
                
            
          
        
    NoteFor Amazon S3 destinations, if you have enabled encryption on the bucket using a KMS key, your function also needs the 
        kms:GenerateDataKey permission.
    The following steps describe how to configure a destination for a function using the Lambda console and the AWS CLI.
    
      Console
          Open the Functions page of the Lambda console.
              Choose a function.
            
              Under Function overview, choose Add destination.
            
              For Source, choose Asynchronous invocation.
            
              For Condition, choose from the following options:
              
                 
                 
              
                  On failure – Send a record when the event fails all processing attempts or
                    exceeds the maximum age.
                
                  On success – Send a record when the function successfully processes an
                    asynchronous invocation.
                
            
              For Destination type, choose the type of resource that receives the invocation
                record.
            
              For Destination, choose a resource.
            
              Choose Save.
            
        
      AWS CLI
          To configure a destination using the AWS CLI, run the update-function-event-invoke-config command. The following example configures Lambda to send a record to a standard
            SQS queue named destination when an event can't be processed.
          aws lambda update-function-event-invoke-config \
  --function-name my-function \
  --destination-config '{"OnFailure":{"Destination": "arn:aws:sqs:us-east-1:123456789012:destination"}}'
        
    
     
      Security best practices for Amazon S3 destinations
       Deleting an S3 bucket that's configured as a destination without removing the destination from your function's configuration can create a security risk. If another 
      user knows your destination bucket's name, they can recreate the bucket in their AWS account. Records of failed invocations will be sent to their bucket, potentially 
      exposing data from your function.
      WarningTo ensure that invocation records from your function can't be sent to an S3 bucket in another AWS account, add a condition to your function's execution role 
        that limits s3:PutObject permissions to buckets in your account. 
      The following example shows an IAM policy that limits your function's s3:PutObject permissions to buckets in your account. This policy also gives Lambda
        the s3:ListBucket permission it needs to use an S3 bucket as a destination.
      {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "S3BucketResourceAccountWrite",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::*/*",
                "arn:aws:s3:::*"
            ],
            "Condition": {
                "StringEquals": {
                    "s3:ResourceAccount": "111122223333"
                }
            }
        }
    ]
}
      To add a permissions policy to your funcion's execution role using the AWS Management Console or AWS CLI, refer to the instructions in the following procedures:
      
        Console
            To add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.
                Select the Lambda function whose execution role you want to modify.
              
                In the Configuration tab, select Permissions.
              
                In the Execution role tab, select your function's Role name to open the role's IAM console page.
              
                Add a permissions policy to the role by doing the following:
                
                    In the Permissions policies pane, choose Add permissions and select Create inline policy. 
                  
                    In Policy editor, select JSON.
                  
                    Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.
                  
                    Under Policy details, enter a Policy name.
                  
                    Choose Create policy.
                  
              
          
        AWS CLI
            To add a permissions policy to a function's execution role (CLI)
                Create a JSON policy document with the required permissions and save it in a local directory.
              
                Use the IAM put-role-policy CLI command to add the permissions to your function's execution role. Run the following command from the 
                directory you saved your JSON policy document in and replace the role name, policy name, and policy document with your own values.
                aws iam put-role-policy \
--role-name my_lambda_role \
--policy-name LambdaS3DestinationPolicy \
--policy-document file://my_policy.json
              
          
      
     
     
      Example invocation record
    When an invocation matches the condition, Lambda sends a JSON document with details about the invocation to the destination. The following example shows an invocation record for an event that failed three processing attempts due to a function error.
    {
    "version": "1.0",
    "timestamp": "2019-11-14T18:16:05.568Z",
    "requestContext": {
        "requestId": "e4b46cbf-b738-xmpl-8880-a18cdf61200e",
        "functionArn": "arn:aws:lambda:us-east-1:123456789012:function:my-function:$LATEST",
        "condition": "RetriesExhausted",
        "approximateInvokeCount": 3
    },
    "requestPayload": {
        "ORDER_IDS": [
            "9e07af03-ce31-4ff3-xmpl-36dce652cb4f",
            "637de236-e7b2-464e-xmpl-baf57f86bb53",
            "a81ddca6-2c35-45c7-xmpl-c3a03a31ed15"
        ]
    },
    "responseContext": {
        "statusCode": 200,
        "executedVersion": "$LATEST",
        "functionError": "Unhandled"
    },
    "responsePayload": {
        "errorMessage": "RequestId: e4b46cbf-b738-xmpl-8880-a18cdf61200e Process exited before completing request"
    }
}
    The invocation record contains details about the event, the response, and the reason that the record was
      sent.
     
       
        Tracing requests to destinations
        You can use AWS X-Ray
        to see a connected view of each request as it's queued, processed by a Lambda function, and passed to the
        destination service. When you activate X-Ray tracing for a function or a service that invokes a function, Lambda
        adds an X-Ray header to the request and passes the header to the destination service. Traces from upstream
        services are automatically linked to traces from downstream Lambda functions and destination services, creating
        an end-to-end view of the entire application. For more information about tracing, see Visualize Lambda function invocations using AWS X-Ray.
       
   
    Adding a dead-letter queue
    
    As an alternative to an on-failure destination, you can
      configure your function with a dead-letter queue to save discarded events for further processing. A dead-letter
      queue acts the same as an on-failure destination in that it is used when an event fails all processing attempts or
      expires without being processed. However, you can only add or remove a dead-letter queue at the function level. Function versions use the same dead-letter queue settings as the unpublished version ($LATEST). On-failure destinations also support additional
      targets and include details about the function's response in the invocation record.
    
    To reprocess events in a dead-letter queue, you can set it as an event source for your Lambda function.
      Alternatively, you can manually retrieve the events.
    
    You can choose an Amazon SQS standard queue or Amazon SNS standard topic for your dead-letter queue. FIFO queues and 
      Amazon SNS FIFO topics are not supported.
    
       
       
    
        Amazon SQS queue – A queue holds failed events
          until they're retrieved. Choose an Amazon SQS standard queue if you expect a single entity, such as a Lambda function or CloudWatch
          alarm, to process the failed event. For more information, see Using Lambda with Amazon SQS.
      
        Amazon SNS topic – A topic relays failed events to
          one or more destinations. Choose an Amazon SNS standard topic if you expect multiple entities to act on a failed event. For
          example, you can configure a topic to send events to an email address, a Lambda function, and/or an HTTP
          endpoint. For more information, see Invoking Lambda functions with Amazon SNS notifications.
      
    
    To send events to a queue or topic, your function needs additional permissions. Add a policy with the
       required permissions to your function's
      execution role. If the target queue or topic
      is encrypted with a customer managed AWS KMS key, ensure that both your function's execution role and the key's
      resource-based policy contains the relevant permissions.
    
    After creating the target and updating your function's execution role, add the dead-letter queue to your
      function. You can configure multiple functions to send events to the same target.
    
      Console
           Open the Functions page of the Lambda console. 
              Choose a function.
            
              Choose Configuration and then choose Asynchronous
                invocation.
            
              Under Asynchronous invocation, choose Edit.
            
              Set Dead-letter queue service to Amazon SQS or Amazon SNS.
            
              Choose the target queue or topic.
            
              Choose Save.
            
        
      AWS CLI
          To configure a dead-letter queue with the AWS CLI, use the update-function-configuration command.
          aws lambda update-function-configuration \
  --function-name my-function \
  --dead-letter-config TargetArn=arn:aws:sns:us-east-1:123456789012:my-topic
        
    
    
    Lambda sends the event to the dead-letter queue as-is, with additional information in attributes. You can use
      this information to identify the error that the function returned, or to correlate the event with logs or an
      AWS X-Ray trace.
    
      Dead-letter queue message attributes
       
       
       
    
        RequestID (String) – The ID of the invocation request. Request IDs
          appear in function logs. You can also use the X-Ray SDK to record the request ID on an attribute in the
          trace. You can then search for traces by request ID in the X-Ray console.
      
        ErrorCode (Number) – The HTTP status code.
      
        ErrorMessage (String) – The first 1 KB of the error
          message.
      
    
    If Lambda can't send a message to the dead-letter queue, it deletes the event and emits the DeadLetterErrors metric. This can happen because of lack of permissions, or
      if the total size of the message exceeds the limit for the target queue or topic. For example, say that an Amazon SNS
      notification with a body close to 256 KB in size triggers a function that results in an error. In
      that case, the event data that Amazon SNS adds, combined with the attributes that Lambda adds, can cause the message to
      exceed the maximum size allowed in the dead-letter queue.
    If you're using Amazon SQS as an event source, configure a dead-letter queue on the Amazon SQS queue itself and not on
      the Lambda function. For more information, see Using Lambda with Amazon SQS.
  Document ConventionsConfigurationEvent source mappingsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUsing an Amazon MSK cluster as an event sourceCreating an event source mapping for an Amazon MSK event sourceConfiguring cluster authentication methodsCustomizable consumer group IDPolling and stream starting positionsEvent poller scaling modesCreating cross-account event source mappingsAll Amazon MSK event source configuration parametersConfiguring Amazon MSK event sources for LambdaTo use an Amazon MSK cluster as an event source for your Lambda function, you create an event source mapping that connects the two resources.
        This page describes how to create an event source mapping for Amazon MSK.This page assumes that you've already properly configured your MSK cluster and the
        Amazon Virtual Private Cloud (VPC)
        it resides in. If you need to set up your cluster or VPC, see Configuring your Amazon MSK cluster and Amazon VPC network for Lambda.TopicsUsing an Amazon MSK cluster as an event sourceCreating an event source mapping for an Amazon MSK event sourceConfiguring cluster authentication methodsCustomizable consumer group IDPolling and stream starting positionsEvent poller scaling modesCreating cross-account event source mappingsAll Amazon MSK event source configuration parameters
        Using an Amazon MSK cluster as an event source
        When you add your Apache Kafka or Amazon MSK cluster as a trigger for your Lambda function, the cluster is used
            as an event source.
        Lambda reads event data from the Kafka topics that you specify as Topics in a
            CreateEventSourceMapping request, based on the starting
            position that you specify. After successful processing, your Kafka topic is committed to your
            Kafka cluster.
        Lambda reads messages sequentially for each Kafka topic partition. A single Lambda payload can contain
            messages from multiple partitions. When more records are available, Lambda continues processing records in
            batches, based on the BatchSize value that you specify in a CreateEventSourceMapping request, until
            your function catches up with the topic.
        After Lambda processes each batch, it commits the offsets of the messages in that batch. If your function
            returns an error for any of the messages in a batch, Lambda retries the whole batch of messages until
            processing succeeds or the messages expire. You can send records that fail all retry attempts to an
            on-failure destination for later processing.
        NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes, event source mappings
                for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with maximum
                timeout limits of 14 minutes.
     
        Creating an event source mapping for an Amazon MSK event source
        To create an event source mapping, you can use the Lambda console, the AWS Command Line Interface (CLI), or an
            AWS SDK.
        NoteWhen you create the event source mapping, Lambda creates a 
                hyperplane ENI in the private subnet that contains your MSK cluster, allowing Lambda to
                establish a secure connection. This hyperplane ENI allows uses the subnet and security group
                configuration of your MSK cluster, not your Lambda function.
        The following console steps add an Amazon MSK cluster as a trigger for your Lambda function. Under the hood,
            this creates an event source mapping resource.
        To add an Amazon MSK trigger to your Lambda function (console)
                Open the Function page
                    of the Lambda console.
            
                Choose the name of the Lambda function you want to add an Amazon MSK trigger to.
            
                Under Function overview, choose Add trigger.
            
                Under Trigger configuration, choose MSK.
            
                To specify your Kafka cluster details, do the following:
                
                     
                     
                     
                
                        For MSK cluster, select your cluster.
                    
                        For Topic name, enter the name of the Kafka topic to consume
                            messages from.
                    
                        For Consumer group ID, enter the ID of a Kafka consumer group
                            to join, if applicable. For more information, see Customizable consumer group ID.
                    
            
                For Cluster authentication, make the necessary configurations. For more
                    information about cluster authentication, see Configuring cluster authentication methods.
                
                     
                     
                     
                
                        Toggle on Use authentication if you want Lambda to perform
                            authentication with your MSK cluster when establishing a connection. Authentication
                            is recommended.
                    
                        If you use authentication, for Authentication method, choose
                            the authentication method to use.
                    
                        If you use authentication, for Secrets Manager key, choose the Secrets Manager key
                            that contains the authentication credentials needed to access your cluster.
                    
            
                Under Event poller configuration, make the necessary configurations.
                
                     
                     
                     
                
                        Choose Activate trigger to enable the trigger immediately after creation.
                    
                        Choose whether you want to Configure provisioned mode for your event source
                            mapping. For more information, see Event poller scaling modes.
                        
                             
                        
                                If you configure provisioned mode, enter a value for Minimum event pollers,
                                    a value for Maximum event pollers, or both values.
                            
                    
                        For Starting position, choose how you want Lambda to start reading from your stream.
                            For more information, see Polling and stream starting positions.
                    
            
                Under Batching, make the necessary configurations. For more information about
                    batching, see Batching behavior.
                
                     
                     
                
                        For Batch size, enter the maximum number of messages to receive in
                            a single batch.
                    
                        For Batch window, enter the maximum number of seconds that Lambda spends
                            gathering records before invoking the function.
                    
            
                Under Filtering, make the necessary configurations. For more information about
                    filtering, see Using event filtering with an Amazon MSK event source.
                
                     
                
                        For Filter criteria, add filter criteria definitions to determine whether
                            or not to process an event.
                    
            
                Under Failure handling, make the necessary configurations. For more information
                    about failure handling, see Capturing discarded batches for an Amazon MSK event source.
                
                     
                
                        For On-failure destination, specify the ARN of your on-failure destination.
                    
            
                For Tags, enter the tags to associate with this event source mapping.
            
                To create the trigger, choose Add.
            
        You can also create the event source mapping using the AWS CLI with the 
            create-event-source-mapping command. The following example creates an event source mapping to map the Lambda
            function my-msk-function to the AWSKafkaTopic topic, starting from the LATEST
            message. This command also uses the SourceAccessConfiguration object to instruct
            Lambda to use SASL/SCRAM authentication when connecting to the cluster.
        aws lambda create-event-source-mapping \
  --event-source-arn arn:aws:kafka:us-east-1:111122223333:cluster/my-cluster/fc2f5bdf-fd1b-45ad-85dd-15b4a5a6247e-2 \
  --topics AWSKafkaTopic \
  --starting-position LATEST \
  --function-name my-kafka-function
  --source-access-configurations '[{"Type": "SASL_SCRAM_512_AUTH","URI": "arn:aws:secretsmanager:us-east-1:111122223333:secret:my-secret"}]'
        If the cluster uses mTLS authentication, include a SourceAccessConfiguration object that specifies
            CLIENT_CERTIFICATE_TLS_AUTH and a Secrets Manager key ARN. This is shown in the following command:
        aws lambda create-event-source-mapping \
  --event-source-arn arn:aws:kafka:us-east-1:111122223333:cluster/my-cluster/fc2f5bdf-fd1b-45ad-85dd-15b4a5a6247e-2 \
  --topics AWSKafkaTopic \
  --starting-position LATEST \
  --function-name my-kafka-function
  --source-access-configurations '[{"Type": "CLIENT_CERTIFICATE_TLS_AUTH","URI": "arn:aws:secretsmanager:us-east-1:111122223333:secret:my-secret"}]'
        When the cluster uses IAM authentication, you don’t need a
            
            SourceAccessConfiguration object. This is shown in the following command:
        aws lambda create-event-source-mapping \
  --event-source-arn arn:aws:kafka:us-east-1:111122223333:cluster/my-cluster/fc2f5bdf-fd1b-45ad-85dd-15b4a5a6247e-2 \
  --topics AWSKafkaTopic \
  --starting-position LATEST \
  --function-name my-kafka-function
     
        Configuring cluster authentication methods
        Lambda needs permission to access your Amazon MSK cluster, retrieve records, and perform other tasks. Amazon MSK supports
            several ways to authenticate with your MSK cluster.
        Cluster authentication methodsUnauthenticated accessSASL/SCRAM authenticationMutual TLS authenticationIAM authenticationHow Lambda chooses a bootstrap broker
         
            Unauthenticated access
            If no clients access the cluster over the internet, you can use unauthenticated access.
         
         
            SASL/SCRAM authentication
            Lambda supports 
                Simple Authentication and Security Layer/Salted Challenge Response Authentication Mechanism (SASL/SCRAM)
                authentication, with the SHA-512 hash function and Transport Layer Security (TLS) encryption. For Lambda to
                connect to the cluster, store the authentication credentials (username and password) in a Secrets Manager secret, and
                reference this secret when configuring your event source mapping.
            For more information about using Secrets Manager, see Sign-in credentials authentication with Secrets Manager in
                the Amazon Managed Streaming for Apache Kafka Developer Guide.
            NoteAmazon MSK doesn’t support SASL/PLAIN authentication.
         
         
            Mutual TLS authentication
            Mutual TLS (mTLS) provides two-way authentication between the client and the server. The client sends a
                certificate to the server for the server to verify the client. The server also sends a certificate to the
                client for the client to verify the server.
            For Amazon MSK integrations with Lambda, your MSK cluster acts as the server, and Lambda acts as the client.
            
                 
                 
            
                    For Lambda to verify your MSK cluster, you configure a client certificate as a secret in Secrets Manager, and
                        reference this certificate in your event source mapping configuration. The client certificate must be
                        signed by a certificate authority (CA) in the server’s trust store.
                
                    The MSK cluster also sends a server certificate to Lambda. The server certificate must be signed by
                        a certificate authority (CA) in the AWS trust store.
                
            Amazon MSK doesn’t support self-signed server certificates. All brokers in Amazon MSK use
                public certificates
                signed by Amazon Trust Services CAs, which Lambda
                trusts by default.
            The CLIENT_CERTIFICATE_TLS_AUTH secret requires a certificate field and a private key field. For an
                        encrypted private key, the secret requires a private key password. Both the certificate and private key must be
                        in PEM format.NoteLambda supports the PBES1 (but not
                        PBES2) private key encryption algorithms.The certificate field must contain a list of certificates, beginning with the client certificate, followed
                        by any intermediate certificates, and ending with the root certificate. Each certificate must start on a new
                        line with the following structure:-----BEGIN CERTIFICATE-----  
        <certificate contents>
-----END CERTIFICATE-----      Secrets Manager supports secrets up to 65,536 bytes, which is enough space for long certificate chains.The private key must be in PKCS #8
                        format, with the following structure:-----BEGIN PRIVATE KEY-----  
         <private key contents>
-----END PRIVATE KEY-----            For an encrypted private key, use the following structure:-----BEGIN ENCRYPTED PRIVATE KEY-----  
          <private key contents>
-----END ENCRYPTED PRIVATE KEY-----           The following example shows the contents of a secret for mTLS authentication using an encrypted private key.
                        For an encrypted private key, you include the private key password in the secret.{
 "privateKeyPassword": "testpassword",
 "certificate": "-----BEGIN CERTIFICATE-----
MIIE5DCCAsygAwIBAgIRAPJdwaFaNRrytHBto0j5BA0wDQYJKoZIhvcNAQELBQAw
...
j0Lh4/+1HfgyE2KlmII36dg4IMzNjAFEBZiCRoPimO40s1cRqtFHXoal0QQbIlxk
cmUuiAii9R0=
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
MIIFgjCCA2qgAwIBAgIQdjNZd6uFf9hbNC5RdfmHrzANBgkqhkiG9w0BAQsFADBb
...
rQoiowbbk5wXCheYSANQIfTZ6weQTgiCHCCbuuMKNVS95FkXm0vqVD/YpXKwA/no
c8PH3PSoAaRwMMgOSA2ALJvbRz8mpg==
-----END CERTIFICATE-----",
 "privateKey": "-----BEGIN ENCRYPTED PRIVATE KEY-----
MIIFKzBVBgkqhkiG9w0BBQ0wSDAnBgkqhkiG9w0BBQwwGgQUiAFcK5hT/X7Kjmgp
...
QrSekqF+kWzmB6nAfSzgO9IaoAaytLvNgGTckWeUkWn/V0Ck+LdGUXzAC4RxZnoQ
zp2mwJn2NYB7AZ7+imp0azDZb+8YG2aUCiyqb6PnnA==
-----END ENCRYPTED PRIVATE KEY-----"
} 
            For more information about mTLS for Amazon MSK, and instructions on how to generate a client certificate,
                see Mutual TLS client
                authentication for Amazon MSK in the Amazon Managed Streaming for Apache Kafka Developer Guide.
         
         
            IAM authentication
            You can use AWS Identity and Access Management (IAM) to authenticate the identity of clients that connect to the MSK cluster.
                With IAM auth, Lambda relies on the permissions in your function’s
                execution role to connect to the cluster, retrieve
                records, and perform other required actions. For a sample policy that contains the necessary permissions,
                see 
                Create authorization policies for the IAM role in the Amazon Managed Streaming for Apache Kafka Developer Guide.
            If IAM auth is active on your MSK cluster, and you don’t provide a secret, Lambda automatically defaults
                to using IAM auth.
            For more information about IAM authentication in Amazon MSK, see IAM access control.
         
         
            How Lambda chooses a bootstrap broker
            Lambda chooses a 
                bootstrap broker based on the authentication methods available on your cluster, and whether you provide a secret
                for authentication. If you provide a secret for mTLS or SASL/SCRAM, Lambda automatically chooses that auth method.
                If you don't provide a secret, Lambda selects the strongest auth method that's active on your cluster. The following is
                the order of priority in which Lambda selects a broker, from strongest to weakest auth:
            
                 
                 
                 
                 
                 
            mTLS (secret provided for mTLS)SASL/SCRAM (secret provided for SASL/SCRAM)SASL IAM (no secret provided, and IAM auth active)Unauthenticated TLS (no secret provided, and IAM auth not active)Plaintext (no secret provided, and both IAM auth and unauthenticated TLS are not active)
            NoteIf Lambda can't connect to the most secure broker type, Lambda doesn't attempt to connect to a different (weaker)
                broker type. If you want Lambda to choose a weaker broker type, deactivate all stronger auth methods on your cluster.
         
     
        Customizable consumer group ID
        When setting up Kafka as an event source, you can specify a
            consumer group ID.
            This consumer group ID is an existing identifier for the Kafka consumer group that you want your Lambda function to
            join. You can use this feature to seamlessly migrate any ongoing Kafka record processing setups from other
            consumers to Lambda.
        Kafka distributes messages across all consumers in a consumer group. If you specify a consumer group ID that
            has other active consumers, Lambda receives only a portion of the messages from the Kafka topic. If you want Lambda
            to handle all messages in the topic, turn off any other consumers in that consumer group.
        Additionally, if you specify a consumer group ID, and Kafka finds a valid existing consumer group with the same
            ID, Lambda ignores the StartingPosition for your event source mapping.
            Instead, Lambda begins processing records according to the committed offset of the consumer group. If you specify
            a consumer group ID, and Kafka cannot find an existing consumer group, then Lambda configures your event source
            with the specified StartingPosition.
        The consumer group ID that you specify must be unique among all your Kafka event sources. After creating a
            Kafka event source mapping with the consumer group ID specified, you cannot update this value.
     
        Polling and stream starting positions
        The 
            StartingPosition parameter tells Lambda when to start reading messages from your stream. There are
            three options to choose from:
        
             
             
             
        
                Latest – Lambda starts reading just after the most recent
                    record in the Kafka topic.
            
                Trim horizon – Lambda starts reading from the last untrimmed
                    record in the Kafka topic. This is also the oldest record in the topic.
            
                At timestamp – Lambda starts reading from a position defined
                    by a timestamp, in Unix time seconds. Use the 
                    StartingPositionTimestamp parameter to specify the timestamp.
            
        Stream polling during an event source mapping create or update is eventually consistent:
        
             
             
        
                During event source mapping creation, it may take several minutes to start polling events
                    from the stream.
            
                During event source mapping updates, it may take up to 90 seconds to stop and restart polling
                    events from the stream.
            
        This behavior means that if you specify LATEST as the starting position for the stream, the event
            source mapping could miss events during a create or update. To ensure that no events are missed, specify either
            TRIM_HORIZON or AT_TIMESTAMP.
     
        Event poller scaling modes
        You can choose between two modes of event poller scaling for your Kafka event source mapping:
        Scaling modesOn-demand mode (default)Provisioned modeBest practices and considerations when using provisioned mode
         
            On-demand mode (default)
            When you initially create an Amazon MSK event source, Lambda allocates a default number of event pollers
                to process all partitions in the Kafka topic. Lambda automatically scales up or down the number of
                event pollers based on message
                load.
            In one-minute intervals, Lambda evaluates the offset lag
                of all the partitions in the topic. If the offset lag is too high, the partition is receiving messages
                faster than Lambda can process them. If necessary, Lambda adds or removes event pollers from the topic.
                This autoscaling process of adding or removing event pollers occurs within three minutes of evaluation.
            If your target Lambda function is throttled, Lambda reduces the number of event pollers. This action
                reduces the workload on the function by reducing the number of messages that event pollers can
                retrieve and send to the function.
         
         
            Provisioned mode
            For workloads where you need to fine-tune the throughput of your event source mapping, you can use
                provisioned mode. In provisioned mode, you define minimum and maximum limits for the amount of
                provisioned event pollers. These provisioned event pollers are dedicated to your event source mapping,
                and can handle unexpected message spikes through responsive autoscaling. We recommend that you use
                provisioned mode for Kafka workloads that have strict performance requirements.
            In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.
    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.
    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,
    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,
    see AWS Lambda pricing.
            NoteWhen using provisioned mode, you don't need to create AWS PrivateLink VPC endpoints or grant the
                    associated permissions as part of your network
                    configuration.
            In provisioned mode, the range of accepted values for the minimum number of event pollers
                (MinimumPollers) is between 1 and 200, inclusive. The range of
                accepted values for the maximum number of event pollers (MaximumPollers)
                is between 1 and 2,000, inclusive. MaximumPollers must be greater than
                or equal to MinimumPollers. In addition, to maintain ordered
                processing within partitions, Lambda caps the MaximumPollers to the
                number of partitions in the topic.
            For more details about choosing appropriate values for minimum and maximum event pollers,
                see Best practices and considerations when using provisioned mode.
            You can configure provisioned mode for your Amazon MSK event source mapping using the console
                or the Lambda API.
            To configure provisioned mode for an existing Amazon MSK event source mapping (console)
                    Open the Functions page of the Lambda console.
                
                    Choose the function with the Amazon MSK event source mapping you want to configure
                        provisioned mode for.
                
                    Choose Configuration, then choose Triggers.
                
                    Choose the Amazon MSK event source mapping that you want to configure provisioned mode for,
                        then choose Edit.
                
                    Under Event source mapping configuration, choose 
                        Configure provisioned mode.
                    
                         
                         
                    
                            For Minimum event pollers, enter a value between 1 and 200.
                                If you don't specify a value, Lambda chooses a default value of 1.
                        
                            For Maximum event pollers, enter a value between 1 and 2,000.
                                This value must be greater than or equal to your value for Minimum event
                                pollers. If you don't specify a value, Lambda chooses a default value of 200.
                        
                
                    Choose Save.
                
            You can configure provisioned mode programmatically using the ProvisionedPollerConfig object
                in your 
                EventSourceMappingConfiguration. For example, the following UpdateEventSourceMapping CLI
                command configures a MinimumPollers value of 5, and a
                MaximumPollers value of 100.
            aws lambda update-event-source-mapping \
    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \
    --provisioned-poller-config '{"MinimumPollers": 5, "MaximumPollers": 100}'

            After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring
    the ProvisionedPollers metric. For more information, see Event source mapping metrics.
            To disable provisioned mode and return to default (on-demand) mode,
                you can use the following UpdateEventSourceMapping CLI
                command:
            aws lambda update-event-source-mapping \
    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \
    --provisioned-poller-config '{}'

         
         
            Best practices and considerations when using provisioned mode
            The optimal configuration of minimum and maximum event pollers for your event source mapping
                depends on your application's performance requirements. We recommend that you start with the
                default minimum event pollers to baseline the performance profile. Adjust your configuration
                based on observed message processing patterns and your desired performance profile.
            For workloads with spiky traffic and strict performance needs, increase the minimum event
                pollers to handle sudden surges in messages. To determine the minimum event pollers required,
                consider your workload's messages per second and average payload size, and use the throughput
                capacity of a single event poller (up to 5 MBps) as a reference.
            To maintain ordered processing within a partition, Lambda limits the maximum event pollers
                to the number of partitions in the topic. Additionally, the maximum event pollers your event
                source mapping can scale to depends on the function's concurrency settings.
            When activating provisioned mode, update your network settings to remove AWS PrivateLink VPC
                endpoints and associated permissions.
         
     
        Creating cross-account event source mappings
        You can use multi-VPC private
            connectivity to connect a Lambda function to a provisioned MSK cluster in a different AWS account.
            Multi-VPC connectivity uses AWS PrivateLink, which keeps all traffic within the AWS network.
        NoteYou can't create cross-account event source mappings for serverless MSK clusters.
        To create a cross-account event source mapping, you must first configure multi-VPC
            connectivity for the MSK cluster. When you create the event source mapping, use the managed VPC connection
            ARN instead of the cluster ARN, as shown in the following examples. The CreateEventSourceMapping operation
            also differs depending on which authentication type the MSK cluster uses.
        Example — Create cross-account event source mapping for cluster that uses IAM authenticationWhen the cluster uses IAM role-based authentication,
                you don't need a SourceAccessConfiguration object. Example:aws lambda create-event-source-mapping \
  --event-source-arn arn:aws:kafka:us-east-1:111122223333:vpc-connection/444455556666/my-cluster-name/51jn98b4-0a61-46cc-b0a6-61g9a3d797d5-7 \
  --topics AWSKafkaTopic \
  --starting-position LATEST \
  --function-name my-kafka-function
        Example — Create cross-account event source mapping for cluster that uses SASL/SCRAM authenticationIf the cluster uses SASL/SCRAM authentication,
                you must include a SourceAccessConfiguration object that specifies SASL_SCRAM_512_AUTH
                and a Secrets Manager secret ARN.There are two ways to use secrets for cross-account Amazon MSK event source mappings with SASL/SCRAM authentication:
                 
                 
            
                    Create a secret in the Lambda function account and sync it with the cluster secret.
                        Create a rotation
                        to keep the two secrets in sync. This option allows you to control the secret from the function account.
                
                    Use the secret that's associated with the MSK cluster. This secret must allow cross-account
                        access to the Lambda function account. For more information, see Permissions to AWS Secrets Manager
                        secrets for users in a different account.
                aws lambda create-event-source-mapping \
  --event-source-arn arn:aws:kafka:us-east-1:111122223333:vpc-connection/444455556666/my-cluster-name/51jn98b4-0a61-46cc-b0a6-61g9a3d797d5-7 \
  --topics AWSKafkaTopic \
  --starting-position LATEST \
  --function-name my-kafka-function \
  --source-access-configurations '[{"Type": "SASL_SCRAM_512_AUTH","URI": "arn:aws:secretsmanager:us-east-1:444455556666:secret:my-secret"}]'
        Example — Create cross-account event source mapping for cluster that uses mTLS authenticationIf the cluster uses mTLS authentication,
                you must include a SourceAccessConfiguration object that specifies CLIENT_CERTIFICATE_TLS_AUTH
                and a Secrets Manager secret ARN. The secret can be stored in the cluster account or the Lambda function account.aws lambda create-event-source-mapping \
  --event-source-arn arn:aws:kafka:us-east-1:111122223333:vpc-connection/444455556666/my-cluster-name/51jn98b4-0a61-46cc-b0a6-61g9a3d797d5-7 \
  --topics AWSKafkaTopic \
  --starting-position LATEST \
  --function-name my-kafka-function \
  --source-access-configurations '[{"Type": "CLIENT_CERTIFICATE_TLS_AUTH","URI": "arn:aws:secretsmanager:us-east-1:444455556666:secret:my-secret"}]'
     
        All Amazon MSK event source configuration parameters
        All Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping
            API operations. However, only some of the parameters apply to Amazon MSK, as shown in the following table.
        
                    
                        Parameter
                        Required
                        Default
                        Notes
                    
                
                    
                        
                            AmazonManagedKafkaEventSourceConfig
                        
                        
                            N
                        
                        
                            Contains the ConsumerGroupId field, which defaults to a unique value.
                        
                        
                            Can set only on Create
                        
                    
                    
                        
                            BatchSize
                        
                        
                            N
                        
                        
                            100
                        
                        
                            Maximum: 10,000
                        
                    
                    
                        
                            DestinationConfig
                        
                        
                            N
                        
                        
                            N/A
                        
                        
                            Capturing discarded batches for an Amazon MSK event source
                        
                    
                    
                        
                            Enabled
                        
                        
                            N
                        
                        
                            True
                        
                        
                            
                        
                    
                    
                        
                            EventSourceArn
                        
                        
                            Y
                        
                        N/A
                        
                            Can set only on Create
                        
                    
                    
                        
                            FilterCriteria
                        
                        
                            N
                        
                        
                            N/A
                        
                        
                            Control which events Lambda sends to your function
                        
                    
                    
                        
                            FunctionName
                        
                        
                            Y
                        
                        
                            N/A
                        
                        
                            
                        
                    
                    
                        
                            KMSKeyArn
                        
                        
                            N
                        
                        
                            N/A
                        
                        
                            Encryption of filter criteria
                        
                    
                    
                        
                            MaximumBatchingWindowInSeconds
                        
                        
                            N
                        
                        
                            500 ms
                        
                        
                            Batching behavior
                        
                    
                    
                        
                            ProvisionedPollersConfig
                        
                        
                            N
                        
                        
                            MinimumPollers: default value of 1 if not specified
                            MaximumPollers: default value of 200 if not specified
                        
                        
                            Provisioned mode
                        
                    
                    
                        
                            SourceAccessConfigurations
                        
                        
                            N
                        
                        
                            No credentials
                        
                        
                            SASL/SCRAM or CLIENT_CERTIFICATE_TLS_AUTH (MutualTLS) authentication credentials for your event source
                        
                    
                    
                        
                            StartingPosition
                        
                        
                            Y
                        
                        N/A
                        
                            AT_TIMESTAMP, TRIM_HORIZON, or LATEST
                            Can set only on Create
                        
                    
                    
                        
                            StartingPositionTimestamp
                        
                        
                            N
                        
                        
                            N/A
                        
                        
                            Required if StartingPosition is set to AT_TIMESTAMP
                        
                    
                    
                        
                            Tags
                        
                        
                            N
                        
                        
                            N/A
                        
                        
                            Using tags on event source mappings
                        
                    
                    
                        
                            Topics
                        
                        
                            Y
                        
                        N/A
                        
                            Kafka topic name
                            Can set only on Create
                        
                    
                
    Document ConventionsCluster and network setupConfigure permissionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideAdding a Kafka cluster as an event sourceSelf-managed Apache Kafka configuration parametersUsing a Kafka cluster as an event sourcePolling and stream starting positionsMessage throughput scaling behavior for self-managed Apache Kafka event source mappingsAmazon CloudWatch metricsProcessing self-managed Apache Kafka messages with LambdaNoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see 
    Amazon EventBridge Pipes.TopicsAdding a Kafka cluster as an event sourceSelf-managed Apache Kafka configuration parametersUsing a Kafka cluster as an event sourcePolling and stream starting positionsMessage throughput scaling behavior for self-managed Apache Kafka event source mappingsAmazon CloudWatch metrics
    Adding a Kafka cluster as an event source
    To create an event source mapping, add your Kafka
      cluster as a Lambda function trigger using the Lambda
      console, an AWS SDK, or the AWS Command Line Interface (AWS CLI).
    This section describes how to create an event source mapping using the Lambda console and the AWS CLI.
     
      Prerequisites
      
         
         
      
          A self-managed Apache Kafka cluster. Lambda supports Apache Kafka version 0.10.1.0 and later.
        
          An execution role with permission to access the AWS resources that your self-managed Kafka
            cluster uses.
        
     
    
    
     
      Customizable consumer group ID
      When setting up Kafka as an event source, you can specify a consumer group ID. This consumer group ID is an
    existing identifier for the Kafka consumer group that you want your Lambda function to join. You can use this feature to seamlessly migrate any
    ongoing Kafka record processing setups from other consumers to Lambda.
    If you specify a consumer group ID and there are other active pollers within that consumer group, Kafka distributes messages across
      all consumers. In other words, Lambda doesn't receive all message for the Kafka topic. If you want Lambda to handle all messages in the
      topic, turn off any other pollers in that consumer group.
    Additionally, if you specify a consumer group ID, and Kafka finds a valid existing consumer group with the same ID, Lambda ignores the
      StartingPosition parameter for your event source mapping. Instead, Lambda begins processing records according to the committed
      offset of the consumer group. If you specify a consumer group ID, and Kafka cannot find an existing consumer group, then Lambda configures your
      event source with the specified StartingPosition.
    The consumer group ID that you specify must be unique among all your Kafka event sources. After creating a Kafka event source mapping
      with the consumer group ID specified, you cannot update this value.
     
    
     
      Adding a self-managed Kafka cluster (console)
      Follow these steps to add your self-managed Apache Kafka cluster and a Kafka topic as a trigger for your Lambda function.
      To add an Apache Kafka trigger to your Lambda function (console)
          Open the Functions page of the Lambda
            console.
        
          Choose the name of your Lambda function.
        
          Under Function overview, choose Add trigger.
        
          Under Trigger configuration, do the following:
          
              Choose the Apache Kafka trigger type.
            
              For Bootstrap servers, enter the host and port pair address of a Kafka broker
                in your cluster, and then choose Add. Repeat for each Kafka broker in the
                cluster.
            
              For Topic name, enter the name of the Kafka topic used to store records in the
                cluster.
            
              (Optional) For Batch size, enter the maximum number of records to receive in a
                single batch.
            
              For Batch window, enter the maximum amount of seconds that Lambda spends
                gathering records before invoking the function.
            
              (Optional) For Consumer group ID, enter the ID of a Kafka consumer group to join.
            
              (Optional) For Starting position, choose Latest to start
                reading the stream from the latest record, Trim horizon to start at the
                earliest available record, or At timestamp to specify a timestamp to start
                reading from.
            
              (Optional) For VPC, choose the Amazon VPC for your Kafka cluster. Then, choose the
                VPC subnets and VPC security groups.
              This setting is required if only users within your VPC access your brokers.
              
            
              (Optional) For Authentication, choose Add, and then do the
                following:
              
                  Choose the access or authentication protocol of the Kafka brokers in your cluster.
                  
                    
                      If your Kafka broker uses SASL/PLAIN authentication, choose
                        BASIC_AUTH.
                    
                    
                      If your broker uses SASL/SCRAM authentication, choose one of the
                        SASL_SCRAM protocols.
                    
                    
                      If you're configuring mTLS authentication, choose the
                        CLIENT_CERTIFICATE_TLS_AUTH protocol.
                    
                  
                
                  For SASL/SCRAM or mTLS authentication, choose the Secrets Manager secret key that contains the
                    credentials for your Kafka cluster.
                
            
              (Optional) For Encryption, choose the Secrets Manager secret containing the root CA
                certificate that your Kafka brokers use for TLS encryption, if your Kafka brokers use certificates
                signed by a private CA.
              This setting applies to TLS encryption for SASL/SCRAM or SASL/PLAIN, and to mTLS
                authentication.
            
              To create the trigger in a disabled state for testing (recommended), clear Enable
                trigger. Or, to enable the trigger immediately, select Enable
                  trigger.
            
        
          To create the trigger, choose Add.
        
     
    
     
      Adding a self-managed Kafka cluster (AWS CLI)
      Use the following example AWS CLI commands to create and view a self-managed Apache Kafka trigger for your Lambda function.
      
       
        Using SASL/SCRAM
        If Kafka users access your Kafka brokers over the internet, specify the Secrets Manager secret that you created
          for SASL/SCRAM authentication. The following example uses the create-event-source-mapping AWS CLI command to map a Lambda function named my-kafka-function to a Kafka topic named AWSKafkaTopic.
        aws lambda create-event-source-mapping \ 
  --topics AWSKafkaTopic \
  --source-access-configuration Type=SASL_SCRAM_512_AUTH,URI=arn:aws:secretsmanager:us-east-1:111122223333:secret:MyBrokerSecretName \
  --function-name arn:aws:lambda:us-east-1:111122223333:function:my-kafka-function \
  --self-managed-event-source '{"Endpoints":{"KAFKA_BOOTSTRAP_SERVERS":["abc3.xyz.com:9092", "abc2.xyz.com:9092"]}}'
       
       
        Using a VPC
        If only Kafka users within your VPC access your Kafka brokers, you must specify your VPC, subnets, and VPC
          security group. The following example uses the create-event-source-mapping AWS CLI command to map a Lambda function named my-kafka-function to a Kafka topic named AWSKafkaTopic.
        aws lambda create-event-source-mapping \ 
  --topics AWSKafkaTopic \
  --source-access-configuration '[{"Type": "VPC_SUBNET", "URI": "subnet:subnet-0011001100"}, {"Type": "VPC_SUBNET", "URI": "subnet:subnet-0022002200"}, {"Type": "VPC_SECURITY_GROUP", "URI": "security_group:sg-0123456789"}]' \
  --function-name arn:aws:lambda:us-east-1:111122223333:function:my-kafka-function \
  --self-managed-event-source '{"Endpoints":{"KAFKA_BOOTSTRAP_SERVERS":["abc3.xyz.com:9092", "abc2.xyz.com:9092"]}}'
       
      
       
        Viewing the status using the AWS CLI
        The following example uses the get-event-source-mapping AWS CLI command to describe the status of the event source mapping that you created.
        aws lambda get-event-source-mapping
          --uuid dh38738e-992b-343a-1077-3478934hjkfd7
       
     
   
    Self-managed Apache Kafka configuration parameters
    All Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping
      API operations. However, only some of the parameters apply to Apache Kafka.
    
          
            Parameter
            Required
            Default
            Notes
          
        
          
            
              BatchSize
            
            
              N
            
            
              100
            
            
              Maximum: 10,000
            
          
          
            
              DestinationConfig
            
            
              N
            
            
              N/A
            
            
              Capturing discarded batches for a self-managed Apache Kafka event source
            
          
          
            
              Enabled
            
            
              N
            
            
              True
            
            
          
          
            
              FilterCriteria
            
            
              N
            
            
              N/A
            
            
              Control which events Lambda sends to your function
            
          
          
            
              FunctionName
            
            
              Y
            
            
              N/A
            
            
              
            
          
          
            
              KMSKeyArn
            
            
              N
            
            
              N/A
            
            
              Encryption of filter criteria
            
          
          
            
              MaximumBatchingWindowInSeconds
            
            
              N
            
            
              500 ms
            
            
              Batching behavior
            
          
          
            
              ProvisionedPollersConfig
            
            
              N
            
            
              MinimumPollers: default value of 1 if not specified
              MaximumPollers: default value of 200 if not specified
            
            
              Configuring provisioned mode
            
          
          
            
              SelfManagedEventSource
            
            
              Y
            
            N/A
            
              List of Kafka Brokers. Can set only on Create
            
          
          
            
              SelfManagedKafkaEventSourceConfig
            
            
              N
            
            
              Contains the ConsumerGroupId field which defaults to a unique value.
            
            
              Can set only on Create
            
          
          
            
              SourceAccessConfigurations
            
            
              N
            
            
              No credentials
            
            
              VPC information or authentication credentials for the cluster 
               For SASL_PLAIN, set to BASIC_AUTH
            
          
          
            
              StartingPosition
            
            
              Y
            
            
              N/A
            
            
              AT_TIMESTAMP, TRIM_HORIZON, or LATEST
              Can set only on Create
            
          
          
            
              StartingPositionTimestamp
            
            
              N
            
            
              N/A
            
            
              Required if StartingPosition is set to AT_TIMESTAMP
            
          
          
            
              Tags
            
            
              N
            
            
              N/A
            
            
              Using tags on event source mappings
            
          
          
            
              Topics
            
            
              Y
            
            
              N/A
            
            
              Topic name
              Can set only on Create
            
          
        
   
    Using a Kafka cluster as an event source
    When you add your Apache Kafka or Amazon MSK cluster as a trigger for your Lambda function, the cluster is used as an event source.
    Lambda reads event data from the Kafka topics that you specify as Topics in a
      CreateEventSourceMapping request, based on the StartingPosition that you specify. After
      successful processing, your Kafka topic is committed to your Kafka cluster.
    If you specify the StartingPosition as LATEST, Lambda starts reading from the latest
      message in each partition belonging to the topic. Because there can be some delay after trigger configuration
      before Lambda starts reading the messages, Lambda doesn't read any messages produced during this window.
    Lambda processes records from one or more Kafka topic partitions that you specify and sends a JSON payload to
      your function. A single Lambda payload can contain messages from multiple partitions. When more records are available, 
      Lambda continues processing records in batches, based on the
      BatchSize value that you specify in a CreateEventSourceMapping request, until your function
      catches up with the topic.
    If your function returns an error for any of the messages in a batch, Lambda retries the whole batch of
      messages until processing succeeds or the messages expire. You can send records that fail all retry attempts
      to an on-failure destination for later processing.
    NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes,
      event source mappings for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with
      maximum timeout limits of 14 minutes. This constraint ensures that the event source mapping can properly
      handle function errors and retries.
   
    Polling and stream starting positions
    Be aware that stream polling during event source mapping creation and updates is eventually consistent.
    
       
       
    
        During event source mapping creation, it may take several minutes to start polling events from the stream.
      
        During event source mapping updates, it may take several minutes to stop and restart polling events from the stream.
      
    This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could 
    miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON 
    or AT_TIMESTAMP.
   
    Message throughput scaling behavior for self-managed Apache Kafka event source mappings
    You can choose between two modes of message throughput scaling behavior for your Amazon MSK
      event source mapping:
    
       
       
    
        Default (on-demand) mode
      
        Provisioned mode
      
     
      Default (on-demand) mode
      When you initially create an self-managed Apache Kafka event source, Lambda allocates a default number of event
        pollers to process all partitions in the Kafka topic. Lambda automatically scales up or down the
        number of event pollers based on message load.
      In one-minute intervals, Lambda evaluates the consumer offset lag of all the partitions in the
        topic. If the offset lag is too high, the partition is receiving messages faster than Lambda can
        process them. If necessary, Lambda adds or removes event pollers from the topic. This autoscaling
        process of adding or removing event pollers occurs within three minutes of evaluation.
      If your target Lambda function is throttled, Lambda reduces the number of event pollers. This
        action reduces the workload on the function by reducing the number of messages that event
        pollers can retrieve and send to the function.
      To monitor the throughput of your Kafka topic, you can view the Apache Kafka consumer metrics,
        such as consumer_lag and consumer_offset.
     
     
      Configuring provisioned mode
      For workloads where you need to fine-tune the throughput of your event source mapping,
        you can use provisioned mode. In provisioned mode, you define minimum and maximum limits
        for the amount of provisioned event pollers. These provisioned event pollers are dedicated
        to your event source mapping, and can handle unexpected message spikes instantly when they
        occur. We recommend that you use provisioned mode for Kafka workloads that have strict
        performance requirements.
      In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.
    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.
    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,
    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,
    see AWS Lambda pricing.
      In provisioned mode, the range of accepted values for the minimum number of event pollers
                (MinimumPollers) is between 1 and 200, inclusive. The range of
                accepted values for the maximum number of event pollers (MaximumPollers)
                is between 1 and 2,000, inclusive. MaximumPollers must be greater than
                or equal to MinimumPollers. In addition, to maintain ordered
                processing within partitions, Lambda caps the MaximumPollers to the
                number of partitions in the topic.
      For more details about choosing appropriate values for minimum and maximum event pollers,
        see Best practices and considerations when using provisioned mode.
      You can configure provisioned mode for your self-managed Apache Kafka event source mapping using the console
        or the Lambda API.
      To configure provisioned mode for an existing self-managed Apache Kafka event source mapping (console)
          Open the Functions page of the Lambda console.
        
          Choose the function with the self-managed Apache Kafka event source mapping you want to configure
            provisioned mode for.
        
          Choose Configuration, then choose Triggers.
        
          Choose the self-managed Apache Kafka event source mapping that you want to configure provisioned mode for,
            then choose Edit.
        
          Under Event source mapping configuration, choose 
            Configure provisioned mode.
          
                         
                         
                    
                            For Minimum event pollers, enter a value between 1 and 200.
                                If you don't specify a value, Lambda chooses a default value of 1.
                        
                            For Maximum event pollers, enter a value between 1 and 2,000.
                                This value must be greater than or equal to your value for Minimum event
                                pollers. If you don't specify a value, Lambda chooses a default value of 200.
                        
        
          Choose Save.
        
      You can configure provisioned mode programmatically using the ProvisionedPollerConfig object
                in your 
                EventSourceMappingConfiguration. For example, the following UpdateEventSourceMapping CLI
                command configures a MinimumPollers value of 5, and a
                MaximumPollers value of 100.
            aws lambda update-event-source-mapping \
    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \
    --provisioned-poller-config '{"MinimumPollers": 5, "MaximumPollers": 100}'

      After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring
    the ProvisionedPollers metric. For more information, see Event source mapping metrics.
      To disable provisioned mode and return to default (on-demand) mode,
                you can use the following UpdateEventSourceMapping CLI
                command:
            aws lambda update-event-source-mapping \
    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \
    --provisioned-poller-config '{}'

     
     
      Best practices and considerations when using provisioned mode
      The optimal configuration of minimum and maximum event pollers for your event source mapping
                depends on your application's performance requirements. We recommend that you start with the
                default minimum event pollers to baseline the performance profile. Adjust your configuration
                based on observed message processing patterns and your desired performance profile.
            For workloads with spiky traffic and strict performance needs, increase the minimum event
                pollers to handle sudden surges in messages. To determine the minimum event pollers required,
                consider your workload's messages per second and average payload size, and use the throughput
                capacity of a single event poller (up to 5 MBps) as a reference.
            To maintain ordered processing within a partition, Lambda limits the maximum event pollers
                to the number of partitions in the topic. Additionally, the maximum event pollers your event
                source mapping can scale to depends on the function's concurrency settings.
            When activating provisioned mode, update your network settings to remove AWS PrivateLink VPC
                endpoints and associated permissions.
     
   
    Amazon CloudWatch metrics
    Lambda emits the OffsetLag metric while your function processes records. The value of this metric
      is the difference in offset between the last record written to the Kafka event source topic and the last record that your function's 
      consumer group processed. You can use OffsetLag to estimate the latency between when a record is added and when
      your consumer group processes it.
    An increasing trend in OffsetLag can indicate issues with pollers in your function's consumer group. For more information, see
      Using CloudWatch metrics with Lambda.
  Document ConventionsConfigure event sourceEvent filteringDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideInvocation metricsPerformance metricsConcurrency metricsAsynchronous invocation metricsEvent source mapping metricsTypes of metrics for Lambda functionsThis section describes the types of Lambda metrics available in the CloudWatch console.TopicsInvocation metricsPerformance metricsConcurrency metricsAsynchronous invocation metricsEvent source mapping metrics
        Invocation metrics
        
        Invocation metrics are binary indicators of the outcome of a Lambda function invocation.
            View these metrics with the Sum statistic. For example, if the function
            returns an error, then Lambda sends the Errors metric with a value of 1.
            To get a count of the number of function errors that occurred each minute, view the
            Sum of the Errors metric with a period of 1 minute.
        
             
             
             
             
             
             
             
             
             
        
                Invocations – The number of times that your function code is
                    invoked, including successful invocations and invocations that result in a function
                    error. Invocations aren't recorded if the invocation request is throttled or otherwise
                    results in an invocation error. The value of Invocations equals the number
                    of requests billed.
            
                Errors – The number of invocations that result in a function
                    error. Function errors include exceptions that your code throws and exceptions that the
                    Lambda runtime throws. The runtime returns errors for issues such as timeouts and
                    configuration errors. To calculate the error rate, divide the value of
                    Errors by the value of Invocations. Note that the timestamp
                    on an error metric reflects when the function was invoked, not when the error
                    occurred.
            
                DeadLetterErrors – For asynchronous invocation, the number of times that Lambda attempts to send an
                    event to a dead-letter queue (DLQ) but fails. Dead-letter errors can occur due to
                    incorrectly set resources or size limits.
            
                DestinationDeliveryFailures – For asynchronous invocation and supported event source mappings, the
                    number of times that Lambda attempts to send an event to a destination but fails. For event source mappings, 
                    Lambda supports destinations for stream sources (DynamoDB and Kinesis). Delivery errors can occur due to permissions errors, incorrectly configured resources, or size
                    limits. Errors can also occur if the destination you have configured is an unsupported type such as an Amazon SQS FIFO queue or an Amazon SNS FIFO topic.
            
                Throttles – The number of invocation requests that are
                    throttled. When all function instances are processing requests and no concurrency is
                    available to scale up, Lambda rejects additional requests with a
                    TooManyRequestsException error. Throttled requests and other invocation
                    errors don't count as either Invocations or Errors.
            
                OversizedRecordCount – For Amazon DocumentDB event sources, the number
                    of events your function receives from your change stream that are over 6 MB in size.
                    Lambda drops the message and emits this metric.
            
                ProvisionedConcurrencyInvocations – The number of times that
                    your function code is invoked using provisioned
                        concurrency.
            
                ProvisionedConcurrencySpilloverInvocations – The number of times
                    that your function code is invoked using standard concurrency when all provisioned
                    concurrency is in use.
            
                RecursiveInvocationsDropped – The number of times that Lambda has stopped invocation of your function because
                    it has detected that your function is part of an infinite recursive loop. Recursive loop detection monitors 
                    how many times a function is invoked as part of a chain of requests by tracking metadata added by supported AWS SDKs. By default, if your function is 
                    invoked as part of a chain of requests approximately 16 times, Lambda drops the next invocation. If you disable recursive loop detection, this
                    metric is not emitted. For more information about this feature, see Use Lambda recursive loop detection to prevent infinite loops.
            
     
        Performance metrics
        
        Performance metrics provide performance details about a single function invocation. For
            example, the Duration metric indicates the amount of time in milliseconds that
            your function spends processing an event. To get a sense of how fast your function processes
            events, view these metrics with the Average or Max
            statistic.
        
             
             
             
             
        
                Duration – The amount of time that your function code spends
                    processing an event. The billed duration for an invocation is the value of
                    Duration rounded up to the nearest millisecond. Duration does not 
                    include cold start time.
            
                PostRuntimeExtensionsDuration – The cumulative amount of time
                    that the runtime spends running code for extensions after the function code has
                    completed.
            
                IteratorAge – For DynamoDB, Kinesis, and Amazon DocumentDB event sources, the
                    age of the last record in the event in milliseconds. This metric measures the time
                    between when a stream receives the record and when the event source mapping sends the
                    event to the function.
            
                OffsetLag – For self-managed Apache Kafka and Amazon Managed Streaming for Apache Kafka (Amazon MSK) event sources, the
                    difference in offset between the last record written to a topic and the last record that
                    your function's consumer group processed. Though a Kafka topic can have multiple
                    partitions, this metric measures the offset lag at the topic level.
            
        
        Duration also supports percentile (p) statistics. Use
            percentiles to exclude outlier values that skew Average and
            Maximum statistics. For example, the p95 statistic shows the
            maximum duration of 95 percent of invocations, excluding the slowest 5 percent. For more
            information, see Percentiles in the Amazon CloudWatch User Guide.
     
        Concurrency metrics
        
        Lambda reports concurrency metrics as an aggregate count of the number of instances
            processing events across a function, version, alias, or AWS Region. To see how close you
            are to hitting concurrency limits, view these
            metrics with the Max statistic.
        
             
             
             
             
             
        
                ConcurrentExecutions – The number of function instances that are
                    processing events. If this number reaches your concurrent executions quota for the Region, or the reserved concurrency limit on the function,
                    then Lambda throttles additional invocation requests.
            
                ProvisionedConcurrentExecutions – The number of function
                    instances that are processing events using provisioned concurrency. For each invocation of an alias or version with
                    provisioned concurrency, Lambda emits the current count. If your function is inactive
                    or not receiving requests, Lambda doesn't emit this metric.
            
                ProvisionedConcurrencyUtilization – For a version or alias, the
                    value of ProvisionedConcurrentExecutions divided by the total amount of
                    provisioned concurrency configured. For example, if you configure a provisioned
                    concurrency of 10 for your function, and your
                    ProvisionedConcurrentExecutions is 7, then your
                    ProvisionedConcurrencyUtilization is 0.7.
                If your function is inactive or not receiving requests, Lambda doesn't emit this
                    metric because it is based on ProvisionedConcurrentExecutions. Keep this
                    in mind if you use ProvisionedConcurrencyUtilization as the basis for
                    CloudWatch alarms.
            
                UnreservedConcurrentExecutions – For a Region, the number of
                    events that functions without reserved concurrency are processing.
            
                ClaimedAccountConcurrency – For a Region, the amount of
                    concurrency that is unavailable for on-demand invocations.
                    ClaimedAccountConcurrency is equal to UnreservedConcurrentExecutions
                    plus the amount of allocated concurrency (i.e. the total reserved concurrency plus
                    total provisioned concurrency). For more information, see
                    Working with the
        ClaimedAccountConcurrency metric.
            
     
        Asynchronous invocation metrics
        Asynchronous invocation metrics provide details about asynchronous invocations from
            event sources and direct invocations. You can set thresholds and alarms to notify you of
            certain changes. For example, when there's an undesired increase in the number of events
            queued for processing (AsyncEventsReceived). Or, when an event has been waiting
            a long time to be processed (AsyncEventAge).
        
             
             
             
        
                AsyncEventsReceived – The number of events that Lambda
                    successfully queues for processing. This metric provides insight into the number of
                    events that a Lambda function receives. Monitor this metric and set alarms for thresholds
                    to check for issues. For example, to detect an undesirable number of events sent to
                    Lambda, and to quickly diagnose issues resulting from incorrect trigger or function
                    configurations. Mismatches between AsyncEventsReceived and
                    Invocations can indicate a disparity in processing, events being dropped,
                    or a potential queue backlog.
            
                AsyncEventAge – The time between when Lambda successfully queues
                    the event and when the function is invoked. The value of this metric increases when
                    events are being retried due to invocation failures or throttling. Monitor this metric
                    and set alarms for thresholds on different statistics for when a queue buildup occurs.
                    To troubleshoot an increase in this metric, look at the Errors metric to
                    identify function errors and the Throttles metric to identify concurrency
                    issues.
            
                AsyncEventsDropped – The number of events that are dropped
                    without successfully executing the function. If you configure a dead-letter queue (DLQ)
                    or OnFailure destination, then events are sent there before they're
                    dropped. Events are dropped for various reasons. For example, events can exceed the
                    maximum event age or exhaust the maximum retry attempts, or reserved concurrency might
                    be set to 0. To troubleshoot why events are dropped, look at the Errors
                    metric to identify function errors and the Throttles metric to identify
                    concurrency issues.
            
     
        Event source mapping metrics
        Event source mapping metrics provide insights into the processing behavior of your event
            source mapping. These metrics help you monitor the flow and status of events, including
            events that your event source mapping successfully processed, filtered, or dropped.
        You must opt-in to receive metrics related to counts (PolledEventCount,
            FilteredOutEventCount, InvokedEventCount,
            FailedInvokeEventCount, DroppedEventCount,
            OnFailureDestinationDeliveredEventCount, and DeletedEventCount).
            To opt-in, you can use the console or the Lambda API.
        To enable metrics or an event source mapping (console)
                Open the Functions page of the Lambda console.
            
                Choose the function you want to enable metrics for.
            
                Choose Configuration, then choose Triggers.
            
                Choose the event source mapping that you want to enable metrics for, then choose
                    Edit.
            
                Under Event source mapping configuration, choose 
                    Enable metrics.
            
                Choose Save.
            
        Alternatively, you can enable metrics for your event source mapping programmatically using
            the 
            EventSourceMappingMetricsConfig object in your EventSourceMappingConfiguration.
            For example, the following UpdateEventSourceMapping CLI command enables metrics for an
            event source mapping:
        aws lambda update-event-source-mapping \
    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \
    --metrics-config Metrics=EventCount
        View metrics related to event counts with the Sum statistic.
        WarningLambda event source mappings process each event at least once, and duplicate processing of
                records can occur. Because of this, events may be counted multiple times in metrics that
                involve event counts.
        
             
             
             
             
             
             
             
        
                PolledEventCount – The number of events that Lambda reads
                    successfully from the event source. If Lambda polls for events but receives an empty
                    poll (no new records), Lambda emits a 0 value for this metric. Use this metric to detect
                    whether your event source mapping is correctly polling for new events.
            
                FilteredOutEventCount – For event source mapping with a
                    filter criteria, the number of
                    events filtered out by that filter criteria. Use this metric to detect whether your
                    event source mapping is properly filtering out events. For events that match the filter
                    criteria, Lambda emits a 0 metric.
            
                InvokedEventCount – The number of events that invoked your Lambda
                    function. Use this metric to verify that events are properly invoking your function. If an
                    event results in a function error or throttling, InvokedEventCount may count
                    multiple times for the same polled event due to automatic retries.
            
                FailedInvokeEventCount – The number of events that Lambda tried to
                    invoke your function with, but failed. Invocations can fail due to reasons such as network
                    configuration issues, incorrect permissions, or a deleted Lambda function, version, or
                    alias. If your event source mapping has 
                    partial batch responses enabled, FailedInvokeEventCount includes any
                    event with a non-empty BatchItemFailures in the response.
                NoteThe timestamp for the FailedInvokeEventCount metric represents the end
                        of the function invocation. This behavior differs from other Lambda invocation error
                        metrics, which are timestamped at the start of the function invocation.
            
                DroppedEventCount – The number of events that Lambda dropped due to
                    expiry or retry exhaustion. Specifically, this is the number of records that exceed your
                    configured values for MaximumRecordAgeInSeconds or
                    MaximumRetryAttempts. Importantly, this doesn't include the number of records
                    that expire due to exceeding your event source's retention settings. Dropped events also
                    excludes events that you send to an 
                    on-failure destination. Use this metric to detect an increasing backlog of events.
            
                OnFailureDestinationDeliveredEventCount – For event source mappings
                    with an on-failure destination
                    configured, the number of events sent to that destination. Use this metric to monitor for
                    function errors related to invocations from this event source. If delivery to the
                    destination fails, Lambda handles metrics as follows:
                
                     
                     
                     
                
                        Lambda doesn't emit the OnFailureDestinationDeliveredEventCount
                            metric.
                    
                        For the DestinationDeliveryFailures metric, Lambda emits a 1.
                    
                        For the DroppedEventCount metric, Lambda emits a number equal to
                            the number of events that failed delivery.
                    
            
                DeletedEventCount – The number of events that Lambda successfully
                    deletes after processing. If Lambda tries to delete an event but fails, Lambda emits a 0
                    metric. Use this metric to ensure that successfully processed events are deleted from
                    your event source.
            
        If your event source mapping is disabled, you won't receive event source mapping metrics. You
            may also see missing metrics if CloudWatch or Lambda is experiencing degraded availability.
        Not every event source mapping metric is available for each event source. Currently, event
            source mapping metrics are available for Amazon SQS, Kinesis, and DynamoDB streams event sources. The
            following availability matrix summarizes the supported metrics for each type of event source.
        
                    
                        Event source mapping metric
                        Support for Amazon SQS
                        Support for Kinesis and DynamoDB streams
                    
                
                    
                        
                            PolledEventCount
                        
                        
                            Yes
                        
                        
                            Yes
                        
                    
                    
                        
                            FilteredOutEventCount
                        
                        
                            Yes
                        
                        
                            Yes
                        
                    
                    
                        
                            InvokedEventCount
                        
                        
                            Yes
                        
                        
                            Yes
                        
                    
                    
                        
                            FailedInvokeEventCount
                        
                        
                            Yes
                        
                        
                            Yes
                        
                    
                    
                        
                            DroppedEventCount
                        
                        
                            No
                        
                        
                            Yes
                        
                    
                    
                        
                            OnFailureDestinationDeliveredEventCount
                        
                        
                            No
                        
                        
                            Yes
                        
                    
                    
                        
                            DeletedEventCount
                        
                        
                            Yes
                        
                        
                            No
                        
                    
                
        In addition, if your event source mapping is in 
            provisioned mode, Lambda provides the following metric:
        
             
        
                ProvisionedPollers – For event source mappings in provisioned mode,
                    the number of event pollers that are actively running. View this metric using the
                    MAX metric.
            
    Document ConventionsView function metricsFunction logsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS Command Line InterfaceUser Guide for Version 2AWS CLI install and update
                instructionsTroubleshooting AWS CLI install and uninstall
                errorsNext stepsInstalling or updating to the latest version of
            the AWS CLIThis topic describes how to install or update the latest release of the AWS Command Line Interface (AWS CLI)
        on supported operating systems. For information on the latest releases of AWS CLI, see the
            AWS CLI version 2
            Changelog on GitHub.To install a past release of the AWS CLI, see Installing past releases of the AWS CLI version 2. For
        uninstall instructions, see Uninstalling the AWS CLI version 2.ImportantAWS CLI versions 1 and 2 use the same aws command name. If you previously
            installed AWS CLI version 1, see Migration guide for the AWS CLI version 2.TopicsAWS CLI install and update
                instructionsTroubleshooting AWS CLI install and uninstall
                errorsNext steps
        AWS CLI install and update
                instructions
        For installation instructions, expand the section for your operating system.
        
                    Install and update requirements
                
                    
                     
                     
                     
                     
                
                        You must be able to extract or "unzip" the downloaded package. If
                            your operating system doesn't have the built-in unzip
                            command, use an equivalent.
                    
                        The AWS CLI uses glibc, groff, and
                            less. These are included by default in most major
                            distributions of Linux.
                    
                        We support the AWS CLI on 64-bit versions of recent distributions of
                            CentOS, Fedora, Ubuntu, Amazon Linux 1, Amazon Linux 2, Amazon Linux 2023, and Linux
                            ARM.
                    
                        Because AWS doesn't maintain third-party repositories other than
                                snap, we can’t guarantee that they contain the latest
                            version of the AWS CLI.
                    
                    Install or
                        update the AWS CLI
                WarningIf this is your first time updating on Amazon Linux, to install the latest
                        version of the AWS CLI, you must uninstall the pre-installed
                        yum version using the following command:$ sudo yum remove awscliAfter the yum installation of the AWS CLI is removed,
                        follow the below Linux install instructions.
                You can install the AWS CLI by using one of the following methods:
                    
                         
                         
                    
                            The command line installer is
                                good option for version control, as you can specify the version to
                                install. This option does not auto-update and you must download a
                                new installer each time you update to overwrite previous
                                version.
                        
                            The officially supported snap
                                    package is a good option to always have the latest
                                version of the AWS CLI as snap packages automatically refresh. There
                                is no built-in support for selecting minor versions of AWS CLI and
                                therefore is not an optimal install method if your team needs to pin
                                versions.
                        
                
                    Command line installer - Linux x86 (64-bit)
                                To update your current installation of AWS CLI, download a new
                                    installer each time you update to overwrite previous versions.
                                    Follow these steps from the command line to install the AWS CLI on
                                    Linux.
                                The following are quick installation steps in a single copy
                                    and paste group that provide a basic installation. For guided
                                    instructions, see the steps that follow.
                                            Note(Optional)
                                                  The following command block downloads and installs
                                                  the AWS CLI without first verifying the integrity of
                                                  your download. To verify the integrity of your
                                                  download, use the below step by step
                                                  instructions.
                                            To install the
                                                AWS CLI, run the following commands.
                                            $ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
                                            To update your current
                                        installation of the AWS CLI, add your existing
                                    symlink and installer information to construct the
                                        install command using the
                                        --bin-dir, --install-dir, and
                                        --update parameters. The following command
                                    block uses an example symlink of
                                        /usr/local/bin and example
                                    installer location of
                                        /usr/local/aws-cli to install the
                                    AWS CLI locally for the current user.
                                            $ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update
                                Guided installation steps
                                        Download the installation file in one of the following
                                            ways:
                                                  
                                                   
                                                   
                                                  
                                                  Use the
                                                  curl command – The
                                                  -o option specifies the file name
                                                  that the downloaded package is written to. The
                                                  options on the following example command write the
                                                  downloaded file to the current directory with the
                                                  local name awscliv2.zip. 
                                                  $ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
                                                  
                                                  Downloading from the
                                                  URL – To download the installer
                                                  with your browser, use the following URL: https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip
                                                  
                                    
                                        (Optional) Verifying the
                                                integrity of your downloaded zip
                                            file
                                        If you chose to manually download the AWS CLI installer
                                            package .zip in the above steps,
                                            you can use the following steps to verify the signatures
                                            by using the GnuPG tool.
                                        The AWS CLI installer package .zip
                                            files are cryptographically signed using PGP signatures.
                                            If there is any damage or alteration of the files, this
                                            verification fails and you should not proceed with
                                            installation.
                                        
                                                Download and install the gpg
                                                  command using your package manager. For more
                                                  information about GnuPG, see the
                                                  GnuPG
                                                  website. 
                                            
                                                To create the public key file, create a text
                                                  file and paste in the following text.
                                                -----BEGIN PGP PUBLIC KEY BLOCK-----

mQINBF2Cr7UBEADJZHcgusOJl7ENSyumXh85z0TRV0xJorM2B/JL0kHOyigQluUG
ZMLhENaG0bYatdrKP+3H91lvK050pXwnO/R7fB/FSTouki4ciIx5OuLlnJZIxSzx
PqGl0mkxImLNbGWoi6Lto0LYxqHN2iQtzlwTVmq9733zd3XfcXrZ3+LblHAgEt5G
TfNxEKJ8soPLyWmwDH6HWCnjZ/aIQRBTIQ05uVeEoYxSh6wOai7ss/KveoSNBbYz
gbdzoqI2Y8cgH2nbfgp3DSasaLZEdCSsIsK1u05CinE7k2qZ7KgKAUIcT/cR/grk
C6VwsnDU0OUCideXcQ8WeHutqvgZH1JgKDbznoIzeQHJD238GEu+eKhRHcz8/jeG
94zkcgJOz3KbZGYMiTh277Fvj9zzvZsbMBCedV1BTg3TqgvdX4bdkhf5cH+7NtWO
lrFj6UwAsGukBTAOxC0l/dnSmZhJ7Z1KmEWilro/gOrjtOxqRQutlIqG22TaqoPG
fYVN+en3Zwbt97kcgZDwqbuykNt64oZWc4XKCa3mprEGC3IbJTBFqglXmZ7l9ywG
EEUJYOlb2XrSuPWml39beWdKM8kzr1OjnlOm6+lpTRCBfo0wa9F8YZRhHPAkwKkX
XDeOGpWRj4ohOx0d2GWkyV5xyN14p2tQOCdOODmz80yUTgRpPVQUtOEhXQARAQAB
tCFBV1MgQ0xJIFRlYW0gPGF3cy1jbGlAYW1hem9uLmNvbT6JAlQEEwEIAD4CGwMF
CwkIBwIGFQoJCAsCBBYCAwECHgECF4AWIQT7Xbd/1cEYuAURraimMQrMRnJHXAUC
ZqFYbwUJCv/cOgAKCRCmMQrMRnJHXKYuEAC+wtZ611qQtOl0t5spM9SWZuszbcyA
0xBAJq2pncnp6wdCOkuAPu4/R3UCIoD2C49MkLj9Y0Yvue8CCF6OIJ8L+fKBv2DI
yWZGmHL0p9wa/X8NCKQrKxK1gq5PuCzi3f3SqwfbZuZGeK/ubnmtttWXpUtuU/Iz
VR0u/0sAy3j4uTGKh2cX7XnZbSqgJhUk9H324mIJiSwzvw1Ker6xtH/LwdBeJCck
bVBdh3LZis4zuD4IZeBO1vRvjot3Oq4xadUv5RSPATg7T1kivrtLCnwvqc6L4LnF
0OkNysk94L3LQSHyQW2kQS1cVwr+yGUSiSp+VvMbAobAapmMJWP6e/dKyAUGIX6+
2waLdbBs2U7MXznx/2ayCLPH7qCY9cenbdj5JhG9ibVvFWqqhSo22B/URQE/CMrG
+3xXwtHEBoMyWEATr1tWwn2yyQGbkUGANneSDFiTFeoQvKNyyCFTFO1F2XKCcuDs
19nj34PE2TJilTG2QRlMr4D0NgwLLAMg2Los1CK6nXWnImYHKuaKS9LVaCoC8vu7
IRBik1NX6SjrQnftk0M9dY+s0ZbAN1gbdjZ8H3qlbl/4TxMdr87m8LP4FZIIo261
Eycv34pVkCePZiP+dgamEiQJ7IL4ZArio9mv6HbDGV6mLY45+l6/0EzCwkI5IyIf
BfWC9s/USgxchg==
=ptgS
-----END PGP PUBLIC KEY BLOCK-----
                                                For reference, the following are the details
                                                  of the public key.
                                                Key ID:           A6310ACC4672475C
Type:             RSA
Size:             4096/4096
Created:          2019-09-18
Expires:          2025-07-24
User ID:          AWS CLI Team <aws-cli@amazon.com>
Key fingerprint:  FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C
                                            
                                                Import the AWS CLI public key with the following
                                                  command, substituting
                                                  public-key-file-name
                                                  with the file name of the public key you
                                                  created.
                                                $ gpg --import public-key-file-name
gpg: /home/username/.gnupg/trustdb.gpg: trustdb created
gpg: key A6310ACC4672475C: public key "AWS CLI Team <aws-cli@amazon.com>" imported
gpg: Total number processed: 1
gpg:               imported: 1
                                            
                                                Download the AWS CLI signature file for the
                                                  package you downloaded. It has the same path and
                                                  name as the .zip file it corresponds
                                                  to, but has the extension .sig. In
                                                  the following examples, we save it to the current
                                                  directory as a file named
                                                  awscliv2.sig.
                                                  For the latest version
                                                  of the AWS CLI, use the following command
                                                  block:
                                                  $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip.sig
                                                  For a specific version
                                                  of the AWS CLI, append a hyphen and the
                                                  version number to the filename. For this example
                                                  the filename for version
                                                  2.0.30 would be
                                                  awscli-exe-linux-x86_64-2.0.30.zip.sig
                                                  resulting in the following command:
                                                  $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip.sig
                                                   For a list of versions, see the AWS CLI version 2 Changelog on GitHub.
                                            
                                                Verify the signature, passing both the
                                                  downloaded .sig and
                                                  .zip file names as parameters to the
                                                  gpg command.
                                                $ gpg --verify awscliv2.sig awscliv2.zip
                                                The output should look similar to the
                                                  following.
                                                gpg: Signature made Mon Nov  4 19:00:01 2019 PST
gpg:                using RSA key FB5D B77F D5C1 18B8 0511 ADA8 A631 0ACC 4672 475C
gpg: Good signature from "AWS CLI Team <aws-cli@amazon.com>" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C
                                                ImportantThe warning in the output is expected and
                                                  doesn't indicate a problem. It occurs because
                                                  there isn't a chain of trust between your personal
                                                  PGP key (if you have one) and the AWS CLI PGP key.
                                                  For more information, see Web of trust.
                                            
                                    
                                        Unzip the installer. If your Linux distribution
                                            doesn't have a built-in unzip command, use
                                            an equivalent to unzip it. The following example command
                                            unzips the package and creates a directory named
                                                aws under the current
                                            directory.
                                        $ unzip awscliv2.zip
                                        NoteWhen updating from a previous version, the
                                                  unzip command prompts to overwrite
                                                existing files. To skip these prompts, such as with
                                                script automation, use the -u update
                                                flag for unzip. This flag automatically
                                                updates existing files and creates new ones as
                                                needed.$ unzip -u awscliv2.zip
                                    
                                        Run the install program. The installation command uses
                                            a file named install in the newly
                                            unzipped aws directory. By default,
                                            the files are all installed to
                                                /usr/local/aws-cli, and a
                                            symbolic link is created in
                                                /usr/local/bin. The command
                                            includes sudo to grant write permissions to
                                            those directories. 
                                        $ sudo ./aws/install
                                        You can install without sudo if you
                                            specify directories that you already have write
                                            permissions to. Use the following instructions for the
                                                install command to specify the
                                            installation location:
                                        
                                             
                                             
                                             
                                        
                                                Ensure that the paths you provide to the
                                                  -i and -b parameters
                                                  contain no volume name or directory names that
                                                  contain any space characters or other white space
                                                  characters. If there is a space, the installation
                                                  fails.
                                            
                                                --install-dir or -i
                                                  – This option specifies the directory to
                                                  copy all of the files to.
                                                The default value is
                                                  /usr/local/aws-cli.
                                            
                                                --bin-dir or -b
                                                  – This option specifies that the main
                                                  aws program in the install directory
                                                  is symbolically linked to the file
                                                  aws in the specified path.
                                                  You must have write permissions to the specified
                                                  directory. Creating a symlink to a directory that
                                                  is already in your path eliminates the need to add
                                                  the install directory to the user's
                                                  $PATH variable. 
                                                The default value is
                                                  /usr/local/bin.
                                            
                                        $ ./aws/install -i /usr/local/aws-cli -b /usr/local/bin
                                        NoteTo update your current installation of the AWS CLI,
                                                add your existing symlink and installer information
                                                to construct the install command with
                                                the --update parameter.$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --updateTo locate the existing symlink and installation
                                                directory, use the following steps:
                                                 
                                                 
                                            
                                                  Use the which command to find
                                                  your symlink. This gives you the path to use with
                                                  the --bin-dir parameter.
                                                  $ which aws
/usr/local/bin/aws
                                                
                                                  Use the ls command to find the
                                                  directory that your symlink points to. This gives
                                                  you the path to use with the
                                                  --install-dir parameter.
                                                  $ ls -l /usr/local/bin/aws
lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -> /usr/local/aws-cli/v2/current/bin/aws
                                                
                                    
                                        Confirm the installation with the following command. 
                                        $ aws --version
aws-cli/2.25.11 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 
                                        If the aws command cannot be found, you
                                            might need to restart your terminal or follow the
                                            troubleshooting in Troubleshooting errors for the AWS CLI.
                                    
                            
                        Command line - Linux ARM
                                To update your current installation of AWS CLI, download a new
                                    installer each time you update to overwrite previous versions.
                                    Follow these steps from the command line to install the AWS CLI on
                                    Linux.
                                The following are quick installation steps in a single copy
                                    and paste group that provide a basic installation. For guided
                                    instructions, see the steps that follow.
                                Note(Optional) The following
                                        command block downloads and installs the AWS CLI without first
                                        verifying the integrity of your download. To verify the
                                        integrity of your download, use the below step by step
                                        instructions.
                                To install the AWS CLI, run the
                                    following commands.
                                $ curl "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
                                To update your current
                                        installation of the AWS CLI, add your existing
                                    symlink and installer information to construct the
                                        install command using the
                                        --bin-dir, --install-dir, and
                                        --update parameters. The following command
                                    block uses an example symlink of
                                        /usr/local/bin and example
                                    installer location of
                                        /usr/local/aws-cli.
                                $ curl "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update
                                Guided installation steps
                                        Download the installation file in one of the following
                                            ways:
                                        
                                             
                                             
                                        
                                                Use the
                                                  curl command – The
                                                  -o option specifies the file name
                                                  that the downloaded package is written to. The
                                                  options on the following example command write the
                                                  downloaded file to the current directory with the
                                                  local name awscliv2.zip. 
                                                $ curl "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" -o "awscliv2.zip"
                                            
                                                Downloading from the
                                                  URL – To download the installer
                                                  with your browser, use the following URL: https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip
                                            
                                    
                                        (Optional) Verifying the
                                                integrity of your downloaded zip
                                            file
                                        If you chose to manually download the AWS CLI installer
                                            package .zip in the above steps,
                                            you can use the following steps to verify the signatures
                                            by using the GnuPG tool.
                                        The AWS CLI installer package .zip
                                            files are cryptographically signed using PGP signatures.
                                            If there is any damage or alteration of the files, this
                                            verification fails and you should not proceed with
                                            installation.
                                        
                                                Download and install the gpg
                                                  command using your package manager. For more
                                                  information about GnuPG, see the
                                                  GnuPG
                                                  website. 
                                            
                                                To create the public key file, create a text
                                                  file and paste in the following text.
                                                -----BEGIN PGP PUBLIC KEY BLOCK-----

mQINBF2Cr7UBEADJZHcgusOJl7ENSyumXh85z0TRV0xJorM2B/JL0kHOyigQluUG
ZMLhENaG0bYatdrKP+3H91lvK050pXwnO/R7fB/FSTouki4ciIx5OuLlnJZIxSzx
PqGl0mkxImLNbGWoi6Lto0LYxqHN2iQtzlwTVmq9733zd3XfcXrZ3+LblHAgEt5G
TfNxEKJ8soPLyWmwDH6HWCnjZ/aIQRBTIQ05uVeEoYxSh6wOai7ss/KveoSNBbYz
gbdzoqI2Y8cgH2nbfgp3DSasaLZEdCSsIsK1u05CinE7k2qZ7KgKAUIcT/cR/grk
C6VwsnDU0OUCideXcQ8WeHutqvgZH1JgKDbznoIzeQHJD238GEu+eKhRHcz8/jeG
94zkcgJOz3KbZGYMiTh277Fvj9zzvZsbMBCedV1BTg3TqgvdX4bdkhf5cH+7NtWO
lrFj6UwAsGukBTAOxC0l/dnSmZhJ7Z1KmEWilro/gOrjtOxqRQutlIqG22TaqoPG
fYVN+en3Zwbt97kcgZDwqbuykNt64oZWc4XKCa3mprEGC3IbJTBFqglXmZ7l9ywG
EEUJYOlb2XrSuPWml39beWdKM8kzr1OjnlOm6+lpTRCBfo0wa9F8YZRhHPAkwKkX
XDeOGpWRj4ohOx0d2GWkyV5xyN14p2tQOCdOODmz80yUTgRpPVQUtOEhXQARAQAB
tCFBV1MgQ0xJIFRlYW0gPGF3cy1jbGlAYW1hem9uLmNvbT6JAlQEEwEIAD4CGwMF
CwkIBwIGFQoJCAsCBBYCAwECHgECF4AWIQT7Xbd/1cEYuAURraimMQrMRnJHXAUC
ZqFYbwUJCv/cOgAKCRCmMQrMRnJHXKYuEAC+wtZ611qQtOl0t5spM9SWZuszbcyA
0xBAJq2pncnp6wdCOkuAPu4/R3UCIoD2C49MkLj9Y0Yvue8CCF6OIJ8L+fKBv2DI
yWZGmHL0p9wa/X8NCKQrKxK1gq5PuCzi3f3SqwfbZuZGeK/ubnmtttWXpUtuU/Iz
VR0u/0sAy3j4uTGKh2cX7XnZbSqgJhUk9H324mIJiSwzvw1Ker6xtH/LwdBeJCck
bVBdh3LZis4zuD4IZeBO1vRvjot3Oq4xadUv5RSPATg7T1kivrtLCnwvqc6L4LnF
0OkNysk94L3LQSHyQW2kQS1cVwr+yGUSiSp+VvMbAobAapmMJWP6e/dKyAUGIX6+
2waLdbBs2U7MXznx/2ayCLPH7qCY9cenbdj5JhG9ibVvFWqqhSo22B/URQE/CMrG
+3xXwtHEBoMyWEATr1tWwn2yyQGbkUGANneSDFiTFeoQvKNyyCFTFO1F2XKCcuDs
19nj34PE2TJilTG2QRlMr4D0NgwLLAMg2Los1CK6nXWnImYHKuaKS9LVaCoC8vu7
IRBik1NX6SjrQnftk0M9dY+s0ZbAN1gbdjZ8H3qlbl/4TxMdr87m8LP4FZIIo261
Eycv34pVkCePZiP+dgamEiQJ7IL4ZArio9mv6HbDGV6mLY45+l6/0EzCwkI5IyIf
BfWC9s/USgxchg==
=ptgS
-----END PGP PUBLIC KEY BLOCK-----
                                                For reference, the following are the details
                                                  of the public key.
                                                Key ID:           A6310ACC4672475C
Type:             RSA
Size:             4096/4096
Created:          2019-09-18
Expires:          2025-07-24
User ID:          AWS CLI Team <aws-cli@amazon.com>
Key fingerprint:  FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C
                                            
                                                Import the AWS CLI public key with the following
                                                  command, substituting
                                                  public-key-file-name
                                                  with the file name of the public key you
                                                  created.
                                                $ gpg --import public-key-file-name
gpg: /home/username/.gnupg/trustdb.gpg: trustdb created
gpg: key A6310ACC4672475C: public key "AWS CLI Team <aws-cli@amazon.com>" imported
gpg: Total number processed: 1
gpg:               imported: 1
                                            
                                                Download the AWS CLI signature file for the
                                                  package you downloaded. It has the same path and
                                                  name as the .zip file it corresponds
                                                  to, but has the extension .sig. In
                                                  the following examples, we save it to the current
                                                  directory as a file named
                                                  awscliv2.sig.
                                                For the latest version
                                                  of the AWS CLI, use the following command
                                                  block:
                                                $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip.sig
                                                For a specific version
                                                  of the AWS CLI, append a hyphen and the
                                                  version number to the filename. For this example
                                                  the filename for version
                                                  2.0.30 would be
                                                  awscli-exe-linux-aarch64-2.0.30.zip.sig
                                                  resulting in the following command:
                                                $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-aarch64-2.0.30.zip.sig
                                                 For a list of versions, see the AWS CLI version 2 Changelog on GitHub.
                                            
                                                Verify the signature, passing both the
                                                  downloaded .sig and
                                                  .zip file names as parameters to the
                                                  gpg command.
                                                $ gpg --verify awscliv2.sig awscliv2.zip
                                                The output should look similar to the
                                                  following.
                                                gpg: Signature made Mon Nov  4 19:00:01 2019 PST
gpg:                using RSA key FB5D B77F D5C1 18B8 0511 ADA8 A631 0ACC 4672 475C
gpg: Good signature from "AWS CLI Team <aws-cli@amazon.com>" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C
                                                ImportantThe warning in the output is expected and
                                                  doesn't indicate a problem. It occurs because
                                                  there isn't a chain of trust between your personal
                                                  PGP key (if you have one) and the AWS CLI PGP key.
                                                  For more information, see Web of trust.
                                            
                                    
                                        Unzip the installer. If your Linux distribution
                                            doesn't have a built-in unzip command, use
                                            an equivalent to unzip it. The following example command
                                            unzips the package and creates a directory named
                                                aws under the current
                                            directory.
                                        $ unzip awscliv2.zip
                                        NoteWhen updating from a previous version, the
                                                  unzip command prompts to overwrite
                                                existing files. To skip these prompts, such as with
                                                script automation, use the -u update
                                                flag for unzip. This flag automatically
                                                updates existing files and creates new ones as
                                                needed.$ unzip -u awscliv2.zip
                                    
                                        Run the install program. The installation command uses
                                            a file named install in the newly
                                            unzipped aws directory. By default,
                                            the files are all installed to
                                                /usr/local/aws-cli, and a
                                            symbolic link is created in
                                                /usr/local/bin. The command
                                            includes sudo to grant write permissions to
                                            those directories. 
                                        $ sudo ./aws/install
                                        You can install without sudo if you
                                            specify directories that you already have write
                                            permissions to. Use the following instructions for the
                                                install command to specify the
                                            installation location:
                                        
                                             
                                             
                                             
                                        
                                                Ensure that the paths you provide to the
                                                  -i and -b parameters
                                                  contain no volume name or directory names that
                                                  contain any space characters or other white space
                                                  characters. If there is a space, the installation
                                                  fails.
                                            
                                                --install-dir or -i
                                                  – This option specifies the directory to
                                                  copy all of the files to.
                                                The default value is
                                                  /usr/local/aws-cli.
                                            
                                                --bin-dir or -b
                                                  – This option specifies that the main
                                                  aws program in the install directory
                                                  is symbolically linked to the file
                                                  aws in the specified path.
                                                  You must have write permissions to the specified
                                                  directory. Creating a symlink to a directory that
                                                  is already in your path eliminates the need to add
                                                  the install directory to the user's
                                                  $PATH variable. 
                                                The default value is
                                                  /usr/local/bin.
                                            
                                        $ ./aws/install -i /usr/local/aws-cli -b /usr/local/bin
                                        NoteTo update your current installation of the AWS CLI,
                                                add your existing symlink and installer information
                                                to construct the install command with
                                                the --update parameter.$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --updateTo locate the existing symlink and installation
                                                directory, use the following steps:
                                                 
                                                 
                                            
                                                  Use the which command to find
                                                  your symlink. This gives you the path to use with
                                                  the --bin-dir parameter.
                                                  $ which aws
/usr/local/bin/aws
                                                
                                                  Use the ls command to find the
                                                  directory that your symlink points to. This gives
                                                  you the path to use with the
                                                  --install-dir parameter.
                                                  $ ls -l /usr/local/bin/aws
lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -> /usr/local/aws-cli/v2/current/bin/aws
                                                
                                    
                                        Confirm the installation with the following command. 
                                        $ aws --version
aws-cli/2.25.11 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 
                                        If the aws command cannot be found, you
                                            might need to restart your terminal or follow the
                                            troubleshooting in Troubleshooting errors for the AWS CLI.
                                    
                            
                    Snap package
                                We provide an official AWS supported version of the AWS CLI on
                                        snap. If you want to always have the latest
                                    version of the AWS CLI installed on your system, a snap package
                                    provides this for you as it auto-updates. There is no built-in
                                    support for selecting minor versions of AWS CLI and therefore it
                                    is not an optimal install method if your team needs to pin
                                    versions. If you want to install a specific minor version of the
                                    AWS CLI, we suggest you use the command line installer.
                                
                                        If your Linux platform does not already have
                                                snap installed, install
                                                snap on your platform. 
                                        
                                                For information on installing
                                                  snap, see Installing the daemon in the Snap
                                                  documentation.
                                            
                                                You may need to restart your system so that
                                                  your PATH variables are updated
                                                  correctly. If you are having installation issues,
                                                  follow steps in Fix common issues in the Snap
                                                  documentation.
                                            
                                                To verify that snap is installed
                                                  correctly, run the following command.
                                                $ snap version
                                            
                                    
                                        Run the following snap install command
                                            for the AWS CLI.
                                        $ snap install aws-cli --classic
                                        Depending on your permissions, you may need to add
                                                sudo to the command.
                                        $ sudo snap install aws-cli --classic
                                        NoteTo view the snap repository for the AWS CLI,
                                                including additional snap instructions,
                                                see the aws-cli page in the
                                                  Canonical Snapcraft
                                                  website.
                                    
                                        Verify that the AWS CLI installed correctly.
                                        $ aws --version
aws-cli/2.25.11 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 
                                        If you get an error, see Troubleshooting errors for the AWS CLI.
                                    
                            
                
                
        
                    Install and update
                            requirements
                    
                         
                         
                    
                            We support the AWS CLI on macOS versions 11 and later. For more
                                information, see macOS support policy updates for the AWS CLI v2 on the
                                    AWS Developer Tools
                                Blog.
                        
                            Because AWS doesn't maintain third-party repositories, we can’t
                                guarantee that they contain the latest version of the AWS CLI.
                        
                    
                    
    macOS version support matrix
    
                
                    AWS CLI version
                    Supported macOS version
                
            
                
                    2.21.0 – current
                    11+
                
                
                    2.17.0 –2.20.0
                    10.15+
                
                
                    2.0.0 – 2.16.12
                    10.14 and below
                
            

                 
                    Install or update the
                            AWS CLI
                    If you are updating to the latest version, use the same installation
                        method that you used in your current version. You can install the AWS CLI on
                        macOS in the following ways.
                    
                        GUI installer
                                The following steps show how to install the latest version
                                        of the AWS CLI by using the standard macOS user interface and
                                        your browser.
                                        In your browser, download the macOS
                                                pkg file: https://awscli.amazonaws.com/AWSCLIV2.pkg
                                    
                                        Run your downloaded file and follow the on-screen
                                            instructions. You can choose to install the AWS CLI in the
                                            following ways:
                                        
                                             
                                             
                                        
                                                For all users on the
                                                  computer (requires
                                                  sudo)
                                                
                                                   
                                                   
                                                
                                                  You can install to any folder, or choose the
                                                  recommended default folder of
                                                  /usr/local/aws-cli.
                                                  
                                                  The installer automatically creates a
                                                  symlink at /usr/local/bin/aws that
                                                  links to the main program in the installation
                                                  folder you chose.
                                                  
                                            
                                                For only the current
                                                  user (doesn't require
                                                  sudo)
                                                
                                                   
                                                   
                                                
                                                  You can install to any folder to which you
                                                  have write permission.
                                                  
                                                  Due to standard user permissions, after the
                                                  installer finishes, you must manually create a
                                                  symlink file in your $PATH that
                                                  points to the aws and
                                                  aws_completer programs by using the
                                                  following commands at the command prompt. If your
                                                  $PATH includes a folder you can write
                                                  to, you can run the following command without
                                                  sudo if you specify that folder as
                                                  the target's path. If you don't have a writable
                                                  folder in your $PATH, you must use
                                                  sudo in the commands to get
                                                  permissions to write to the specified target
                                                  folder. The default location for a symlink is
                                                  /usr/local/bin/.
                                                  $ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws
$ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer
                                                  
                                            
                                        NoteYou can view debug logs for the installation by
                                                pressing Cmd+L
                                                anywhere in the installer. This opens a log pane
                                                that enables you to filter and save the log. The log
                                                file is also automatically saved to
                                                  /var/log/install.log.
                                    
                                        To verify that the shell can find and run the
                                                aws command in your $PATH,
                                            use the following commands. 
                                        $ which aws
/usr/local/bin/aws 
$ aws --version
aws-cli/2.25.11 Python/3.11.6 Darwin/23.3.0
                                        If the aws command cannot be found, you
                                            might need to restart your terminal or follow the
                                            troubleshooting in Troubleshooting errors for the AWS CLI.
                                    
                            
                        Command line installer - All users
                                If you have sudo permissions, you can install the
                                    AWS CLI for all users on the computer. We provide the steps in one
                                    easy to copy and paste group. See the descriptions of each line
                                    in the following steps. 
                                $ curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
$ sudo installer -pkg AWSCLIV2.pkg -target /
                                Guided installation instructions
                                        Download the file using the curl command.
                                            The -o option specifies the file name that
                                            the downloaded package is written to. In this example,
                                            the file is written to
                                                AWSCLIV2.pkg in the
                                            current folder.
                                        $ curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
                                    
                                        Run the standard macOS installer program,
                                            specifying the downloaded .pkg file
                                            as the source. Use the -pkg parameter to
                                            specify the name of the package to install, and the
                                                -target / parameter for which drive to
                                            install the package to. The files are installed to
                                                /usr/local/aws-cli, and a
                                            symlink is automatically created in
                                                /usr/local/bin. You must
                                            include sudo on the command to grant write
                                            permissions to those folders. 
                                        $ sudo installer -pkg ./AWSCLIV2.pkg -target /
                                        After installation is complete, debug logs are written
                                            to /var/log/install.log.
                                    
                                        To verify that the shell can find and run the
                                                aws command in your $PATH,
                                            use the following commands. 
                                        $ which aws
/usr/local/bin/aws 
$ aws --version
aws-cli/2.25.11 Python/3.11.6 Darwin/23.3.0
                                        If the aws command cannot be found, you
                                            might need to restart your terminal or follow the
                                            troubleshooting in Troubleshooting errors for the AWS CLI.
                                    
                            
                        Command line - Current user
                                
                                        To specify which folder the AWS CLI is installed to, you
                                            must create an XML file with any file name. This file is
                                            an XML-formatted file that looks like the following
                                            example. Leave all values as shown, except you must
                                            replace the path
                                                /Users/myusername in line
                                            9 with the path to the folder you want the AWS CLI
                                            installed to. The folder must
                                                already exist, or the command fails. The
                                            following XML example, named
                                                choices.xml, specifies
                                            the installer to install the AWS CLI in the folder
                                                /Users/myusername, where it
                                            creates a folder named
                                            aws-cli.
                                        <?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
  <array>
    <dict>
      <key>choiceAttribute</key>
      <string>customLocation</string>
      <key>attributeSetting</key>
      <string>/Users/myusername</string>
      <key>choiceIdentifier</key>
      <string>default</string>
    </dict>
  </array>
</plist>
                                    
                                        Download the pkg installer using
                                            the curl command. The -o
                                            option specifies the file name that the downloaded
                                            package is written to. In this example, the file is
                                            written to AWSCLIV2.pkg
                                            in the current folder.
                                        $ curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
                                    
                                        Run the standard macOS installer program
                                            with the following options:
                                        
                                             
                                             
                                             
                                        
                                                Specify the name of the package to install by
                                                  using the -pkg parameter.
                                            
                                                Specify installing to a current user only by
                                                  setting the -target parameter to
                                                  CurrentUserHomeDirectory.
                                            
                                                Specify the path (relative to the current
                                                  folder) and name of the XML file that you created
                                                  in the -applyChoiceChangesXML
                                                  parameter.
                                            
                                        The following example installs the AWS CLI in the folder
                                                /Users/myusername/aws-cli.
                                        $ installer -pkg AWSCLIV2.pkg \
            -target CurrentUserHomeDirectory \
            -applyChoiceChangesXML choices.xml
                                    
                                        Because standard user permissions typically don't
                                            allow writing to folders in your $PATH, the
                                            installer in this mode doesn't try to add the symlinks
                                            to the aws and aws_completer
                                            programs. For the AWS CLI to run correctly, you must
                                            manually create the symlinks after the installer
                                            finishes. If your $PATH includes a folder
                                            you can write to and you specify the folder as the
                                            target's path, you can run the following command without
                                                sudo. If you don't have a writable
                                            folder in your $PATH, you must use
                                                sudo for permissions to write to the
                                            specified target folder. The default location for a
                                            symlink is /usr/local/bin/. Replace
                                                folder/installed with the path to your
                                            AWS CLI installation.
                                        $ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws
$ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer
                                        After installation is complete, debug logs are written
                                            to /var/log/install.log.
                                    
                                        To verify that the shell can find and run the
                                                aws command in your $PATH,
                                            use the following commands. 
                                        $ which aws
/usr/local/bin/aws 
$ aws --version
aws-cli/2.25.11 Python/3.11.6 Darwin/23.3.0
                                        If the aws command cannot be found, you
                                            might need to restart your terminal or follow the
                                            troubleshooting in Troubleshooting errors for the AWS CLI.
                                    
                            
                    
                
        
                    Install and update
                            requirements
                    
                         
                         
                    
                            We support the AWS CLI on Microsoft-supported versions of 64-bit
                                Windows.
                        
                            Admin rights to install software
                        
                 
                    Install or update the
                            AWS CLI
                    To update your current installation of AWS CLI on Windows, download a new
                        installer each time you update to overwrite previous versions. AWS CLI is
                        updated regularly. To see when the latest version was released, see the
                            AWS CLI version 2 Changelog on GitHub. 
                    
                            Download and run the AWS CLI MSI installer for Windows
                                (64-bit):
                            https://awscli.amazonaws.com/AWSCLIV2.msi
                            Alternatively, you can run the msiexec command to run
                                the MSI installer.
                            C:\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi
                            For various parameters that can be used with msiexec,
                                see msiexec on the Microsoft
                                    Docs website. For example, you can use the
                                    /qn flag for a silent installation.
                            C:\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi /qn
                        
                            To confirm the installation, open the Start
                                menu, search for cmd to open a command prompt window,
                                and at the command prompt use the aws --version
                                command. 
                            C:\> aws --version
aws-cli/2.25.11 Python/3.11.6 Windows/10 exe/AMD64 prompt/off
                            If Windows is unable to find the program, you might need to close
                                and reopen the command prompt window to refresh the path, or follow
                                the troubleshooting in Troubleshooting errors for the AWS CLI.
                        
                
     
        Troubleshooting AWS CLI install and uninstall
                errors
        If you come across issues after installing or uninstalling the AWS CLI, see Troubleshooting errors for the AWS CLI for troubleshooting steps. For the most
            relevant troubleshooting steps, see Command not found errors, The "aws --version" command
                returns a different version than you installed, and The "aws --version" command returns a
                version after uninstalling the AWS CLI.
     
        Next steps
        After you successfully install the AWS CLI, you can safely delete your downloaded
            installer files. After completing the steps in Prerequisites to use the AWS CLI version 2
            and installing the AWS CLI, you should perform a Setting up the AWS CLI.
    Document ConventionsPrerequisitesPast releasesDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoListEventSourceMappingsLists event source mappings. Specify an EventSourceArn to show only event source mappings for a
      single event source.
      Request Syntax
      GET /2015-03-31/event-source-mappings/?EventSourceArn=EventSourceArn&FunctionName=FunctionName&Marker=Marker&MaxItems=MaxItems HTTP/1.1

    
      URI Request Parameters
      The request uses the following URI parameters.
      
          
          
          
          
      
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – The ARN of the data stream or a stream consumer.
                  
                     
                        Amazon DynamoDB Streams – The ARN of the stream.
                  
                     
                        Amazon Simple Queue Service – The ARN of the queue.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – The ARN of the cluster or the ARN of the VPC connection (for cross-account event source mappings).
                  
                     
                        Amazon MQ – The ARN of the broker.
                  
                     
                        Amazon DocumentDB – The ARN of the DocumentDB change stream.
                  
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  FunctionName
               
            
            
               The name or ARN of the Lambda function.
               
                  Name formats
                   
                   
                   
                   
               
                     
                        Function name – MyFunction.
                  
                     
                        Function ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction.
                  
                     
                        Version or Alias ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD.
                  
                     
                        Partial ARN – 123456789012:function:MyFunction.
                  
               The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64
      characters in length.
               Length Constraints: Minimum length of 1. Maximum length of 140.
               Pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\d{1}:)?(\d{12}:)?(function:)?([a-zA-Z0-9-_]+)(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  Marker
               
            
            
               A pagination token returned by a previous call.
            
          
            
               
                  MaxItems
               
            
            
               The maximum number of event source mappings to return. Note that ListEventSourceMappings returns a maximum of
      100 items in each response, even if you set the number higher.
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
         
    
      Request Body
      The request does not have a request body.
    
      Response Syntax
      HTTP/1.1 200
Content-type: application/json

{
   "EventSourceMappings": [ 
      { 
         "AmazonManagedKafkaEventSourceConfig": { 
            "ConsumerGroupId": "string"
         },
         "BatchSize": number,
         "BisectBatchOnFunctionError": boolean,
         "DestinationConfig": { 
            "OnFailure": { 
               "Destination": "string"
            },
            "OnSuccess": { 
               "Destination": "string"
            }
         },
         "DocumentDBEventSourceConfig": { 
            "CollectionName": "string",
            "DatabaseName": "string",
            "FullDocument": "string"
         },
         "EventSourceArn": "string",
         "EventSourceMappingArn": "string",
         "FilterCriteria": { 
            "Filters": [ 
               { 
                  "Pattern": "string"
               }
            ]
         },
         "FilterCriteriaError": { 
            "ErrorCode": "string",
            "Message": "string"
         },
         "FunctionArn": "string",
         "FunctionResponseTypes": [ "string" ],
         "KMSKeyArn": "string",
         "LastModified": number,
         "LastProcessingResult": "string",
         "MaximumBatchingWindowInSeconds": number,
         "MaximumRecordAgeInSeconds": number,
         "MaximumRetryAttempts": number,
         "MetricsConfig": { 
            "Metrics": [ "string" ]
         },
         "ParallelizationFactor": number,
         "ProvisionedPollerConfig": { 
            "MaximumPollers": number,
            "MinimumPollers": number
         },
         "Queues": [ "string" ],
         "ScalingConfig": { 
            "MaximumConcurrency": number
         },
         "SelfManagedEventSource": { 
            "Endpoints": { 
               "string" : [ "string" ]
            }
         },
         "SelfManagedKafkaEventSourceConfig": { 
            "ConsumerGroupId": "string"
         },
         "SourceAccessConfigurations": [ 
            { 
               "Type": "string",
               "URI": "string"
            }
         ],
         "StartingPosition": "string",
         "StartingPositionTimestamp": number,
         "State": "string",
         "StateTransitionReason": "string",
         "Topics": [ "string" ],
         "TumblingWindowInSeconds": number,
         "UUID": "string"
      }
   ],
   "NextMarker": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 200 response.
      The following data is returned in JSON format by the service.
      
          
          
      
            
               
                  EventSourceMappings
               
            
            
               A list of event source mappings.
               Type: Array of EventSourceMappingConfiguration objects
            
          
            
               
                  NextMarker
               
            
            
               A pagination token that's returned when the response doesn't contain all event source mappings.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsListCodeSigningConfigsListFunctionEventInvokeConfigsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoGetEventSourceMappingReturns details about an event source mapping. You can get the identifier of a mapping from the output of
        ListEventSourceMappings.
      Request Syntax
      GET /2015-03-31/event-source-mappings/UUID HTTP/1.1

    
      URI Request Parameters
      The request uses the following URI parameters.
      
          
      
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Required: Yes
            
         
    
      Request Body
      The request does not have a request body.
    
      Response Syntax
      HTTP/1.1 200
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "EventSourceArn": "string",
   "EventSourceMappingArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FilterCriteriaError": { 
      "ErrorCode": "string",
      "Message": "string"
   },
   "FunctionArn": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "LastModified": number,
   "LastProcessingResult": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "State": "string",
   "StateTransitionReason": "string",
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number,
   "UUID": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 200 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
               Default value: Varies by service. For Amazon SQS, the default is 10. For all other services, the default is 100.
               Related setting: When you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.
               Type: Boolean
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  EventSourceMappingArn
               
            
            
               The Amazon Resource Name (ARN) of the event source mapping.
               Type: String
               Length Constraints: Minimum length of 85. Maximum length of 120.
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso([a-z]?)))?-[a-z]+-\d{1}:\d{12}:event-source-mapping:[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}
               
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               If filter criteria is encrypted, this field shows up as null in the response
      of ListEventSourceMapping API calls. You can view this field in plaintext in the response of
      GetEventSourceMapping and DeleteEventSourceMapping calls if you have
      kms:Decrypt permissions for the correct AWS KMS key.
               Type: FilterCriteria object
            
          
            
               
                  FilterCriteriaError
               
            
            
               An object that contains details about an error related to filter criteria encryption.
               Type: FilterCriteriaError object
            
          
            
               
                  FunctionArn
               
            
            
               The ARN of the Lambda function.
               Type: String
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\d{1}:\d{12}:function:[a-zA-Z0-9-_]+(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
            
          
            
               
                  LastModified
               
            
            
               The date that the event source mapping was last updated or that its state changed, in Unix time seconds.
               Type: Timestamp
            
          
            
               
                  LastProcessingResult
               
            
            
               The result of the last Lambda invocation of your function.
               Type: String
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For streams and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1,
which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records.
               NoteThe minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1,
which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
            
          
            
               
                  Queues
               
            
            
                (Amazon MQ) The name of the Amazon MQ broker destination queue to consume.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster for your event source.
               Type: SelfManagedEventSource object
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
            
          
            
               
                  State
               
            
            
               The state of the event source mapping. It can be one of the following: Creating,
        Enabling, Enabled, Disabling, Disabled,
        Updating, or Deleting.
               Type: String
            
          
            
               
                  StateTransitionReason
               
            
            
               Indicates whether a user or Lambda made the last change to the event source mapping.
               Type: String
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
            
          
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsGetCodeSigningConfigGetFunctionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoUpdateEventSourceMappingUpdates an event source mapping. You can change the function that AWS Lambda invokes, or pause
      invocation and resume later from the same location.For details about how to configure different event sources, see the following topics. 
       
       
       
       
       
       
       
   
         
            
            Amazon DynamoDB Streams
         
      
         
            
            Amazon Kinesis
         
      
         
            
            Amazon SQS
         
      
         
            
            Amazon MQ and RabbitMQ
         
      
         
            
            Amazon MSK
         
      
         
            
            Apache Kafka
         
      
         
            
            Amazon DocumentDB
         
      The following error handling options are available only for DynamoDB and Kinesis event sources:
       
       
       
       
   
         
            BisectBatchOnFunctionError – If the function returns an error, split the batch in two and retry.
      
         
            MaximumRecordAgeInSeconds – Discard records older than the specified age. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires
      
         
            MaximumRetryAttempts – Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
      
         
            ParallelizationFactor – Process multiple batches from each shard concurrently.
      For stream sources (DynamoDB, Kinesis, Amazon MSK, and self-managed Apache Kafka), the following option is also available:
       
   
         
            DestinationConfig – Send discarded records to an Amazon SQS queue, Amazon SNS topic, or 
            Amazon S3 bucket.
      For information about which configuration parameters apply to each event source, see the following topics.
       
       
       
       
       
       
       
   
         
            
          Amazon DynamoDB Streams
         
      
         
            
          Amazon Kinesis
         
      
         
            
          Amazon SQS
         
      
         
            
          Amazon MQ and RabbitMQ
         
      
         
            
          Amazon MSK
         
      
         
            
          Apache Kafka
         
      
         
            
          Amazon DocumentDB
         
      
      Request Syntax
      PUT /2015-03-31/event-source-mappings/UUID HTTP/1.1
Content-type: application/json

{
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "Enabled": boolean,
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FunctionName": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "TumblingWindowInSeconds": number
}
    
      URI Request Parameters
      The request uses the following URI parameters.
      
          
      
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Required: Yes
            
         
    
      Request Body
      The request accepts the following data in JSON format.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation
  (6 MB).
               
                   
                   
                   
                   
                   
                   
                   
               
                     
                        Amazon Kinesis – Default 100. Max 10,000.
                  
                     
                        Amazon DynamoDB Streams – Default 100. Max 10,000.
                  
                     
                        Amazon Simple Queue Service – Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.
                  
                     
                        Amazon Managed Streaming for Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Self-managed Apache Kafka – Default 100. Max 10,000.
                  
                     
                        Amazon MQ (ActiveMQ and RabbitMQ) – Default 100. Max 10,000.
                  
                     
                        DocumentDB – Default 100. Max 10,000.
                  
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
               Required: No
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry.
               Type: Boolean
               Required: No
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Kafka only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
               Required: No
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
               Required: No
            
          
            
               
                  Enabled
               
            
            
               When true, the event source mapping is active. When false, Lambda pauses polling and invocation.
               Default: True
               Type: Boolean
               Required: No
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               Type: FilterCriteria object
               Required: No
            
          
            
               
                  FunctionName
               
            
            
               The name or ARN of the Lambda function.
               
                  Name formats
                   
                   
                   
                   
               
                     
                        Function name – MyFunction.
                  
                     
                        Function ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction.
                  
                     
                        Version or Alias ARN – arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD.
                  
                     
                        Partial ARN – 123456789012:function:MyFunction.
                  
               The length constraint applies only to the full ARN. If you specify only the function name, it's limited to 64
      characters in length.
               Type: String
               Length Constraints: Minimum length of 1. Maximum length of 140.
               Pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\d{1}:)?(\d{12}:)?(function:)?([a-zA-Z0-9-_]+)(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
               Required: No
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
               Required: No
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
      By default, Lambda does not encrypt your filter criteria object. Specify this
      property to encrypt data using your own customer managed key.
    
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
               Required: No
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For Kinesis, DynamoDB, and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For Kinesis, DynamoDB, and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
               Required: No
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is infinite (-1).
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
               Required: No
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
               Required: No
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
               Required: No
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process from each shard concurrently.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
               Required: No
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
               Required: No
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
               Required: No
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of authentication protocols or VPC components required to secure your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
               Required: No
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
               Required: No
            
         
    
      Response Syntax
      HTTP/1.1 202
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "EventSourceArn": "string",
   "EventSourceMappingArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FilterCriteriaError": { 
      "ErrorCode": "string",
      "Message": "string"
   },
   "FunctionArn": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "LastModified": number,
   "LastProcessingResult": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "State": "string",
   "StateTransitionReason": "string",
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number,
   "UUID": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 202 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
               Default value: Varies by service. For Amazon SQS, the default is 10. For all other services, the default is 100.
               Related setting: When you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.
               Type: Boolean
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  EventSourceMappingArn
               
            
            
               The Amazon Resource Name (ARN) of the event source mapping.
               Type: String
               Length Constraints: Minimum length of 85. Maximum length of 120.
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso([a-z]?)))?-[a-z]+-\d{1}:\d{12}:event-source-mapping:[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}
               
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               If filter criteria is encrypted, this field shows up as null in the response
      of ListEventSourceMapping API calls. You can view this field in plaintext in the response of
      GetEventSourceMapping and DeleteEventSourceMapping calls if you have
      kms:Decrypt permissions for the correct AWS KMS key.
               Type: FilterCriteria object
            
          
            
               
                  FilterCriteriaError
               
            
            
               An object that contains details about an error related to filter criteria encryption.
               Type: FilterCriteriaError object
            
          
            
               
                  FunctionArn
               
            
            
               The ARN of the Lambda function.
               Type: String
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\d{1}:\d{12}:function:[a-zA-Z0-9-_]+(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
            
          
            
               
                  LastModified
               
            
            
               The date that the event source mapping was last updated or that its state changed, in Unix time seconds.
               Type: Timestamp
            
          
            
               
                  LastProcessingResult
               
            
            
               The result of the last Lambda invocation of your function.
               Type: String
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For streams and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1,
which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records.
               NoteThe minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1,
which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
            
          
            
               
                  Queues
               
            
            
                (Amazon MQ) The name of the Amazon MQ broker destination queue to consume.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster for your event source.
               Type: SelfManagedEventSource object
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
            
          
            
               
                  State
               
            
            
               The state of the event source mapping. It can be one of the following: Creating,
        Enabling, Enabled, Disabling, Disabled,
        Updating, or Deleting.
               Type: String
            
          
            
               
                  StateTransitionReason
               
            
            
               Indicates whether a user or Lambda made the last change to the event source mapping.
               Type: String
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
            
          
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceConflictException
               
            
            
               The resource already exists, or another operation is in progress.
               HTTP Status Code: 409
            
          
            
               
                  ResourceInUseException
               
            
            
               The operation conflicts with the resource's availability. For example, you tried to update an event source
      mapping in the CREATING state, or you tried to delete an event source mapping currently UPDATING.
               HTTP Status Code: 400
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsUpdateCodeSigningConfigUpdateFunctionCodeDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceRequest SyntaxURI Request ParametersRequest BodyResponse SyntaxResponse ElementsErrorsSee AlsoDeleteEventSourceMappingDeletes an event source
        mapping. You can get the identifier of a mapping from the output of ListEventSourceMappings.When you delete an event source mapping, it enters a Deleting state and might not be completely
      deleted for several seconds.
      Request Syntax
      DELETE /2015-03-31/event-source-mappings/UUID HTTP/1.1

    
      URI Request Parameters
      The request uses the following URI parameters.
      
          
      
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Required: Yes
            
         
    
      Request Body
      The request does not have a request body.
    
      Response Syntax
      HTTP/1.1 202
Content-type: application/json

{
   "AmazonManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "BatchSize": number,
   "BisectBatchOnFunctionError": boolean,
   "DestinationConfig": { 
      "OnFailure": { 
         "Destination": "string"
      },
      "OnSuccess": { 
         "Destination": "string"
      }
   },
   "DocumentDBEventSourceConfig": { 
      "CollectionName": "string",
      "DatabaseName": "string",
      "FullDocument": "string"
   },
   "EventSourceArn": "string",
   "EventSourceMappingArn": "string",
   "FilterCriteria": { 
      "Filters": [ 
         { 
            "Pattern": "string"
         }
      ]
   },
   "FilterCriteriaError": { 
      "ErrorCode": "string",
      "Message": "string"
   },
   "FunctionArn": "string",
   "FunctionResponseTypes": [ "string" ],
   "KMSKeyArn": "string",
   "LastModified": number,
   "LastProcessingResult": "string",
   "MaximumBatchingWindowInSeconds": number,
   "MaximumRecordAgeInSeconds": number,
   "MaximumRetryAttempts": number,
   "MetricsConfig": { 
      "Metrics": [ "string" ]
   },
   "ParallelizationFactor": number,
   "ProvisionedPollerConfig": { 
      "MaximumPollers": number,
      "MinimumPollers": number
   },
   "Queues": [ "string" ],
   "ScalingConfig": { 
      "MaximumConcurrency": number
   },
   "SelfManagedEventSource": { 
      "Endpoints": { 
         "string" : [ "string" ]
      }
   },
   "SelfManagedKafkaEventSourceConfig": { 
      "ConsumerGroupId": "string"
   },
   "SourceAccessConfigurations": [ 
      { 
         "Type": "string",
         "URI": "string"
      }
   ],
   "StartingPosition": "string",
   "StartingPositionTimestamp": number,
   "State": "string",
   "StateTransitionReason": "string",
   "Topics": [ "string" ],
   "TumblingWindowInSeconds": number,
   "UUID": "string"
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 202 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  AmazonManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for an Amazon Managed Streaming for Apache Kafka (Amazon MSK) event source.
               Type: AmazonManagedKafkaEventSourceConfig object
            
          
            
               
                  BatchSize
               
            
            
               The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
               Default value: Varies by service. For Amazon SQS, the default is 10. For all other services, the default is 100.
               Related setting: When you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10000.
            
          
            
               
                  BisectBatchOnFunctionError
               
            
            
               (Kinesis and DynamoDB Streams only) If the function returns an error, split the batch in two and retry. The default value is false.
               Type: Boolean
            
          
            
               
                  DestinationConfig
               
            
            
               (Kinesis, DynamoDB Streams, Amazon MSK, and self-managed Apache Kafka event sources only) A configuration object that specifies the destination of an event after Lambda processes it.
               Type: DestinationConfig object
            
          
            
               
                  DocumentDBEventSourceConfig
               
            
            
               Specific configuration settings for a DocumentDB event source.
               Type: DocumentDBEventSourceConfig object
            
          
            
               
                  EventSourceArn
               
            
            
               The Amazon Resource Name (ARN) of the event source.
               Type: String
               Pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\-])+:([a-z]{2}(-gov)?-[a-z]+-\d{1})?:(\d{12})?:(.*)
               
            
          
            
               
                  EventSourceMappingArn
               
            
            
               The Amazon Resource Name (ARN) of the event source mapping.
               Type: String
               Length Constraints: Minimum length of 85. Maximum length of 120.
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso([a-z]?)))?-[a-z]+-\d{1}:\d{12}:event-source-mapping:[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}
               
            
          
            
               
                  FilterCriteria
               
            
            
               An object that defines the filter criteria that
    determine whether Lambda should process an event. For more information, see Lambda event filtering.
               If filter criteria is encrypted, this field shows up as null in the response
      of ListEventSourceMapping API calls. You can view this field in plaintext in the response of
      GetEventSourceMapping and DeleteEventSourceMapping calls if you have
      kms:Decrypt permissions for the correct AWS KMS key.
               Type: FilterCriteria object
            
          
            
               
                  FilterCriteriaError
               
            
            
               An object that contains details about an error related to filter criteria encryption.
               Type: FilterCriteriaError object
            
          
            
               
                  FunctionArn
               
            
            
               The ARN of the Lambda function.
               Type: String
               Pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\d{1}:\d{12}:function:[a-zA-Z0-9-_]+(:(\$LATEST|[a-zA-Z0-9-_]+))?
               
            
          
            
               
                  FunctionResponseTypes
               
            
            
               (Kinesis, DynamoDB Streams, and Amazon SQS) A list of current response type enums applied to the event source mapping.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 1 item.
               Valid Values: ReportBatchItemFailures
               
            
          
            
               
                  KMSKeyArn
               
            
            
               
      The ARN of the AWS Key Management Service (AWS KMS) customer managed key that Lambda
      uses to encrypt your function's filter criteria.
               Type: String
               Pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()
               
            
          
            
               
                  LastModified
               
            
            
               The date that the event source mapping was last updated or that its state changed, in Unix time seconds.
               Type: Timestamp
            
          
            
               
                  LastProcessingResult
               
            
            
               The result of the last Lambda invocation of your function.
               Type: String
            
          
            
               
                  MaximumBatchingWindowInSeconds
               
            
            
               The maximum amount of time, in seconds, that Lambda spends gathering records before invoking the function.
  You can configure MaximumBatchingWindowInSeconds to any value from 0 seconds to 300 seconds in increments of seconds.
               For streams and Amazon SQS event sources, the default batching window is 0 seconds. For Amazon MSK, Self-managed Apache Kafka, Amazon MQ, and DocumentDB event sources, the default
  batching window is 500 ms. Note that because you can only change MaximumBatchingWindowInSeconds in increments of seconds, you cannot revert back to the 500 ms default batching window after you have changed it.
  To restore the default batching window, you must create a new event source mapping.
               Related setting: For streams and Amazon SQS event sources, when you set BatchSize to a value greater than 10, you must set MaximumBatchingWindowInSeconds to at least 1.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 300.
            
          
            
               
                  MaximumRecordAgeInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records older than the specified age. The default value is -1,
which sets the maximum age to infinite. When the value is set to infinite, Lambda never discards old records.
               NoteThe minimum valid value for maximum record age is 60s. Although values less than 60 and greater than -1 fall within the parameter's absolute range, they are not allowed
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 604800.
            
          
            
               
                  MaximumRetryAttempts
               
            
            
               (Kinesis and DynamoDB Streams only) Discard records after the specified number of retries. The default value is -1,
which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, Lambda retries failed records until the record expires in the event source.
               Type: Integer
               Valid Range: Minimum value of -1. Maximum value of 10000.
            
          
            
               
                  MetricsConfig
               
            
            
               The metrics configuration for your event source. For more information, see Event source mapping metrics.
               Type: EventSourceMappingMetricsConfig object
            
          
            
               
                  ParallelizationFactor
               
            
            
               (Kinesis and DynamoDB Streams only) The number of batches to process concurrently from each shard. The default value is 1.
               Type: Integer
               Valid Range: Minimum value of 1. Maximum value of 10.
            
          
            
               
                  ProvisionedPollerConfig
               
            
            
               (Amazon MSK and self-managed Apache Kafka only) The provisioned mode configuration for the event source.
  For more information, see provisioned mode.
               Type: ProvisionedPollerConfig object
            
          
            
               
                  Queues
               
            
            
                (Amazon MQ) The name of the Amazon MQ broker destination queue to consume.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 1000.
               Pattern: [ \S]*
               
            
          
            
               
                  ScalingConfig
               
            
            
               (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
               Type: ScalingConfig object
            
          
            
               
                  SelfManagedEventSource
               
            
            
               The self-managed Apache Kafka cluster for your event source.
               Type: SelfManagedEventSource object
            
          
            
               
                  SelfManagedKafkaEventSourceConfig
               
            
            
               Specific configuration settings for a self-managed Apache Kafka event source.
               Type: SelfManagedKafkaEventSourceConfig object
            
          
            
               
                  SourceAccessConfigurations
               
            
            
               An array of the authentication protocol, VPC components, or virtual host to secure and define your event source.
               Type: Array of SourceAccessConfiguration objects
               Array Members: Minimum number of 0 items. Maximum number of 22 items.
            
          
            
               
                  StartingPosition
               
            
            
               The position in a stream from which to start reading. Required for Amazon Kinesis and
      Amazon DynamoDB Stream event sources. AT_TIMESTAMP is supported only for
      Amazon Kinesis streams, Amazon DocumentDB, Amazon MSK, and self-managed Apache Kafka.
               Type: String
               Valid Values: TRIM_HORIZON | LATEST | AT_TIMESTAMP
               
            
          
            
               
                  StartingPositionTimestamp
               
            
            
               With StartingPosition set to AT_TIMESTAMP, the time from which to start
      reading, in Unix time seconds. StartingPositionTimestamp cannot be in the future.
               Type: Timestamp
            
          
            
               
                  State
               
            
            
               The state of the event source mapping. It can be one of the following: Creating,
        Enabling, Enabled, Disabling, Disabled,
        Updating, or Deleting.
               Type: String
            
          
            
               
                  StateTransitionReason
               
            
            
               Indicates whether a user or Lambda made the last change to the event source mapping.
               Type: String
            
          
            
               
                  Topics
               
            
            
               The name of the Kafka topic.
               Type: Array of strings
               Array Members: Fixed number of 1 item.
               Length Constraints: Minimum length of 1. Maximum length of 249.
               Pattern: ^[^.]([a-zA-Z0-9\-_.]+)
               
            
          
            
               
                  TumblingWindowInSeconds
               
            
            
               (Kinesis and DynamoDB Streams only) The duration in seconds of a processing window for DynamoDB and Kinesis Streams event sources. A value of 0 seconds indicates no tumbling window.
               Type: Integer
               Valid Range: Minimum value of 0. Maximum value of 900.
            
          
            
               
                  UUID
               
            
            
               The identifier of the event source mapping.
               Type: String
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
          
      
            
               
                  InvalidParameterValueException
               
            
            
               One of the parameters in the request is not valid.
               HTTP Status Code: 400
            
          
            
               
                  ResourceConflictException
               
            
            
               The resource already exists, or another operation is in progress.
               HTTP Status Code: 409
            
          
            
               
                  ResourceInUseException
               
            
            
               The operation conflicts with the resource's availability. For example, you tried to update an event source
      mapping in the CREATING state, or you tried to delete an event source mapping currently UPDATING.
               HTTP Status Code: 400
            
          
            
               
                  ResourceNotFoundException
               
            
            
               The resource specified in the request does not exist.
               HTTP Status Code: 404
            
          
            
               
                  ServiceException
               
            
            
               The AWS Lambda service encountered an internal error.
               HTTP Status Code: 500
            
          
            
               
                  TooManyRequestsException
               
            
            
               The request throughput limit was exceeded. For more information, see Lambda quotas.
               HTTP Status Code: 429
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsDeleteCodeSigningConfigDeleteFunctionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuidePrerequisitesCreate the execution role (Account A)Create the function (Account A)Test the function (Account A)Create an Amazon SQS queue (Account B)Configure the event source (Account A)Test the setupClean up your resourcesTutorial: Using a cross-account Amazon SQS queue as an event
      sourceIn this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a
    different AWS account. This tutorial involves two AWS accounts: Account A refers to the
    account that contains your Lambda function, and Account B refers to the account that contains
    the Amazon SQS queue.
    Prerequisites
    If you have not yet installed the AWS Command Line Interface, follow the steps at  Installing or updating the latest version of the AWS CLI 
          to install it.The tutorial requires a command line terminal or shell to run commands. In Linux and macOS, use your preferred shell and package manager.NoteIn Windows, some Bash CLI commands that you commonly use with Lambda (such as zip) are not supported by the operating system's built-in terminals. 
            To get a Windows-integrated version of Ubuntu and Bash, install the Windows Subsystem for Linux.
          
   
    Create the execution role (Account A)
    In Account A, create an execution role
      that gives your function permission to access the required AWS resources.
    To create an execution role
        Open the Roles page in the AWS Identity and Access Management (IAM)
          console.
      
        Choose Create role.
      
        Create a role with the following properties.
        
           
           
           
        
            Trusted entity –
              AWS Lambda
          
            Permissions –
              AWSLambdaSQSQueueExecutionRole
          
            Role name –
              cross-account-lambda-sqs-role
          
      
    The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to
      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs.
   
    Create the function (Account A)
    In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    The following Node.js 18 code example writes each message to a log in CloudWatch Logs.
    Example index.mjsexport const handler = async function(event, context) {
  event.Records.forEach(record => {
    const { body } = record;
    console.log(body);
  });
  return {};
}
    To create the functionNoteFollowing these steps creates a function in Node.js 18. For other languages, the steps are similar, but
          some details are different.
        Save the code example as a file named index.mjs.
      
        Create a deployment package.
        zip function.zip index.mjs
      
        Create the function using the create-function AWS Command Line Interface (AWS CLI) command.
        aws lambda create-function --function-name CrossAccountSQSExample \
--zip-file fileb://function.zip --handler index.handler --runtime nodejs18.x \
--role arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role
      
   
    Test the function (Account A)
    In Account A, test your Lambda function manually using the invoke AWS CLI
      command and a sample Amazon SQS event.
    If the handler returns normally without exceptions, Lambda considers the message to be successfully processed
      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes
      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully
      processed, and Lambda invokes the function with the same batch of messages.
    
        Save the following JSON as a file named input.txt.
        {
    "Records": [
        {
            "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
            "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
            "body": "test",
            "attributes": {
                "ApproximateReceiveCount": "1",
                "SentTimestamp": "1545082649183",
                "SenderId": "AIDAIENQZJOLO23YVJ4VO",
                "ApproximateFirstReceiveTimestamp": "1545082649185"
            },
            "messageAttributes": {},
            "md5OfBody": "098f6bcd4621d373cade4e832627b4f6",
            "eventSource": "aws:sqs",
            "eventSourceARN": "arn:aws:sqs:us-east-1:111122223333:example-queue",
            "awsRegion": "us-east-1"
        }
    ]
}
        The preceding JSON simulates an event that Amazon SQS might send to your Lambda function, where
            "body" contains the actual message from the queue.
      
        Run the following invoke AWS CLI command.
        aws lambda invoke --function-name CrossAccountSQSExample \
--cli-binary-format raw-in-base64-out \
--payload file://input.txt outputfile.txt
        The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.
      
        Verify the output in the file outputfile.txt.
      
   
    Create an Amazon SQS queue (Account B)
    In Account B, create an Amazon SQS queue that the Lambda function in Account
      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.
    To create a queue
        Open the Amazon SQS console.
      
        Choose Create queue.
      
        Create a queue with the following properties.
        
           
           
           
           
        
            Type – Standard
          
            Name – LambdaCrossAccountQueue
          
            Configuration – Keep the default settings.
          
            Access policy – Choose Advanced. Paste in the
              following JSON policy:
            {
   "Version": "2012-10-17",
   "Id": "Queue1_Policy_UUID",
   "Statement": [{
      "Sid":"Queue1_AllActions",
      "Effect": "Allow",
      "Principal": {
         "AWS": [
            "arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role"
         ]
      },
      "Action": "sqs:*",
      "Resource": "arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
    }
  ]
}
            This policy grants the Lambda execution role in Account A permissions to consume
              messages from this Amazon SQS queue.
          
      
        After creating the queue, record its Amazon Resource Name (ARN). You need this in the next step when you
          associate the queue with your Lambda function.
      
   
    Configure the event source (Account A)
    In Account A, create an event source mapping between the Amazon SQS queue in Account
        B and your Lambda function by running the following create-event-source-mapping AWS CLI
      command.
    aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
    To get a list of your event source mappings, run the following command.
    aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \
--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue
   
    Test the setup
    You can now test the setup as follows:
    
       
       
       
       
       
    
        In Account B, open the Amazon SQS console.
      
        Choose LambdaCrossAccountQueue, which you created earlier.
      
        Choose Send and receive messages.
      
        Under Message body, enter a test message.
      
        Choose Send message.
      
    Your Lambda function in Account A should receive the message. Lambda will continue to poll
      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the
      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console.
   
    Clean up your resources
    
    You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.
    In Account A, clean up your execution role and Lambda function.
    
    To delete the execution role
    Open the Roles page of the IAM console.
  
    Select the execution role that you created.
  
    Choose Delete.
  
    Enter the name of the role in the text input field and choose Delete.
  
    
    To delete the Lambda function
    Open the Functions page of the Lambda console.
  
    Select the function that you created.
  
    Choose Actions, Delete.
  
    Type confirm in the text input field and choose Delete.
  
    In Account B, clean up the Amazon SQS queue.
    
    To delete the Amazon SQS queue
    Sign in to the AWS Management Console and open the Amazon SQS console at
         https://console.aws.amazon.com/sqs/.
  
    Select the queue you created.
  
    Choose Delete.
  
    Enter confirm in the text input field.
  
    Choose Delete.
  
  Document ConventionsTutorialStep FunctionsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideCreating an execution role in the IAM consoleCreating and managing roles with the AWS CLIGrant least privilege access to your Lambda execution roleDefining Lambda function permissions with an execution roleA Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access
    AWS services and resources. For example, you might create an execution role that has permission to send logs to
    Amazon CloudWatch and upload trace data to AWS X-Ray. This page provides information on how to create, view, and manage a
    Lambda function's execution role.Lambda automatically assumes your execution role when you invoke your function. You should avoid manually
    calling sts:AssumeRole to assume the execution role in your function code. If your use case requires
    that the role assumes itself, you must include the role itself as a trusted principal in your role's trust policy.
    For more information on how to modify a role trust policy, see 
    Modifying a role trust policy (console) in the IAM User Guide.In order for Lambda to properly assume your execution role, the role's
    trust policy must specify the Lambda service principal
    (lambda.amazonaws.com) as a trusted service.TopicsCreating an execution role in the IAM consoleCreating and managing roles with the AWS CLIGrant least privilege access to your Lambda execution roleViewing and updating permissions in the execution roleWorking with AWS managed policies in the execution roleUsing source function ARN to control function access behavior
    Creating an execution role in the IAM console
    
    By default, Lambda creates an execution role with minimal permissions when you create a function in the Lambda console. Specifically,
      this execution role includes the AWSLambdaBasicExecutionRole managed policy, which gives your function
      basic permissions to log events to Amazon CloudWatch Logs.
    Your functions typically need additional permissions to perform more meaningful
      tasks. For example, you might have a Lambda function that responds to an event by updating
      entries in an Amazon DynamoDB database. You can create an execution role with the necessary
      permissions using the IAM console.
    To create an execution role in the IAM console
        Open the Roles page in the IAM console.
      
        Choose Create role.
      
        Under Trusted entity type, choose AWS service.
      
        Under Use case, choose Lambda.
      
        Choose Next.
      
        Select the AWS managed policies that you want to attach to your role.
          For example, if your function needs to access DynamoDB, select the
          AWSLambdaDynamoDBExecutionRole managed policy.
      
        Choose Next.
      
        Enter a Role name and then choose Create role.
      
    For detailed instructions, see Creating a role
      for an AWS service (console) in the IAM User Guide.
    After you create your execution role, attach it to your function. When you
      create a function in the Lambda console,
      you can attach any execution role that you previously created to the function. If you want
      to attach a new execution role to an existing function, follow the steps in Updating a function's execution role.
   
    Creating and managing roles with the AWS CLI
    To create an execution role with the AWS Command Line Interface (AWS CLI), use the create-role
      command. When using this command, you can specify the trust policy inline.
      A role's trust policy gives the specified principals permission to assume the role. In the
      following example, you grant the Lambda service principal permission to assume your role. Note that
      requirements for escaping quotes in the JSON string may vary depending on your shell.
    aws iam create-role \
  --role-name lambda-ex \
  --assume-role-policy-document '{"Version": "2012-10-17","Statement": [{ "Effect": "Allow", "Principal": {"Service": "lambda.amazonaws.com"}, "Action": "sts:AssumeRole"}]}'
    You can also define the trust policy for the role using a separate JSON file. In the following example,
      trust-policy.json is a file in the current directory.
    Example trust-policy.json{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}aws iam create-role \
  --role-name lambda-ex \
  --assume-role-policy-document file://trust-policy.json
    You should see the following output:
    {
    "Role": {
        "Path": "/",
        "RoleName": "lambda-ex",
        "RoleId": "AROAQFOXMPL6TZ6ITKWND",
        "Arn": "arn:aws:iam::123456789012:role/lambda-ex",
        "CreateDate": "2020-01-17T23:19:12Z",
        "AssumeRolePolicyDocument": {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Principal": {
                        "Service": "lambda.amazonaws.com"
                    },
                    "Action": "sts:AssumeRole"
                }
            ]
        }
    }
}
    To add permissions to the role, use the attach-policy-to-role command. The following
      command adds the AWSLambdaBasicExecutionRole managed policy to the lambda-ex
      execution role.
    aws iam attach-role-policy --role-name lambda-ex --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
    After you create your execution role, attach it to your function. When you
      create a function in the Lambda console,
      you can attach any execution role that you previously created to the function. If you want
      to attach a new execution role to an existing function, follow the steps in Updating a function's execution role.
   
    Grant least privilege access to your Lambda execution role
    When you first create an IAM role for your Lambda function during the development phase, you might sometimes
      grant permissions beyond what is required. Before publishing your function in the production environment, as a
      best practice, adjust the policy to include only the required permissions. For more information, see Apply least-privilege
        permissions in the IAM User Guide.
    Use IAM Access Analyzer to help identify the required permissions for the IAM execution role policy. IAM Access Analyzer
      reviews your AWS CloudTrail logs over the date range that you specify and generates a policy template with only the
      permissions that the function used during that time. You can use the template to create a managed policy with
      fine-grained permissions, and then attach it to the IAM role. That way, you grant only the permissions that the
      role needs to interact with AWS resources for your specific use case.
    For more information, see Generate policies based on access
      activity in the IAM User Guide.
  Document ConventionsLambda permissionsUpdate execution roleDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS Command Line InterfaceUser Guide for Version 2How to use command line optionsAWS CLI supported global command line
        optionsCommon uses of command line optionsCommand line options in the AWS CLIIn the AWS CLI, command line options are global parameters you can use to override the default
    configuration settings, any corresponding profile setting, or environment variable setting for
    that single command. You can't use command line options to directly specify credentials,
    although you can specify which profile to use. TopicsHow to use command line optionsAWS CLI supported global command line
        optionsCommon uses of command line options
    How to use command line options
    Most command line options are simple strings, such as the profile name
        profile1 in the following example:
    $ aws s3 ls --profile profile1
amzn-s3-demo-bucket1
amzn-s3-demo-bucket2
...
    Each option that takes an argument requires a space or equals sign (=) separating the
      argument from the option name. If the argument value is a string that contains a space, you
      must use quotation marks around the argument. For details on argument types and formatting for
      parameters, see Specifying parameter values in the AWS CLI.
   
    AWS CLI supported global command line
        options
    In the AWS CLI you can use the following command line options to override the default
      configuration settings, any corresponding profile setting, or environment variable setting for
      that single command. 
    
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
    
        --ca-bundle
            <string>
        
          Specifies the certificate authority (CA) certificate bundle to use when verifying
            SSL certificates. 
          If defined, this option overrides the value for the profile setting ca_bundle and the AWS_CA_BUNDLE environment
            variable.
        
      
        --cli-auto-prompt
        
          Enables auto-prompt mode for a single command. As the following examples show, you
            can specify it at any point.
          $ aws --cli-auto-prompt
$ aws dynamodb --cli-auto-prompt
$ aws dynamodb describe-table --cli-auto-prompt
          This option overrides the aws_cli_auto_prompt environment variable and the cli_auto_prompt profile
            setting.
          For information on the AWS CLI version 2 auto-prompt feature, see Enabling and using command prompts in the
            AWS CLI.
        
      
        --cli-binary-format
        
          Specifies how the AWS CLI version 2 interprets binary input parameters. It can be one of the
            following values:
          
             
             
          
              base64 – This is the default value. An
                input parameter that is typed as a binary large object (BLOB) accepts a
                base64-encoded string. To pass true binary content, put the content in a file and
                provide the file's path and name with the fileb:// prefix as the
                parameter's value. To pass base64-encoded text contained in a file, provide the
                file's path and name with the file:// prefix as the parameter's
                value.
            
              raw-in-base64-out – Default for the
                AWS CLI version 1. If the setting's value is raw-in-base64-out, files referenced
                using the file:// prefix is read as text and then the AWS CLI attempts to
                encode it to binary.
            
          This overrides the cli_binary_format file configuration setting.
          $ aws lambda invoke \
    --cli-binary-format raw-in-base64-out \
    --function-name my-function \
    --invocation-type Event \
    --payload '{ "name": "Bob" }' \
    response.json
          If you reference a binary value in a file using the fileb:// prefix
            notation, the AWS CLI always expects the file to
            contain raw binary content and does not attempt to convert the value. 
          If you reference a binary value in a file using the file:// prefix
            notation, the AWS CLI handles the file according to the current
              cli_binary_format setting. If that setting's value is base64
            (the default when not explicitly set), the AWS CLI expects the file to contain
            base64-encoded text. If that setting's value is raw-in-base64-out, the
            AWS CLI expects the file to contain raw binary content.
        
      
        --cli-connect-timeout <integer>
        
          Specifies the maximum socket connect time in seconds. If the value is set to zero
            (0), the socket connect waits indefinitely (is blocking) and doesn't timeout.
        
      
        --cli-read-timeout <integer>
        
          Specifies the maximum socket read time in seconds. If the value is set to zero (0)
            the socket read waits indefinitely (is blocking) and doesn't timeout.
        
      
        --color <string>
        
          Specifies support for color output. Valid values are on,
              off, and auto. The default value is
            auto.
        
      
        --debug
        
          A Boolean switch that enables debug logging. The AWS CLI by default provides cleaned
            up information regarding any successes or failures regarding command outcomes in the
            command output. The --debug option provides the full Python logs. This
            includes additional stderr diagnostic information about the operation of
            the command that can be useful when troubleshooting why a command provides unexpected
            results. To easily view debug logs, we suggest sending the logs to a file to more easily
            search the information. You can do this by using one of the following.
          To send only the stderr diagnostic
            information, append 2> debug.txt where debug.txt is
            the name you want to use for your debug file:
          $ aws servicename commandname options --debug 2> debug.txt
          To send both the output and stderr
            diagnostic information, append &> debug.txt where
              debug.txt is the name you want to use for your debug file:
          $ aws servicename commandname options --debug &> debug.txt
        
      
        --endpoint-url
            <string>
        
          Specifies the URL to send the request to. For most commands, the AWS CLI automatically
            determines the URL based on the selected service and the specified AWS Region.
            However, some commands require that you specify an account-specific URL. You can also
            configure some AWS services to host an endpoint
              directly within your private VPC, which might then need to be specified. 
          The following command example uses a custom Amazon S3 endpoint URL.
          $ aws s3 ls --endpoint-url http://localhost:4567
          
          
    Endpoint precedenceEndpoint configuration settings are located in multiple places, such as the system or
            user environment variables, local AWS configuration files, or explicitly declared on
            the command line as a parameter. The AWS CLI checks these endpoint settings in a
            particular order, and uses the endpoint setting with the highest precedence. For the
            endpoint precedence list, see Endpoint configuration and settings
                precedence.

        
      
        --no-cli-auto-prompt
        
          Disables auto-prompt mode for a single command.
          $ aws dynamodb describe-table --table-name Table1 --no-cli-auto-prompt
          This option overrides the aws_cli_auto_prompt environment variable and the cli_auto_prompt profile
            setting.
          For information on the AWS CLI version 2 auto-prompt feature, see Enabling and using command prompts in the
            AWS CLI.
        
      
        --no-cli-pager
        
          A Boolean switch that disables using a pager for the output of the command.
        
      
        --no-paginate
        
          A Boolean switch that disables the multiple calls the automatically AWS CLI makes to
            receive all command results that creates pagination of the output. This means only the
            first page of your output is displayed.
        
      
        --no-sign-request
        
          A Boolean switch that disables signing the HTTP requests to the AWS service
            endpoint. This prevents credentials from being loaded.
        
      
        --no-verify-ssl
        
          By default, the AWS CLI uses SSL when communicating with AWS services. For each SSL
            connection and call, the AWS CLI verifies the SSL certificates. Using this option
            overrides the default behavior of verifying SSL certificates. 
          WarningThis option is not best practice. If you use
                --no-verify-ssl, your traffic between your client and AWS services is
              no longer secured. This means your traffic is a security risk and vulnerable to
              man-in-the-middle exploits. If you're having issues with certificates, it's best to
              resolve those issues instead. For certificate troubleshooting steps, see SSL certificate errors. 
        
      
        --output <string>
        
          Specifies the output format to use for this command. You can specify any of the
            following values:
                
         
         
         
         
         
      
          json – The output is formatted as a JSON string.
              
        
    yaml – The output is formatted as a YAML string.
        
          yaml-stream – The output is streamed and formatted as a YAML string. Streaming allows for faster handling of large data types.
        
          text – The output is formatted as multiple lines of
            tab-separated string values. This can be useful to pass the output to a text
            processor, like grep, sed, or awk.
        
          table – The output is formatted as a table using the
            characters +|- to form the cell borders. It typically presents the information in a
            "human-friendly" format that is much easier to read than the others, but not as
            programmatically useful.
        

        
      
        --profile <string>
        
          Specifies the named
              profile to use for this command. To set up additional named profiles, you can
            use the aws configure command with the --profile
            option.
          $ aws configure --profile <profilename>
        
      
        --query <string>
        
          Specifies a JMESPath query to use in
            filtering the response data. For more information, see Filtering output in the AWS CLI.
        
      
        --region <string>
        
          Specifies which AWS Region to send this command's AWS request to. For a list of
            all of the Regions that you can specify, see AWS
              Regions and Endpoints in the Amazon Web Services General Reference.
        
      
        --version
        
          A Boolean switch that displays the current version of the AWS CLI program that is
            running.
        
      
   
    Common uses of command line options
    Common uses for command line options include checking your resources in multiple AWS
      Regions, and changing the output format for legibility or ease of use when scripting. In the
      following examples, we run the describe-instances command
      against each Region until we find which Region our instance is in. 
    $ aws ec2 describe-instances --output table --region us-west-1
-------------------
|DescribeInstances|
+-----------------+
$ aws ec2 describe-instances --output table --region us-west-2
------------------------------------------------------------------------------
|                              DescribeInstances                             |
+----------------------------------------------------------------------------+
||                               Reservations                               ||
|+-------------------------------------+------------------------------------+|
||  OwnerId                            |  012345678901                      ||
||  ReservationId                      |  r-abcdefgh                        ||
|+-------------------------------------+------------------------------------+|
|||                                Instances                               |||
||+------------------------+-----------------------------------------------+||
|||  AmiLaunchIndex        |  0                                            |||
|||  Architecture          |  x86_64                                       |||
...
  Document ConventionsEnvironment Variables Command completionDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideConfiguring a queue to use with LambdaSetting up Lambda execution role permissionsCreating an SQS event source mappingCreating and configuring an Amazon SQS event source mappingTo process Amazon SQS messages with Lambda, configure your queue with the appropriate settings,
        then create a Lambda event source mapping.
        Configuring a queue to use with Lambda
        If you don't already have an existing Amazon SQS queue, create one
            to serve as an event source for your Lambda function. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
        To allow your function time to process each batch of records, set the source queue's
            
            visibility timeout to at least six times the configuration
            timeout on your function. The extra time allows Lambda to retry if your function is throttled
            while processing a previous batch.
        By default, if Lambda encounters an error at any point while processing a batch, all
            messages in that batch return to the queue. After the 
            visibility timeout, the messages become visible to Lambda again. You can
            configure your event source mapping to use 
            partial batch responses to return only the failed messages back to the queue. In
            addition, if your function fails to process a message multiple times, Amazon SQS can send it to a
            
            dead-letter queue. We recommend setting the maxReceiveCount on your
            source queue's 
            redrive policy to at least 5. This gives Lambda a few chances to retry before
            sending failed messages directly to the dead-letter queue.
     
        Setting up Lambda execution role permissions
        The 
            AWSLambdaSQSQueueExecutionRole AWS managed policy includes the permissions that Lambda needs to read
            from your Amazon SQS queue. You can add this managed policy to your function's
            execution role.
        Optionally, if you're using an encrypted queue, you also need to add the following permission to your
            execution role:
        
             
        
                kms:Decrypt
            
     
        Creating an SQS event source mapping
        Create an event source mapping to tell Lambda to send items from your queue to a Lambda function.
            You can create multiple event source mappings to process items from multiple queues with a single
            function. When Lambda invokes the target function, the event can contain multiple items, up to a
            configurable maximum batch size.
        To configure your function to read from Amazon SQS, attach the 
            AWSLambdaSQSQueueExecutionRole AWS managed policy to your execution role.
            Then, create an SQS event source mapping from the console using
            the following steps.
        To add permissions and create a triggerOpen the Functions page of the Lambda console.
        Choose the name of a function.
      
        Choose the Configuration tab, and then choose Permissions.
      
        Under Role name, choose the link to your execution role. This link opens the role in the IAM console.
          
             
               
             
             
                  
      
        Choose Add permissions, and then choose Attach policies.
          
             
               
             
             
                    
      
        In the search field, enter AWSLambdaSQSQueueExecutionRole.
    Add this policy to your execution role. This is an AWS managed policy that contains the permissions
    your function needs to read from an Amazon SQS queue. For more information about this policy, see
    
    AWSLambdaSQSQueueExecutionRole in the AWS Managed Policy Reference.
      
        Go back to your function in the Lambda console. Under Function overview, choose Add trigger.
          
             
               
             
             
             
      
        Choose a trigger type.
      
        Configure the required options, and then choose Add.
      
        
            Lambda supports the following configuration options for Amazon SQS event sources:
             
             
             
             
             
             
        
                SQS queue
                
                    The Amazon SQS queue to read records from. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
                
            
                Enable trigger
                
                    The status of the event source mapping. Enable trigger is selected by default.
                
            
                Batch size
                
                    The maximum number of records to send to the function in each batch. For a standard queue,
                        this can be up to 10,000 records. For a FIFO queue, the maximum is 10. For a batch size
                        over 10, you must also set the batch window (MaximumBatchingWindowInSeconds)
                        to at least 1 second.
                    Configure your 
                        function timeout to allow enough time to process an entire batch of items. If items
                        take a long time to process, choose a smaller batch size. A large batch size can improve
                        efficiency for workloads that are very fast or have a lot of overhead. If you configure
                        reserved concurrency on your function, set
                        a minimum of five concurrent executions to reduce the chance of throttling errors when Lambda
                        invokes your function. 
                    Lambda passes all of the records in the batch to the function in a single call, as long as
                        the total size of the events doesn't exceed the 
                        invocation payload size quota for synchronous invocation (6 MB). Both Lambda and Amazon SQS
                        generate metadata for each record. This additional metadata is counted towards the total
                        payload size and can cause the total number of records sent in a batch to be lower than your
                        configured batch size. The metadata fields that Amazon SQS sends can be variable in length.
                        For more information about the Amazon SQS metadata fields, see the ReceiveMessage
                        API operation documentation in the Amazon Simple Queue Service API Reference.
                
            
                Batch window
                
                    The maximum amount of time to gather records before invoking the function, in seconds.
                        This applies only to standard queues.
                    If you're using a batch window greater than 0 seconds, you must account for the increased
                        processing time in your queue's
                        
                        visibility timeout. We recommend setting your queue's visibility timeout to six times your
                        function timeout, plus the value of
                        MaximumBatchingWindowInSeconds. This allows time for your Lambda function to process each
                        batch of events and to retry in the event of a throttling error.
                    When messages become available, Lambda starts processing messages in batches. Lambda starts
                        processing five batches at a time with five concurrent invocations of your function. If messages
                        are still available, Lambda adds up to 300 more instances of your function a minute, up to a
                        maximum of 1,000 function instances. To learn more about function scaling and concurrency,
                        see Lambda function scaling.
                    To process more messages, you can optimize your Lambda function for higher throughput.
                        For more information, see 
                        Understanding how AWS Lambda scales with Amazon SQS standard queues.
                
            
                Maximum concurrency
                
                    The maximum number of concurrent functions that the event source can invoke. For more information,
                        see Configuring maximum concurrency for Amazon SQS event sources.
                
            
                Filter criteria
                
                    Add filter criteria to control which events Lambda sends to your function for processing.
                        For more information, see Control which events Lambda sends to your function.
                
            
    Document ConventionsSQSScaling behaviorDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideConfiguring a queue to use with LambdaSetting up Lambda execution role permissionsCreating an SQS event source mappingCreating and configuring an Amazon SQS event source mappingTo process Amazon SQS messages with Lambda, configure your queue with the appropriate settings,
        then create a Lambda event source mapping.
        Configuring a queue to use with Lambda
        If you don't already have an existing Amazon SQS queue, create one
            to serve as an event source for your Lambda function. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
        To allow your function time to process each batch of records, set the source queue's
            
            visibility timeout to at least six times the configuration
            timeout on your function. The extra time allows Lambda to retry if your function is throttled
            while processing a previous batch.
        By default, if Lambda encounters an error at any point while processing a batch, all
            messages in that batch return to the queue. After the 
            visibility timeout, the messages become visible to Lambda again. You can
            configure your event source mapping to use 
            partial batch responses to return only the failed messages back to the queue. In
            addition, if your function fails to process a message multiple times, Amazon SQS can send it to a
            
            dead-letter queue. We recommend setting the maxReceiveCount on your
            source queue's 
            redrive policy to at least 5. This gives Lambda a few chances to retry before
            sending failed messages directly to the dead-letter queue.
     
        Setting up Lambda execution role permissions
        The 
            AWSLambdaSQSQueueExecutionRole AWS managed policy includes the permissions that Lambda needs to read
            from your Amazon SQS queue. You can add this managed policy to your function's
            execution role.
        Optionally, if you're using an encrypted queue, you also need to add the following permission to your
            execution role:
        
             
        
                kms:Decrypt
            
     
        Creating an SQS event source mapping
        Create an event source mapping to tell Lambda to send items from your queue to a Lambda function.
            You can create multiple event source mappings to process items from multiple queues with a single
            function. When Lambda invokes the target function, the event can contain multiple items, up to a
            configurable maximum batch size.
        To configure your function to read from Amazon SQS, attach the 
            AWSLambdaSQSQueueExecutionRole AWS managed policy to your execution role.
            Then, create an SQS event source mapping from the console using
            the following steps.
        To add permissions and create a triggerOpen the Functions page of the Lambda console.
        Choose the name of a function.
      
        Choose the Configuration tab, and then choose Permissions.
      
        Under Role name, choose the link to your execution role. This link opens the role in the IAM console.
          
             
               
             
             
                  
      
        Choose Add permissions, and then choose Attach policies.
          
             
               
             
             
                    
      
        In the search field, enter AWSLambdaSQSQueueExecutionRole.
    Add this policy to your execution role. This is an AWS managed policy that contains the permissions
    your function needs to read from an Amazon SQS queue. For more information about this policy, see
    
    AWSLambdaSQSQueueExecutionRole in the AWS Managed Policy Reference.
      
        Go back to your function in the Lambda console. Under Function overview, choose Add trigger.
          
             
               
             
             
             
      
        Choose a trigger type.
      
        Configure the required options, and then choose Add.
      
        
            Lambda supports the following configuration options for Amazon SQS event sources:
             
             
             
             
             
             
        
                SQS queue
                
                    The Amazon SQS queue to read records from. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.
                
            
                Enable trigger
                
                    The status of the event source mapping. Enable trigger is selected by default.
                
            
                Batch size
                
                    The maximum number of records to send to the function in each batch. For a standard queue,
                        this can be up to 10,000 records. For a FIFO queue, the maximum is 10. For a batch size
                        over 10, you must also set the batch window (MaximumBatchingWindowInSeconds)
                        to at least 1 second.
                    Configure your 
                        function timeout to allow enough time to process an entire batch of items. If items
                        take a long time to process, choose a smaller batch size. A large batch size can improve
                        efficiency for workloads that are very fast or have a lot of overhead. If you configure
                        reserved concurrency on your function, set
                        a minimum of five concurrent executions to reduce the chance of throttling errors when Lambda
                        invokes your function. 
                    Lambda passes all of the records in the batch to the function in a single call, as long as
                        the total size of the events doesn't exceed the 
                        invocation payload size quota for synchronous invocation (6 MB). Both Lambda and Amazon SQS
                        generate metadata for each record. This additional metadata is counted towards the total
                        payload size and can cause the total number of records sent in a batch to be lower than your
                        configured batch size. The metadata fields that Amazon SQS sends can be variable in length.
                        For more information about the Amazon SQS metadata fields, see the ReceiveMessage
                        API operation documentation in the Amazon Simple Queue Service API Reference.
                
            
                Batch window
                
                    The maximum amount of time to gather records before invoking the function, in seconds.
                        This applies only to standard queues.
                    If you're using a batch window greater than 0 seconds, you must account for the increased
                        processing time in your queue's
                        
                        visibility timeout. We recommend setting your queue's visibility timeout to six times your
                        function timeout, plus the value of
                        MaximumBatchingWindowInSeconds. This allows time for your Lambda function to process each
                        batch of events and to retry in the event of a throttling error.
                    When messages become available, Lambda starts processing messages in batches. Lambda starts
                        processing five batches at a time with five concurrent invocations of your function. If messages
                        are still available, Lambda adds up to 300 more instances of your function a minute, up to a
                        maximum of 1,000 function instances. To learn more about function scaling and concurrency,
                        see Lambda function scaling.
                    To process more messages, you can optimize your Lambda function for higher throughput.
                        For more information, see 
                        Understanding how AWS Lambda scales with Amazon SQS standard queues.
                
            
                Maximum concurrency
                
                    The maximum number of concurrent functions that the event source can invoke. For more information,
                        see Configuring maximum concurrency for Amazon SQS event sources.
                
            
                Filter criteria
                
                    Add filter criteria to control which events Lambda sends to your function for processing.
                        For more information, see Control which events Lambda sends to your function.
                
            
    Document ConventionsSQSScaling behaviorDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideWhen to increase timeoutUsing the consoleUsing the AWS CLIUsing AWS SAMConfigure Lambda function timeoutLambda runs your code for a set amount of time before timing out.
        Timeout is the maximum amount of time in seconds that a Lambda function can run. The 
        default value for this setting is 3 seconds, but you can adjust this in increments of 1 second up to a maximum value
        of 900 seconds (15 minutes).This page describes how and when to update the timeout setting for a Lambda function.SectionsDetermining the appropriate timeout value for a Lambda functionConfiguring timeout (console)Configuring timeout (AWS CLI)Configuring timeout (AWS SAM)
        Determining the appropriate timeout value for a Lambda function
        If the timeout value is close to the average duration of a function, there is a higher
            risk that the function will time out unexpectedly. The duration of a function can vary
            based on the amount of data transfer and processing, and the latency of any services the
            function interacts with. Some common causes of timeout include:
        
             
             
             
        
                Downloads from Amazon Simple Storage Service (Amazon S3) are larger or take longer than average.
            
                A function makes a request to another service, which takes longer to respond.
            
                The parameters provided to a function require more computational complexity in the function, which causes the invocation to take longer.
            
        When testing your application, ensure that your tests accurately reflect the size and quantity of data and realistic parameter values. Tests often use small samples for convenience, but you should use datasets at the upper bounds of what is reasonably expected for your workload.
     
        Configuring timeout (console)
        You can configure function timeout in the Lambda console.
        To modify the timeout for a functionOpen the Functions page of the Lambda console.
                Choose a function.
            
                Choose the Configuration tab and then choose General configuration.
                
                     
                        
                     
                     
                
            
                Under General configuration, choose Edit.
            
                For Timeout, set a value between 1 and
                    900 seconds (15 minutes).
            
                Choose Save.
            
     
        Configuring timeout (AWS CLI)
        You can use the  update-function-configuration command to configure the timeout value, in seconds. The following example command increases the function timeout to 120 seconds (2 minutes).
        aws lambda update-function-configuration \
  --function-name my-function \
  --timeout 120    
     
        Configuring timeout (AWS SAM)
        You can use the  AWS Serverless Application Model to configure the timeout value for your function. Update the Timeout property in your template.yaml file and then run sam deploy.
        Example template.yamlAWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: An AWS Serverless Application Model template describing your function.
Resources:
  my-function:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: .
      Description: ''
      MemorySize: 128
      Timeout: 120
      # Other function properties...    
    Document ConventionsInstruction sets (ARM/x86)Environment variablesDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideUsing policies for dead-letter
                queuesUnderstanding message
                retention periods for dead-letter queuesUsing dead-letter queues in Amazon SQS Amazon SQS supports dead-letter queues (DLQs), which source queues can target for messages that
        are not processed successfully. DLQs are useful for debugging your application because you
        can isolate unconsumed messages to determine why processing did not succeed. For optimal
        performance, it is a best practice to keep the source queue and DLQ within the same
        AWS account and Region. Once messages are in a dead-letter queue, you can:
         
         
         
         
    
            Examine logs for exceptions that might have caused messages to be moved to a
                dead-letter queue.
        
            Analyze the contents of messages moved to the dead-letter queue to diagnose
                application issues.
        
            Determine whether you have given your consumer sufficient time to process
                messages.
        
            Move messages out of the dead-letter queue using dead-letter queue
                redrive.
        You must first create a new queue before configuring it as a dead-letter queue. For
        information about configuring a dead-letter queue using the Amazon SQS console, see Configure a dead-letter queue using the
			Amazon SQS console. For help with dead-letter queues,
        such as how to configure an alarm for any messages moved to a dead-letter queue, see Creating alarms for dead-letter
            queues using Amazon CloudWatch.NoteDon't use a dead-letter queue with a FIFO queue if you don't want to break the exact
            order of messages or operations. For example, don't use a dead-letter queue with
            instructions in an Edit Decision List (EDL) for a video editing suite, where changing
            the order of edits changes the context of subsequent edits.
        Using policies for dead-letter
                queues
        Use a redrive policy to specify the
                maxReceiveCount. The maxReceiveCount is the number of
            times a consumer can receive a message from a source queue before it is moved to a
            dead-letter queue. For example, if the maxReceiveCount is set to a low
            value such as 1, one failure to receive a message would cause the message to move to the
            dead-letter queue. To ensure that your system is resilient against errors, set the
                maxReceiveCount high enough to allow for sufficient retries. 
        The redrive allow policy specifies which source
            queues can access the dead-letter queue. You can choose whether to allow all source
            queues, allow specific source queues, or deny all source queues use of the dead-letter
            queue. The default allows all source queues to use the dead-letter queue. If you choose
            to allow specific queues using the byQueue option, you can specify up to 10
            source queues using the source queue Amazon Resource Name (ARN). If you specify
                denyAll, the queue cannot be used as a dead-letter queue. 
     
        Understanding message
                retention periods for dead-letter queues
        For standard queues, the expiration of a message is always based on its original
            enqueue timestamp. When a message is moved to a dead-letter queue, the enqueue timestamp
            is unchanged. The ApproximateAgeOfOldestMessage metric indicates when the
            message moved to the dead-letter queue, not when the message was originally sent. For
            example, assume that a message spends 1 day in the original queue before it's moved to a
            dead-letter queue. If the dead-letter queue's retention period is 4 days, the message is
            deleted from the dead-letter queue after 3 days and the
                ApproximateAgeOfOldestMessage is 3 days. Thus, it is a best practice to
            always set the retention period of a dead-letter queue to be longer than the retention
            period of the original queue.
        For FIFO queues, the enqueue timestamp resets when the message is moved to a
            dead-letter queue. The ApproximateAgeOfOldestMessage metric indicates when
            the message moved to the dead-letter queue. In the same example above, the message is
            deleted from the dead-letter queue after four days and the
                ApproximateAgeOfOldestMessage is four days. 
    Document ConventionsFeatures and capabilitiesConfiguring a dead-letter queueDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideUsing policies for dead-letter
                queuesUnderstanding message
                retention periods for dead-letter queuesUsing dead-letter queues in Amazon SQS Amazon SQS supports dead-letter queues (DLQs), which source queues can target for messages that
        are not processed successfully. DLQs are useful for debugging your application because you
        can isolate unconsumed messages to determine why processing did not succeed. For optimal
        performance, it is a best practice to keep the source queue and DLQ within the same
        AWS account and Region. Once messages are in a dead-letter queue, you can:
         
         
         
         
    
            Examine logs for exceptions that might have caused messages to be moved to a
                dead-letter queue.
        
            Analyze the contents of messages moved to the dead-letter queue to diagnose
                application issues.
        
            Determine whether you have given your consumer sufficient time to process
                messages.
        
            Move messages out of the dead-letter queue using dead-letter queue
                redrive.
        You must first create a new queue before configuring it as a dead-letter queue. For
        information about configuring a dead-letter queue using the Amazon SQS console, see Configure a dead-letter queue using the
			Amazon SQS console. For help with dead-letter queues,
        such as how to configure an alarm for any messages moved to a dead-letter queue, see Creating alarms for dead-letter
            queues using Amazon CloudWatch.NoteDon't use a dead-letter queue with a FIFO queue if you don't want to break the exact
            order of messages or operations. For example, don't use a dead-letter queue with
            instructions in an Edit Decision List (EDL) for a video editing suite, where changing
            the order of edits changes the context of subsequent edits.
        Using policies for dead-letter
                queues
        Use a redrive policy to specify the
                maxReceiveCount. The maxReceiveCount is the number of
            times a consumer can receive a message from a source queue before it is moved to a
            dead-letter queue. For example, if the maxReceiveCount is set to a low
            value such as 1, one failure to receive a message would cause the message to move to the
            dead-letter queue. To ensure that your system is resilient against errors, set the
                maxReceiveCount high enough to allow for sufficient retries. 
        The redrive allow policy specifies which source
            queues can access the dead-letter queue. You can choose whether to allow all source
            queues, allow specific source queues, or deny all source queues use of the dead-letter
            queue. The default allows all source queues to use the dead-letter queue. If you choose
            to allow specific queues using the byQueue option, you can specify up to 10
            source queues using the source queue Amazon Resource Name (ARN). If you specify
                denyAll, the queue cannot be used as a dead-letter queue. 
     
        Understanding message
                retention periods for dead-letter queues
        For standard queues, the expiration of a message is always based on its original
            enqueue timestamp. When a message is moved to a dead-letter queue, the enqueue timestamp
            is unchanged. The ApproximateAgeOfOldestMessage metric indicates when the
            message moved to the dead-letter queue, not when the message was originally sent. For
            example, assume that a message spends 1 day in the original queue before it's moved to a
            dead-letter queue. If the dead-letter queue's retention period is 4 days, the message is
            deleted from the dead-letter queue after 3 days and the
                ApproximateAgeOfOldestMessage is 3 days. Thus, it is a best practice to
            always set the retention period of a dead-letter queue to be longer than the retention
            period of the original queue.
        For FIFO queues, the enqueue timestamp resets when the message is moved to a
            dead-letter queue. The ApproximateAgeOfOldestMessage metric indicates when
            the message moved to the dead-letter queue. In the same example above, the message is
            deleted from the dead-letter queue after four days and the
                ApproximateAgeOfOldestMessage is four days. 
    Document ConventionsFeatures and capabilitiesConfiguring a dead-letter queueDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS Managed PolicyReference GuideUsing this policyPolicy
                detailsPolicy versionJSON policy documentLearn moreAWSLambdaSQSQueueExecutionRoleDescription: Provides receive message, delete message, and read attribute access to SQS queues, and write permissions to CloudWatch logs.AWSLambdaSQSQueueExecutionRole is an AWS managed policy.
        Using this policy

                You can attach AWSLambdaSQSQueueExecutionRole to your users, groups, and roles.
     
        Policy
                details


        
             
             
             
             
        
                Type: Service role policy 
            
                Creation time: June 14, 2018, 21:50 UTC
                
            
                Edited time:  June 14, 2018, 21:50 UTC
            
                ARN:
                    arn:aws:iam::aws:policy/service-role/AWSLambdaSQSQueueExecutionRole
            
     
        Policy version
        Policy version: v1 (default)
        The policy's default version is the version that defines the permissions for the policy. When a user or role with the policy makes a
            request to access an AWS resource, AWS checks the default version of the policy to determine whether to allow the request. 
     
        JSON policy document
        {
  "Version" : "2012-10-17",
  "Statement" : [
    {
      "Effect" : "Allow",
      "Action" : [
        "sqs:ReceiveMessage",
        "sqs:DeleteMessage",
        "sqs:GetQueueAttributes",
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource" : "*"
    }
  ]
}
     
        Learn more
        
                 
                 
             
             
        
                    Create a permission set using AWS managed policies in IAM Identity Center
                    
                
                    Adding and removing IAM identity permissions
                    
                Understand versioning for IAM policies
            Get started with AWS managed policies and move toward least-privilege permissions
            
    Document ConventionsAWSLambdaRoleAWSLambdaVPCAccessExecutionRoleDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationKMSAPI ReferenceRequest SyntaxRequest ParametersResponse SyntaxResponse ElementsErrorsExamplesSee AlsoDecryptDecrypts ciphertext that was encrypted by a KMS key using any of the following
      operations:
       
       
       
       
       
   
         
            Encrypt
         
      
         
            GenerateDataKey
         
      
         
            GenerateDataKeyPair
         
      
         
            GenerateDataKeyWithoutPlaintext
         
      
         
            GenerateDataKeyPairWithoutPlaintext
         
      You can use this operation to decrypt ciphertext that was encrypted under a symmetric
      encryption KMS key or an asymmetric encryption KMS key. When the KMS key is asymmetric, you
      must specify the KMS key and the encryption algorithm that was used to encrypt the ciphertext.
      For information about asymmetric KMS keys, see Asymmetric KMS keys in the 
         AWS Key Management Service Developer Guide.The Decrypt operation also decrypts ciphertext that was encrypted outside of
      AWS KMS by the public key in an AWS KMS asymmetric KMS key. However, it cannot decrypt symmetric
      ciphertext produced by other libraries, such as the AWS Encryption SDK or Amazon S3 client-side encryption.
      These libraries return a ciphertext format that is incompatible with AWS KMS.If the ciphertext was encrypted under a symmetric encryption KMS key, the
        KeyId parameter is optional. AWS KMS can get this information from metadata that
      it adds to the symmetric ciphertext blob. This feature adds durability to your implementation
      by ensuring that authorized users can decrypt ciphertext decades after it was encrypted, even
      if they've lost track of the key ID. However, specifying the KMS key is always recommended as
      a best practice. When you use the KeyId parameter to specify a KMS key, AWS KMS
      only uses the KMS key you specify. If the ciphertext was encrypted under a different KMS key,
      the Decrypt operation fails. This practice ensures that you use the KMS key that
      you intend.Whenever possible, use key policies to give users permission to call the
        Decrypt operation on a particular KMS key, instead of using IAM policies.
      Otherwise, you might create an IAM policy that gives the user Decrypt
      permission on all KMS keys. This user could decrypt ciphertext that was encrypted by KMS keys
      in other accounts if the key policy for the cross-account KMS key permits it. If you must use
      an IAM policy for Decrypt permissions, limit the user to particular KMS keys or
      particular trusted accounts. For details, see Best practices for IAM
        policies in the 
         AWS Key Management Service Developer Guide.
      Decrypt also supports AWS Nitro Enclaves, which provide an
      isolated compute environment in Amazon EC2. To call Decrypt for a Nitro enclave, use
      the AWS Nitro Enclaves SDK or any AWS SDK. Use the Recipient parameter to provide the
      attestation document for the enclave. Instead of the plaintext data, the response includes the
      plaintext data encrypted with the public key from the attestation document
        (CiphertextForRecipient). For information about the interaction between AWS KMS and AWS Nitro Enclaves, see How AWS Nitro Enclaves uses AWS KMS in the 
         AWS Key Management Service Developer Guide.The KMS key that you use for this operation must be in a compatible key state. For
details, see Key states of AWS KMS keys in the 
         AWS Key Management Service Developer Guide.
      Cross-account use: Yes. If you use the KeyId
      parameter to identify a KMS key in a different AWS account, specify the key ARN or the alias
      ARN of the KMS key.
      Required permissions: kms:Decrypt (key policy)
      Related operations:
   
       
       
       
       
   
         
            Encrypt
         
      
         
            GenerateDataKey
         
      
         
            GenerateDataKeyPair
         
      
         
            ReEncrypt
         
      
      Eventual consistency: The AWS KMS API follows an eventual consistency model. 
  For more information, see AWS KMS eventual consistency.
      Request Syntax
      {
   "CiphertextBlob": blob,
   "DryRun": boolean,
   "EncryptionAlgorithm": "string",
   "EncryptionContext": { 
      "string" : "string" 
   },
   "GrantTokens": [ "string" ],
   "KeyId": "string",
   "Recipient": { 
      "AttestationDocument": blob,
      "KeyEncryptionAlgorithm": "string"
   }
}
    
      Request Parameters
      For information about the parameters that are common to all actions, see Common Parameters.
      The request accepts the following data in JSON format.
      NoteIn the following list, the required parameters are described first.
      
          
          
          
          
          
          
          
      
            
               
                  CiphertextBlob
               
            
            
               Ciphertext to be decrypted. The blob includes metadata.
               Type: Base64-encoded binary data object
               Length Constraints: Minimum length of 1. Maximum length of 6144.
               Required: Yes
            
          
            
               
                  DryRun
               
            
            
               Checks if your request will succeed. DryRun is an optional parameter. 
               To learn more about how to use this parameter, see Testing your permissions in the 
                     AWS Key Management Service Developer Guide.
               Type: Boolean
               Required: No
            
          
            
               
                  EncryptionAlgorithm
               
            
            
               Specifies the encryption algorithm that will be used to decrypt the ciphertext. Specify
      the same algorithm that was used to encrypt the data. If you specify a different algorithm,
      the Decrypt operation fails.
               This parameter is required only when the ciphertext was encrypted under an asymmetric KMS
      key. The default value, SYMMETRIC_DEFAULT, represents the only supported
      algorithm that is valid for symmetric encryption KMS keys.
               Type: String
               Valid Values: SYMMETRIC_DEFAULT | RSAES_OAEP_SHA_1 | RSAES_OAEP_SHA_256 | SM2PKE
               
               Required: No
            
          
            
               
                  EncryptionContext
               
            
            
               Specifies the encryption context to use when decrypting the data.
      An encryption context is valid only for cryptographic operations with a symmetric encryption KMS key. The standard asymmetric encryption algorithms and HMAC algorithms that AWS KMS uses do not support an encryption context.
               An encryption context is a collection of non-secret key-value pairs that represent additional authenticated data. 
When you use an encryption context to encrypt data, you must specify the same (an exact case-sensitive match) encryption context to decrypt the data. An encryption context is supported
only on operations with symmetric encryption KMS keys. On operations with symmetric encryption KMS keys, an encryption context is optional, but it is strongly recommended.
               For more information, see
Encryption context in the 
                     AWS Key Management Service Developer Guide.
               Type: String to string map
               Required: No
            
          
            
               
                  GrantTokens
               
            
            
               A list of grant tokens. 
               Use a grant token when your permission to call this operation comes from a new grant that has not yet achieved eventual consistency. For more information, see Grant token and Using a grant token in the
    
                     AWS Key Management Service Developer Guide.
               Type: Array of strings
               Array Members: Minimum number of 0 items. Maximum number of 10 items.
               Length Constraints: Minimum length of 1. Maximum length of 8192.
               Required: No
            
          
            
               
                  KeyId
               
            
            
               Specifies the KMS key that AWS KMS uses to decrypt the ciphertext.
               Enter a key ID of the KMS key that was used to encrypt the ciphertext. If you identify a
      different KMS key, the Decrypt operation throws an
        IncorrectKeyException.
               This parameter is required only when the ciphertext was encrypted under an asymmetric KMS
      key. If you used a symmetric encryption KMS key, AWS KMS can get the KMS key from metadata that
      it adds to the symmetric ciphertext blob. However, it is always recommended as a best
      practice. This practice ensures that you use the KMS key that you intend.
               To specify a KMS key, use its key ID, key ARN, alias name, or alias ARN. When using an alias name, prefix it with "alias/". To specify a KMS key in a different AWS account, you must use the key ARN or alias ARN.
               For example:
               
                   
                   
                   
                   
               
                     Key ID: 1234abcd-12ab-34cd-56ef-1234567890ab
                     
                  
                     Key ARN: arn:aws:kms:us-east-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab
                     
                  
                     Alias name: alias/ExampleAlias
                     
                  
                     Alias ARN: arn:aws:kms:us-east-2:111122223333:alias/ExampleAlias
                     
                  
               To get the key ID and key ARN for a KMS key, use ListKeys or DescribeKey. To get the alias name and alias ARN, use ListAliases.
               Type: String
               Length Constraints: Minimum length of 1. Maximum length of 2048.
               Required: No
            
          
            
               
                  Recipient
               
            
            
               A signed attestation
        document from an AWS Nitro enclave and the encryption algorithm to use with the
      enclave's public key. The only valid encryption algorithm is RSAES_OAEP_SHA_256. 
               This parameter only supports attestation documents for AWS Nitro Enclaves. To include this
      parameter, use the AWS Nitro Enclaves SDK or any AWS SDK.
               When you use this parameter, instead of returning the plaintext data, AWS KMS encrypts the
      plaintext data with the public key in the attestation document, and returns the resulting
      ciphertext in the CiphertextForRecipient field in the response. This ciphertext
      can be decrypted only with the private key in the enclave. The Plaintext field in
      the response is null or empty.
               For information about the interaction between AWS KMS and AWS Nitro Enclaves, see How AWS Nitro Enclaves uses AWS KMS in the 
                     AWS Key Management Service Developer Guide.
               Type: RecipientInfo object
               Required: No
            
         
    
      Response Syntax
      {
   "CiphertextForRecipient": blob,
   "EncryptionAlgorithm": "string",
   "KeyId": "string",
   "Plaintext": blob
}
    
      Response Elements
      If the action is successful, the service sends back an HTTP 200 response.
      The following data is returned in JSON format by the service.
      
          
          
          
          
      
            
               
                  CiphertextForRecipient
               
            
            
               The plaintext data encrypted with the public key in the attestation document. 
               This field is included in the response only when the Recipient parameter in
      the request includes a valid attestation document from an AWS Nitro enclave.
      For information about the interaction between AWS KMS and AWS Nitro Enclaves, see How AWS Nitro Enclaves uses AWS KMS in the 
                     AWS Key Management Service Developer Guide.
               Type: Base64-encoded binary data object
               Length Constraints: Minimum length of 1. Maximum length of 6144.
            
          
            
               
                  EncryptionAlgorithm
               
            
            
               The encryption algorithm that was used to decrypt the ciphertext.
               Type: String
               Valid Values: SYMMETRIC_DEFAULT | RSAES_OAEP_SHA_1 | RSAES_OAEP_SHA_256 | SM2PKE
               
            
          
            
               
                  KeyId
               
            
            
               The Amazon Resource Name (key ARN) of the KMS key that was used to decrypt the ciphertext.
               Type: String
               Length Constraints: Minimum length of 1. Maximum length of 2048.
            
          
            
               
                  Plaintext
               
            
            
               Decrypted plaintext data. When you use the HTTP API or the AWS CLI, the value is Base64-encoded. Otherwise, it is not Base64-encoded.
               If the response includes the CiphertextForRecipient field, the
        Plaintext field is null or empty.
               Type: Base64-encoded binary data object
               Length Constraints: Minimum length of 1. Maximum length of 4096.
            
         
    
      Errors
      For information about the errors that are common to all actions, see Common Errors.
      
          
          
          
          
          
          
          
          
          
          
          
      
            
               
                  DependencyTimeoutException
               
            
            
               The system timed out while trying to fulfill the request. You can retry the
      request.
               HTTP Status Code: 500
            
          
            
               
                  DisabledException
               
            
            
               The request was rejected because the specified KMS key is not enabled.
               HTTP Status Code: 400
            
          
            
               
                  DryRunOperationException
               
            
            
                The request was rejected because the DryRun parameter was specified. 
               HTTP Status Code: 400
            
          
            
               
                  IncorrectKeyException
               
            
            
               The request was rejected because the specified KMS key cannot decrypt the data. The
        KeyId in a Decrypt request and the SourceKeyId
      in a ReEncrypt request must identify the same KMS key that was used to
      encrypt the ciphertext.
               HTTP Status Code: 400
            
          
            
               
                  InvalidCiphertextException
               
            
            
               From the Decrypt or ReEncrypt operation, the request
      was rejected because the specified ciphertext, or additional authenticated data incorporated
      into the ciphertext, such as the encryption context, is corrupted, missing, or otherwise
      invalid.
               From the ImportKeyMaterial operation, the request was rejected because
      AWS KMS could not decrypt the encrypted (wrapped) key material. 
               HTTP Status Code: 400
            
          
            
               
                  InvalidGrantTokenException
               
            
            
               The request was rejected because the specified grant token is not valid.
               HTTP Status Code: 400
            
          
            
               
                  InvalidKeyUsageException
               
            
            
               The request was rejected for one of the following reasons: 
               
                   
                   
               
                     The KeyUsage value of the KMS key is incompatible with the API
          operation.
                  
                     The encryption algorithm or signing algorithm specified for the operation is
          incompatible with the type of key material in the KMS key (KeySpec).
                  
               For encrypting, decrypting, re-encrypting, and generating data keys, the
        KeyUsage must be ENCRYPT_DECRYPT. For signing and verifying
      messages, the KeyUsage must be SIGN_VERIFY. For generating and
      verifying message authentication codes (MACs), the KeyUsage must be
        GENERATE_VERIFY_MAC. For deriving key agreement secrets, the
        KeyUsage must be KEY_AGREEMENT. To find the KeyUsage
      of a KMS key, use the DescribeKey operation.
               To find the encryption or signing algorithms supported for a particular KMS key, use the
        DescribeKey operation.
               HTTP Status Code: 400
            
          
            
               
                  KeyUnavailableException
               
            
            
               The request was rejected because the specified KMS key was not available. You can retry
      the request.
               HTTP Status Code: 500
            
          
            
               
                  KMSInternalException
               
            
            
               The request was rejected because an internal exception occurred. The request can be
      retried.
               HTTP Status Code: 500
            
          
            
               
                  KMSInvalidStateException
               
            
            
               The request was rejected because the state of the specified resource is not valid for this
      request.
               This exceptions means one of the following:
               
                   
                   
               
                     The key state of the KMS key is not compatible with the operation. 
                     To find the key state, use the DescribeKey operation. For more
          information about which key states are compatible with each AWS KMS operation, see
          Key states of AWS KMS keys in the 
                           
                              AWS Key Management Service Developer Guide
                        .
                  
                     For cryptographic operations on KMS keys in custom key stores, this exception
          represents a general failure with many possible causes. To identify the cause, see the
          error message that accompanies the exception.
                  
               HTTP Status Code: 400
            
          
            
               
                  NotFoundException
               
            
            
               The request was rejected because the specified entity or resource could not be
      found.
               HTTP Status Code: 400
            
         
    
      Examples
      The following examples are formatted for
    legibility.
       
         Example Request
         This example illustrates one usage of Decrypt.
          
            POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
Content-Length: 293
X-Amz-Target: TrentService.Decrypt
X-Amz-Date: 20160517T204035Z
Content-Type: application/x-amz-json-1.1
Authorization: AWS4-HMAC-SHA256\
 Credential=AKIAI44QH8DHBEXAMPLE/20160517/us-west-2/kms/aws4_request,\
 SignedHeaders=content-type;host;x-amz-date;x-amz-target,\
 Signature=545b0c3bfd9223b8ef7e6293ef3ccac37a83d415ee3112d2e5c70727d2a49c46

{
  "KeyId": "arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab",  
  "CiphertextBlob": "CiDPoCH188S65r5Cy7pAhIFJMXDlU7mewhSlYUpuQIVBrhKmAQEBAgB4z6Ah9fPEuua+Qsu6QISBSTFw5VO5nsIUpWFKbkCFQa4AAAB9MHsGCSqGSIb3DQEHBqBuMGwCAQAwZwYJKoZIhvcNAQcBMB4GCWCGSAFlAwQBLjARBAxLc9b6QThC9jB/ZjYCARCAOt8la8qXLO5wB3JH2NlwWWzWRU2RKqpO9A/0psE5UWwkK6CnwoeC3Zj9Q0A66apZkbRglFfY1lTY+Tc="
}
          
       
       
         Example Response
         This example illustrates one usage of Decrypt.
          
            HTTP/1.1 200 OK
Server: Server
Date: Tue, 17 May 2016 20:40:40 GMT
Content-Type: application/x-amz-json-1.1
Content-Length: 146
Connection: keep-alive
x-amzn-RequestId: 9e02f41f-1c6f-11e6-af63-ab8791945da7

{
  "KeyId": "arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab",
  "Plaintext": "VGhpcyBpcyBEYXkgMSBmb3IgdGhlIEludGVybmV0Cg==",
  "EncryptionAlgorithm": "SYMMETRIC_DEFAULT" 
}
          
       
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
          
          
          
          
          
          
          
      
            
               AWS Command Line Interface
            
         
            
               AWS SDK for .NET
            
         
            
               AWS SDK for C++
            
         
            
               AWS SDK for Go v2
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for JavaScript V3
            
         
            
               AWS SDK for Kotlin
            
         
            
               AWS SDK for PHP V3
            
         
            
               AWS SDK for Python
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsCreateKeyDeleteAliasDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideConfiguring reserved concurrencyAccurately estimating required reserved concurrency for a functionConfiguring reserved concurrency for a functionIn Lambda, concurrency is the number of
    in-flight requests that your function is currently handling. There are two types
    of concurrency controls available:
     
     
  
      Reserved concurrency – This represents the maximum number of
        concurrent instances allocated to your function. When a function 
        has reserved concurrency, no other function can use that concurrency.
        Reserved concurrency is useful for ensuring that your most critical
        functions always have enough concurrency to handle incoming requests.
        Configuring reserved concurrency for a function incurs no additional charges.
    
      Provisioned concurrency – This is the number of pre-initialized
        execution environments allocated to your function. These execution environments
        are ready to respond immediately to incoming function requests. Provisioned
        concurrency is useful for reducing cold start latencies for functions. Configuring
        provisioned concurrency incurs additional charges to your AWS account.
    This topic details how to manage and configure reserved concurrency. For a conceptual
    overview of these two types of concurrency controls, see Reserved concurrency
    and provisioned concurrency. For information on configuring provisioned concurrency,
    see Configuring provisioned concurrency for a function.NoteLambda functions linked to an Amazon MQ event source mapping have a default maximum concurrency. For Apache Active MQ, 
the maximum number of concurrent instances is 5. For Rabbit MQ, the maximum number of concurrent instances is 1. Setting reserved or provisioned concurrency for your 
function doesn't change these limits. To request an increase in the default maximum concurrency when using Amazon MQ, contact Support.SectionsConfiguring reserved concurrencyAccurately estimating required reserved concurrency for a function
    Configuring reserved concurrency
    
    You can configure reserved concurrency settings for a function using the Lambda
      console or the Lambda API.
    To reserve concurrency for a function (console)Open the Functions page of the Lambda console.
        Choose the function you want to reserve concurrency for.
      
		    Choose Configuration and then choose
		      Concurrency.
	    
        Under Concurrency, choose Edit. 
      
        Choose Reserve concurrency. Enter the amount of concurrency
          to reserve for the function.
      
        Choose Save.
      
    You can reserve up to the Unreserved account concurrency value
      minus 100. The remaining 100 units of concurrency are for functions that aren't using
      reserved concurrency. For example, if your account has a concurrency limit of 1,000,
      you cannot reserve all 1,000 units of concurrency to a single function.
    
       
        
       
       
    
    Reserving concurrency for a function impacts the concurrency pool that's
      available to other functions. For example, if you reserve 100 units of concurrency
      for function-a, other functions in your account must share the
      remaining 900 units of concurrency, even if function-a doesn't use
      all 100 reserved concurrency units.
    To intentionally throttle a function, set its reserved concurrency to 0. This
      stops your function from processing any events until you remove the limit.
    To configure reserved concurrency with the Lambda API, use the following API
      operations.
    
       
       
       
    
        PutFunctionConcurrency
      
        GetFunctionConcurrency
      
        DeleteFunctionConcurrency
      
    For example, to configure reserved concurrency with the AWS Command Line Interface (CLI), use
      the put-function-concurrency command. The following command reserves
      100 concurrency units for a function named my-function:
    aws lambda put-function-concurrency --function-name my-function \
    --reserved-concurrent-executions 100
    You should see output that looks like the following:
    {
    "ReservedConcurrentExecutions": 100
}
   
    Accurately estimating required reserved concurrency for a function
    If your function is currently serving traffic, you can easily view its concurrency
      metrics using CloudWatch metrics.
      Specifically, the ConcurrentExecutions metric shows you the number of
      concurrent invocations for each function in your account.
    
       
        
       
       
    
    The previous graph suggests that this function serves an average of 5 to 10
      concurrent requests at any given time, and peaks at 20 requests on a typical day.
      Suppose that there are many other functions in your account. 
      If this function is critical to your application and you don't want to drop any
      requests, use a number greater than or equal to 20 as your reserved
      concurrency setting.
    Alternatively, recall that you can also 
      calculate concurrency using the following formula:
    Concurrency = (average requests per second) * (average request duration in seconds)
    Multiplying average requests per second with the average request duration in
      seconds gives you a rough estimate of how much concurrency you need to reserve.
      You can estimate average requests per second using the Invocation
      metric, and the average request duration in seconds using the Duration
      metric. See Using CloudWatch metrics with Lambda
      for more details.
    You should also be familiar with your upstream and downstream throughput constraints.
      While Lambda functions scale seamlessly with load, upstream and downstream dependencies may not have the
      same throughput capabilities. If you need to limit how high your function can scale, configure reserved 
      concurrency on your function.
  Document ConventionsFunction scalingConfiguring provisioned concurrencyDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideMaximum concurrencyConfiguring scaling behavior for SQS event source mappingsFor standard queues, Lambda uses  long polling to poll a queue until it becomes active. When messages are
        available, Lambda starts processing five batches at a time with five concurrent invocations
        of your function. If messages are still available, Lambda increases the number of processes that are reading batches by up to 300 more instances per minute. The maximum number of batches that an event source mapping can process simultaneously is 1,000. When traffic is low, Lambda scales back the processing to
        five concurrent batches, and can optimize to as few as 2 concurrent batches to reduce the
        SQS calls and corresponding costs. However, this optimization is not available when you
        enable the maximum concurrency setting.For FIFO queues, Lambda sends messages to your function in the order that it receives them. When you send a
        message to a FIFO queue, you specify a message group
            ID. Amazon SQS ensures that messages in the same group are delivered to Lambda in order. When Lambda reads 
        your messages into batches, each batch may contain messages from more than one message group, but the order 
        of the messages is maintained. If your function returns an error, the function attempts all retries on the 
        affected messages before Lambda receives additional messages from the same
        group.
        Configuring maximum concurrency for Amazon SQS event sources
        You can use the maximum concurrency setting to control scaling behavior for your SQS event sources.
            The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS
            event source can invoke. Maximum concurrency is an event source-level setting. If you have multiple Amazon SQS
            event sources mapped to one function, each event source can have a separate maximum concurrency setting.
            You can use maximum concurrency to prevent one queue from using all of the function's
            reserved concurrency or the rest of the
            account's concurrency quota. There is no charge for
            configuring maximum concurrency on an Amazon SQS event source.
        Importantly, maximum concurrency and reserved concurrency are two independent settings. Don't set
            maximum concurrency higher than the function's reserved concurrency. If you configure maximum concurrency,
            make sure that your function's reserved concurrency is greater than or equal to the total maximum
            concurrency for all Amazon SQS event sources on the function. Otherwise, Lambda may throttle your messages.
        When your account's concurrency quota is set to the default value of 1,000, an Amazon SQS event source mapping can scale 
            to invoke function instances up to this value, unless you specify a maximum concurrency.
        If you receive an increase to your account's default concurrency quota, Lambda may not be able to invoke concurrent functions 
            instances up to your new quota. By default, Lambda can scale to invoke up to 1,250 concurrent function instances 
            for an Amazon SQS event source mapping. If this is insufficient for your use case, contact AWS support to 
            discuss an increase to your account's Amazon SQS event source mapping concurrency.
        NoteFor FIFO queues, concurrent invocations are capped either by the number of
                message group IDs
                (messageGroupId) or the maximum concurrency setting—whichever is lower. For example,
                if you have six message group IDs and maximum concurrency is set to 10, your function can have a maximum
                of six concurrent invocations.
        You can configure maximum concurrency on new and existing Amazon SQS event source mappings.
        Configure maximum concurrency using the Lambda consoleOpen the Functions page of the Lambda console.
                Choose the name of a function.
            
                Under Function overview, choose SQS. This opens the Configuration tab.
            
                Select the Amazon SQS trigger and choose Edit.
            
                For Maximum concurrency, enter a number between 2 and 1,000. To turn off maximum concurrency, leave the box empty.
            
                Choose Save.
            
         
            Configure maximum concurrency using the AWS Command Line Interface (AWS CLI)
            Use the update-event-source-mapping command with the --scaling-config option. Example:
         
        aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --scaling-config '{"MaximumConcurrency":5}'
        To turn off maximum concurrency, enter an empty value for --scaling-config:
        aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --scaling-config "{}"
         
            Configure maximum concurrency using the Lambda API
            Use the CreateEventSourceMapping or UpdateEventSourceMapping action with a ScalingConfig object.
         
    Document ConventionsCreate mappingError handlingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideUnderstanding event filtering basicsHandling records that don't meet filter criteriaFilter rule syntaxAttaching filter criteria to an event source mapping (console)Attaching filter criteria to an event source mapping (AWS CLI)Attaching filter criteria to an event source mapping (AWS SAM)Encryption of filter criteriaUsing filters with different AWS servicesControl which events Lambda sends to your functionYou can use event filtering to control which records from a stream or queue Lambda sends to your function.
    For example, you can add a filter so that your function only processes Amazon SQS messages containing certain
    data parameters. Event filtering works only with certain event source mappings. You can add filters to
    event source mappings for the following AWS services:
     
     
     
     
     
     
  
      Amazon DynamoDB
    
      Amazon Kinesis Data Streams
    
      Amazon MQ
    
      Amazon Managed Streaming for Apache Kafka (Amazon MSK)
    
      Self-managed Apache Kafka
    
      Amazon Simple Queue Service (Amazon SQS)
    For specific information about filtering with specific event sources, see
    Using filters with different AWS services. Lambda doesn't support event filtering for Amazon DocumentDB.By default, you can define up to five different filters for a single event source mapping.
    Your filters are logically ORed together. If a record from your event source satisfies one or more
    of your filters, Lambda includes the record in the next event it sends to your function. If none of
    your filters are satisfied, Lambda discards the record.NoteIf you need to define more than five filters for an event source, you can request a quota
      increase for up to 10 filters for each event source. If you attempt to add more filters than
      your current quota permits, Lambda will return an error when you try to create the event source.TopicsUnderstanding event filtering basicsHandling records that don't meet filter criteriaFilter rule syntaxAttaching filter criteria to an event source mapping (console)Attaching filter criteria to an event source mapping (AWS CLI)Attaching filter criteria to an event source mapping (AWS SAM)Encryption of filter criteriaUsing filters with different AWS services
    Understanding event filtering basics
    A filter criteria (FilterCriteria) object is a structure that consists of a
      list of filters (Filters). Each filter is a structure that defines an event
      filtering pattern (Pattern). A pattern is a string representation of a JSON filter
      rule. The structure of a FilterCriteria object is as follows.
    {
   "Filters": [
        {
            "Pattern": "{ \"Metadata1\": [ rule1 ], \"data\": { \"Data1\": [ rule2 ] }}"
        }
    ]
}
    For added clarity, here is the value of the filter's Pattern expanded
      in plain JSON.
    {
    "Metadata1": [ rule1 ],
    "data": {
        "Data1": [ rule2 ]
    }
}
    Your filter pattern can include metadata properties, data properties, or both. The
      available metadata parameters and the format of the data parameters vary according to the
      AWS service which is acting as the event source. For example, suppose your event source
      mapping receives the following record from an Amazon SQS queue:
  {
    "messageId": "059f36b4-87a3-44ab-83d2-661975830a7d",
    "receiptHandle": "AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...",
    "body": "{\n "City": "Seattle",\n "State": "WA",\n "Temperature": "46"\n}",
    "attributes": {
        "ApproximateReceiveCount": "1",
        "SentTimestamp": "1545082649183",
        "SenderId": "AIDAIENQZJOLO23YVJ4VO",
        "ApproximateFirstReceiveTimestamp": "1545082649185"
    },
    "messageAttributes": {},
    "md5OfBody": "e4e68fb7bd0e697a0ae8f1bb342846b3",
    "eventSource": "aws:sqs",
    "eventSourceARN": "arn:aws:sqs:us-east-2:123456789012:my-queue",
    "awsRegion": "us-east-2"
}  
 
    
    
 
     Metadata properties are the fields containing information
       about the event that created the record. In the example Amazon SQS record, the metadata properties
       include fields such as messageID, eventSourceArn, and
       awsRegion.
   
     Data properties are the fields of the record containing
       the data from your stream or queue. In the Amazon SQS event example, the key for the data field is
       body, and the data properties are the fields City State,
       and Temperature.
   
    Different types of event source use different key values for their data fields. To filter
      on data properties, make sure that you use the correct key in your filter’s pattern. For a list
      of data filtering keys, and to see examples of filter patterns for each supported AWS service, 
      refer to Using filters with different AWS services.
    Event filtering can handle multi-level JSON filtering. For example, consider the following
      fragment of a record from a DynamoDB stream:
    "dynamodb": {
    "Keys": {
        "ID": {
            "S": "ABCD"
        }
        "Number": {
            "N": "1234"
    },
    ...
}
    Suppose you want to process only those records where the value of the sort key
      Number is 4567. In this case, your FilterCriteria object would look
      like this:
    {
    "Filters": [
        {
            "Pattern": "{ \"dynamodb\": { \"Keys\": { \"Number\": { \"N\": [ "4567" ] } } } }"
        }
    ]
}
    For added clarity, here is the value of the filter's Pattern expanded
      in plain JSON. 
    {
    "dynamodb": {
        "Keys": {
            "Number": {
                "N": [ "4567" ]
                }
            }
        }
}
   
      Handling records that don't meet filter criteria
      How Lambda handles records that don't meet your filter criteria depends on the event source.
      
         
         
         
      For Amazon SQS, if a message doesn't satisfy your filter
          criteria, Lambda automatically removes the message from the queue. You don't have to manually
          delete these messages in Amazon SQS.
        
          For Kinesis and DynamoDB, after
            your filter criteria evaluates a record, the streams iterator advances past this record. If the
            record doesn't satisfy your filter criteria, you don't have to manually delete the record from
            your event source. After the retention period, Kinesis and DynamoDB automatically delete these old
            records. If you want records to be deleted sooner, see Changing the Data Retention Period.
        
          For Amazon MSK, self-managed Apache Kafka, and
            Amazon MQ messages, Lambda drops messages that don't match all fields
            included in the filter. For Amazon MSK and self-managed Apache Kafka, Lambda commits offsets for matched and unmatched
            messages after successfully invoking the function. For Amazon MQ, Lambda acknowledges matched messages
            after successfully invoking the function, and acknowledges unmatched messages when filtering them.
        
     
    Filter rule syntax
    For filter rules, Lambda supports the Amazon EventBridge rules and uses the same syntax as EventBridge. For more
      information, see 
      Amazon EventBridge event patterns in the Amazon EventBridge User Guide.
    The following is a summary of all the comparison operators available for Lambda event filtering.
    
          
            Comparison operator
            Example
            Rule syntax
          
        
          
            
              Null
            
            
              UserID is null
            
            
              "UserID": [ null ]
            
          
          
            
              Empty
            
            
              LastName is empty
            
            
              "LastName": [""]
            
          
          
            
              Equals
            
            
              Name is "Alice"
            
            
              "Name": [ "Alice" ]
            
          
          
            
              Equals (ignore case)
            
            
              Name is "Alice"
            
            
              "Name": [ { "equals-ignore-case": "alice" } ]
            
          
          
            
              And
            
            
              Location is "New York" and Day is "Monday"
            
            
              "Location": [ "New York" ], "Day": ["Monday"]
            
          
          
            
              Or
            
            
              PaymentType is "Credit" or "Debit"
            
            
              "PaymentType": [ "Credit", "Debit"]
            
          
          
            
              Or (multiple fields)
            
            
              Location is "New York", or Day is "Monday".
            
            
              "$or": [ { "Location": [ "New York" ] }, { "Day": [ "Monday" ] } ] 
            
          
          
            
              Not
            
            
              Weather is anything but "Raining"
            
            
              "Weather": [ { "anything-but": [ "Raining" ] } ]
            
          
          
            
              Numeric (equals)
            
            
              Price is 100
            
            
              "Price": [ { "numeric": [ "=", 100 ] } ]
            
          
          
            
              Numeric (range)
            
            
              Price is more than 10, and less than or equal to 20
            
            
              "Price": [ { "numeric": [ ">", 10, "<=", 20 ] } ]
            
          
          
            
              Exists
            
            
              ProductName exists
            
            
              "ProductName": [ { "exists": true } ]
            
          
          
            
              Does not exist
            
            
              ProductName does not exist
            
            
              "ProductName": [ { "exists": false } ]
            
          
          
            
              Begins with
            
            
              Region is in the US
            
            
              "Region": [ {"prefix": "us-" } ]
            
          
          
            
              Ends with
            
            
              FileName ends with a .png extension.
            
            
              "FileName": [ { "suffix": ".png" } ] 
            
          
        
    NoteLike EventBridge, for strings, Lambda uses exact character-by-character matching without case-folding
        or any other string normalization. For numbers, Lambda also uses string representation. For example,
        300, 300.0, and 3.0e2 are not considered equal.
      Note that the Exists operator only works on leaf nodes in your event source JSON. It doesn't match
        intermediate nodes. For example, with the following JSON, the filter pattern
        { "person": { "address": [ { "exists": true } ] } }" wouldn't find a match because
        "address" is an intermediate node.
    {
  "person": {
    "name": "John Doe",
    "age": 30,
    "address": {
      "street": "123 Main St",
      "city": "Anytown",
      "country": "USA"
    }
  }
}
   
    Attaching filter criteria to an event source mapping (console)
    Follow these steps to create a new event source mapping with filter criteria using the Lambda console.
    To create a new event source mapping with filter criteria (console)
        Open the Functions page of the Lambda
          console.
      
        Choose the name of a function to create an event source mapping for.
      
        Under Function overview, choose Add trigger.
      
        For Trigger configuration, choose a trigger type that supports event filtering.
          For a list of supported services, refer to the list at the beginning of this page.
      
        Expand Additional settings.
      
        Under Filter criteria, choose Add, and then define and
          enter your filters. For example, you can enter the following.
        { "Metadata" : [ 1, 2 ] }
        This instructs Lambda to process only the records where field Metadata is equal to
          1 or 2. You can continue to select Add to add more filters up to the maximum
          allowed amount.
      
        When you have finished adding your filters, choose Save.
      
    When you enter filter criteria using the console, you enter only the filter pattern and don't need
      to provide the Pattern key or escape quotes. In step 6 of the preceding instructions,
      { "Metadata" : [ 1, 2 ] } corresponds to the following FilterCriteria.
    {
   "Filters": [
      {
          "Pattern": "{ \"Metadata\" : [ 1, 2 ] }"
      }
   ]
}
    After creating your event source mapping in the console, you can see the formatted
      FilterCriteria in the trigger details. For more examples of creating event filters using
      the console, see Using filters with different AWS services.
   
    Attaching filter criteria to an event source mapping (AWS CLI)
    Suppose you want an event source mapping to have the following FilterCriteria:
    {
   "Filters": [
      {
          "Pattern": "{ \"Metadata\" : [ 1, 2 ] }"
      }
   ]
}
    To create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run
      the following command.
    aws lambda create-event-source-mapping \
    --function-name my-function \
    --event-source-arn arn:aws:sqs:us-east-2:123456789012:my-queue \
    --filter-criteria '{"Filters": [{"Pattern": "{ \"Metadata\" : [ 1, 2 ]}"}]}'
    This 
      create-event-source-mapping command creates a new Amazon SQS event source mapping for function
      my-function with the specified FilterCriteria.
    To add these filter criteria to an existing event source mapping, run the following command.
    aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --filter-criteria '{"Filters": [{"Pattern": "{ \"Metadata\" : [ 1, 2 ]}"}]}'
    Note that to update an event source mapping, you need its UUID. You can get the UUID from a
      
      list-event-source-mappings call. Lambda also returns the UUID in the
      
      create-event-source-mapping CLI response.
    To remove filter criteria from an event source, you can run the following
      
      update-event-source-mapping command with an empty FilterCriteria object.
    aws lambda update-event-source-mapping \
    --uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
    --filter-criteria "{}"
    For more examples of creating event filters using the AWS CLI, see
      Using filters with different AWS services.
   
    Attaching filter criteria to an event source mapping (AWS SAM)
    
      Suppose you want to configure an event source in AWS SAM to use the following filter criteria:
    
    {
   "Filters": [
      {
          "Pattern": "{ \"Metadata\" : [ 1, 2 ] }"
      }
   ]
}
    
      To add these filter criteria to your event source mapping, insert the following snippet into the YAML
      template for your event source.
    FilterCriteria: 
  Filters: 
    - Pattern: '{"Metadata": [1, 2]}'
    
      For more information on creating and configuring an AWS SAM template for an event source mapping, see
      the 
      EventSource section of the AWS SAM Developer Guide. Fore more examples of creating event filters
      using AWS SAM templates, see Using filters with different AWS services.
    
   
    Encryption of filter criteria
    By default, Lambda doesn't encrypt your filter criteria object. For use cases
      where you may include sensitive information in your filter criteria object, you can
      use your own KMS key to encrypt it.
    After you encrypt your filter criteria object, you can view its plaintext version
      using a GetEventSourceMapping API call. You must have kms:Decrypt
      permissions to be able to successfully view the filter criteria in plaintext.
    NoteIf your filter criteria object is encrypted, Lambda redacts the value of the
        FilterCriteria field in the response of ListEventSourceMappings
        calls. Instead, this field displays as null. To see the true value
        of FilterCriteria, use the GetEventSourceMapping API.To view the decrypted value of FilterCriteria in the console,
        ensure that your IAM role contains permissions for GetEventSourceMapping.
    You can specify your own KMS key via the console, API/CLI, or AWS CloudFormation.
    To encrypt filter criteria with a customer-owned KMS key (console)
        Open the Functions page of the Lambda console.
      
        Choose Add trigger. If you already have an existing trigger,
          choose the Configuration tab, and then choose 
            Triggers. Select the existing trigger, and choose Edit.
      
        Select the checkbox next to Encrypt with customer managed KMS key.
      
        For Choose a customer managed KMS encryption key, select an
          existing enabled key or create a new key. Depending on the operation, you need some or
          all of the following permissions: kms:DescribeKey,
          kms:GenerateDataKey, and kms:Decrypt. Use the KMS key
          policy to grant these permissions.
      
    If you use your own KMS key, the following API operations must be permitted in the
      key policy:
    
       
       
       
       
    
        kms:Decrypt – Must be granted to the regional Lambda service
          principal (lambda.AWS_region.amazonaws.com).
          This allows Lambda to decrypt data with this KMS key.
        
           
        
            To prevent a
              
                cross-service confused deputy problem, the key policy uses the
              aws:SourceArn global condition key. The correct value
              of the aws:SourceArn key is the ARN of your event source mapping
              resource, so you can add this to your policy only after you know its ARN.
              Lambda also forwards the aws:lambda:FunctionArn and
              aws:lambda:EventSourceArn keys and their respective values
              in the encryption context
              when making a decryption request to KMS. These values must match the specified
              conditions in the key policy for the decryption request to succeed. You don't need to
              include EventSourceArn for Self-managed Kafka event sources since they don't have an
              EventSourceArn.
          
      
        kms:Decrypt – Must also be granted to the principal that
          intends to use the key to view the plaintext filter criteria in
          GetEventSourceMapping or DeleteEventSourceMapping API calls.
      
        kms:DescribeKey – Provides the customer managed key details to allow
          the specified principal to use the key.
      
        kms:GenerateDataKey – Provides permissions for Lambda to generate
          a data key to encrypt the filter criteria, on behalf of the specified principal
          (envelope encryption).
      
    You can use AWS CloudTrail to track AWS KMS requests that Lambda makes on your behalf. For sample
      CloudTrail events, see Monitoring your encryption keys for Lambda.
    We also recommend using the kms:ViaService condition key to limit the use of the KMS
      key to requests from Lambda only. The value of this key is the regional
      Lambda service principal
      (lambda.AWS_region.amazonaws.com). The following
      is a sample key policy that grants all the relevant permissions:
    Example AWS KMS key policy{
    "Version": "2012-10-17",
    "Id": "example-key-policy-1",
    "Statement": [
        {
            "Sid": "Allow Lambda to decrypt using the key",
            "Effect": "Allow",
            "Principal": {
                "Service": "lambda.us-east-1.amazonaws.com"
            },
            "Action": [
                "kms:Decrypt"
            ],
            "Resource": "*",
            "Condition": {
                "ArnEquals" : {
                    "aws:SourceArn": [
                        "arn:aws:lambda:us-east-1:123456789012:event-source-mapping:<esm_uuid>"
                    ]
                },
                "StringEquals": {
                    "kms:EncryptionContext:aws:lambda:FunctionArn": "arn:aws:lambda:us-east-1:123456789012:function:test-function",
                    "kms:EncryptionContext:aws:lambda:EventSourceArn": "arn:aws:sqs:us-east-1:123456789012:test-queue"
                }
            }
        },
        {
            "Sid": "Allow actions by an AWS account on the key",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:root"
            },
            "Action": "kms:*",
            "Resource": "*"
        },
        {
            "Sid": "Allow use of the key to specific roles",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:role/ExampleRole"
            },
            "Action": [
                "kms:Decrypt",
                "kms:DescribeKey",
                "kms:GenerateDataKey"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals" : {
                    "kms:ViaService": "lambda.us-east-1.amazonaws.com"
                }
            }
        }
    ]
}
    To use your own KMS key to encrypt filter criteria, you can also use the
      following CreateEventSourceMapping AWS CLI command. Specify the KMS key ARN
      with the --kms-key-arn flag.
    aws lambda create-event-source-mapping --function-name my-function \
    --maximum-batching-window-in-seconds 60 \
    --event-source-arn arn:aws:sqs:us-east-1:123456789012:my-queue \
    --filter-criteria "{\"filters\": [{\"pattern\": \"{\"a\": [\"1\", \"2\"]}\" }]}" \
    --kms-key-arn arn:aws:kms:us-east-1:123456789012:key/055efbb4-xmpl-4336-ba9c-538c7d31f599
    If you have an existing event source mapping, use the UpdateEventSourceMapping
      AWS CLI command instead. Specify the KMS key ARN with the --kms-key-arn flag.
    aws lambda update-event-source-mapping --function-name my-function \
    --maximum-batching-window-in-seconds 60 \
    --event-source-arn arn:aws:sqs:us-east-1:123456789012:my-queue \
    --filter-criteria "{\"filters\": [{\"pattern\": \"{\"a\": [\"1\", \"2\"]}\" }]}" \
    --kms-key-arn arn:aws:kms:us-east-1:123456789012:key/055efbb4-xmpl-4336-ba9c-538c7d31f599
    This operation overwrites any KMS key that was previously specified. If you specify
      the --kms-key-arn flag along with an empty argument, Lambda stops using your
      KMS key to encrypt filter criteria. Instead, Lambda defaults back to using an
      Amazon-owned key.
    To specify your own KMS key in a AWS CloudFormation template, use the KMSKeyArn
      property of the AWS::Lambda::EventSourceMapping resource type. For
      example, you can insert the following snippet into the YAML template for your event
      source.
    MyEventSourceMapping:
  Type: AWS::Lambda::EventSourceMapping
  Properties:
    ...
    FilterCriteria:
      Filters:
        - Pattern: '{"a": [1, 2]}'
    KMSKeyArn: "arn:aws:kms:us-east-1:123456789012:key/055efbb4-xmpl-4336-ba9c-538c7d31f599"
    ...

    To be able to view your encrypted filter criteria in plaintext in a
      GetEventSourceMapping or DeleteEventSourceMapping API call,
      you must have kms:Decrypt permissions.
    
    Starting August 6, 2024, the FilterCriteria field no longer shows up in
      AWS CloudTrail logs from CreateEventSourceMapping, UpdateEventSourceMapping, and
      DeleteEventSourceMapping API calls if your function doesn't use event filtering.
      If your function does use event filtering, the FilterCriteria field shows
      up as empty ({}). You can still view your filter criteria in plaintext in
      the response of GetEventSourceMapping API calls if you have
      kms:Decrypt permissions for the correct KMS key.
    In the following AWS CloudTrail sample log entry for a CreateEventSourceMapping call,
          FilterCriteria shows up as empty ({}) because the
          function uses event filtering. This is the case even if FilterCriteria
          object contains valid filter criteria that your function is actively using. If the
          function doesn't use event filtering, CloudTrail won't display the FilterCriteria
          field at all in log entries.{
    "eventVersion": "1.08",
    "userIdentity": {
        "type": "AssumedRole",
        "principalId": "AROA123456789EXAMPLE:userid1",
        "arn": "arn:aws:sts::123456789012:assumed-role/Example/example-role",
        "accountId": "123456789012",
        "accessKeyId": "ASIAIOSFODNN7EXAMPLE",
        "sessionContext": {
            "sessionIssuer": {
                "type": "Role",
                "principalId": "AROA987654321EXAMPLE",
                "arn": "arn:aws:iam::123456789012:role/User1",
                "accountId": "123456789012",
                "userName": "User1"
            },
            "webIdFederationData": {},
            "attributes": {
                "creationDate": "2024-05-09T20:35:01Z",
                "mfaAuthenticated": "false"
            }
        },
        "invokedBy": "AWS Internal"
    },
    "eventTime": "2024-05-09T21:05:41Z",
    "eventSource": "lambda.amazonaws.com",
    "eventName": "CreateEventSourceMapping20150331",
    "awsRegion": "us-east-2",
    "sourceIPAddress": "AWS Internal",
    "userAgent": "AWS Internal",
    "requestParameters": {
        "eventSourceArn": "arn:aws:sqs:us-east-2:123456789012:example-queue",
        "functionName": "example-function",
        "enabled": true,
        "batchSize": 10,
        "filterCriteria": {},
        "kMSKeyArn": "arn:aws:kms:us-east-2:123456789012:key/a1b2c3d4-5678-90ab-cdef-EXAMPLE11111",
        "scalingConfig": {},
        "maximumBatchingWindowInSeconds": 0,
        "sourceAccessConfigurations": []
    },
    "responseElements": {
        "uUID": "a1b2c3d4-5678-90ab-cdef-EXAMPLEaaaaa",
        "batchSize": 10,
        "maximumBatchingWindowInSeconds": 0,
        "eventSourceArn": "arn:aws:sqs:us-east-2:123456789012:example-queue",
        "filterCriteria": {},
        "kMSKeyArn": "arn:aws:kms:us-east-2:123456789012:key/a1b2c3d4-5678-90ab-cdef-EXAMPLE11111",
        "functionArn": "arn:aws:lambda:us-east-2:123456789012:function:example-function",
        "lastModified": "May 9, 2024, 9:05:41 PM",
        "state": "Creating",
        "stateTransitionReason": "USER_INITIATED",
        "functionResponseTypes": [],
        "eventSourceMappingArn": "arn:aws:lambda:us-east-2:123456789012:event-source-mapping:a1b2c3d4-5678-90ab-cdef-EXAMPLEbbbbb"
    },
    "requestID": "a1b2c3d4-5678-90ab-cdef-EXAMPLE33333",
    "eventID": "a1b2c3d4-5678-90ab-cdef-EXAMPLE22222",
    "readOnly": false,
    "eventType": "AwsApiCall",
    "managementEvent": true,
    "recipientAccountId": "123456789012",
    "eventCategory": "Management",
    "sessionCredentialFromConsole": "true"
}
   
    Using filters with different AWS services
    Different types of event source use different key values for their data fields. To filter on
      data properties, make sure that you use the correct key in your filter’s pattern. The following table
      gives the filtering keys for each supported AWS service.
    
          
            AWS service
            Filtering key
          
        
          
            DynamoDB
            dynamodb
          
          
            Kinesis
            data
          
          
            Amazon MQ
            data
          
          
            Amazon MSK
            value
          
          
            Self-managed Apache Kafka
            value
          
          
            Amazon SQS
            body
          
        
    The following sections give examples of filter patterns for different types of event sources. They also provide definitions of 
      supported incoming data formats and filter pattern body formats for each supported service.
    
       
       
       
       
       
       
    
        Using event filtering with a DynamoDB event source
      
        Using event filtering with a Kinesis event source
      
        Filter events from an Amazon MQ event source
      
        Using event filtering with an Amazon MSK event source
      
        Using event filtering with a self-managed Apache Kafka event source
      
        Using event filtering with an Amazon SQS event source
      
  Document ConventionsEvent source mapping tagsTesting in consoleDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideConsuming messages using short pollingConsuming messages using long pollingDifferences between long and short
                pollingAmazon SQS short and long pollingAmazon SQS offers short and long polling options for receiving messages from a queue. Consider
        your application's requirements for responsiveness and cost efficiency when choosing between
        these two polling options:
         
         
    
            Short polling (default) – The ReceiveMessage
                request queries a subset of servers (based on a weighted random distribution) to
                find available messages and sends an immediate response, even if no messages are
                found. 
        
            Long polling – ReceiveMessage
                queries all servers for messages, sending a response once at least one message is
                available, up to the specified maximum. An empty response is sent only if the
                polling wait time expires. This option can reduce the number of empty responses and
                potentially lower costs.
        The following sections explain the details of short polling and long polling.
        Consuming messages using short polling
        When you consume messages from a queue (FIFO or standard) using short polling, Amazon SQS
            samples a subset of its servers (based on a weighted random distribution) and returns
            messages from only those servers. Thus, a particular ReceiveMessage
            request might not return all of your messages. However, if you have fewer than 1,000
            messages in your queue, a subsequent request will return your messages. If you keep
            consuming from your queues, Amazon SQS samples all of its servers, and you receive all of
            your messages.
        The following diagram shows the short-polling behavior of messages returned from a
            standard queue after one of your system components makes a receive request. Amazon SQS
            samples several of its servers (in gray) and returns messages A, C, D, and B from these
            servers. Message E isn't returned for this request, but is returned for a subsequent
            request.
        
             
                
             
             
        
     
        Consuming messages using long polling
        When the wait time for the ReceiveMessage API action is greater than 0, long polling
    is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses
    (when there are no messages available for a ReceiveMessage request) and false
    empty responses (when messages are available but aren't included in a response). For information about enabling long polling for a new or
            existing queue using the Amazon SQS console, see the Configuring queue parameters using the Amazon SQS
      console. For best practices, see Setting-up long polling in
				Amazon SQS.
        Long polling offers the following benefits:
        
             
             
             
        
                Reduce empty responses by allowing Amazon SQS to wait until a message is available
                    in a queue before sending a response. Unless the connection times out, the
                    response to the ReceiveMessage request contains at least one of the
                    available messages, up to the maximum number of messages specified in the
                        ReceiveMessage action. In rare cases, you might receive empty
                    responses even when a queue still contains messages, especially if you specify a
                    low value for the ReceiveMessageWaitTimeSeconds parameter.
            
                Reduce false empty responses by querying all—rather than a subset
                    of—Amazon SQS servers.
            
                Return messages as soon as they become available.
            
        For information about how to confirm that a queue is empty, see Confirming that an Amazon SQS queue is empty.
     
        Differences between long and short
                polling
        Short polling occurs when the WaitTimeSeconds parameter of a ReceiveMessage request
            is set to 0 in one of two ways:
        
             
             
        
                The ReceiveMessage call sets WaitTimeSeconds to
                        0.
            
                The ReceiveMessage call doesn’t set WaitTimeSeconds,
                    but the queue attribute ReceiveMessageWaitTimeSeconds is set to
                        0.
            
    Document ConventionsCost allocation tagsVisibility timeoutDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAmazon Simple Queue ServiceDeveloper GuideUsing the Amazon SQS message group IDMessageGroupId is an attribute used only in Amazon SQS FIFO (First-In-First-Out) queues to organize messages into distinct groups. Messages within the same message group are always processed one at a time, in strict order, ensuring that no two messages from the same group are processed simultaneously. Standard queues do not use MessageGroupId and do not provide ordering guarantees. If strict ordering is required, use a FIFO queue instead.TopicsInterleaving multiple
					ordered message groups in Amazon SQSPreventing duplicate processing in a multiple-producer/consumer system in
					Amazon SQSAvoid large message
					backlogs with the same message group ID in Amazon SQSAvoid
					reusing the same message group ID with virtual queues in Amazon SQSDocument ConventionsConfiguring visibility timeouts
					in Amazon SQSInterleaving multiple
					ordered message groups in Amazon SQSDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaAPI ReferenceContentsSee AlsoScalingConfig(Amazon SQS only) The scaling configuration for the event source. To remove the configuration, pass an empty value.
      Contents
      
          
      
            
               
                  MaximumConcurrency
               
            
            
               Limits the number of concurrent instances that the Amazon SQS event source can invoke.
               Type: Integer
               Valid Range: Minimum value of 2. Maximum value of 1000.
               Required: No
            
         
    
      See Also
      For more information about using this API in one of the language-specific AWS SDKs, see the following:
      
          
          
          
      
            
               AWS SDK for C++
            
         
            
               AWS SDK for Java V2
            
         
            
               AWS SDK for Ruby V3
            
         
   Document ConventionsRuntimeVersionErrorSelfManagedEventSourceDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\nThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Thanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\nDocumentationAWS LambdaDeveloper GuideBackoff strategy for failed invocationsImplementing partial batch responsesHandling errors for an SQS event source in LambdaTo handle errors related to an SQS event source, Lambda automatically uses a retry strategy with a
        backoff strategy. You can also customize error handling behavior by configuring your SQS event
        source mapping to return partial batch responses.
        Backoff strategy for failed invocations
        When an invocation fails, Lambda attempts to retry the invocation while implementing a backoff strategy.
            The backoff strategy differs slightly depending on whether Lambda encountered the failure due to an error in
            your function code, or due to throttling.
        
             
             
        
                
                    If your function code caused the error, Lambda will stop processing and retrying the invocation.
                    In the meantime, Lambda gradually backs off, reducing the amount of concurrency allocated to your Amazon SQS event source mapping.
                    After your queue's visibility timeout runs out, the message will again reappear in the queue.
                
            
                If the invocation fails due to throttling, Lambda gradually backs off
                    retries by reducing the amount of concurrency allocated to your Amazon SQS event source mapping. Lambda continues
                    to retry the message until the message's timestamp exceeds your queue's visibility timeout, at which point
                    Lambda drops the message.
            
     
        Implementing partial batch responses
        When your Lambda function encounters an error while processing a batch, all messages in that batch become
            visible in the queue again by default, including messages that Lambda processed successfully. As a result, your
            function can end up processing the same message several times.
        To avoid reprocessing successfully processed messages in a failed batch, you can configure your event
            source mapping to make only the failed messages visible again. This is called a partial batch response.
            To turn on partial batch responses, specify ReportBatchItemFailures for the
            FunctionResponseTypes
            action when configuring your event source mapping. This lets your function
            return a partial success, which can help reduce the number of unnecessary retries on records.
        When ReportBatchItemFailures is activated, Lambda doesn't scale down message polling when function invocations fail. If you expect some messages to fail—and you don't want those failures to impact the message processing rate—use ReportBatchItemFailures.
        NoteKeep the following in mind when using partial batch responses:
                 
                 
            
                    If your function throws an exception, the entire batch is considered a complete failure.
                
                    If you're using this feature with a FIFO queue, your function should stop processing messages after the
                        first failure and return all failed and unprocessed messages in batchItemFailures. This helps
                        preserve the ordering of messages in your queue.
                
        To activate partial batch reporting
                Review the Best practices for implementing partial batch responses.
            
                Run the following command to activate ReportBatchItemFailures for your function. To retrieve your event source mapping's UUID, run the list-event-source-mappings AWS CLI command.
                aws lambda update-event-source-mapping \
--uuid "a1b2c3d4-5678-90ab-cdef-11111EXAMPLE" \
--function-response-types "ReportBatchItemFailures"
            
                Update your function code to catch all exceptions and return failed messages in a batchItemFailures JSON response. The batchItemFailures response must include a list of message IDs, as itemIdentifier JSON values.
                For example, suppose you have a batch of five messages, with message IDs id1, id2, id3, id4, and id5. Your function successfully processes id1, id3, and id5. To make messages id2 and id4 visible again in your queue, your function should return the following response:  
                { 
  "batchItemFailures": [ 
        {
            "itemIdentifier": "id2"
        },
        {
            "itemIdentifier": "id4"
        }
    ]
}
                Here are some examples of function code that return the list of failed message IDs in the batch:
                
    .NET
            
     

        SDK for .NET
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using .NET.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
using Amazon.Lambda.Core;
using Amazon.Lambda.SQSEvents;

// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.
[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]
namespace sqsSample;

public class Function
{
    public async Task<SQSBatchResponse> FunctionHandler(SQSEvent evnt, ILambdaContext context)
    {
        List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new List<SQSBatchResponse.BatchItemFailure>();
        foreach(var message in evnt.Records)
        {
            try
            {
                //process your message
                await ProcessMessageAsync(message, context);
            }
            catch (System.Exception)
            {
                //Add failed message identifier to the batchItemFailures list
                batchItemFailures.Add(new SQSBatchResponse.BatchItemFailure{ItemIdentifier=message.MessageId}); 
            }
        }
        return new SQSBatchResponse(batchItemFailures);
    }

    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)
    {
        if (String.IsNullOrEmpty(message.Body))
        {
            throw new Exception("No Body in SQS Message.");
        }
        context.Logger.LogInformation($"Processed message {message.Body}");
        // TODO: Do interesting work based on the new message
        await Task.CompletedTask;
    }
}

             
        
    
        
    Go
            
     

        SDK for Go V2
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Go.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"github.com/aws/aws-lambda-go/events"
	"github.com/aws/aws-lambda-go/lambda"
)

func handler(ctx context.Context, sqsEvent events.SQSEvent) (map[string]interface{}, error) {
	batchItemFailures := []map[string]interface{}{}

	for _, message := range sqsEvent.Records {
		
		if /* Your message processing condition here */ {			
			batchItemFailures = append(batchItemFailures, map[string]interface{}{"itemIdentifier": message.MessageId})
		}
	}

	sqsBatchResponse := map[string]interface{}{
		"batchItemFailures": batchItemFailures,
	}
	return sqsBatchResponse, nil
}

func main() {
	lambda.Start(handler)
}


             
        
    
        
    Java
            
     

        SDK for Java 2.x
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Java.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SQSEvent;
import com.amazonaws.services.lambda.runtime.events.SQSBatchResponse;
 
import java.util.ArrayList;
import java.util.List;
 
public class ProcessSQSMessageBatch implements RequestHandler<SQSEvent, SQSBatchResponse> {
    @Override
    public SQSBatchResponse handleRequest(SQSEvent sqsEvent, Context context) {
 
         List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new ArrayList<SQSBatchResponse.BatchItemFailure>();
         String messageId = "";
         for (SQSEvent.SQSMessage message : sqsEvent.getRecords()) {
             try {
                 //process your message
             } catch (Exception e) {
                 //Add failed message identifier to the batchItemFailures list
                 batchItemFailures.add(new SQSBatchResponse.BatchItemFailure(message.getMessageId()));
             }
         }
         return new SQSBatchResponse(batchItemFailures);
     }
}

             
        
    
        
    JavaScript
            
     

        SDK for JavaScript (v3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using JavaScript.
                
                // Node.js 20.x Lambda runtime, AWS SDK for Javascript V3
export const handler = async (event, context) => {
    const batchItemFailures = [];
    for (const record of event.Records) {
        try {
            await processMessageAsync(record, context);
        } catch (error) {
            batchItemFailures.push({ itemIdentifier: record.messageId });
        }
    }
    return { batchItemFailures };
};

async function processMessageAsync(record, context) {
    if (record.body && record.body.includes("error")) {
        throw new Error("There is an error in the SQS Message.");
    }
    console.log(`Processed message: ${record.body}`);
}

             
             
                    Reporting SQS batch item failures with Lambda using TypeScript.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
import { SQSEvent, SQSBatchResponse, Context, SQSBatchItemFailure, SQSRecord } from 'aws-lambda';

export const handler = async (event: SQSEvent, context: Context): Promise<SQSBatchResponse> => {
    const batchItemFailures: SQSBatchItemFailure[] = [];

    for (const record of event.Records) {
        try {
            await processMessageAsync(record);
        } catch (error) {
            batchItemFailures.push({ itemIdentifier: record.messageId });
        }
    }

    return {batchItemFailures: batchItemFailures};
};

async function processMessageAsync(record: SQSRecord): Promise<void> {
    if (record.body && record.body.includes("error")) {
        throw new Error('There is an error in the SQS Message.');
    }
    console.log(`Processed message ${record.body}`);
}


             
        
    
        
    PHP
            
     

        SDK for PHP
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using PHP.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
<?php

use Bref\Context\Context;
use Bref\Event\Sqs\SqsEvent;
use Bref\Event\Sqs\SqsHandler;
use Bref\Logger\StderrLogger;

require __DIR__ . '/vendor/autoload.php';

class Handler extends SqsHandler
{
    private StderrLogger $logger;
    public function __construct(StderrLogger $logger)
    {
        $this->logger = $logger;
    }

    /**
     * @throws JsonException
     * @throws \Bref\Event\InvalidLambdaEvent
     */
    public function handleSqs(SqsEvent $event, Context $context): void
    {
        $this->logger->info("Processing SQS records");
        $records = $event->getRecords();

        foreach ($records as $record) {
            try {
                // Assuming the SQS message is in JSON format
                $message = json_decode($record->getBody(), true);
                $this->logger->info(json_encode($message));
                // TODO: Implement your custom processing logic here
            } catch (Exception $e) {
                $this->logger->error($e->getMessage());
                // failed processing the record
                $this->markAsFailed($record);
            }
        }
        $totalRecords = count($records);
        $this->logger->info("Successfully processed $totalRecords SQS records");
    }
}

$logger = new StderrLogger();
return new Handler($logger);


             
        
    
        
    Python
            
     

        SDK for Python (Boto3)
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Python.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

def lambda_handler(event, context):
    if event:
        batch_item_failures = []
        sqs_batch_response = {}
     
        for record in event["Records"]:
            try:
                # process message
            except Exception as e:
                batch_item_failures.append({"itemIdentifier": record['messageId']})
        
        sqs_batch_response["batchItemFailures"] = batch_item_failures
        return sqs_batch_response

             
        
    
        
    Ruby
            
     

        SDK for Ruby
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Ruby.
                
                # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
require 'json'

def lambda_handler(event:, context:)
  if event
    batch_item_failures = []
    sqs_batch_response = {}

    event["Records"].each do |record|
      begin
        # process message
      rescue StandardError => e
        batch_item_failures << {"itemIdentifier" => record['messageId']}
      end
    end

    sqs_batch_response["batchItemFailures"] = batch_item_failures
    return sqs_batch_response
  end
end


             
        
    
        
    Rust
            
     

        SDK for Rust
        
Note
        There's more on GitHub. Find the complete example and learn how to set up and run in the
        Serverless examples
        repository.
    
             
                    Reporting SQS batch item failures with Lambda using Rust.
                
                // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
use aws_lambda_events::{
    event::sqs::{SqsBatchResponse, SqsEvent},
    sqs::{BatchItemFailure, SqsMessage},
};
use lambda_runtime::{run, service_fn, Error, LambdaEvent};

async fn process_record(_: &SqsMessage) -> Result<(), Error> {
    Err(Error::from("Error processing message"))
}

async fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<SqsBatchResponse, Error> {
    let mut batch_item_failures = Vec::new();
    for record in event.payload.records {
        match process_record(&record).await {
            Ok(_) => (),
            Err(_) => batch_item_failures.push(BatchItemFailure {
                item_identifier: record.message_id.unwrap(),
            }),
        }
    }

    Ok(SqsBatchResponse {
        batch_item_failures,
    })
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    run(service_fn(function_handler)).await
}


             
        
    
        

            
        If the failed events do not return to the queue, see How do I troubleshoot Lambda function SQS ReportBatchItemFailures? in the AWS Knowledge Center.
         
            Success and failure conditions
            Lambda treats a batch as a complete success if your function returns any of the following:
            
                 
                 
                 
                 
            
                    An empty batchItemFailures list
                
                    A null batchItemFailures list
                
                    An empty EventResponse
                
                    A null EventResponse
                
            Lambda treats a batch as a complete failure if your function returns any of the following:
            
                 
                 
                 
                 
                 
            
                    An invalid JSON response
                
                    An empty string itemIdentifier
                
                    A null itemIdentifier
                
                    An itemIdentifier with a bad key name
                
                    An itemIdentifier value with a message ID that doesn't exist
                
         
         
            CloudWatch metrics
            To determine whether your function is correctly reporting batch item failures, you can monitor the
                NumberOfMessagesDeleted and ApproximateAgeOfOldestMessage Amazon SQS metrics in
                Amazon CloudWatch.
            
                 
                 
            
                    NumberOfMessagesDeleted tracks the number of messages removed from your queue. If this
                        drops to 0, this is a sign that your function response is not correctly returning failed messages.
                
                    ApproximateAgeOfOldestMessage tracks how long the oldest message has stayed in your queue.
                        A sharp increase in this metric can indicate that your function is not correctly returning failed
                        messages.
                
         
    Document ConventionsScaling behaviorParametersDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.\n\n\n\n