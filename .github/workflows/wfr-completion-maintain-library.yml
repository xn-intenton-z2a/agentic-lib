# .github/workflows/wfr-completion-maintain-library.yml

#
# agentic-lib
# Copyright (C) 2025 Polycode Limited
#
# This file is part of agentic-lib.
#
# agentic-lib is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License v3.0 (GPL‑3).
# along with this program. If not, see <https://www.gnu.org/licenses/>.
#
# IMPORTANT: Any derived work must include the following attribution:
# "This work is derived from https://github.com/xn-intenton-z2a/agentic-lib"
#

name: ∞ maintain-library

on:
  workflow_call:
    inputs:
      document:
        description: 'Text to drive the library maintenance (if "house choice", the repository will be assessed and an action chosen). e.g. "Get a document about agents."'
        type: string
        required: false
        default: 'house choice'
      libraryDir:
        description: 'The directory to create/update the documents in. e.g. "library/"'
        type: string
        required: false
        default: 'library/'
      sourcesFile:
        description: 'The file containing the sources to extract sections from. e.g. "SOURCES.md"'
        type: string
        required: false
        default: 'SOURCES.md'
      documentsLimit:
        description: 'The maximum number of documents to have in the library to create. e.g. "3"'
        type: string
        required: false
        default: '3'
      summaryWordLimit:
        description: 'The maximum number of words for the URL content summary. e.g. "200"'
        type: string
        required: false
        default: '200'
      model:
        description: 'The OpenAI model to use. e.g. "o3-mini"'
        type: string
        required: false
        default: ${{ vars.CHATGPT_API_MODEL || 'o3-mini' }}
      npmAuthOrganisation:
        description: 'The GitHub organisation to authenticate with for npm. e.g. "xn-intenton-z2a"'
        type: string
        required: false
        default: ''
      gitUserEmail:
        description: 'The email to use for git commits. e.g. "action@github.com"'
        type: string
        required: false
        default: 'action@github.com'
      gitUserName:
        description: 'The name to use for git commits. e.g. "GitHub Actions[bot]"'
        type: string
        required: false
        default: 'GitHub Actions[bot]'
      s3BucketUrl:
        description: 'The S3 bucket URL with prefix to use. e.g. "s3://my-bucket/prefix"'
        type: string
        required: false
        default: ''
      iamRoleArn:
        description: 'The ARN of the IAM role to assume. e.g. "arn:aws:iam::123456789012:role/my-role"'
        type: string
        required: false
        default: ''
    secrets:
      PERSONAL_ACCESS_TOKEN:
        required: false
      CHATGPT_API_SECRET_KEY:
        required: true
    outputs:
      documentName:
        value: ${{ jobs.maintain-library.outputs.documentName }}
      documentDetailedDigest:
        value: ${{ jobs.maintain-library.outputs.documentDetailedDigest }}
      crawlSummary:
        value: ${{ jobs.maintain-library.outputs.crawlSummary }}
      normalisedExtract:
        value: ${{ jobs.maintain-library.outputs.normalisedExtract }}
      supplementaryDetails:
        value: ${{ jobs.maintain-library.outputs.supplementaryDetails }}
      referenceDetails:
        value: ${{ jobs.maintain-library.outputs.referenceDetails }}

jobs:
  maintain-library:
    runs-on: ubuntu-latest

    env:
      document: ${{ inputs.document || '' }}
      libraryDir: ${{ inputs.libraryDir || 'library/' }}
      sourcesFile: ${{ inputs.sourcesFile || 'SOURCES.md' }}
      documentsLimit: ${{ inputs.documentsLimit || '3' }}
      summaryWordLimit: ${{ inputs.summaryWordLimit || '10000' }}
      model: ${{ inputs.model || vars.CHATGPT_API_MODEL || 'o3-mini' }}
      npmAuthOrganisation: ${{ inputs.npmAuthOrganisation || '' }}
      gitUserEmail: ${{ inputs.gitUserEmail || 'action@github.com' }}
      gitUserName: ${{ inputs.gitUserName || 'GitHub Actions[bot]' }}
      chatgptApiSecretKey: ${{ secrets.CHATGPT_API_SECRET_KEY }}
      s3BucketUrl: ${{ inputs.s3BucketUrl || '' }}
      iamRoleArn: ${{ inputs.iamRoleArn || '' }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Check GitHub authentication
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: |
          curl --include --header "Authorization: token ${{ secrets.PERSONAL_ACCESS_TOKEN || secrets.GITHUB_TOKEN }}" https://api.github.com/user

      - name: Set up .npmrc
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: |
          echo "${{ env.npmAuthOrganisation }}:registry=https://npm.pkg.github.com" >> .npmrc
          echo "//npm.pkg.github.com/:_authToken=${{ secrets.PERSONAL_ACCESS_TOKEN || secrets.GITHUB_TOKEN }}" >> .npmrc
          echo "always-auth=true" >> .npmrc

      - run: npm ci

      - name: Install web crawling dependencies
        run: npm install axios cheerio

      - name: Tear down .npmrc
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: rm -f .npmrc

      - name: maintain-library
        id: maintain-library
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const document = process.env.document;
            const libraryDir = process.env.libraryDir;
            const sourcesFile = process.env.sourcesFile;
            const documentsLimit = process.env.documentsLimit;
            const model = process.env.model;
            const apiKey = process.env.chatgptApiSecretKey;
            const dependenciesListOutput = process.env.dependenciesListOutput;
            const summaryWordLimit = process.env.summaryWordLimit;

            const fs = require('fs');
            const path = require('path');
            const OpenAI = require('openai').default;
            const { z } = require('zod');
            const axios = require('axios');
            const cheerio = require('cheerio');
            require('dotenv').config();

            if (!apiKey) { 
              core.setFailed("Missing CHATGPT_API_SECRET_KEY"); 
            }
            const openai = new OpenAI({ apiKey });

            core.info(`document: "${document}"`);
            core.info(`libraryDir: "${libraryDir}"`);
            core.info(`sourcesFile: "${sourcesFile}"`);

            // Load the sources file
            const sourcesFileContent = fs.readFileSync(sourcesFile, 'utf8');
            core.info(`Sources file '${sourcesFile}' has been loaded (length ${sourcesFileContent.length}).`);

            // Get list of existing library documents
            let existingDocuments = [];
            try {
              if (fs.existsSync(libraryDir)) {
                existingDocuments = fs.readdirSync(libraryDir)
                  .filter(file => file.endsWith('.md'))
                  .map(file => path.join(libraryDir, file));
              } else {
                fs.mkdirSync(libraryDir, { recursive: true });
              }
            } catch (e) {
              core.warning(`Error reading library directory: ${e.message}`);
              fs.mkdirSync(libraryDir, { recursive: true });
            }

            core.info(`Found ${existingDocuments.length} existing documents in ${libraryDir}`);

            // Parse the sources file to extract sections
            // Each source entry starts with a level 1 heading (#) and contains URL, description, and license
            const sourceEntries = [];
            const sourceLines = sourcesFileContent.split('\n');
            let currentEntry = null;

            for (let i = 0; i < sourceLines.length; i++) {
              const line = sourceLines[i];

              if (line.startsWith('# ')) {
                // If we have a current entry, push it to the array
                if (currentEntry) {
                  sourceEntries.push(currentEntry);
                }

                // Start a new entry
                currentEntry = {
                  name: line.substring(2).trim(),
                  url: '',
                  description: '',
                  license: '',
                  content: line + '\n'
                };
              } else if (currentEntry) {
                currentEntry.content += line + '\n';

                if (line.startsWith('## http')) {
                  currentEntry.url = line.substring(3).trim();
                } else if (line.startsWith('## ')) {
                  currentEntry.license = line.substring(3).trim();
                } else if (line && !line.startsWith('#')) {
                  if (!currentEntry.description) {
                    currentEntry.description = line.trim();
                  }
                }
              }
            }

            // Don't forget the last entry
            if (currentEntry) {
              sourceEntries.push(currentEntry);
            }

            core.info(`Parsed ${sourceEntries.length} source entries from ${sourcesFile}`);

            // Function to crawl a URL and its links for 16 seconds
            async function crawlUrl(url, maxTime = 16000) {
              if (!url || !url.startsWith('http')) {
                return { content: '', links: [], dataSize: 0, error: 'Invalid URL' };
              }

              const startTime = Date.now();
              const visitedUrls = new Set();
              const result = {
                content: '',
                links: [],
                dataSize: 0,
                error: null
              };

              try {
                // Queue of URLs to visit
                const urlQueue = [url];

                while (urlQueue.length > 0 && (Date.now() - startTime) < maxTime) {
                  const currentUrl = urlQueue.shift();

                  // Skip if already visited
                  if (visitedUrls.has(currentUrl)) {
                    continue;
                  }

                  visitedUrls.add(currentUrl);

                  try {
                    core.info(`Crawling URL: ${currentUrl}`);
                    const response = await axios.get(currentUrl, { 
                      timeout: 5000,
                      headers: {
                        'User-Agent': 'Mozilla/5.0 (compatible; AgenticLibBot/1.0; +https://github.com/xn-intenton-z2a/agentic-lib)'
                      }
                    });

                    const contentType = response.headers['content-type'] || '';

                    // Only process HTML content
                    if (contentType.includes('text/html')) {
                      const $ = cheerio.load(response.data);

                      // Extract text content with priority to meaningful content
                      // Remove script, style, and other non-content elements
                      $('script, style, meta, link, noscript, iframe, svg').remove();

                      // Prioritize content from main content areas
                      let mainContent = '';
                      const contentSelectors = ['article', 'main', '.content', '.main', '#content', '#main', '.post', '.entry', '.blog-post'];

                      for (const selector of contentSelectors) {
                        if ($(selector).length > 0) {
                          mainContent += $(selector).text().replace(/\\s+/g, ' ').trim() + '\\n\\n';
                        }
                      }

                      // If no main content found, extract from body
                      if (!mainContent) {
                        // Extract text from paragraphs, headings, lists, and other text elements
                        $('p, h1, h2, h3, h4, h5, h6, li, td, th, div, span, a').each((i, el) => {
                          const text = $(el).text().trim();
                          if (text.length > 20) { // Only include substantial text
                            mainContent += text + '\\n';
                          }
                        });
                      }

                      // If still no content, fall back to body text
                      if (!mainContent) {
                        mainContent = $('body').text().replace(/\\s+/g, ' ').trim();
                      }

                      result.content += mainContent + '\\n\\n';
                      result.dataSize += mainContent.length;

                      // Extract links
                      $('a').each((i, link) => {
                        const href = $(link).attr('href');
                        if (href) {
                          // Convert relative URLs to absolute
                          let absoluteUrl = href;
                          if (!href.startsWith('http')) {
                            try {
                              absoluteUrl = new URL(href, currentUrl).href;
                            } catch (e) {
                              // Skip invalid URLs
                              return;
                            }
                          }

                          // Only follow links on the same domain
                          const currentDomain = new URL(currentUrl).hostname;
                          const linkDomain = new URL(absoluteUrl).hostname;

                          if (linkDomain === currentDomain && !visitedUrls.has(absoluteUrl)) {
                            result.links.push(absoluteUrl);
                            urlQueue.push(absoluteUrl);
                          }
                        }
                      });
                    }
                  } catch (error) {
                    core.warning(`Error crawling ${currentUrl}: ${error.message}`);
                  }
                }

                core.info(`Crawling completed. Visited ${visitedUrls.size} URLs. Data size: ${result.dataSize} bytes.`);
                return result;
              } catch (error) {
                core.warning(`Crawling error: ${error.message}`);
                result.error = error.message;
                return result;
              }
            }

            // Randomly select a source entry
            const randomIndex = Math.floor(Math.random() * sourceEntries.length);
            const randomSourceEntry = sourceEntries[randomIndex];
            core.info(`Randomly selected source entry: ${randomSourceEntry.name} (${randomSourceEntry.url})`);

            // generate the document prompt either by using the supplied document prompt or by reviewing the current features and full context
            let prompt = document;
            if (document === 'house choice') {
              prompt = `Please review the current documents in the library and either;
                * add a new document to the repository, or
                * extend an existing document to add a new aspect to it, or
                * update an existing document to bring it to a high standard matching other documents in the repository.
                The document name should either be a current document name or be supplied with a summary which is distinct from any other document in the repository.
              `;
            }

            // Crawl the URL from the randomly selected source entry
            core.info(`Crawling URL: ${randomSourceEntry.url}`);
            let crawlResult = { content: '', links: [], dataSize: 0, error: 'Not crawled' };
            try {
              crawlResult = await crawlUrl(randomSourceEntry.url);
            } catch (error) {
              core.warning(`Error during crawling: ${error.message}`);
            }

            const chatGptPrompt = `
            Please generate a document detailed digest based on the supplied prompt and project files.
            The detailed digest should be thorough but concise and state the usefulness and authority of the source.
            Ensure any content generated would be suitable for publication directly on the internet, in particular do not include private information about people.
            The word limit is ${summaryWordLimit} words but it not just a limit, it's a target to aim for provided the content is justified and not padded.
            Apply an internal peer review to ensure the detailed digest meets all of, a useful internal tech write up and a high academic for an informal paper providing a digest and insight into a topic.
            Where possible find situations to support or refute statements made in the originally crawled material.
            Use a journalistic style in the writing and include a glossary of any obscure terms at the end of the detailed digest.
            Before adding a new document ensure that this document is distinct from any other document in the library, otherwise update an existing document.
            The document name should be one or two words in SCREAMING_SNAKECASE.

            You should extract a section from the sources file to create the document. Each document should contain:
            1. The original content from the source section in SOURCES.md
            2. A detailed digest as a detailed digest of the crawled content from the URL (limited to ${summaryWordLimit} words)
            3. A normalised extract of the crawled content, containing:
               a. A summary of the key points
               b. A table of contents listing the main topics
               c. A full paragraph for each item in the table of contents, providing comprehensive information on each topic
            4. A supplementary details section containing the latest information from your own knowledge base that complements the crawled content
            5. A reference details section containing extracted or supplemented details about API specifications, SDK method signatures, or instructional material related to the topic
            6. The date when the content was retrieved (current date)
            7. Attribution information and data size obtained during crawling

            For the normalised extract, ensure that the information in the crawled data is retained and transformed into topics that can be used to create product features and make engineering decisions. The content should be useful, dense, and presented in a consistent readable style. Each item in the table of contents must have a full paragraph that thoroughly explains the topic.

            Consider the following when refining your response:
            * Document prompt details
            * Available source entries from SOURCES.md

            Document prompt:
            DOCUMENT_PROMPT_START
            ${prompt}
            DOCUMENT_PROMPT_END            

            Available source entries from SOURCES.md (${sourceEntries.length} entries):
            SOURCES_ENTRIES_START
            ${sourceEntries.map((entry, index) => `Entry ${index + 1}: ${entry.name} (${entry.url})`).join('\n')}
            SOURCES_ENTRIES_END

            Crawled content from URL (${randomSourceEntry.url}):
            CRAWLED_CONTENT_START
            Data Size: ${crawlResult.dataSize} bytes
            Links Found: ${crawlResult.links.length}
            Error: ${crawlResult.error || 'None'}
            Content Preview: ${crawlResult.content.substring(0, 1000)}...
            CRAWLED_CONTENT_END

            Answer strictly with a JSON object following this schema:
            {
              "documentName": "The document name as one or two words in SCREAMING_SNAKECASE.",
              "documentDetailedDigest": "The document detailed digest as multiline markdown with a few level 1 headings, including the original content from the source and the date when the content was retrieved.",
              "documentNamesToBeDeleted": "The comma separated list of document names to be deleted or 'none' if no document is to be deleted.",
              "sourceEntryIndex": "The index (1-based) of the source entry used for this document."
            }
            Ensure valid JSON.
            `;

            const ResponseSchema = z.object({ 
              documentName: z.string(), 
              documentDetailedDigest: z.string(), 
              documentNamesToBeDeleted: z.string(),
              sourceEntryIndex: z.string(),
              crawlSummary: z.string(),
              normalisedExtract: z.string(),
              supplementaryDetails: z.string(),
              referenceDetails: z.string()
            });

            // Define the function schema for function calling
            const tools = [{
              type: "function",
              function: {
                name: "generate_document",
                description: "Elaborate on the supplied prompt and project files to create the documentName, documentDetailedDigest, and crawlSummary of a repository document, and the document names to be deleted. Return an object with documentName (string), documentDetailedDigest (string), documentNamesToBeDeleted (string), sourceEntryIndex (string), and crawlSummary (string).",
                parameters: {
                  type: "object",
                  properties: {
                    documentName: { type: "string", description: "The document name as one or two words in SCREAMING_SNAKECASE." },
                    documentDetailedDigest: { type: "string", description: "The document detailed digest as multiline markdown with a few level 1 headings, including the original content from the source and the date when the content was retrieved." },
                    documentNamesToBeDeleted: { type: "string", description: "The comma separated list of document names to be deleted or 'none' if no document is to be deleted." },
                    sourceEntryIndex: { type: "string", description: "The index (1-based) of the source entry used for this document." },
                    crawlSummary: { type: "string", description: "A summary of the crawled content from the URL, limited to the specified word limit." },
                    normalisedExtract: { type: "string", description: "A normalised extract of the crawled content, containing a summary, table of contents, and a full paragraph for each item in the table of contents. The information should be retained and transformed into topics for product features and engineering decisions. Content should be useful, dense, and in a consistent readable style." },
                    supplementaryDetails: { type: "string", description: "Additional information from the LLM's own knowledge base that supplements the crawled content with the latest information on the topic." },
                    referenceDetails: { type: "string", description: "Extracted or supplemented details about API specifications, SDK method signatures, or instructional material related to the topic." }
                  },
                  required: ["documentName", "documentDetailedDigest", "documentNamesToBeDeleted", "sourceEntryIndex", "crawlSummary", "normalisedExtract", "supplementaryDetails", "referenceDetails"],
                  additionalProperties: false
                },
                strict: true
              }
            }];

            // Call OpenAI using function calling format
            const response = await openai.chat.completions.create({
              model,
              messages: [
                { role: "system", content: "You are maintaining a library set by providing expert contemporary insight into both the product market and you will perform a detailed analysis of the current state of the repository and current document set in search of value opportunities and unique selling points. Answer strictly with a JSON object following the provided function schema." },
                { role: "user", content: chatGptPrompt }
              ],
              tools: tools
            });

            let result;
            if (response.choices[0].message.tool_calls && response.choices[0].message.tool_calls.length > 0) {
              try {
                result = JSON.parse(response.choices[0].message.tool_calls[0].function.arguments);
              } catch (e) {
                core.setFailed(`Failed to parse function call arguments: ${e.message}`);
              }
            } else if (response.choices[0].message.content) {
              try {
                result = JSON.parse(response.choices[0].message.content);
              } catch (e) {
                core.setFailed(`Failed to parse response content: ${e.message}`);
              }
            } else {
              core.setFailed("No valid response received from OpenAI.");
            }

            try {
              const parsed = ResponseSchema.parse(result);
              core.setOutput("documentName", parsed.documentName);
              core.setOutput("documentDetailedDigest", parsed.documentDetailedDigest);
              core.setOutput("documentNamesToBeDeleted", parsed.documentNamesToBeDeleted);
              core.setOutput("sourceEntryIndex", parsed.sourceEntryIndex);
              core.setOutput("crawlSummary", parsed.crawlSummary);
              core.setOutput("normalisedExtract", parsed.normalisedExtract);
              core.setOutput("supplementaryDetails", parsed.supplementaryDetails);
              core.setOutput("referenceDetails", parsed.referenceDetails);
              core.info(`documentName: "${parsed.documentName}"`);
              core.info(`documentDetailedDigest: "${parsed.documentDetailedDigest}"`);
              core.info(`documentNamesToBeDeleted: "${parsed.documentNamesToBeDeleted}"`);
              core.info(`sourceEntryIndex: "${parsed.sourceEntryIndex}"`);
              core.info(`crawlSummary: "${parsed.crawlSummary}"`);
              core.info(`normalisedExtract: "${parsed.normalisedExtract}"`);
              core.info(`supplementaryDetails: "${parsed.supplementaryDetails}"`);
              core.info(`referenceDetails: "${parsed.referenceDetails}"`);

              // Get the source entry used for this document
              const sourceEntryIndex = parseInt(parsed.sourceEntryIndex, 10) - 1;
              let sourceEntry = null;
              if (sourceEntryIndex >= 0 && sourceEntryIndex < sourceEntries.length) {
                sourceEntry = sourceEntries[sourceEntryIndex];
                core.info(`Using source entry: ${sourceEntry.name} (${sourceEntry.url})`);
              } else {
                core.warning(`Invalid source entry index: ${parsed.sourceEntryIndex}`);
              }

              // Add the current date to the document detailed digest
              const now = new Date();
              const dateStr = now.toISOString().split('T')[0]; // YYYY-MM-DD format

              // Create the document content
              let documentContent = "";

              // If we have a valid source entry, add the original content, crawl detailed digest, and attribution fields
              if (sourceEntry) {
                const crawlDate = new Date();
                const crawlDateStr = crawlDate.toISOString();

                documentContent = `# ${parsed.documentName}\n\n`;
                documentContent += `## Crawl Summary\n${parsed.crawlSummary}\n\n`;
                documentContent += `## Normalised Extract\n${parsed.normalisedExtract}\n\n`;
                documentContent += `## Supplementary Details\n${parsed.supplementaryDetails}\n\n`;
                documentContent += `## Reference Details\n${parsed.referenceDetails}\n\n`;
                documentContent += `## Original Source\n${sourceEntry.name}\n${sourceEntry.url}\n\n`;
                documentContent += `## Digest of ${parsed.documentName}\n\n`;
                documentContent += `${parsed.documentDetailedDigest}\n\n`;
                documentContent += `## Attribution\n`;
                documentContent += `- Source: ${sourceEntry.name}\n`;
                documentContent += `- URL: ${sourceEntry.url}\n`;
                documentContent += `- License: ${sourceEntry.license || 'Unknown'}\n`;
                documentContent += `- Crawl Date: ${crawlDateStr}\n`;
                documentContent += `- Data Size: ${crawlResult.dataSize} bytes\n`;
                documentContent += `- Links Found: ${crawlResult.links.length}\n\n`;
                documentContent += `## Retrieved\n${dateStr}\n`;

                // Save the raw crawl data to a text file
                const rawCrawlFilePath = path.join(libraryDir, `${documentName}_RAW_CRAWL.txt`);
                try {
                  fs.writeFileSync(rawCrawlFilePath, crawlResult.content);
                  core.info(`Raw crawl data saved to ${rawCrawlFilePath}`);
                } catch (e) {
                  core.warning(`Failed to save raw crawl data: ${e.message}`);
                }
              } else {
                // If no valid source entry, just add the date at the end
                documentContent += `\n\n_Retrieved: ${dateStr}_\n`;
              }

              // Save the document to a file
              const documentName = parsed.documentName.replace(/ /g, "_").toUpperCase();
              const documentFilePath = path.join(libraryDir, `${documentName}.md`);
              try {
                fs.mkdirSync(libraryDir, { recursive: true });
                fs.writeFileSync(documentFilePath, documentContent);
                core.info(`Document saved to ${documentFilePath}`);
              } catch (e) {
                core.setFailed(`Failed to save document: ${e.message}`);
              }

              // Delete any documents that should be deleted
              const documentNamesToBeDeleted = parsed.documentNamesToBeDeleted
                .split(',')
                .map(name => name.trim())
                .filter(name => name && name !== 'none');

              for (const name of documentNamesToBeDeleted) {
                const documentNameToDelete = name.replace(/ /g, "_").toUpperCase().replace(/\.md$/, "") + ".md";
                const documentPathToDelete = path.join(libraryDir, documentNameToDelete);
                try {
                  if (fs.existsSync(documentPathToDelete)) {
                    core.info(`Deleting document: ${documentPathToDelete}`);
                    fs.unlinkSync(documentPathToDelete);
                  } else {
                    core.warning(`Document not found for deletion: ${documentPathToDelete}`);
                  }
                } catch (e) {
                  core.warning(`Error deleting document ${documentPathToDelete}: ${e.message}`);
                }
              }

            } catch (e) {
              core.setFailed(`Failed to parse ChatGPT response: ${e.message}`);
            }

            core.setOutput("response", JSON.stringify(response));
            core.setOutput("usage", JSON.stringify(response.usage));
            core.info(`response: "${JSON.stringify(response)}"`);
            core.info(`usage: "${JSON.stringify(response.usage)}"`);

      - name: Commit changes
        id: commit
        continue-on-error: true
        run: |
          git config --local user.email '${{ env.gitUserEmail }}'
          git config --local user.name '${{ env.gitUserName }}'
          git status -v
          git add -v --all '${{ env.libraryDir }}'
          git commit -m 'Maintain ${{ steps.maintain-library.outputs.documentName }}'
          git status -v
          git push -v origin ${{ github.ref }}
          git status -v

    outputs:
      documentName: ${{ steps.maintain-library.outputs.documentName }}
      documentDetailedDigest: ${{ steps.maintain-library.outputs.documentDetailedDigest }}
      crawlSummary: ${{ steps.maintain-library.outputs.crawlSummary }}
      normalisedExtract: ${{ steps.maintain-library.outputs.normalisedExtract }}
      supplementaryDetails: ${{ steps.maintain-library.outputs.supplementaryDetails }}
      referenceDetails: ${{ steps.maintain-library.outputs.referenceDetails }}
