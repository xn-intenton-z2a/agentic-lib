# .github/workflows/wfr-completion-maintain-library.yml

#
# agentic-lib
# Copyright (C) 2025 Polycode Limited
#
# This file is part of agentic-lib.
#
# agentic-lib is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License v3.0 (GPL‑3).
# along with this program. If not, see <https://www.gnu.org/licenses/>.
#
# IMPORTANT: Any derived work must include the following attribution:
# "This work is derived from https://github.com/xn-intenton-z2a/agentic-lib"
#

name: ∞ maintain-library

on:
  workflow_call:
    inputs:
      document:
        description: 'Text to drive the library maintenance (if "house choice", the repository will be assessed and an action chosen). e.g. "Get a document about agents."'
        type: string
        required: false
        default: 'house choice'
      libraryDir:
        description: 'The directory to create/update the documents in. e.g. "library/"'
        type: string
        required: false
        default: 'library/'
      sourcesFile:
        description: 'The file containing the sources to extract sections from. e.g. "SOURCES.md"'
        type: string
        required: false
        default: 'SOURCES.md'
      documentsLimit:
        description: 'The maximum number of documents to have in the library to create. e.g. "3"'
        type: string
        required: false
        default: '3'
      summaryWordLimit:
        description: 'The maximum number of words for the URL content summary. e.g. "200"'
        type: string
        required: false
        default: '200'
      featuresDir:
        description: 'The directory containing feature files. e.g. "features/"'
        type: string
        required: false
        default: 'features/'
      target:
        description: 'The target file to create the issue to change. e.g. "src/lib/main.js"'
        type: string
        required: false
        default: 'src/lib/main.js'
      testFile:
        description: 'The test file. e.g. "tests/unit/main.test.js"'
        type: string
        required: false
        default: 'tests/unit/main.test.js'
      readmeFile:
        description: 'The README file. e.g. "README.md"'
        type: string
        required: false
        default: 'README.md'
      missionFile:
        description: 'The MISSION statement file. e.g. "MISSION.md"'
        type: string
        required: false
        default: 'MISSION.md'
      contributingFile:
        description: 'The CONTRIBUTING file. e.g. "CONTRIBUTING.md"'
        type: string
        required: false
        default: 'CONTRIBUTING.md'
      dependenciesFile:
        description: 'The dependencies file. e.g. "package.json"'
        type: string
        required: false
        default: 'package.json'
      buildScript:
        description: 'The script must be runnable as: `npm ci ; <script>` and succeed with a zero exit code. e.g. `npm run build`'
        type: string
        required: false
        default: 'echo "No build script specified."'
      testScript:
        description: 'The script must be runnable as: `npm ci ; <script>` and succeed with a zero exit code. e.g. `npm test`'
        type: string
        required: false
        default: 'npm test'
      mainScript:
        description: 'The script must be runnable as: `npm ci ; <script>` and succeed with a zero exit code. e.g. `npm run start`'
        type: string
        required: false
        default: 'npm run start'
      mainScriptTimeout:
        description: 'The timeout for the main script. e.g. "5m"'
        type: string
        required: false
        default: '5m'
      testScriptTimeout:
        description: 'The timeout for the test script. e.g. "5m"'
        type: string
        required: false
        default: '5m'
      buildScriptTimeout:
        description: 'The timeout for the build script. e.g. "5m"'
        type: string
        required: false
        default: '5m'
      model:
        description: 'The OpenAI model to use. e.g. "o3-mini"'
        type: string
        required: false
        default: ${{ vars.CHATGPT_API_MODEL || 'o3-mini' }}
      npmAuthOrganisation:
        description: 'The GitHub organisation to authenticate with for npm. e.g. "xn-intenton-z2a"'
        type: string
        required: false
        default: ''
      gitUserEmail:
        description: 'The email to use for git commits. e.g. "action@github.com"'
        type: string
        required: false
        default: 'action@github.com'
      gitUserName:
        description: 'The name to use for git commits. e.g. "GitHub Actions[bot]"'
        type: string
        required: false
        default: 'GitHub Actions[bot]'
      s3BucketUrl:
        description: 'The S3 bucket URL with prefix to use. e.g. "s3://my-bucket/prefix"'
        type: string
        required: false
        default: ''
      iamRoleArn:
        description: 'The ARN of the IAM role to assume. e.g. "arn:aws:iam::123456789012:role/my-role"'
        type: string
        required: false
        default: ''
    secrets:
      PERSONAL_ACCESS_TOKEN:
        required: false
      CHATGPT_API_SECRET_KEY:
        required: true
    outputs:
      documentName:
        value: ${{ jobs.maintain-library.outputs.documentName }}
      documentSummary:
        value: ${{ jobs.maintain-library.outputs.documentSummary }}
      crawlSummary:
        value: ${{ jobs.maintain-library.outputs.crawlSummary }}

jobs:
  maintain-library:
    runs-on: ubuntu-latest

    env:
      document: ${{ inputs.document || '' }}
      libraryDir: ${{ inputs.libraryDir || 'library/' }}
      sourcesFile: ${{ inputs.sourcesFile || 'SOURCES.md' }}
      documentsLimit: ${{ inputs.documentsLimit || '3' }}
      summaryWordLimit: ${{ inputs.summaryWordLimit || '10000' }}
      featuresDir: ${{ inputs.featuresDir || 'features/' }}
      target: ${{ inputs.target || 'src/lib/main.js' }}
      testFile: ${{ inputs.testFile || 'tests/unit/main.test.js' }}
      readmeFile: ${{ inputs.readmeFile || 'README.md' }}
      missionFile: ${{ inputs.missionFile || 'MISSION.md' }}
      contributingFile: ${{ inputs.contributingFile || 'CONTRIBUTING.md' }}
      dependenciesFile: ${{ inputs.dependenciesFile || 'package.json' }}
      buildScript: ${{ inputs.buildScript || 'npm run build' }}
      testScript: ${{ inputs.testScript || 'npm test' }}
      mainScript: ${{ inputs.mainScript || 'npm run start' }}
      mainScriptTimeout: ${{ inputs.mainScriptTimeout || '5m' }}
      testScriptTimeout: ${{ inputs.testScriptTimeout || '5m' }}
      buildScriptTimeout: ${{ inputs.buildScriptTimeout || '5m' }}
      model: ${{ inputs.model || vars.CHATGPT_API_MODEL || 'o3-mini' }}
      npmAuthOrganisation: ${{ inputs.npmAuthOrganisation || '' }}
      gitUserEmail: ${{ inputs.gitUserEmail || 'action@github.com' }}
      gitUserName: ${{ inputs.gitUserName || 'GitHub Actions[bot]' }}
      chatgptApiSecretKey: ${{ secrets.CHATGPT_API_SECRET_KEY }}
      s3BucketUrl: ${{ inputs.s3BucketUrl || '' }}
      iamRoleArn: ${{ inputs.iamRoleArn || '' }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Check GitHub authentication
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: |
          curl --include --header "Authorization: token ${{ secrets.PERSONAL_ACCESS_TOKEN || secrets.GITHUB_TOKEN }}" https://api.github.com/user

      - name: Set up .npmrc
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: |
          echo "${{ env.npmAuthOrganisation }}:registry=https://npm.pkg.github.com" >> .npmrc
          echo "//npm.pkg.github.com/:_authToken=${{ secrets.PERSONAL_ACCESS_TOKEN || secrets.GITHUB_TOKEN }}" >> .npmrc
          echo "always-auth=true" >> .npmrc

      - run: npm ci

      - name: Install web crawling dependencies
        run: npm install axios cheerio

      - name: List current features
        id: features
        shell: bash
        run: |
          output=$(find "${{ env.featuresDir }}" -maxdepth 1 -type f -name '*.md' -print -exec echo "# {}" \; -exec cat {} \; 2>&1 || echo 'none')
          echo "output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$output" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "output=${output}"

      - name: List rejected feature named
        id: rejectedFeatures
        shell: bash
        run: |
          mkdir -p "${{ env.featuresDir }}/rejects"
          output=$(ls -1 "${{ env.featuresDir }}/rejects" | sed 's/\.md//' | xargs echo )
          echo "output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$output" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "output=${output}"

      - name: List dependencies
        id: list
        shell: bash
        run: |
          output=$(npm list 2>&1)
          echo "output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$output" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "output=${output}"

      - name: Build project
        id: build
        shell: bash
        run: |
          output=$(timeout ${{ env.buildScriptTimeout }} ${{ env.buildScript }} 2>&1)
          echo "output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$output" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "output=${output}"

      - name: Tear down .npmrc
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: rm -f .npmrc

      - name: Run tests
        id: test
        shell: bash
        run: |
          output=$(timeout ${{ env.testScriptTimeout }} ${{ env.testScript }} 2>&1)
          echo "output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$output" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "output=${output}"

      - name: Run main
        id: main
        shell: bash
        run: |
          output=$(timeout ${{ env.mainScriptTimeout }} ${{ env.mainScript }} 2>&1)
          echo "output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$output" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "output=${output}"

      - name: maintain-library
        id: maintain-library
        uses: actions/github-script@v7
        env:
          currentFeatures: ${{ steps.features.outputs.output }}
          rejectedFeatures: ${{ steps.rejectedFeatures.outputs.output }}
          dependenciesListOutput: ${{ steps.list.outputs.output }}
          buildOutput: ${{ steps.build.outputs.output }}
          testOutput: ${{ steps.test.outputs.output }}
          mainOutput: ${{ steps.main.outputs.output }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const document = process.env.document;
            const libraryDir = process.env.libraryDir;
            const sourcesFile = process.env.sourcesFile;
            const documentsLimit = process.env.documentsLimit;
            const featuresDir = process.env.featuresDir;
            const currentFeatures = process.env.currentFeatures;
            const rejectedFeatures = process.env.rejectedFeatures;
            const target = process.env.target;
            const testFile = process.env.testFile;
            const readmeFile = process.env.readmeFile;
            const missionFile = process.env.missionFile;
            const contributingFile = process.env.contributingFile;
            const dependenciesFile = process.env.dependenciesFile;
            const model = process.env.model;
            const apiKey = process.env.chatgptApiSecretKey;
            const dependenciesListOutput = process.env.dependenciesListOutput;
            const buildScript = process.env.buildScript;
            const buildOutput = process.env.buildOutput;
            const testScript = process.env.testScript;
            const testOutput = process.env.testOutput;
            const mainScript = process.env.mainScript;
            const mainOutput = process.env.mainOutput;
            const summaryWordLimit = process.env.summaryWordLimit;

            const fs = require('fs');
            const path = require('path');
            const OpenAI = require('openai').default;
            const { z } = require('zod');
            const axios = require('axios');
            const cheerio = require('cheerio');
            require('dotenv').config();

            if (!apiKey) { 
              core.setFailed("Missing CHATGPT_API_SECRET_KEY"); 
            }
            const openai = new OpenAI({ apiKey });

            core.info(`document: "${document}"`);
            core.info(`libraryDir: "${libraryDir}"`);
            core.info(`sourcesFile: "${sourcesFile}"`);
            core.info(`featuresDir: "${featuresDir}"`);
            core.info(`currentFeatures: "${currentFeatures}"`);
            core.info(`target: "${target}"`);
            core.info(`testFile: "${testFile}"`);
            core.info(`readmeFile: "${readmeFile}"`);
            core.info(`missionFile: "${missionFile}"`);
            core.info(`contributingFile: "${contributingFile}"`);
            core.info(`dependenciesFile: "${dependenciesFile}"`);

            // Load the sources file
            const sourcesFileContent = fs.readFileSync(sourcesFile, 'utf8');
            core.info(`Sources file '${sourcesFile}' has been loaded (length ${sourcesFileContent.length}).`);

            const sourceFileContent = fs.readFileSync(target, 'utf8');
            const testFileContent = fs.readFileSync(testFile, 'utf8');
            const readmeFileContent = fs.readFileSync(readmeFile, 'utf8');
            const missionFileContent = fs.readFileSync(missionFile, 'utf8');
            const contributingFileContent = fs.readFileSync(contributingFile, 'utf8');
            const dependenciesFileContent = fs.readFileSync(dependenciesFile, 'utf8');
            core.info(`Target file '${target}' has been loaded (length ${sourceFileContent.length}).`);
            core.info(`Test file '${testFile}' has been loaded (length ${testFileContent.length}).`);
            core.info(`Readme file '${readmeFile}' has been loaded (length ${readmeFileContent.length}).`);
            core.info(`Dependencies file '${dependenciesFile}' has been loaded (length ${dependenciesFileContent.length}).`);

            // Get list of existing library documents
            let existingDocuments = [];
            try {
              if (fs.existsSync(libraryDir)) {
                existingDocuments = fs.readdirSync(libraryDir)
                  .filter(file => file.endsWith('.md'))
                  .map(file => path.join(libraryDir, file));
              } else {
                fs.mkdirSync(libraryDir, { recursive: true });
              }
            } catch (e) {
              core.warning(`Error reading library directory: ${e.message}`);
              fs.mkdirSync(libraryDir, { recursive: true });
            }

            core.info(`Found ${existingDocuments.length} existing documents in ${libraryDir}`);

            // Parse the sources file to extract sections
            // Each source entry starts with a level 1 heading (#) and contains URL, description, and license
            const sourceEntries = [];
            const sourceLines = sourcesFileContent.split('\n');
            let currentEntry = null;

            for (let i = 0; i < sourceLines.length; i++) {
              const line = sourceLines[i];

              if (line.startsWith('# ')) {
                // If we have a current entry, push it to the array
                if (currentEntry) {
                  sourceEntries.push(currentEntry);
                }

                // Start a new entry
                currentEntry = {
                  name: line.substring(2).trim(),
                  url: '',
                  description: '',
                  license: '',
                  content: line + '\n'
                };
              } else if (currentEntry) {
                currentEntry.content += line + '\n';

                if (line.startsWith('## http')) {
                  currentEntry.url = line.substring(3).trim();
                } else if (line.startsWith('## ')) {
                  currentEntry.license = line.substring(3).trim();
                } else if (line && !line.startsWith('#')) {
                  if (!currentEntry.description) {
                    currentEntry.description = line.trim();
                  }
                }
              }
            }

            // Don't forget the last entry
            if (currentEntry) {
              sourceEntries.push(currentEntry);
            }

            core.info(`Parsed ${sourceEntries.length} source entries from ${sourcesFile}`);

            // Function to crawl a URL and its links for 10 seconds
            async function crawlUrl(url, maxTime = 10000) {
              if (!url || !url.startsWith('http')) {
                return { content: '', links: [], dataSize: 0, error: 'Invalid URL' };
              }

              const startTime = Date.now();
              const visitedUrls = new Set();
              const result = {
                content: '',
                links: [],
                dataSize: 0,
                error: null
              };

              try {
                // Queue of URLs to visit
                const urlQueue = [url];

                while (urlQueue.length > 0 && (Date.now() - startTime) < maxTime) {
                  const currentUrl = urlQueue.shift();

                  // Skip if already visited
                  if (visitedUrls.has(currentUrl)) {
                    continue;
                  }

                  visitedUrls.add(currentUrl);

                  try {
                    core.info(`Crawling URL: ${currentUrl}`);
                    const response = await axios.get(currentUrl, { 
                      timeout: 5000,
                      headers: {
                        'User-Agent': 'Mozilla/5.0 (compatible; AgenticLibBot/1.0; +https://github.com/xn-intenton-z2a/agentic-lib)'
                      }
                    });

                    const contentType = response.headers['content-type'] || '';

                    // Only process HTML content
                    if (contentType.includes('text/html')) {
                      const $ = cheerio.load(response.data);

                      // Extract text content
                      const pageText = $('body').text().replace(/\\s+/g, ' ').trim();
                      result.content += pageText + '\\n\\n';
                      result.dataSize += pageText.length;

                      // Extract links
                      $('a').each((i, link) => {
                        const href = $(link).attr('href');
                        if (href && href.startsWith('http') && !visitedUrls.has(href)) {
                          result.links.push(href);
                          urlQueue.push(href);
                        }
                      });
                    }
                  } catch (error) {
                    core.warning(`Error crawling ${currentUrl}: ${error.message}`);
                  }
                }

                core.info(`Crawling completed. Visited ${visitedUrls.size} URLs. Data size: ${result.dataSize} bytes.`);
                return result;
              } catch (error) {
                core.warning(`Crawling error: ${error.message}`);
                result.error = error.message;
                return result;
              }
            }

            // Randomly select a source entry
            const randomIndex = Math.floor(Math.random() * sourceEntries.length);
            const randomSourceEntry = sourceEntries[randomIndex];
            core.info(`Randomly selected source entry: ${randomSourceEntry.name} (${randomSourceEntry.url})`);

            // generate the document prompt either by using the supplied document prompt or by reviewing the current features and full context
            let prompt = document;
            if (document === 'house choice') {
              prompt = `Please review the current documents in the library and either;
                * add a new document to the repository, or
                * extend an existing document to add a new aspect to it, or
                * update an existing document to bring it to a high standard matching other documents in the repository.
                The document name should either be a current document name or be supplied with a summary which is distinct from any other document in the repository.
              `;
            }

            // Crawl the URL from the randomly selected source entry
            core.info(`Crawling URL: ${randomSourceEntry.url}`);
            let crawlResult = { content: '', links: [], dataSize: 0, error: 'Not crawled' };
            try {
              crawlResult = await crawlUrl(randomSourceEntry.url);
            } catch (error) {
              core.warning(`Error during crawling: ${error.message}`);
            }

            const chatGptPrompt = `
            Please generate a document summary based on the supplied prompt and project files.
            The summary should be thorough but concise and take out bravado and speculation, include a critical assessment of the work and also of the authority of the source.
            Ensure any content generated would be suitable for publication directly on the internet, in particular do not include private information about people.
            The word limit is ${summaryWordLimit} words but it not just a limit, it's a target to aim for provided the content is justified and not padded.
            Apply an internal peer review to ensure the summary meets all of, a useful internal tech write up and a high academic for an informal paper providing a digest and insight into a topic.
            Where possible find situations to support or refute statements made in the originally crawled material.
            Use a journalistic style in the writing and include a glossary of any obscure terms at the end of the summary.
            Before adding a new document ensure that this document is distinct from any other document in the library, otherwise update an existing document.
            The document name should be one or two words in SCREAMING_SNAKECASE.

            You should extract a section from the sources file to create the document. Each document should contain:
            1. The original content from the source section in SOURCES.md
            2. A summary of the crawled content from the URL (limited to ${summaryWordLimit} words)
            3. The date when the content was retrieved (current date)
            4. Attribution information and data size obtained during crawling

            Consider the following when refining your response:
            * Document prompt details
            * Current feature names and specifications in the repository
            * Rejected feature names
            * Source file content
            * Test file content
            * README file content
            * MISSION file content
            * Contributing file content
            * Dependencies file content
            * Dependency list
            * Build output
            * Test output
            * Main execution output
            * Available source entries from SOURCES.md

            Document prompt:
            DOCUMENT_PROMPT_START
            ${prompt}
            DOCUMENT_PROMPT_END            

            Current feature names and specifications:
            CURRENT_FEATURES_START
            ${currentFeatures}
            CURRENT_FEATURES_END

            Rejected feature names:
            REJECTED_FEATURES_START
            ${rejectedFeatures}
            REJECTED_FEATURES_END

            Source file: ${target}
            SOURCE_FILE_START
            ${sourceFileContent}
            SOURCE_FILE_END

            Test file: ${testFile}
            TEST_FILE_START
            ${testFileContent}
            TEST_FILE_END

            README file: ${readmeFile}
            README_FILE_START
            ${readmeFileContent}
            README_FILE_END

            MISSION file: ${missionFile}
            MISSION_FILE_START
            ${missionFileContent}
            MISSION_FILE_END

            Contributing file: ${contributingFile}
            CONTRIBUTING_FILE_START
            ${contributingFileContent}
            CONTRIBUTING_FILE_END

            Dependencies file: ${dependenciesFile}
            DEPENDENCIES_FILE_START
            ${dependenciesFileContent}
            DEPENDENCIES_FILE_END

            Dependencies list from command: npm list
            DEPENDENCIES_LIST_START
            ${dependenciesListOutput}
            DEPENDENCIES_LIST_END    

            Build output from command: ${buildScript}
            TEST_OUTPUT_START
            ${buildOutput}
            TEST_OUTPUT_END      

            Test output from command: ${testScript}
            TEST_OUTPUT_START
            ${testOutput}
            TEST_OUTPUT_END            

            Main output from command: ${mainScript}
            MAIN_OUTPUT_START
            ${mainOutput}
            MAIN_OUTPUT_END

            Available source entries from SOURCES.md (${sourceEntries.length} entries):
            SOURCES_ENTRIES_START
            ${sourceEntries.map((entry, index) => `Entry ${index + 1}: ${entry.name} (${entry.url})`).join('\n')}
            SOURCES_ENTRIES_END

            Crawled content from URL (${randomSourceEntry.url}):
            CRAWLED_CONTENT_START
            Data Size: ${crawlResult.dataSize} bytes
            Links Found: ${crawlResult.links.length}
            Error: ${crawlResult.error || 'None'}
            Content Preview: ${crawlResult.content.substring(0, 1000)}...
            CRAWLED_CONTENT_END

            Answer strictly with a JSON object following this schema:
            {
              "documentName": "The document name as one or two words in SCREAMING_SNAKECASE.",
              "documentSummary": "The document summary as multiline markdown with a few level 1 headings, including the original content from the source and the date when the content was retrieved.",
              "documentNamesToBeDeleted": "The comma separated list of document names to be deleted or 'none' if no document is to be deleted.",
              "sourceEntryIndex": "The index (1-based) of the source entry used for this document."
            }
            Ensure valid JSON.
            `;

            const ResponseSchema = z.object({ 
              documentName: z.string(), 
              documentSummary: z.string(), 
              documentNamesToBeDeleted: z.string(),
              sourceEntryIndex: z.string(),
              crawlSummary: z.string()
            });

            // Define the function schema for function calling
            const tools = [{
              type: "function",
              function: {
                name: "generate_document",
                description: "Elaborate on the supplied prompt and project files to create the documentName, documentSummary, and crawlSummary of a repository document, and the document names to be deleted. Return an object with documentName (string), documentSummary (string), documentNamesToBeDeleted (string), sourceEntryIndex (string), and crawlSummary (string).",
                parameters: {
                  type: "object",
                  properties: {
                    documentName: { type: "string", description: "The document name as one or two words in SCREAMING_SNAKECASE." },
                    documentSummary: { type: "string", description: "The document summary as multiline markdown with a few level 1 headings, including the original content from the source and the date when the content was retrieved." },
                    documentNamesToBeDeleted: { type: "string", description: "The comma separated list of document names to be deleted or 'none' if no document is to be deleted." },
                    sourceEntryIndex: { type: "string", description: "The index (1-based) of the source entry used for this document." },
                    crawlSummary: { type: "string", description: "A summary of the crawled content from the URL, limited to the specified word limit." }
                  },
                  required: ["documentName", "documentSummary", "documentNamesToBeDeleted", "sourceEntryIndex", "crawlSummary"],
                  additionalProperties: false
                },
                strict: true
              }
            }];

            // Call OpenAI using function calling format
            const response = await openai.chat.completions.create({
              model,
              messages: [
                { role: "system", content: "You are maintaining a library set by providing expert contemporary insight into both the product market and you will perform a detailed analysis of the current state of the repository and current document set in search of value opportunities and unique selling points. Answer strictly with a JSON object following the provided function schema." },
                { role: "user", content: chatGptPrompt }
              ],
              tools: tools
            });

            let result;
            if (response.choices[0].message.tool_calls && response.choices[0].message.tool_calls.length > 0) {
              try {
                result = JSON.parse(response.choices[0].message.tool_calls[0].function.arguments);
              } catch (e) {
                core.setFailed(`Failed to parse function call arguments: ${e.message}`);
              }
            } else if (response.choices[0].message.content) {
              try {
                result = JSON.parse(response.choices[0].message.content);
              } catch (e) {
                core.setFailed(`Failed to parse response content: ${e.message}`);
              }
            } else {
              core.setFailed("No valid response received from OpenAI.");
            }

            try {
              const parsed = ResponseSchema.parse(result);
              core.setOutput("documentName", parsed.documentName);
              core.setOutput("documentSummary", parsed.documentSummary);
              core.setOutput("documentNamesToBeDeleted", parsed.documentNamesToBeDeleted);
              core.setOutput("sourceEntryIndex", parsed.sourceEntryIndex);
              core.setOutput("crawlSummary", parsed.crawlSummary);
              core.info(`documentName: "${parsed.documentName}"`);
              core.info(`documentSummary: "${parsed.documentSummary}"`);
              core.info(`documentNamesToBeDeleted: "${parsed.documentNamesToBeDeleted}"`);
              core.info(`sourceEntryIndex: "${parsed.sourceEntryIndex}"`);
              core.info(`crawlSummary: "${parsed.crawlSummary}"`);

              // Get the source entry used for this document
              const sourceEntryIndex = parseInt(parsed.sourceEntryIndex, 10) - 1;
              let sourceEntry = null;
              if (sourceEntryIndex >= 0 && sourceEntryIndex < sourceEntries.length) {
                sourceEntry = sourceEntries[sourceEntryIndex];
                core.info(`Using source entry: ${sourceEntry.name} (${sourceEntry.url})`);
              } else {
                core.warning(`Invalid source entry index: ${parsed.sourceEntryIndex}`);
              }

              // Add the current date to the document summary
              const now = new Date();
              const dateStr = now.toISOString().split('T')[0]; // YYYY-MM-DD format

              // Create the document content
              let documentContent = parsed.documentSummary;

              // If we have a valid source entry, add the original content, crawl summary, and attribution fields
              if (sourceEntry) {
                const crawlDate = new Date();
                const crawlDateStr = crawlDate.toISOString();

                documentContent = `# ${parsed.documentName}\n\n`;
                documentContent += `## Original Source\n${sourceEntry.name}\n${sourceEntry.url}\n\n`;
                documentContent += `## Content\n${sourceEntry.content}\n\n`;
                documentContent += `## Crawl Summary\n${parsed.crawlSummary}\n\n`;
                documentContent += `## Attribution\n`;
                documentContent += `- Source: ${sourceEntry.name}\n`;
                documentContent += `- URL: ${sourceEntry.url}\n`;
                documentContent += `- License: ${sourceEntry.license || 'Unknown'}\n`;
                documentContent += `- Crawl Date: ${crawlDateStr}\n`;
                documentContent += `- Data Size: ${crawlResult.dataSize} bytes\n`;
                documentContent += `- Links Found: ${crawlResult.links.length}\n\n`;
                documentContent += `## Retrieved\n${dateStr}\n`;
              } else {
                // If no valid source entry, just add the date at the end
                documentContent += `\n\n_Retrieved: ${dateStr}_\n`;
              }

              // Save the document to a file
              const documentName = parsed.documentName.replace(/ /g, "_").toUpperCase();
              const documentFilePath = path.join(libraryDir, `${documentName}.md`);
              try {
                fs.mkdirSync(libraryDir, { recursive: true });
                fs.writeFileSync(documentFilePath, documentContent);
                core.info(`Document saved to ${documentFilePath}`);
              } catch (e) {
                core.setFailed(`Failed to save document: ${e.message}`);
              }

              // Delete any documents that should be deleted
              const documentNamesToBeDeleted = parsed.documentNamesToBeDeleted
                .split(',')
                .map(name => name.trim())
                .filter(name => name && name !== 'none');

              for (const name of documentNamesToBeDeleted) {
                const documentNameToDelete = name.replace(/ /g, "_").toUpperCase().replace(/\.md$/, "") + ".md";
                const documentPathToDelete = path.join(libraryDir, documentNameToDelete);
                try {
                  if (fs.existsSync(documentPathToDelete)) {
                    core.info(`Deleting document: ${documentPathToDelete}`);
                    fs.unlinkSync(documentPathToDelete);
                  } else {
                    core.warning(`Document not found for deletion: ${documentPathToDelete}`);
                  }
                } catch (e) {
                  core.warning(`Error deleting document ${documentPathToDelete}: ${e.message}`);
                }
              }

            } catch (e) {
              core.setFailed(`Failed to parse ChatGPT response: ${e.message}`);
            }

            core.setOutput("response", JSON.stringify(response));
            core.setOutput("usage", JSON.stringify(response.usage));
            core.info(`response: "${JSON.stringify(response)}"`);
            core.info(`usage: "${JSON.stringify(response.usage)}"`);

      - name: Commit changes
        id: commit
        continue-on-error: true
        run: |
          git config --local user.email '${{ env.gitUserEmail }}'
          git config --local user.name '${{ env.gitUserName }}'
          git status -v
          git add -v --all '${{ env.libraryDir }}'
          git commit -m 'Maintain ${{ steps.maintain-library.outputs.documentName }}'
          git status -v
          git push -v origin ${{ github.ref }}
          git status -v

    outputs:
      documentName: ${{ steps.maintain-library.outputs.documentName }}
      documentSummary: ${{ steps.maintain-library.outputs.documentSummary }}
      crawlSummary: ${{ steps.maintain-library.outputs.crawlSummary }}
