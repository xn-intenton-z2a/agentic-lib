# .github/workflows/wfr-completion-maintain-library.yml

#
# agentic-lib
# Copyright (C) 2025 Polycode Limited
#
# This file is part of agentic-lib.
#
# agentic-lib is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License v3.0 (GPL‑3).
# along with this program. If not, see <https://www.gnu.org/licenses/>.
#
# IMPORTANT: Any derived work must include the following attribution:
# "This work is derived from https://github.com/xn-intenton-z2a/agentic-lib"
#

name: ∞ maintain-library

on:
  workflow_call:
    inputs:
      libraryDocumentsPath:
        description: 'The directory to create/update the documents in. e.g. "library/"'
        type: string
        required: false
        default: 'library/'
      librarySourcesFilepath:
        description: 'The file containing the sources to extract sections from. e.g. "SOURCES.md"'
        type: string
        required: false
        default: 'SOURCES.md'
      libraryDocumentsLimit:
        description: 'The maximum number of documents to have in the library to create. e.g. "3"'
        type: string
        required: false
        default: '3'
      model:
        description: 'The OpenAI model to use. e.g. "o3-mini"'
        type: string
        required: false
        default: ${{ vars.CHATGPT_API_MODEL || 'o4-mini' }}
      npmAuthOrganisation:
        description: 'The GitHub organisation to authenticate with for npm. e.g. "xn-intenton-z2a"'
        type: string
        required: false
        default: ''
      gitUserEmail:
        description: 'The email to use for git commits. e.g. "action@github.com"'
        type: string
        required: false
        default: 'action@github.com'
      gitUserName:
        description: 'The name to use for git commits. e.g. "GitHub Actions[bot]"'
        type: string
        required: false
        default: 'GitHub Actions[bot]'
      s3BucketUrl:
        description: 'The S3 bucket URL with prefix to use. e.g. "s3://my-bucket/prefix"'
        type: string
        required: false
        default: ''
      iamRoleArn:
        description: 'The ARN of the IAM role to assume. e.g. "arn:aws:iam::123456789012:role/my-role"'
        type: string
        required: false
        default: ''
      promptFilepath:
        description: 'The file containing the prompt text. e.g. ".github/agents/agent-ready-issue.md"'
        type: string
        required: true
      agentConfigContent:
        description: 'The content of the agent config file. e.g. Yaml read from "./github/agents/agent-config.yaml"'
        type: string
        required: false
        default: ''
      writeableFilepaths:
        description: 'Semicolon-separated list of file paths that can be written by the workflow. e.g. "elaborator-sandbox/SOURCES.md;elaborator-sandbox/library/;engineer-sandbox/features/;engineer-sandbox/source/;engineer-sandbox/tests/;engineer-sandbox/docs/"'
        type: string
        required: false
        default: ''
    secrets:
      PERSONAL_ACCESS_TOKEN:
        required: false
      CHATGPT_API_SECRET_KEY:
        required: true
    outputs:
      documentName:
        value: ${{ jobs.maintain-library.outputs.documentName }}
      documentDetailedDigest:
        value: ${{ jobs.maintain-library.outputs.documentDetailedDigest }}
      crawlSummary:
        value: ${{ jobs.maintain-library.outputs.crawlSummary }}
      normalisedExtract:
        value: ${{ jobs.maintain-library.outputs.normalisedExtract }}
      supplementaryDetails:
        value: ${{ jobs.maintain-library.outputs.supplementaryDetails }}
      referenceDetails:
        value: ${{ jobs.maintain-library.outputs.referenceDetails }}
      informationDenseExtract:
        value: ${{ jobs.maintain-library.outputs.informationDenseExtract }}

jobs:
  maintain-library:
    runs-on: ubuntu-latest

    env:
      document: ${{ inputs.document || '' }}
      libraryDocumentsPath: ${{ inputs.libraryDocumentsPath || 'library/' }}
      librarySourcesFilepath: ${{ inputs.librarySourcesFilepath || 'SOURCES.md' }}
      libraryDocumentsLimit: ${{ inputs.libraryDocumentsLimit || '3' }}
      crawlResultCharacterLimit: 50000
      model: ${{ inputs.model || vars.CHATGPT_API_MODEL || 'o4-mini' }}
      npmAuthOrganisation: ${{ inputs.npmAuthOrganisation || '' }}
      gitUserEmail: ${{ inputs.gitUserEmail || 'action@github.com' }}
      gitUserName: ${{ inputs.gitUserName || 'GitHub Actions[bot]' }}
      chatgptApiSecretKey: ${{ secrets.CHATGPT_API_SECRET_KEY }}
      promptFilepath: ${{ inputs.promptFilepath || '.github/agents/agent-maintain-library.md' }}
      s3BucketUrl: ${{ inputs.s3BucketUrl || '' }}
      iamRoleArn: ${{ inputs.iamRoleArn || '' }}
      writeableFilepaths: ${{ inputs.writeableFilepaths || '["features/*", "library/*", "src/lib/*", "tests/unit/*", "package.json", "README.md"]' }}
      agentConfigContent: ${{ inputs.agentConfigContent || '' }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Check GitHub authentication
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: |
          curl --include --header "Authorization: token ${{ secrets.PERSONAL_ACCESS_TOKEN || secrets.GITHUB_TOKEN }}" https://api.github.com/user

      - name: Set up .npmrc
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: |
          echo "${{ env.npmAuthOrganisation }}:registry=https://npm.pkg.github.com" >> .npmrc
          echo "//npm.pkg.github.com/:_authToken=${{ secrets.PERSONAL_ACCESS_TOKEN || secrets.GITHUB_TOKEN }}" >> .npmrc
          echo "always-auth=true" >> .npmrc

      - run: npm ci

      - name: Install web crawling dependencies
        run: npm install axios cheerio

      - name: Tear down .npmrc
        if: ${{ env.npmAuthOrganisation != '' }}
        shell: bash
        run: rm -f .npmrc

      - name: maintain-library
        id: maintain-library
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const document = process.env.document;
            const libraryDocumentsPath = process.env.libraryDocumentsPath;
            const librarySourcesFilepath = process.env.librarySourcesFilepath;
            const libraryDocumentsLimit = process.env.libraryDocumentsLimit;
            const crawlResultCharacterLimit = process.env.crawlResultCharacterLimit;
            const model = process.env.model;
            const apiKey = process.env.chatgptApiSecretKey;
            const promptFilepath = process.env.promptFilepath;
            const dependenciesListOutput = process.env.dependenciesListOutput;
            const agentConfigContent = process.env.agentConfigContent;
            const writeableFilepaths = process.env.writeableFilepaths;

            core.info(`document: "${document}"`);
            core.info(`libraryDocumentsPath: "${libraryDocumentsPath}"`);
            core.info(`librarySourcesFilepath: "${librarySourcesFilepath}"`);
            core.info(`libraryDocumentsLimit: "${libraryDocumentsLimit}"`);
            core.info(`crawlResultCharacterLimit: "${crawlResultCharacterLimit}"`);
            core.info(`model: "${model}"`);
            core.info(`apiKey: "${apiKey}"`);
            core.info(`promptFilepath: "${promptFilepath}"`);
            core.info(`dependenciesListOutput: "${dependenciesListOutput}"`);
            core.info(`agentConfigContent: "${agentConfigContent}"`);

            function isWriteable(filepath) {
              const patterns = writeableFilepaths.split(';').map(file => file.trim());
              core.info(`Checking if writing to filepath '${filepath}' is allowed in writeableFilepaths: ${patterns}`);
              return patterns.some(pattern => (
                minimatch(filepath, pattern) || 
                minimatch(filepath, `${pattern}*`) || 
                minimatch(filepath, `${pattern}**`) || 
                minimatch(filepath, `${pattern}/**`)
              ));
            }

            const fs = require('fs');
            const path = require('path');
            const OpenAI = require('openai').default;
            const { z } = require('zod');
            const axios = require('axios');
            const cheerio = require('cheerio');
            const { minimatch } = require('minimatch');
            require('dotenv').config();

            // Function to check if a file path matches any of the writeable filepaths
            function isFilePathAllowed(filePath, writeableFilepaths) {
              // Ensure writeableFilepaths is an array
              const patterns = Array.isArray(writeableFilepaths) ? writeableFilepaths : String(writeableFilepaths).split(',');
              core.info(`Checking if file path '${filePath}' is allowed in writeableFilepaths: ${patterns}`);
              return patterns.some(pattern => (
                minimatch(filePath, pattern.trim()) || 
                minimatch(filePath, `${pattern.trim()}*`) || 
                minimatch(filePath, `${pattern.trim()}**`) || 
                minimatch(filePath, `${pattern.trim()}/**`)
              ));
            }

            if (!apiKey) { 
              core.setFailed("Missing CHATGPT_API_SECRET_KEY"); 
            }
            const openai = new OpenAI({ apiKey });

            // Load the sources files (support for multiple SOURCES*.md files)
            let librarySourcesFilepathContent = '';
            let promptContent = '';

            // Read the prompt file
            promptContent = fs.readFileSync(promptFilepath, 'utf8');
            core.info(`Prompt file '${promptFilepath}' has been loaded (length ${promptContent.length}).`);

            // Load a single sources file
            librarySourcesFilepathContent = fs.readFileSync(librarySourcesFilepath, 'utf8');
            core.info(`Sources file '${librarySourcesFilepath}' has been loaded (length ${librarySourcesFilepathContent.length}).`);

            // Get list of existing library documents
            let existingDocuments = [];
            try {
              if (fs.existsSync(libraryDocumentsPath)) {
                existingDocuments = fs.readdirSync(libraryDocumentsPath)
                  .filter(file => file.endsWith('.md'))
                  .map(file => path.join(libraryDocumentsPath, file));
              } else {
                fs.mkdirSync(libraryDocumentsPath, { recursive: true });
              }
            } catch (e) {
              core.warning(`Error reading library directory: ${e.message}`);
              fs.mkdirSync(libraryDocumentsPath, { recursive: true });
            }

            core.info(`Found ${existingDocuments.length} existing documents in ${libraryDocumentsPath}`);

            // Parse the sources file to extract sections
            // Each source entry starts with a level 1 heading (#) and contains URL, description, and license
            const sourceEntries = [];
            const sourceLines = librarySourcesFilepathContent.split('\n');
            let currentEntry = null;

            for (let i = 0; i < sourceLines.length; i++) {
              const line = sourceLines[i];

              if (line.startsWith('# ')) {
                // If we have a current entry, push it to the array
                if (currentEntry) {
                  sourceEntries.push(currentEntry);
                }

                // Start a new entry
                currentEntry = {
                  name: line.substring(2).trim(),
                  url: '',
                  description: '',
                  license: '',
                  content: line + '\n'
                };
              } else if (currentEntry) {
                currentEntry.content += line + '\n';

                if (line.startsWith('## http')) {
                  currentEntry.url = line.substring(3).trim();
                } else if (line.startsWith('## ')) {
                  currentEntry.license = line.substring(3).trim();
                } else if (line && !line.startsWith('#')) {
                  if (!currentEntry.description) {
                    currentEntry.description = line.trim();
                  }
                }
              }
            }

            // Don't forget the last entry
            if (currentEntry) {
              sourceEntries.push(currentEntry);
            }

            core.info(`Parsed ${sourceEntries.length} source entries from ${librarySourcesFilepath}`);

            // Function to crawl a URL and its links for 16 seconds
            async function crawlUrl(url, maxTime = 16000) {
              if (!url || !url.startsWith('http')) {
                return { content: '', links: [], dataSize: 0, error: 'Invalid URL' };
              }

              const startTime = Date.now();
              const visitedUrls = new Set();
              const result = {
                content: '',
                links: [],
                dataSize: 0,
                error: null
              };

              try {
                // Queue of URLs to visit
                const urlQueue = [url];

                while (urlQueue.length > 0 && (Date.now() - startTime) < maxTime) {
                  const currentUrl = urlQueue.shift();

                  // Skip if already visited
                  if (visitedUrls.has(currentUrl)) {
                    continue;
                  }

                  visitedUrls.add(currentUrl);

                  try {
                    core.info(`Crawling URL: ${currentUrl}`);
                    const response = await axios.get(currentUrl, { 
                      timeout: 5000,
                      headers: {
                        'User-Agent': 'Mozilla/5.0 (compatible; AgenticLibBot/1.0; +https://github.com/xn-intenton-z2a/agentic-lib)'
                      }
                    });

                    const contentType = response.headers['content-type'] || '';

                    // Only process HTML content
                    if (contentType.includes('text/html')) {
                      const $ = cheerio.load(response.data);

                      // Extract text content with priority to meaningful content
                      // Remove script, style, and other non-content elements
                      $('script, style, meta, link, noscript, iframe, svg').remove();

                      // Prioritize content from main content areas
                      let mainContent = '';
                      const contentSelectors = ['article', 'main', '.content', '.main', '#content', '#main', '.post', '.entry', '.blog-post'];

                      for (const selector of contentSelectors) {
                        if ($(selector).length > 0) {
                          mainContent += $(selector).text().replace(/\\s+/g, ' ').trim() + '\\n\\n';
                        }
                      }

                      // If no main content found, extract from body
                      if (!mainContent) {
                        // Extract text from paragraphs, headings, lists, and other text elements
                        $('p, h1, h2, h3, h4, h5, h6, li, td, th, div, span, a').each((i, el) => {
                          const text = $(el).text().trim();
                          if (text.length > 20) { // Only include substantial text
                            mainContent += text + '\\n';
                          }
                        });
                      }

                      // If still no content, fall back to body text
                      if (!mainContent) {
                        mainContent = $('body').text().replace(/\\s+/g, ' ').trim();
                      }

                      result.content += mainContent + '\\n\\n';
                      result.dataSize += mainContent.length;

                      // Extract links
                      $('a').each((i, link) => {
                        const href = $(link).attr('href');
                        if (href) {
                          // Convert relative URLs to absolute
                          let absoluteUrl = href;
                          if (!href.startsWith('http')) {
                            try {
                              absoluteUrl = new URL(href, currentUrl).href;
                            } catch (e) {
                              // Skip invalid URLs
                              return;
                            }
                          }

                          // Only follow links on the same domain
                          const currentDomain = new URL(currentUrl).hostname;
                          const linkDomain = new URL(absoluteUrl).hostname;

                          if (linkDomain === currentDomain && !visitedUrls.has(absoluteUrl)) {
                            result.links.push(absoluteUrl);
                            urlQueue.push(absoluteUrl);
                          }
                        }
                      });
                    }
                  } catch (error) {
                    core.warning(`Error crawling ${currentUrl}: ${error.message}`);
                  }
                }

                core.info(`Crawling completed. Visited ${visitedUrls.size} URLs. Data size: ${result.dataSize} bytes.`);
                return result;
              } catch (error) {
                core.warning(`Crawling error: ${error.message}`);
                result.error = error.message;
                return result;
              }
            }

            // Randomly select a source entry
            const randomIndex = Math.floor(Math.random() * sourceEntries.length);
            const randomSourceEntry = sourceEntries[randomIndex];
            core.info(`Randomly selected source entry: ${randomSourceEntry.name} (${randomSourceEntry.url})`);

            // Crawl the URL from the randomly selected source entry
            core.info(`Crawling URL: ${randomSourceEntry.url}`);
            let crawlResult = { content: '', links: [], dataSize: 0, error: 'Not crawled' };
            try {
              crawlResult = await crawlUrl(randomSourceEntry.url);
            } catch (error) {
              core.warning(`Error during crawling: ${error.message}`);
            }

            const chatGptPrompt = `
            ${promptContent}

            Crawled content from URL (${randomSourceEntry.url}):
            CRAWLED_CONTENT_START
            Data Size: ${crawlResult.dataSize} bytes
            Links Found: ${crawlResult.links.length}
            Error: ${crawlResult.error || 'None'}
            Content Preview: ${crawlResult.content.substring(0, crawlResultCharacterLimit)}...
            CRAWLED_CONTENT_END        

            Available source entries from SOURCES.md (${sourceEntries.length} entries):
            SOURCES_ENTRIES_START
            ${sourceEntries.map((entry, index) => `Entry ${index + 1}: ${entry.name} (${entry.url})`).join('\n')}
            SOURCES_ENTRIES_END

            Agent configuration file: ${agentConfigPath}
            AGENT_CONFIG_FILE_START
            ${agentConfigContent}
            AGENT_CONFIG_FILE_END

            Critical: Extract the actual technical content, not summaries or descriptions. Include the complete API specifications with all parameters, return types, and exceptions. Extract the exact SDK method signatures including all arguments and their types. Include full code examples with comments. Document the precise implementation patterns with all steps. List all configuration options with their exact values, defaults, and effects. Provide concrete best practices with implementation code. Include detailed troubleshooting procedures with exact commands and expected outputs. The referenceDetails section must contain the actual technical specifications that developers can directly use without needing to look elsewhere.

            Answer strictly with a JSON object following this schema:
            {
              "documentName": "The document name as one or two words in SCREAMING_SNAKECASE.",
              "documentDetailedDigest": "The actual technical content from the source in multiline markdown with level 1 headings, including exact specifications, method signatures, and configuration details.",
              "documentNamesToBeDeleted": "The comma separated list of document names to be deleted or 'none' if no document is to be deleted.",
              "sourceEntryIndex": "The index (1-based) of the source entry used for this document.",
              "crawlSummary": "The condensed technical details from the crawled content, with actual specifications rather than descriptions of what specifications exist.",
              "normalisedExtract": "The actual technical information extracted from the crawled content, including specific implementation details, exact configuration parameters, and concrete examples. Include a table of contents of technical topics and complete technical details for each topic that can be directly used by developers.",
              "supplementaryDetails": "The actual technical specifications and implementation details that complement the crawled content, including exact parameter values, configuration options, and implementation steps.",
              "referenceDetails": "The complete API specifications with all parameters and return types, full SDK method signatures, actual code examples, exact implementation patterns with all steps, specific configuration options with their values and effects, concrete best practices with implementation code, and detailed troubleshooting procedures with exact commands.",
              "informationDenseExtract": "A highly condensed, information-rich version of the technical content optimized for LLM consumption. This should contain all key technical details in a dense format without redundancy or explanatory text. Focus on facts, specifications, parameters, and code patterns."
            }
            Ensure valid JSON.
            `;

            const ResponseSchema = z.object({ 
              documentName: z.string(), 
              documentDetailedDigest: z.string(), 
              documentNamesToBeDeleted: z.string(),
              sourceEntryIndex: z.string(),
              crawlSummary: z.string(),
              normalisedExtract: z.string(),
              supplementaryDetails: z.string(),
              referenceDetails: z.string(),
              informationDenseExtract: z.string()
            });

            // Define the function schema for function calling
            const tools = [{
              type: "function",
              function: {
                name: "generate_document",
                description: "Extract and condense the actual technical details from the supplied prompt and project files to create a document with specific technical information. Return an object with documentName, documentDetailedDigest (containing actual technical content), documentNamesToBeDeleted, sourceEntryIndex, crawlSummary (with condensed technical details), normalisedExtract (with actual technical information), supplementaryDetails (with actual specifications), and referenceDetails (with complete API specifications).",
                parameters: {
                  type: "object",
                  properties: {
                    documentName: { type: "string", description: "The document name as one or two words in SCREAMING_SNAKECASE." },
                    documentDetailedDigest: { type: "string", description: "The actual technical content from the source in multiline markdown with level 1 headings, including exact specifications, method signatures, and configuration details." },
                    documentNamesToBeDeleted: { type: "string", description: "The comma separated list of document names to be deleted or 'none' if no document is to be deleted." },
                    sourceEntryIndex: { type: "string", description: "The index (1-based) of the source entry used for this document." },
                    crawlSummary: { type: "string", description: "The condensed technical details from the crawled content, with actual specifications rather than descriptions of what specifications exist." },
                    normalisedExtract: { type: "string", description: "The actual technical information extracted from the crawled content, including specific implementation details, exact configuration parameters, and concrete examples. Include a table of contents of technical topics and complete technical details for each topic that can be directly used by developers." },
                    supplementaryDetails: { type: "string", description: "The actual technical specifications and implementation details that complement the crawled content, including exact parameter values, configuration options, and implementation steps." },
                    referenceDetails: { type: "string", description: "The complete API specifications with all parameters and return types, full SDK method signatures, actual code examples, exact implementation patterns with all steps, specific configuration options with their values and effects, concrete best practices with implementation code, and detailed troubleshooting procedures with exact commands." },
                    informationDenseExtract: { type: "string", description: "A highly condensed, information-rich version of the technical content optimized for LLM consumption. This should contain all key technical details in a dense format without redundancy or explanatory text. Focus on facts, specifications, parameters, and code patterns." }
                  },
                  required: ["documentName", "documentDetailedDigest", "documentNamesToBeDeleted", "sourceEntryIndex", "crawlSummary", "normalisedExtract", "supplementaryDetails", "referenceDetails", "informationDenseExtract"],
                  additionalProperties: false
                },
                strict: true
              }
            }];

            // Call OpenAI using function calling format
            const response = await openai.chat.completions.create({
              model,
              messages: [
                { role: "system", content: "You are maintaining a technical library by extracting and condensing actual technical details from source materials. Your task is to provide specific technical information, not summaries or descriptions. Extract complete API specifications, exact method signatures, full code examples, precise implementation patterns, specific configuration options, concrete best practices, and detailed troubleshooting procedures. Answer strictly with a JSON object following the provided function schema." },
                { role: "user", content: chatGptPrompt }
              ],
              tools: tools
            });

            let result;
            if (response.choices[0].message.tool_calls && response.choices[0].message.tool_calls.length > 0) {
              try {
                result = JSON.parse(response.choices[0].message.tool_calls[0].function.arguments);
              } catch (e) {
                core.setFailed(`Failed to parse function call arguments: ${e.message}`);
              }
            } else if (response.choices[0].message.content) {
              try {
                result = JSON.parse(response.choices[0].message.content);
              } catch (e) {
                core.setFailed(`Failed to parse response content: ${e.message}`);
              }
            } else {
              core.setFailed("No valid response received from OpenAI.");
            }

            try {
              core.info("raw response start: **********************************************");
              core.info(`raw response (${(result ? result.length : "undefined")} characters): "${result}"`);
              core.info("raw response mid: ================================================");
              core.info(`raw response as JSON: "${JSON.stringify(result)}"`);
              core.info("raw response end: ************************************************");
              const parsed = ResponseSchema.parse(result);
              core.info(`parsed response: "${JSON.stringify(parsed)}"`);
              core.setOutput("documentName", parsed.documentName);
              core.setOutput("documentDetailedDigest", parsed.documentDetailedDigest);
              core.setOutput("documentNamesToBeDeleted", parsed.documentNamesToBeDeleted);
              core.setOutput("sourceEntryIndex", parsed.sourceEntryIndex);
              core.setOutput("crawlSummary", parsed.crawlSummary);
              core.setOutput("normalisedExtract", parsed.normalisedExtract);
              core.setOutput("supplementaryDetails", parsed.supplementaryDetails);
              core.setOutput("referenceDetails", parsed.referenceDetails);
              core.setOutput("informationDenseExtract", parsed.informationDenseExtract);
              core.info(`documentName: "${parsed.documentName}"`);
              core.info(`documentDetailedDigest: "${parsed.documentDetailedDigest}"`);
              core.info(`documentNamesToBeDeleted: "${parsed.documentNamesToBeDeleted}"`);
              core.info(`sourceEntryIndex: "${parsed.sourceEntryIndex}"`);
              core.info(`crawlSummary: "${parsed.crawlSummary}"`);
              core.info(`normalisedExtract: "${parsed.normalisedExtract}"`);
              core.info(`supplementaryDetails: "${parsed.supplementaryDetails}"`);
              core.info(`referenceDetails: "${parsed.referenceDetails}"`);
              core.info(`informationDenseExtract: "${parsed.informationDenseExtract}"`);

              // Get the source entry used for this document
              const sourceEntryIndex = parseInt(parsed.sourceEntryIndex, 10) - 1;
              let sourceEntry = null;
              if (sourceEntryIndex >= 0 && sourceEntryIndex < sourceEntries.length) {
                sourceEntry = sourceEntries[sourceEntryIndex];
                core.info(`Using source entry: ${sourceEntry.name} (${sourceEntry.url})`);
              } else {
                core.warning(`Invalid source entry index: ${parsed.sourceEntryIndex}`);
              }

              // Add the current date to the document detailed digest
              const now = new Date();
              const dateStr = now.toISOString().split('T')[0]; // YYYY-MM-DD format

              // Create the document content
              let documentContent = "";

              // If we have a valid source entry, add the original content, crawl detailed digest, and attribution fields
              if (sourceEntry) {
                const crawlDate = new Date();
                const crawlDateStr = crawlDate.toISOString();

                documentContent = `# ${parsed.documentName}\n\n`;
                documentContent += `## Crawl Summary\n${parsed.crawlSummary}\n\n`;
                documentContent += `## Normalised Extract\n${parsed.normalisedExtract}\n\n`;
                documentContent += `## Supplementary Details\n${parsed.supplementaryDetails}\n\n`;
                documentContent += `## Reference Details\n${parsed.referenceDetails}\n\n`;

                // Use the LLM-generated Information Dense Extract
                let llmInformationDenseExtract = '';
                if (parsed.informationDenseExtract && parsed.informationDenseExtract.trim()) {
                  llmInformationDenseExtract = parsed.informationDenseExtract;
                } else {
                  // If LLM-generated Information Dense Extract is empty or missing, use a combination of other sections
                  const combinedContent = [
                    parsed.crawlSummary || '',
                    parsed.supplementaryDetails || '',
                    parsed.referenceDetails || '',
                    parsed.documentDetailedDigest || ''
                  ].filter(Boolean).join('\n\n');

                  llmInformationDenseExtract = combinedContent;
                }

                documentContent += `## Information Dense Extract\n${llmInformationDenseExtract}\n\n`;

                // Create Sanitised Extract by sanitizing the Normalised Extract
                let sanitisedExtract = '';
                if (parsed.normalisedExtract && parsed.normalisedExtract.trim()) {
                  sanitisedExtract = parsed.normalisedExtract
                    .replace(/[^\x20-\x7E\n]/g, '') // Remove non-ASCII characters
                    .replace(/[\\"`]/g, "'") // Replace quotes and backslashes with single quotes
                    .replace(/[\t\r]/g, ' ') // Replace tabs and carriage returns with spaces
                    .trim();
                } else {
                  // If Normalised Extract is empty or missing, use a combination of other sections
                  const combinedContent = [
                    parsed.crawlSummary || '',
                    parsed.supplementaryDetails || '',
                    parsed.referenceDetails || '',
                    parsed.documentDetailedDigest || ''
                  ].filter(Boolean).join('\n\n');

                  sanitisedExtract = combinedContent
                    .replace(/[^\x20-\x7E\n]/g, '') // Remove non-ASCII characters
                    .replace(/[\\"`]/g, "'") // Replace quotes and backslashes with single quotes
                    .replace(/[\t\r]/g, ' ') // Replace tabs and carriage returns with spaces
                    .trim();
                }

                documentContent += `## Sanitised Extract\n${sanitisedExtract}\n\n`;
                documentContent += `## Original Source\n${sourceEntry.name}\n${sourceEntry.url}\n\n`;
                documentContent += `## Digest of ${parsed.documentName}\n\n`;
                documentContent += `${parsed.documentDetailedDigest}\n\n`;
                documentContent += `## Attribution\n`;
                documentContent += `- Source: ${sourceEntry.name}\n`;
                documentContent += `- URL: ${sourceEntry.url}\n`;
                documentContent += `- License: ${sourceEntry.license || 'Unknown'}\n`;
                documentContent += `- Crawl Date: ${crawlDateStr}\n`;
                documentContent += `- Data Size: ${crawlResult.dataSize} bytes\n`;
                documentContent += `- Links Found: ${crawlResult.links.length}\n\n`;
                documentContent += `## Retrieved\n${dateStr}\n`;

                // Save the raw crawl data to a text file
                const rawCrawlFilePath = path.join(libraryDocumentsPath, `${parsed.documentName}_RAW_CRAWL.txt`);
                try {
                  if (isWriteable(rawCrawlFilePath)) {
                    fs.writeFileSync(rawCrawlFilePath, crawlResult.content);
                    core.info(`Raw crawl data saved to ${rawCrawlFilePath}`);
                  } else {
                    core.warning(`Raw crawl file path '${rawCrawlFilePath}' is not in the allowed filepath patterns. Skipping update.`);
                  }
                } catch (e) {
                  core.warning(`Failed to save raw crawl data: ${e.message}`);
                }
              } else {
                // If no valid source entry, just add the date at the end
                documentContent += `\n\n_Retrieved: ${dateStr}_\n`;
              }

              // Save the document to a file
              const documentName = parsed.documentName.replace(/ /g, "_").toUpperCase();
              const documentFilePath = path.join(libraryDocumentsPath, `${documentName}.md`);
              try {
                fs.mkdirSync(libraryDocumentsPath, { recursive: true });
                if (isWriteable(documentFilePath)) {
                  fs.writeFileSync(documentFilePath, documentContent);
                  core.info(`Document saved to ${documentFilePath}`);
                } else {
                  core.warning(`Document file path '${documentFilePath}' is not in the allowed filepath patterns. Skipping update.`);
                  core.setFailed(`Cannot save document to '${documentFilePath}' as it is not in the allowed filepath patterns.`);
                }
              } catch (e) {
                core.setFailed(`Failed to save document: ${e.message}`);
              }

              // Delete any documents that should be deleted
              const documentNamesToBeDeleted = parsed.documentNamesToBeDeleted
                .split(',')
                .map(name => name.trim())
                .filter(name => name && name !== 'none');

              for (const name of documentNamesToBeDeleted) {
                const documentNameToDelete = name.replace(/ /g, "_").toUpperCase().replace(/\.md$/, "") + ".md";
                const documentPathToDelete = path.join(libraryDocumentsPath, documentNameToDelete);
                try {
                  if (fs.existsSync(documentPathToDelete)) {
                    if (isWriteable(documentPathToDelete)) {
                      core.info(`Deleting document: ${documentPathToDelete}`);
                      fs.unlinkSync(documentPathToDelete);
                    } else {
                      core.warning(`Document path '${documentPathToDelete}' is not in the allowed filepath patterns. Skipping deletion.`);
                    }
                  } else {
                    core.warning(`Document not found for deletion: ${documentPathToDelete}`);
                  }
                } catch (e) {
                  core.warning(`Error deleting document ${documentPathToDelete}: ${e.message}`);
                }
              }

            } catch (e) {
              core.setFailed(`Failed to parse ChatGPT response: ${e.message}`);
            }

            core.setOutput("response", JSON.stringify(response));
            core.setOutput("usage", JSON.stringify(response.usage));
            core.info(`response: "${JSON.stringify(response)}"`);
            core.info(`usage: "${JSON.stringify(response.usage)}"`);

      - name: Commit changes
        id: commit
        continue-on-error: true
        run: |
          git config --local user.email '${{ env.gitUserEmail }}'
          git config --local user.name '${{ env.gitUserName }}'
          git status -v
          git add -v --all '${{ env.libraryDocumentsPath }}'
          git commit -m 'Maintain ${{ steps.maintain-library.outputs.documentName }}'
          git status -v
          git pull --ff-only origin ${{ github.ref }}
          git push -v origin ${{ github.ref }}
          git status -v

    outputs:
      documentName: ${{ steps.maintain-library.outputs.documentName }}
      documentDetailedDigest: ${{ steps.maintain-library.outputs.documentDetailedDigest }}
      crawlSummary: ${{ steps.maintain-library.outputs.crawlSummary }}
      normalisedExtract: ${{ steps.maintain-library.outputs.normalisedExtract }}
      supplementaryDetails: ${{ steps.maintain-library.outputs.supplementaryDetails }}
      referenceDetails: ${{ steps.maintain-library.outputs.referenceDetails }}
      informationDenseExtract: ${{ steps.maintain-library.outputs.informationDenseExtract }}
